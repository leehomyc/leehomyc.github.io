window.trendingPapers = {
    "today": [{"paper": {"id": "2601.20614", "authors": [{"_id": "697ac91bdf3e800774f13c12", "name": "Yanqi Dai", "hidden": false}, {"_id": "697ac91bdf3e800774f13c13", "name": "Yuxiang Ji", "hidden": false}, {"_id": "697ac91bdf3e800774f13c14", "name": "Xiao Zhang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c15", "name": "Yong Wang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c16", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "697ac91bdf3e800774f13c17", "name": "Zhiwu Lu", "hidden": false}], "publishedAt": "2026-01-28T13:49:23.000Z", "submittedOnDailyAt": "2026-01-29T00:15:35.371Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "submittedOnDailyBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "isPro": false, "fullname": "Yanqi Dai", "user": "YanqiDai", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "upvotes": 93, "discussionId": "697ac91bdf3e800774f13c18", "githubRepo": "https://github.com/AMAP-ML/MathForge", "githubRepoAddedBy": "user", "ai_summary": "MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "Group Relative Policy Optimization", "Difficulty-Aware Group Policy Optimization", "Multi-Aspect Question Reformulation", "mathematical reasoning", "policy updates", "group advantage estimation", "question-level weighting", "data augmentation"], "githubStars": 84, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMathForge\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5927\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u96be\u9898\u65f6\u5b58\u5728\u7b97\u6cd5\u548c\u6570\u636e\u4e0a\u7684\u4e0d\u8db3\uff0c\u96be\u9898\u7684\u66f4\u65b0\u5e45\u5ea6\u8f83\u5c0f\u3002</li>\n    <li>MathForge\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\uff1a\u96be\u5ea6\u611f\u77e5\u7684\u7fa4\u4f53\u7b56\u7565\u4f18\u5316\uff08DGPO\uff09\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u8ff0\uff08MQR\uff09\u3002</li>\n    <li>DGPO\u901a\u8fc7\u5e73\u8861\u96be\u5ea6\u6765\u4f18\u5316\u95ee\u9898\u66f4\u65b0\uff0c\u5e76\u4f18\u5148\u5904\u7406\u66f4\u96be\u7684\u95ee\u9898\u3002</li>\n    <li>MQR\u901a\u8fc7\u591a\u65b9\u9762\u91cd\u8ff0\u95ee\u9898\u6765\u589e\u52a0\u96be\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7b54\u6848\u4e0d\u53d8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) helps improve mathematical reasoning in large models.</li>\n    <li>Current methods don't focus enough on difficult questions, which are important for developing better reasoning skills.</li>\n    <li>The proposed MathForge framework includes two parts: a new algorithm (DGPO) that balances question difficulty and a strategy (MQR) that reformulates questions to make them harder.</li>\n    <li>DGPO improves the way questions are handled by focusing on more difficult ones and ensuring fair updates in learning.</li>\n    <li>MathForge has been shown to work much better than existing methods in various math reasoning tasks, with all resources available online.</li>\n</ul>"}, "publishedAt": "2026-01-28T08:49:23.000Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20614.png", "numComments": 12, "submittedBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "fullname": "Yanqi Dai", "name": "YanqiDai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20540", "authors": [{"_id": "697ac48cdf3e800774f13bc1", "name": "Robbyant Team", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc2", "name": "Zelin Gao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc3", "user": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "name": "Qiuyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:29.190Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc4", "name": "Yanhong Zeng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc5", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc6", "user": {"_id": "64acd2ec39fcfebff8c79c00", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64acd2ec39fcfebff8c79c00/Avq66l5hO-aggNtk4Y1ss.png", "isPro": false, "fullname": "Ka Leong Cheng", "user": "felixcheng97", "type": "user"}, "name": "Ka Leong Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:36.041Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc7", "name": "Yixuan Li", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc8", "user": {"_id": "665f059a8947302aa2c63afe", "avatarUrl": "/avatars/50f560285946532321a0bd526494148d.svg", "isPro": false, "fullname": "hanlin wang", "user": "hlwang06", "type": "user"}, "name": "Hanlin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:18:10.952Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc9", "name": "Yinghao Xu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bca", "name": "Shuailei Ma", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcb", "name": "Yihang Chen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcc", "name": "Jie Liu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcd", "name": "Yansong Cheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bce", "name": "Yao Yao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcf", "name": "Jiayi Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd0", "user": {"_id": "656084f44e8918182d4f07c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg", "isPro": false, "fullname": "Yihao Meng", "user": "Yhmeng1106", "type": "user"}, "name": "Yihao Meng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:37.840Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd1", "name": "Kecheng Zheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd2", "user": {"_id": "63f0baf66309c84d5f4a2226", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg", "isPro": true, "fullname": "Qingyan", "user": "QingyanBai", "type": "user"}, "name": "Qingyan Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:24.535Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd3", "user": {"_id": "6478a982256b62e219917d67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg", "isPro": false, "fullname": "JingyeChen22", "user": "JingyeChen22", "type": "user"}, "name": "Jingye Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:37.828Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd4", "name": "Zehong Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd5", "user": {"_id": "662128ec9ca2cd4e6db2fb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg", "isPro": false, "fullname": "Bruce Yu", "user": "bruceyyu", "type": "user"}, "name": "Yue Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:27.115Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd6", "name": "Xing Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd7", "name": "Yujun Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd8", "name": "Hao Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "publishedAt": "2026-01-28T12:37:01.000Z", "submittedOnDailyAt": "2026-01-29T00:09:39.166Z", "title": "Advancing Open-source World Models", "submittedOnDailyBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "upvotes": 66, "discussionId": "697ac48cdf3e800774f13bd9", "projectPage": "https://technology.robbyant.com/lingbot-world", "githubRepo": "https://github.com/Robbyant/lingbot-world/", "githubRepoAddedBy": "user", "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.", "ai_keywords": ["world simulator", "video generation", "world model", "long-term memory", "real-time interactivity"], "githubStars": 756, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "summary_zh": "<ul>\n    <li>LingBot-World\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4e16\u754c\u6a21\u62df\u5668\uff0c\u6e90\u81ea\u89c6\u9891\u751f\u6210\u6280\u672f\u3002</li>\n    <li>\u5b83\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u5f3a\u5927\u7684\u52a8\u6001\u8868\u73b0\uff0c\u5305\u62ec\u73b0\u5b9e\u4e3b\u4e49\u3001\u79d1\u5b66\u80cc\u666f\u548c\u5361\u901a\u98ce\u683c\u7b49\u3002</li>\n    <li>\u652f\u6301\u5206\u949f\u7ea7\u7684\u89c6\u91ce\uff0c\u5e76\u4fdd\u6301\u65f6\u95f4\u4e0a\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u5373\u201c\u957f\u671f\u8bb0\u5fc6\u201d\u3002</li>\n    <li>\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\uff0c\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\uff0c\u80fd\u591f\u4ee5\u6bcf\u79d216\u5e27\u7684\u901f\u5ea6\u751f\u6210\u56fe\u50cf\u3002</li>\n    <li>\u63d0\u4f9b\u4ee3\u7801\u548c\u6a21\u578b\u7684\u516c\u5171\u8bbf\u95ee\uff0c\u65e8\u5728\u7f29\u5c0f\u5f00\u6e90\u4e0e\u95ed\u6e90\u6280\u672f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u5185\u5bb9\u521b\u4f5c\u3001\u6e38\u620f\u548c\u673a\u5668\u4eba\u5b66\u4e60\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LingBot-World is an open-source world simulator that creates video content.</li>\n    <li>It supports various types of environments, from realistic to cartoonish styles.</li>\n    <li>It has a long-term memory feature that keeps context consistent over time.</li>\n    <li>It allows real-time interaction with quick response times (under 1 second).</li>\n    <li>The code and model are publicly available to help improve access to technology for content creation, gaming, and robot learning.</li>\n</ul>"}, "publishedAt": "2026-01-28T07:37:01.000Z", "title": "Advancing Open-source World Models", "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20540.png", "numComments": 1, "submittedBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "fullname": "Qiuyu Wang", "name": "qiuyuu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.19325", "authors": [{"_id": "69798298df44b75fa47e47a9", "name": "Zichen Wen", "hidden": false}, {"_id": "69798298df44b75fa47e47aa", "user": {"_id": "688c72c011ef3399b561dee7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/688c72c011ef3399b561dee7/puhgnTOAfZYetsC46hqGm.jpeg", "isPro": false, "fullname": "BoxueYang", "user": "Boxue", "type": "user"}, "name": "Boxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:12.982Z", "hidden": false}, {"_id": "69798298df44b75fa47e47ab", "name": "Shuang Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47ac", "name": "Yaojie Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ad", "name": "Yuhang Han", "hidden": false}, {"_id": "69798298df44b75fa47e47ae", "name": "Junlong Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47af", "name": "Cong Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b0", "name": "Yicheng Fu", "hidden": false}, {"_id": "69798298df44b75fa47e47b1", "name": "Jiawang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47b2", "name": "Jiangchao Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b3", "name": "Xi Fang", "hidden": false}, {"_id": "69798298df44b75fa47e47b4", "name": "Zhen Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b5", "name": "Henxing Cai", "hidden": false}, {"_id": "69798298df44b75fa47e47b6", "name": "Lin Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b7", "name": "Zhifeng Gao", "hidden": false}, {"_id": "69798298df44b75fa47e47b8", "name": "Yanhui Hong", "hidden": false}, {"_id": "69798298df44b75fa47e47b9", "name": "Nang Yuan", "hidden": false}, {"_id": "69798298df44b75fa47e47ba", "name": "Yixuan Li", "hidden": false}, {"_id": "69798298df44b75fa47e47bb", "name": "Guojiang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47bc", "name": "Haoyi Tao", "hidden": false}, {"_id": "69798298df44b75fa47e47bd", "name": "Nan Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47be", "name": "Han Lyu", "hidden": false}, {"_id": "69798298df44b75fa47e47bf", "name": "Guolin Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47c0", "name": "Ning Liao", "hidden": false}, {"_id": "69798298df44b75fa47e47c1", "name": "Xiaoxing Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c2", "name": "Kai Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c3", "name": "Zhiyu Li", "hidden": false}, {"_id": "69798298df44b75fa47e47c4", "name": "Feiyu Xiong", "hidden": false}, {"_id": "69798298df44b75fa47e47c5", "name": "Sihan Hu", "hidden": false}, {"_id": "69798298df44b75fa47e47c6", "name": "Kun Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c7", "name": "Yanfeng Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c8", "name": "Weinan E", "hidden": false}, {"_id": "69798298df44b75fa47e47c9", "name": "Linfeng Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ca", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-01-27T08:12:18.000Z", "submittedOnDailyAt": "2026-01-29T01:20:58.570Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "submittedOnDailyBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "upvotes": 53, "discussionId": "69798298df44b75fa47e47cb", "projectPage": "https://innovatorlm.github.io/Innovator-VL", "githubRepo": "https://github.com/InnovatorLM/Innovator-VL", "githubRepoAddedBy": "user", "ai_summary": "Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.", "ai_keywords": ["multimodal large language model", "scientific multimodal large language model", "end-to-end reproducible training pipeline", "supervised fine-tuning", "reinforcement learning", "scientific reasoning", "data efficiency", "principled data selection", "generalization", "scientific alignment"], "githubStars": 70, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n  <li>Innovator-VL\u662f\u4e00\u4e2a\u79d1\u5b66\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u5bf9\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7684\u7406\u89e3\u548c\u63a8\u7406\u3002</li>\n  <li>\u8be5\u6a21\u578b\u91c7\u7528\u900f\u660e\u7684\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u6570\u636e\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u5f3a\u5927\u7684\u79d1\u5b66\u667a\u80fd\u3002</li>\n  <li>\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u91cd\u590d\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u6e05\u6d17\u3001\u9884\u5904\u7406\u7b49\uff0c\u65b9\u4fbf\u793e\u533a\u6269\u5c55\u3002</li>\n  <li>Innovator-VL\u5728\u79d1\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u7528\u4e0d\u5230\u4e94\u767e\u4e07\u4e2a\u6837\u672c\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002</li>\n  <li>\u8be5\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u548c\u591a\u6a21\u6001\u63a8\u7406\u7b49\u65b9\u9762\u7684\u8868\u73b0\u4e5f\u5f88\u5f3a\uff0c\u8bc1\u660e\u79d1\u5b66\u5bf9\u9f50\u53ef\u4ee5\u4e0e\u901a\u7528\u80fd\u529b\u7ed3\u5408\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Innovator-VL is a new large language model that helps with understanding and reasoning in various scientific fields while also doing well in general vision tasks.</li>\n    <li>It uses a clear and reproducible training process, making it easy for others to follow and build upon.</li>\n    <li>The model is very efficient with data, performing well on scientific tasks with fewer than five million curated samples, without needing extensive pretraining.</li>\n    <li>Innovator-VL shows strong generalization abilities, performing well in both scientific tasks and general vision and reasoning challenges.</li>\n    <li>This work suggests that high-quality scientific models can be developed without needing large amounts of data, which can help future research efforts.</li>\n</ul>"}, "publishedAt": "2026-01-27T03:12:18.000Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19325.png", "numComments": 1, "submittedBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "fullname": "Zichen Wen", "name": "zichenwen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20552", "authors": [{"_id": "697acf36df3e800774f13c41", "name": "Haoran Wei", "hidden": false}, {"_id": "697acf36df3e800774f13c42", "name": "Yaofeng Sun", "hidden": false}, {"_id": "697acf36df3e800774f13c43", "name": "Yukun Li", "hidden": false}], "publishedAt": "2026-01-28T12:46:07.000Z", "submittedOnDailyAt": "2026-01-29T00:38:50.729Z", "title": "DeepSeek-OCR 2: Visual Causal Flow", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.", "upvotes": 26, "discussionId": "697acf37df3e800774f13c44", "githubRepo": "https://github.com/deepseek-ai/DeepSeek-OCR-2", "githubRepoAddedBy": "user", "ai_summary": "DeepSeek-OCR 2 introduces DeepEncoder V2 that dynamically reorders visual tokens based on semantic content, enabling more human-like causal reasoning in 2D image understanding through cascaded 1D causal structures.", "ai_keywords": ["encoder-DeepEncoder V2", "visual tokens", "vision-language models", "raster-scan order", "positional encoding", "causal reasoning", "2D image understanding", "cascaded 1D causal reasoning"], "githubStars": 1558, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-OCR 2 \u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u7684\u7f16\u7801\u5668DeepEncoder V2\uff0c\u80fd\u591f\u6839\u636e\u56fe\u50cf\u8bed\u4e49\u52a8\u6001\u91cd\u65b0\u6392\u5217\u89c6\u89c9\u6807\u8bb0\u3002</li>\n    <li>\u4f20\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u987a\u5e8f\u5904\u7406\u89c6\u89c9\u6807\u8bb0\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u7684\u89c6\u89c9\u8bc6\u522b\u65b9\u5f0f\u4e0d\u7b26\u3002</li>\n    <li>DeepEncoder V2 \u53d7\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\u542f\u53d1\uff0c\u5177\u5907\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u4ee5\u5728\u5185\u5bb9\u89e3\u91ca\u524d\u667a\u80fd\u5730\u91cd\u65b0\u6392\u5217\u89c6\u89c9\u6807\u8bb0\u3002</li>\n    <li>\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u901a\u8fc7\u4e24\u4e2a\u7ea7\u8054\u7684\u4e00\u7ef4\u56e0\u679c\u63a8\u7406\u7ed3\u6784\u6709\u6548\u7406\u89e3\u4e8c\u7ef4\u56fe\u50cf\u7684\u65b0\u65b9\u6cd5\u3002</li>\n    <li>\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5df2\u516c\u5f00\uff0c\u4f9b\u5927\u5bb6\u4f7f\u7528\u548c\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-OCR 2 introduces a new encoder called DeepEncoder V2 that can rearrange visual information based on the meaning of the image.</li>\n    <li>Traditional models analyze images in a fixed order, which is not how humans naturally perceive them.</li>\n    <li>Human vision uses flexible scanning patterns influenced by logical structures, especially for complex images.</li>\n    <li>DeepEncoder V2 aims to mimic this human cognitive process by allowing the model to reorder visual data before interpretation.</li>\n    <li>This research tests a new way to understand 2D images using two linked 1D reasoning processes, presenting a fresh architectural idea for image analysis.</li>\n</ul>"}, "publishedAt": "2026-01-28T07:46:07.000Z", "title": "DeepSeek-OCR 2: Visual Causal Flow", "summary": "We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20552.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 219, "isUserFollowing": false}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20209", "authors": [{"_id": "697acf8ddf3e800774f13c46", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:13.804Z", "hidden": false}, {"_id": "697acf8ddf3e800774f13c47", "name": "Shuo Yang", "hidden": false}, {"_id": "697acf8ddf3e800774f13c48", "name": "Changpeng Yang", "hidden": false}, {"_id": "697acf8ddf3e800774f13c49", "name": "Yuhao Shen", "hidden": false}, {"_id": "697acf8ddf3e800774f13c4a", "name": "Shuai Zhang", "hidden": false}, {"_id": "697acf8ddf3e800774f13c4b", "name": "Zhengqi Wen", "hidden": false}, {"_id": "697acf8ddf3e800774f13c4c", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-28T03:15:34.000Z", "submittedOnDailyAt": "2026-01-29T00:45:22.548Z", "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose Spark (Strategic Policy-Aware exploRation via Key-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that Spark achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.", "upvotes": 12, "discussionId": "697acf8edf3e800774f13c4d", "ai_summary": "Spark is a reinforcement learning framework that strategically allocates computational resources by branching at critical decision states, improving sample efficiency and generalization for long-horizon tasks.", "ai_keywords": ["reinforcement learning", "large language models", "long-horizon tasks", "trajectory scarcity", "rollout size", "computational resource allocation", "adaptive branching exploration", "decision-making signals", "sample efficiency", "generalization"], "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4f5c\u4e3a\u667a\u80fd\u4ee3\u7406\uff0c\u4f46\u957f\u65f6\u95f4\u4efb\u52a1\u7684\u8bad\u7ec3\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u4e2d\u95f4\u6b65\u9aa4\u4e0a\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u672a\u80fd\u4fdd\u8bc1\u6837\u672c\u8d28\u91cf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Spark\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5173\u952e\u51b3\u7b56\u72b6\u6001\u5904\u9009\u62e9\u6027\u5206\u652f\uff0c\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u63a2\u7d22\u3002</li>\n    <li>Spark\u4f18\u5148\u8003\u8651\u91c7\u6837\u8d28\u91cf\u800c\u975e\u76f2\u76ee\u8986\u76d6\uff0c\u52a9\u529b\u81ea\u4e3b\u63a2\u7d22\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSpark\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u4ee5\u66f4\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6cdb\u5316\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning helps large language models act intelligently, but training them for long tasks is hard due to a lack of good examples.</li>\n    <li>Current methods waste computing power by focusing equally on all steps, even trivial ones, which doesn't ensure quality results.</li>\n    <li>We introduce Spark, a new framework that focuses on important decision points to use resources more efficiently during exploration.</li>\n    <li>Spark improves sampling quality by adapting exploration based on the agent's own decision-making signals instead of relying on human input.</li>\n    <li>Tests show that Spark performs better with fewer training samples and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-27T22:15:34.000Z", "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning", "summary": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose Spark (Strategic Policy-Aware exploRation via Key-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that Spark achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20209.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.20834", "authors": [{"_id": "697ad00edf3e800774f13c4f", "name": "Andrew Kyle Lampinen", "hidden": false}, {"_id": "697ad00edf3e800774f13c50", "name": "Yuxuan Li", "hidden": false}, {"_id": "697ad00edf3e800774f13c51", "name": "Eghbal Hosseini", "hidden": false}, {"_id": "697ad00edf3e800774f13c52", "name": "Sangnie Bhardwaj", "hidden": false}, {"_id": "697ad00edf3e800774f13c53", "name": "Murray Shanahan", "hidden": false}], "publishedAt": "2026-01-28T18:33:17.000Z", "submittedOnDailyAt": "2026-01-29T00:42:23.006Z", "title": "Linear representations in language models can change dramatically over a conversation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.", "upvotes": 8, "discussionId": "697ad00edf3e800774f13c54", "ai_summary": "Linear representation directions in language models dynamically shift during conversations, affecting how factual information is encoded while preserving generic content, with implications for interpretability and context-adaptive model behavior.", "ai_keywords": ["linear directions", "language model representations", "conversation dynamics", "factual representation", "representational drift", "context adaptation", "model steering", "interpretability"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n  <li>\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u793a\u5177\u6709\u7ebf\u6027\u65b9\u5411\uff0c\u4ee3\u8868\u9ad8\u5c42\u6b21\u7684\u6982\u5ff5\u3002</li>\n  <li>\u5728\u6a21\u62df\u5bf9\u8bdd\u4e2d\uff0c\u8fd9\u4e9b\u8868\u793a\u4f1a\u968f\u7740\u5bf9\u8bdd\u7684\u53d1\u5c55\u800c\u663e\u8457\u53d8\u5316\uff0c\u4f8b\u5982\u4e8b\u5b9e\u4fe1\u606f\u53ef\u80fd\u5728\u5bf9\u8bdd\u5f00\u59cb\u65f6\u88ab\u89c6\u4e3a\u771f\u5b9e\uff0c\u4f46\u5728\u7ed3\u675f\u65f6\u53ef\u80fd\u88ab\u89c6\u4e3a\u4e0d\u771f\u5b9e\u3002</li>\n  <li>\u8fd9\u4e9b\u53d8\u5316\u4f9d\u8d56\u4e8e\u5185\u5bb9\uff0c\u76f8\u5173\u4fe1\u606f\u7684\u8868\u793a\u4f1a\u6539\u53d8\uff0c\u800c\u4e00\u822c\u4fe1\u606f\u901a\u5e38\u4fdd\u6301\u4e0d\u53d8\u3002</li>\n  <li>\u5373\u4f7f\u662f\u5728\u4e0d\u540c\u6a21\u578b\u548c\u5c42\u6b21\u4e4b\u95f4\uff0c\u8fd9\u4e9b\u8868\u793a\u53d8\u5316\u4e5f\u662f\u663e\u8457\u7684\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5b9e\u65f6\u5bf9\u8bdd\uff0c\u5373\u4f7f\u91cd\u64ad\u4e0d\u540c\u6a21\u578b\u7684\u5bf9\u8bdd\u811a\u672c\u4e5f\u80fd\u4ea7\u751f\u7c7b\u4f3c\u7684\u53d8\u5316\u3002</li>\n  <li>\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u7684\u8868\u793a\u53ef\u80fd\u4f1a\u54cd\u5e94\u5bf9\u8bdd\u4e2d\u6240\u626e\u6f14\u7684\u89d2\u8272\u800c\u6f14\u53d8\uff0c\u8fd9\u5bf9\u7406\u89e3\u6a21\u578b\u5982\u4f55\u9002\u5e94\u4e0a\u4e0b\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Language models can show changes in how they represent information during conversations.</li>\n    <li>Information can shift from being seen as factual to non-factual and vice versa based on the conversation's context.</li>\n    <li>While important conversation-related information changes, general information tends to stay the same.</li>\n    <li>These changes happen across different types of models and layers, and can occur even when replaying conversations from different models.</li>\n    <li>The findings suggest that using fixed interpretations of model features may be misleading and highlight new areas for research on how models adapt to context.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:33:17.000Z", "title": "Linear representations in language models can change dramatically over a conversation", "summary": "Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20834.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 219, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20802", "authors": [{"_id": "697b378a870173bd9177734d", "user": {"_id": "6645dbab80ee4eb5ddc8aed8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6645dbab80ee4eb5ddc8aed8/-IckaooWMXeACKew9THv_.jpeg", "isPro": false, "fullname": "Jonas H\u00fcbotter", "user": "jonhue", "type": "user"}, "name": "Jonas H\u00fcbotter", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:34.106Z", "hidden": false}, {"_id": "697b378a870173bd9177734e", "name": "Frederike L\u00fcbeck", "hidden": false}, {"_id": "697b378a870173bd9177734f", "name": "Lejs Behric", "hidden": false}, {"_id": "697b378a870173bd91777350", "name": "Anton Baumann", "hidden": false}, {"_id": "697b378a870173bd91777351", "name": "Marco Bagatella", "hidden": false}, {"_id": "697b378a870173bd91777352", "name": "Daniel Marta", "hidden": false}, {"_id": "697b378a870173bd91777353", "name": "Ido Hakimi", "hidden": false}, {"_id": "697b378a870173bd91777354", "name": "Idan Shenfeld", "hidden": false}, {"_id": "697b378a870173bd91777355", "name": "Thomas Kleine Buening", "hidden": false}, {"_id": "697b378a870173bd91777356", "name": "Carlos Guestrin", "hidden": false}, {"_id": "697b378a870173bd91777357", "name": "Andreas Krause", "hidden": false}], "publishedAt": "2026-01-28T17:45:12.000Z", "submittedOnDailyAt": "2026-01-29T09:40:07.498Z", "title": "Reinforcement Learning via Self-Distillation", "submittedOnDailyBy": {"_id": "6645dbab80ee4eb5ddc8aed8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6645dbab80ee4eb5ddc8aed8/-IckaooWMXeACKew9THv_.jpeg", "isPro": false, "fullname": "Jonas H\u00fcbotter", "user": "jonhue", "type": "user"}, "summary": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.", "upvotes": 6, "discussionId": "697b378a870173bd91777358", "projectPage": "https://self-distillation.github.io/SDPO", "githubRepo": "https://github.com/lasgroup/SDPO", "githubRepoAddedBy": "user", "ai_summary": "Self-Distillation Policy Optimization (SDPO) enhances reinforcement learning with verifiable rewards by utilizing rich textual feedback to improve sample efficiency and accuracy in language model training.", "ai_keywords": ["reinforcement learning", "verifiable rewards", "rich feedback", "Self-Distillation Policy Optimization", "SDPO", "tokenized feedback", "dense learning signal", "reward model", "policy distillation", "in-context learning", "sample efficiency", "accuracy"], "githubStars": 1, "organization": {"_id": "68923f153ceba9f21ce33320", "name": "lasgroup", "fullname": "LAS @ ETH Zurich", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6645dbab80ee4eb5ddc8aed8/-wp0yB2cNzO2B0wte9nzF.png"}, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u9886\u57df\uff08\u5982\u4ee3\u7801\u548c\u6570\u5b66\uff09\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u540e\u8bad\u7ec3\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u4ec5\u4ece\u6bcf\u6b21\u5c1d\u8bd5\u7684\u6807\u91cf\u7ed3\u679c\u5956\u52b1\u4e2d\u5b66\u4e60\uff0c\u5bfc\u81f4\u4fe1\u7528\u5206\u914d\u74f6\u9888\u3002</li>\n    <li>\u8bb8\u591a\u53ef\u9a8c\u8bc1\u73af\u5883\u63d0\u4f9b\u4e30\u5bcc\u7684\u6587\u672c\u53cd\u9988\uff0c\u5982\u8fd0\u884c\u65f6\u9519\u8bef\u6216\u8bc4\u5ba1\u8bc4\u4f30\uff0c\u89e3\u91ca\u5931\u8d25\u539f\u56e0\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5Self-Distillation Policy Optimization (SDPO)\uff0c\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\uff0c\u65e0\u9700\u5916\u90e8\u6559\u5e08\u6216\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u3002</li>\n    <li>SDPO\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u51c6\u786e\u6027\uff0c\u5e76\u5728\u6807\u51c6\u73af\u5883\u4e2d\u4e5f\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are being improved using reinforcement learning in areas like coding and math.</li>\n    <li>Current methods only use a single outcome reward, which makes it hard to learn from mistakes.</li>\n    <li>Many environments offer detailed feedback (like error messages) that can help models learn better.</li>\n    <li>Self-Distillation Policy Optimization (SDPO) uses this feedback to enhance learning without needing external guidance.</li>\n    <li>SDPO shows better learning efficiency and accuracy in various tasks, even outperforming traditional methods with just scalar feedback.</li>\n</ul>"}, "publishedAt": "2026-01-28T12:45:12.000Z", "title": "Reinforcement Learning via Self-Distillation", "summary": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20802.png", "numComments": 2, "submittedBy": {"_id": "6645dbab80ee4eb5ddc8aed8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6645dbab80ee4eb5ddc8aed8/-IckaooWMXeACKew9THv_.jpeg", "fullname": "Jonas H\u00fcbotter", "name": "jonhue", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68923f153ceba9f21ce33320", "name": "lasgroup", "fullname": "LAS @ ETH Zurich", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6645dbab80ee4eb5ddc8aed8/-wp0yB2cNzO2B0wte9nzF.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.20055", "authors": [{"_id": "697baf7ea67238fac88cbf0d", "user": {"_id": "6702352c547acbd64cddf31e", "avatarUrl": "/avatars/37fbacb299f217505b2a03549128f10d.svg", "isPro": false, "fullname": "Vikash Singh", "user": "optimusPrimeBee", "type": "user"}, "name": "Vikash Singh", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T21:15:21.775Z", "hidden": false}, {"_id": "697baf7ea67238fac88cbf0e", "name": "Darion Cassel", "hidden": false}, {"_id": "697baf7ea67238fac88cbf0f", "name": "Nathaniel Weir", "hidden": false}, {"_id": "697baf7ea67238fac88cbf10", "name": "Nick Feng", "hidden": false}, {"_id": "697baf7ea67238fac88cbf11", "name": "Sam Bayless", "hidden": false}], "publishedAt": "2026-01-27T20:59:11.000Z", "submittedOnDailyAt": "2026-01-29T16:37:00.504Z", "title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning", "submittedOnDailyBy": {"_id": "6702352c547acbd64cddf31e", "avatarUrl": "/avatars/37fbacb299f217505b2a03549128f10d.svg", "isPro": false, "fullname": "Vikash Singh", "user": "optimusPrimeBee", "type": "user"}, "summary": "Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.", "upvotes": 5, "discussionId": "697baf7ea67238fac88cbf12", "ai_summary": "A neurosymbolic framework integrates large language models with SMT solvers to enhance logical correctness through verification-guided iterative refinement, utilizing formal semantic equivalence checking, semantic routing, and minimal correction subsets for precise error localization and consensus verification.", "ai_keywords": ["Large Language Models", "SMT solvers", "verification-guided answers", "iterative refinement", "atomic claims", "first-order logic", "automated theorem proving", "formal semantic equivalence checking", "semantic routing", "symbolic solvers", "LLM ensembles", "commonsense reasoning", "Minimal Correction Subsets", "logical error localization", "structured feedback", "convergence", "formal guarantees", "consensus verification"], "organization": {"_id": "65424e525bb41e82fba69276", "name": "amazonwebservices", "fullname": "Amazon Web Services (AWS)", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65424dd6afa1020a9c31759e/wt7-MpCfO2O8pVG8mISJS.png"}, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u6cd5\u4e0a\u6d41\u5229\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u9886\u57df\u786e\u4fdd\u903b\u8f91\u6b63\u786e\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLMs\u548cSMT\u6c42\u89e3\u5668\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u4ea7\u751f\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7b54\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5c06LLM\u8f93\u51fa\u5206\u89e3\u4e3a\u539f\u5b50\u58f0\u660e\uff0c\u81ea\u52a8\u5f62\u5f0f\u5316\u4e3a\u4e00\u9636\u903b\u8f91\uff0c\u5e76\u4f7f\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u9a8c\u8bc1\u903b\u8f91\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u591a\u6a21\u578b\u5171\u8bc6\u3001\u8bed\u4e49\u8def\u7531\u548c\u7cbe\u786e\u7684\u903b\u8f91\u9519\u8bef\u5b9a\u4f4d\u3002</li>\n    <li>\u4f7f\u7528GPT-OSS-120B\u6a21\u578b\uff0cVERGE\u5728\u4e00\u7ec4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u63d0\u534718.7%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) can create fluent text, but ensuring their logical correctness is difficult, especially in important areas.</li>\n    <li>A new framework combines LLMs with SMT (Satisfiability Modulo Theories) solvers to improve answer accuracy through repeated adjustments.</li>\n    <li>Key innovations include checking logic between different models, directing claims to the right verification method, and identifying specific errors for correction.</li>\n    <li>The system organizes claims by their logical correctness and uses multiple verification signals to improve answers until they meet standards.</li>\n    <li>The VERGE framework, using the GPT-OSS-120B model, shows an 18.7% improvement in performance over previous methods on reasoning tasks.</li>\n</ul>"}, "publishedAt": "2026-01-27T15:59:11.000Z", "title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning", "summary": "Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20055.png", "numComments": 1, "submittedBy": {"_id": "6702352c547acbd64cddf31e", "avatarUrl": "/avatars/37fbacb299f217505b2a03549128f10d.svg", "fullname": "Vikash Singh", "name": "optimusPrimeBee", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "65424e525bb41e82fba69276", "name": "amazonwebservices", "fullname": "Amazon Web Services (AWS)", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65424dd6afa1020a9c31759e/wt7-MpCfO2O8pVG8mISJS.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.20789", "authors": [{"_id": "697ad0a3df3e800774f13c56", "name": "Ethan Shen", "hidden": false}, {"_id": "697ad0a3df3e800774f13c57", "name": "Danny Tormoen", "hidden": false}, {"_id": "697ad0a3df3e800774f13c58", "name": "Saurabh Shah", "hidden": false}, {"_id": "697ad0a3df3e800774f13c59", "name": "Ali Farhadi", "hidden": false}, {"_id": "697ad0a3df3e800774f13c5a", "name": "Tim Dettmers", "hidden": false}], "publishedAt": "2026-01-28T17:27:08.000Z", "submittedOnDailyAt": "2026-01-29T00:44:55.441Z", "title": "SERA: Soft-Verified Efficient Repository Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.", "upvotes": 4, "discussionId": "697ad0a4df3e800774f13c5b", "githubRepo": "https://github.com/allenai/SERA", "githubRepoAddedBy": "user", "ai_summary": "Soft-Verified Efficient Repository Agents (SERA) enables cost-effective training of coding agents through supervised fine-tuning, achieving state-of-the-art performance while enabling specialization to private codebases at a fraction of the cost of previous methods.", "ai_keywords": ["supervised fine-tuning", "coding agents", "private codebases", "Soft-Verified Generation", "synthetic trajectories", "scaling laws", "ablations", "reinforcement learning"], "githubStars": 75, "organization": {"_id": "65e6310cc7738c6b88970c23", "name": "ai21labs", "fullname": "AI21", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67baf6e5489cb4dc98a4bff4/9Rkvk1VGhK1woxWvhqDyb.png"}, "summary_zh": "<ul>\n    <li>\u5f00\u653e\u6743\u91cd\u7f16\u7801\u4ee3\u7406\u76f8\u6bd4\u95ed\u6e90\u7cfb\u7edf\u5177\u6709\u4f18\u52bf\uff0c\u53ef\u4ee5\u4e13\u95e8\u9488\u5bf9\u79c1\u6709\u4ee3\u7801\u5e93\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u8f6f\u9a8c\u8bc1\u9ad8\u6548\u4ee3\u7801\u5e93\u4ee3\u7406\uff08SERA\uff09\uff0c\u4f7f\u5f97\u5feb\u901f\u4e14\u5ec9\u4ef7\u5730\u521b\u5efa\u4e13\u95e8\u9488\u5bf9\u79c1\u6709\u4ee3\u7801\u5e93\u7684\u4ee3\u7406\u53d8\u4e3a\u53ef\u80fd\u3002</li>\n    <li>SERA\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u578b\u7684\u6548\u679c\uff0c\u4e14\u6210\u672c\u6bd4\u5f3a\u5316\u5b66\u4e60\u4fbf\u5b9c26\u500d\uff0c\u8f83\u4e4b\u524d\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4fbf\u5b9c57\u500d\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ece\u5355\u4e00\u4ee3\u7801\u5e93\u751f\u6210\u6570\u5343\u6761\u8f68\u8ff9\uff0c\u5e76\u80fd\u5bf9\u66f4\u5927\u8303\u56f4\u7684\u4ee3\u7801\u5e93\u8fdb\u884c\u5e94\u7528\uff0c\u751f\u6210\u8d85\u8fc7200,000\u6761\u5408\u6210\u8f68\u8ff9\u3002</li>\n    <li>\u6211\u4eec\u5c06SERA\u4f5c\u4e3aAi2\u5f00\u653e\u7f16\u7801\u4ee3\u7406\u7cfb\u5217\u7684\u9996\u4e2a\u6a21\u578b\u53d1\u5e03\uff0c\u63d0\u4f9b\u76f8\u5173\u4ee3\u7801\u3001\u6570\u636e\u548cClaude Code\u96c6\u6210\uff0c\u652f\u6301\u7814\u7a76\u793e\u533a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SERA (Soft-Verified Efficient Repository Agents) is a new method for training coding agents that allows them to specialize in private codebases.</li>\n    <li>This method uses supervised finetuning and achieves top results among open-source models, matching the performance of leading open-weight models.</li>\n    <li>Creating SERA models is significantly cheaper, being 26 times less expensive than reinforcement learning and 57 times cheaper than previous methods.</li>\n    <li>The method generates thousands of synthetic code examples from a single repository, enabling better training and analysis.</li>\n    <li>SERA is the first model released in Ai2's Open Coding Agents series, with all related resources available for the research community.</li>\n</ul>"}, "publishedAt": "2026-01-28T12:27:08.000Z", "title": "SERA: Soft-Verified Efficient Repository Agents", "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20789.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 219, "isUserFollowing": false}, "organization": {"_id": "65e6310cc7738c6b88970c23", "name": "ai21labs", "fullname": "AI21", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67baf6e5489cb4dc98a4bff4/9Rkvk1VGhK1woxWvhqDyb.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20380", "authors": [{"_id": "697acd54df3e800774f13c30", "name": "Le Zhang", "hidden": false}, {"_id": "697acd54df3e800774f13c31", "name": "Yixiong Xiao", "hidden": false}, {"_id": "697acd54df3e800774f13c32", "name": "Xinjiang Lu", "hidden": false}, {"_id": "697acd54df3e800774f13c33", "name": "Jingjia Cao", "hidden": false}, {"_id": "697acd54df3e800774f13c34", "name": "Yusai Zhao", "hidden": false}, {"_id": "697acd54df3e800774f13c35", "name": "Jingbo Zhou", "hidden": false}, {"_id": "697acd54df3e800774f13c36", "name": "Lang An", "hidden": false}, {"_id": "697acd54df3e800774f13c37", "name": "Zikan Feng", "hidden": false}, {"_id": "697acd54df3e800774f13c38", "name": "Wanxiang Sha", "hidden": false}, {"_id": "697acd54df3e800774f13c39", "name": "Yu Shi", "hidden": false}, {"_id": "697acd54df3e800774f13c3a", "name": "Congxi Xiao", "hidden": false}, {"_id": "697acd54df3e800774f13c3b", "name": "Jian Xiong", "hidden": false}, {"_id": "697acd54df3e800774f13c3c", "name": "Yankai Zhang", "hidden": false}, {"_id": "697acd54df3e800774f13c3d", "name": "Hua Wu", "hidden": false}, {"_id": "697acd54df3e800774f13c3e", "name": "Haifeng Wang", "hidden": false}], "publishedAt": "2026-01-28T08:45:17.000Z", "submittedOnDailyAt": "2026-01-29T00:30:40.013Z", "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.", "upvotes": 4, "discussionId": "697acd55df3e800774f13c3f", "ai_summary": "OmegaUse is a general-purpose GUI agent model that achieves state-of-the-art performance on mobile and desktop platforms through a combination of high-quality data construction, decoupled training methods, and a Mixture-of-Experts architecture.", "ai_keywords": ["GUI agents", "foundation models", "autonomous task execution", "data-construction pipeline", "decoupled training paradigm", "automated synthesis framework", "bottom-up autonomous exploration", "top-down taxonomy-guided generation", "supervised fine-tuning", "group relative policy optimization", "mixture-of-experts", "screenspot-v2", "androidcontrol", "os-nav", "chim-nav", "ubu-nav"], "summary_zh": "<ul>\n    <li>OmegaUse\u662f\u4e00\u4e2a\u901a\u7528\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u79fb\u52a8\u548c\u684c\u9762\u5e73\u53f0\u4e0a\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u548c\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6784\u5efa\u6d41\u7a0b\u548c\u8bad\u7ec3\u8303\u5f0f\u3002</li>\n    <li>\u4e3a\u4e86\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\uff0cOmegaUse\u7ed3\u5408\u4e86\u81ea\u4e3b\u63a2\u7d22\u548c\u5206\u7c7b\u751f\u6210\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u8bad\u7ec3\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7b56\u7565\uff0c\u9996\u5148\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u901a\u8fc7\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u63d0\u5347\u7a7a\u95f4\u5b9a\u4f4d\u548c\u987a\u5e8f\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOmegaUse\u5728\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728ScreenSpot-V2\u548cAndroidControl\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6210\u7ee9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>OmegaUse is a new type of GUI agent that helps complete tasks on mobile and desktop devices.</li>\n    <li>It was developed using high-quality data and effective training methods.</li>\n    <li>The model uses a unique data construction process that creates realistic synthetic data.</li>\n    <li>Training includes two steps: first teaching basic interactions, then improving planning and understanding of the interface.</li>\n    <li>OmegaUse has shown impressive performance on various benchmarks, achieving high success rates in task completion.</li>\n</ul>"}, "publishedAt": "2026-01-28T03:45:17.000Z", "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution", "summary": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20380.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 219, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faLongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u5f3a\u5927\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u79cd\u81ea\u4e3b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u81ea\u4e3b\u641c\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u65b9\u9762\u3002</li>\n    <li>\u6a21\u578b\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u5de5\u5177\u4ea4\u4e92\u548c\u4e0d\u7a33\u5b9a\u7684\u73b0\u5b9e\u73af\u5883\u3002</li>\n    <li>\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\uff0c\u4f18\u5316\u4e86\u4ece\u9884\u8bad\u7ec3\u5230\u540e\u8bad\u7ec3\u7684\u6574\u4e2a\u8fc7\u7a0b\u3002</li>\n    <li>\u5f15\u5165\u91cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u8054\u5408\u6269\u5c55\u5b9e\u73b0\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source model with 560 billion parameters that excels in reasoning tasks.</li>\n    <li>It performs exceptionally well on various benchmarks, particularly in agentic search and tool use.</li>\n    <li>The model is designed to handle complex interactions and perform reliably in noisy, real-world situations.</li>\n    <li>Its effectiveness comes from a unique training approach that combines expert training and advanced data handling techniques.</li>\n    <li>To improve reasoning tasks, it features a Heavy Thinking mode that allows for deeper and broader thought processes during testing.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18418", "authors": [{"_id": "69785315026bdf0473116f6a", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T08:31:51.245Z", "hidden": false}, {"_id": "69785315026bdf0473116f6b", "name": "Dayuan Fu", "hidden": false}, {"_id": "69785315026bdf0473116f6c", "name": "Tiantian Mi", "hidden": false}, {"_id": "69785315026bdf0473116f6d", "name": "Yumin Zhuang", "hidden": false}, {"_id": "69785315026bdf0473116f6e", "user": {"_id": "6865e6b362fc5689c5e67733", "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg", "isPro": false, "fullname": "Yaxing Huang", "user": "Rookie-Noob-Newbie", "type": "user"}, "name": "Yaxing Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:30.220Z", "hidden": false}, {"_id": "69785315026bdf0473116f6f", "user": {"_id": "67638cc0d63e4b348e8a5fa3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png", "isPro": false, "fullname": "Xuefeng Li", "user": "drxuefeng", "type": "user"}, "name": "Xuefeng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:37.248Z", "hidden": false}, {"_id": "69785315026bdf0473116f70", "name": "Lyumanshan Ye", "hidden": false}, {"_id": "69785315026bdf0473116f71", "name": "Muhang Xie", "hidden": false}, {"_id": "69785315026bdf0473116f72", "name": "Qishuo Hua", "hidden": false}, {"_id": "69785315026bdf0473116f73", "name": "Zhen Huang", "hidden": false}, {"_id": "69785315026bdf0473116f74", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:23.438Z", "hidden": false}, {"_id": "69785315026bdf0473116f75", "name": "Hanning Wang", "hidden": false}, {"_id": "69785315026bdf0473116f76", "user": {"_id": "66fa544c54f87b607fbffd2e", "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg", "isPro": false, "fullname": "Jifan Lin", "user": "evanlin2570", "type": "user"}, "name": "Jifan Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:57.029Z", "hidden": false}, {"_id": "69785315026bdf0473116f77", "name": "Yang Xiao", "hidden": false}, {"_id": "69785315026bdf0473116f78", "name": "Jie Sun", "hidden": false}, {"_id": "69785315026bdf0473116f79", "user": {"_id": "684faf712acd915b5afc055f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg", "isPro": false, "fullname": "Yunze Wu", "user": "wyzmike", "type": "user"}, "name": "Yunze Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:06.063Z", "hidden": false}, {"_id": "69785315026bdf0473116f7a", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-01-26T12:20:18.000Z", "submittedOnDailyAt": "2026-01-27T03:34:37.777Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "upvotes": 104, "discussionId": "69785315026bdf0473116f7b", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev", "githubRepoAddedBy": "user", "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.", "ai_keywords": ["Large Language Model", "agentic software engineering", "mid-training", "distribution mismatch", "agent-native data", "contextually-native trajectories", "environmentally-native trajectories", "SWE-Bench Verified", "Kimi-Dev", "resolution rates"], "githubStars": 22, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u6b63\u5728\u4ece\u5355\u6b21\u4ee3\u7801\u751f\u6210\u8f6c\u5411\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u3002</li>\n    <li>\u4e2d\u95f4\u8bad\u7ec3\uff08MT\uff09\u65b9\u6cd5\u5728\u5927\u578b\u6570\u636e\u4e0a\u5e94\u7528\uff0c\u80fd\u66f4\u6709\u6548\u5730\u57f9\u517b\u81ea\u4e3b\u884c\u4e3a\uff0c\u4f46\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u52a8\u6001\u5f00\u53d1\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u4e2d\u95f4\u8bad\u7ec3\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6709\u6548\u7684\u4ee3\u7406\u5f00\u53d1\u6570\u636e\u5408\u6210\u539f\u5219\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u901a\u8fc7\u4e0a\u4e0b\u6587\u548c\u73af\u5883\u539f\u751f\u8f68\u8ff9\u7684\u6570\u636e\u76d1\u7763\uff0c\u9a8c\u8bc1\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving to autonomously handle software engineering tasks like navigating and editing code.</li>\n    <li>Mid-training on large data sets that reflect real-world workflows is a promising but underused method due to high resource demands.</li>\n    <li>A key challenge is the mismatch between static training data and the dynamic nature of actual software development environments.</li>\n    <li>This study introduces a systematic approach for mid-training, focusing on two types of data: contextually-native and environmentally-native trajectories.</li>\n    <li>The new models show better performance in agentic capabilities compared to previous methods, achieving impressive resolution rates with fewer training tokens.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:20:18.000Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png", "numComments": 2, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.20614", "authors": [{"_id": "697ac91bdf3e800774f13c12", "name": "Yanqi Dai", "hidden": false}, {"_id": "697ac91bdf3e800774f13c13", "name": "Yuxiang Ji", "hidden": false}, {"_id": "697ac91bdf3e800774f13c14", "name": "Xiao Zhang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c15", "name": "Yong Wang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c16", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "697ac91bdf3e800774f13c17", "name": "Zhiwu Lu", "hidden": false}], "publishedAt": "2026-01-28T13:49:23.000Z", "submittedOnDailyAt": "2026-01-29T00:15:35.371Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "submittedOnDailyBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "isPro": false, "fullname": "Yanqi Dai", "user": "YanqiDai", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "upvotes": 93, "discussionId": "697ac91bdf3e800774f13c18", "githubRepo": "https://github.com/AMAP-ML/MathForge", "githubRepoAddedBy": "user", "ai_summary": "MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "Group Relative Policy Optimization", "Difficulty-Aware Group Policy Optimization", "Multi-Aspect Question Reformulation", "mathematical reasoning", "policy updates", "group advantage estimation", "question-level weighting", "data augmentation"], "githubStars": 84, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMathForge\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5927\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u96be\u9898\u65f6\u5b58\u5728\u7b97\u6cd5\u548c\u6570\u636e\u4e0a\u7684\u4e0d\u8db3\uff0c\u96be\u9898\u7684\u66f4\u65b0\u5e45\u5ea6\u8f83\u5c0f\u3002</li>\n    <li>MathForge\u5305\u62ec\u4e24\u4e2a\u90e8\u5206\uff1a\u96be\u5ea6\u611f\u77e5\u7684\u7fa4\u4f53\u7b56\u7565\u4f18\u5316\uff08DGPO\uff09\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u8ff0\uff08MQR\uff09\u3002</li>\n    <li>DGPO\u901a\u8fc7\u5e73\u8861\u96be\u5ea6\u6765\u4f18\u5316\u95ee\u9898\u66f4\u65b0\uff0c\u5e76\u4f18\u5148\u5904\u7406\u66f4\u96be\u7684\u95ee\u9898\u3002</li>\n    <li>MQR\u901a\u8fc7\u591a\u65b9\u9762\u91cd\u8ff0\u95ee\u9898\u6765\u589e\u52a0\u96be\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7b54\u6848\u4e0d\u53d8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) helps improve mathematical reasoning in large models.</li>\n    <li>Current methods don't focus enough on difficult questions, which are important for developing better reasoning skills.</li>\n    <li>The proposed MathForge framework includes two parts: a new algorithm (DGPO) that balances question difficulty and a strategy (MQR) that reformulates questions to make them harder.</li>\n    <li>DGPO improves the way questions are handled by focusing on more difficult ones and ensuring fair updates in learning.</li>\n    <li>MathForge has been shown to work much better than existing methods in various math reasoning tasks, with all resources available online.</li>\n</ul>"}, "publishedAt": "2026-01-28T08:49:23.000Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20614.png", "numComments": 12, "submittedBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "fullname": "Yanqi Dai", "name": "YanqiDai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20540", "authors": [{"_id": "697ac48cdf3e800774f13bc1", "name": "Robbyant Team", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc2", "name": "Zelin Gao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc3", "user": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "name": "Qiuyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:29.190Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc4", "name": "Yanhong Zeng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc5", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc6", "user": {"_id": "64acd2ec39fcfebff8c79c00", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64acd2ec39fcfebff8c79c00/Avq66l5hO-aggNtk4Y1ss.png", "isPro": false, "fullname": "Ka Leong Cheng", "user": "felixcheng97", "type": "user"}, "name": "Ka Leong Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:36.041Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc7", "name": "Yixuan Li", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc8", "user": {"_id": "665f059a8947302aa2c63afe", "avatarUrl": "/avatars/50f560285946532321a0bd526494148d.svg", "isPro": false, "fullname": "hanlin wang", "user": "hlwang06", "type": "user"}, "name": "Hanlin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:18:10.952Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc9", "name": "Yinghao Xu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bca", "name": "Shuailei Ma", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcb", "name": "Yihang Chen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcc", "name": "Jie Liu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcd", "name": "Yansong Cheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bce", "name": "Yao Yao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcf", "name": "Jiayi Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd0", "user": {"_id": "656084f44e8918182d4f07c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg", "isPro": false, "fullname": "Yihao Meng", "user": "Yhmeng1106", "type": "user"}, "name": "Yihao Meng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:37.840Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd1", "name": "Kecheng Zheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd2", "user": {"_id": "63f0baf66309c84d5f4a2226", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg", "isPro": true, "fullname": "Qingyan", "user": "QingyanBai", "type": "user"}, "name": "Qingyan Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:24.535Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd3", "user": {"_id": "6478a982256b62e219917d67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg", "isPro": false, "fullname": "JingyeChen22", "user": "JingyeChen22", "type": "user"}, "name": "Jingye Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:37.828Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd4", "name": "Zehong Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd5", "user": {"_id": "662128ec9ca2cd4e6db2fb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg", "isPro": false, "fullname": "Bruce Yu", "user": "bruceyyu", "type": "user"}, "name": "Yue Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:27.115Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd6", "name": "Xing Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd7", "name": "Yujun Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd8", "name": "Hao Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "publishedAt": "2026-01-28T12:37:01.000Z", "submittedOnDailyAt": "2026-01-29T00:09:39.166Z", "title": "Advancing Open-source World Models", "submittedOnDailyBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "upvotes": 66, "discussionId": "697ac48cdf3e800774f13bd9", "projectPage": "https://technology.robbyant.com/lingbot-world", "githubRepo": "https://github.com/Robbyant/lingbot-world/", "githubRepoAddedBy": "user", "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.", "ai_keywords": ["world simulator", "video generation", "world model", "long-term memory", "real-time interactivity"], "githubStars": 756, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "summary_zh": "<ul>\n    <li>LingBot-World\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4e16\u754c\u6a21\u62df\u5668\uff0c\u6e90\u81ea\u89c6\u9891\u751f\u6210\u6280\u672f\u3002</li>\n    <li>\u5b83\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u5f3a\u5927\u7684\u52a8\u6001\u8868\u73b0\uff0c\u5305\u62ec\u73b0\u5b9e\u4e3b\u4e49\u3001\u79d1\u5b66\u80cc\u666f\u548c\u5361\u901a\u98ce\u683c\u7b49\u3002</li>\n    <li>\u652f\u6301\u5206\u949f\u7ea7\u7684\u89c6\u91ce\uff0c\u5e76\u4fdd\u6301\u65f6\u95f4\u4e0a\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u5373\u201c\u957f\u671f\u8bb0\u5fc6\u201d\u3002</li>\n    <li>\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\uff0c\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\uff0c\u80fd\u591f\u4ee5\u6bcf\u79d216\u5e27\u7684\u901f\u5ea6\u751f\u6210\u56fe\u50cf\u3002</li>\n    <li>\u63d0\u4f9b\u4ee3\u7801\u548c\u6a21\u578b\u7684\u516c\u5171\u8bbf\u95ee\uff0c\u65e8\u5728\u7f29\u5c0f\u5f00\u6e90\u4e0e\u95ed\u6e90\u6280\u672f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u5185\u5bb9\u521b\u4f5c\u3001\u6e38\u620f\u548c\u673a\u5668\u4eba\u5b66\u4e60\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LingBot-World is an open-source world simulator that creates video content.</li>\n    <li>It supports various types of environments, from realistic to cartoonish styles.</li>\n    <li>It has a long-term memory feature that keeps context consistent over time.</li>\n    <li>It allows real-time interaction with quick response times (under 1 second).</li>\n    <li>The code and model are publicly available to help improve access to technology for content creation, gaming, and robot learning.</li>\n</ul>"}, "publishedAt": "2026-01-28T07:37:01.000Z", "title": "Advancing Open-source World Models", "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20540.png", "numComments": 1, "submittedBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "fullname": "Qiuyu Wang", "name": "qiuyuu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16746", "authors": [{"_id": "6976d4105d41524304c13517", "name": "Yuhang Wang", "hidden": false}, {"_id": "6976d4105d41524304c13518", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:56.805Z", "hidden": false}, {"_id": "6976d4105d41524304c13519", "name": "Mo Yang", "hidden": false}, {"_id": "6976d4105d41524304c1351a", "name": "Rongrui Zhang", "hidden": false}, {"_id": "6976d4105d41524304c1351b", "name": "Shilin He", "hidden": false}, {"_id": "6976d4105d41524304c1351c", "name": "Heng Lian", "hidden": false}, {"_id": "6976d4105d41524304c1351d", "name": "Yuting Chen", "hidden": false}, {"_id": "6976d4105d41524304c1351e", "name": "Siyu Ye", "hidden": false}, {"_id": "6976d4105d41524304c1351f", "name": "Kai Cai", "hidden": false}, {"_id": "6976d4105d41524304c13520", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-01-23T13:51:59.000Z", "submittedOnDailyAt": "2026-01-26T00:10:23.451Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "upvotes": 63, "discussionId": "6976d4105d41524304c13521", "githubRepo": "https://github.com/Ayanami1314/swe-pruner", "githubRepoAddedBy": "user", "ai_summary": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.", "ai_keywords": ["context compression", "LongLLMLingua", "PPL", "code understanding", "task-aware adaptive pruning", "neural skimmer", "token reduction", "SWE-Bench Verified", "LongCodeQA"], "githubStars": 35, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u4ea4\u4e92\u4e0a\u4e0b\u6587\u5bfc\u81f4\u9ad8API\u6210\u672c\u548c\u5ef6\u8fdf\u3002</li>\n    <li>\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u5982LongLLMLingua\u4f9d\u8d56\u56fa\u5b9a\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u4ee3\u7801\u7406\u89e3\u7684\u4efb\u52a1\u7279\u6027\u3002</li>\n    <li>\u63d0\u51fa\u4e86SWE-Pruner\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u4fee\u526a\u6846\u67b6\uff0c\u4e13\u4e3a\u7f16\u7801\u4ee3\u7406\u8bbe\u8ba1\u3002</li>\n    <li>SWE-Pruner\u7075\u611f\u6765\u81ea\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u5f00\u53d1\u548c\u8c03\u8bd5\u65f6\u7684\u9009\u62e9\u6027\u6d4f\u89c8\uff0c\u80fd\u591f\u6839\u636e\u4efb\u52a1\u81ea\u9002\u5e94\u4fee\u526a\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWE-Pruner\u5b9e\u73b0\u4e8623-54%\u7684\u4ee4\u724c\u51cf\u5c11\uff0c\u5e76\u5728\u5355\u8f6e\u4efb\u52a1\u4e2d\u8fbe\u523014.84\u500d\u7684\u538b\u7f29\uff0c\u4e14\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are good at software development but struggle with long interactions due to high costs and slow responses.</li>\n    <li>Current methods for reducing context length often lose important code details and disrupt structure.</li>\n    <li>SWE-Pruner is a new framework that helps coding agents focus on relevant parts of code based on specific tasks.</li>\n    <li>The framework uses a lightweight neural network to select important lines of code while considering the agent's current goal.</li>\n    <li>Tests show SWE-Pruner can reduce context size significantly (by 23-54%) without harming performance in various tasks.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:51:59.000Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16746.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18491", "authors": [{"_id": "697831d9026bdf0473116e5c", "name": "Dongrui Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e5d", "user": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "name": "Qihan Ren", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:31:15.765Z", "hidden": false}, {"_id": "697831d9026bdf0473116e5e", "name": "Chen Qian", "hidden": false}, {"_id": "697831d9026bdf0473116e5f", "name": "Shuai Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e60", "name": "Yuejin Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e61", "name": "Yu Li", "hidden": false}, {"_id": "697831d9026bdf0473116e62", "name": "Zhonghao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e63", "name": "Haoyu Luo", "hidden": false}, {"_id": "697831d9026bdf0473116e64", "name": "Peng Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e65", "name": "Qingyu Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e66", "name": "Binxin Hu", "hidden": false}, {"_id": "697831d9026bdf0473116e67", "name": "Ling Tang", "hidden": false}, {"_id": "697831d9026bdf0473116e68", "name": "Jilin Mei", "hidden": false}, {"_id": "697831d9026bdf0473116e69", "name": "Dadi Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e6a", "name": "Leitao Yuan", "hidden": false}, {"_id": "697831d9026bdf0473116e6b", "name": "Junyao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e6c", "name": "Guanxu Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e6d", "name": "Qihao Lin", "hidden": false}, {"_id": "697831d9026bdf0473116e6e", "name": "Yi Yu", "hidden": false}, {"_id": "697831d9026bdf0473116e6f", "name": "Bo Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e70", "name": "Jiaxuan Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e71", "name": "Jie Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e72", "name": "Wenqi Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e73", "name": "Huiqi Deng", "hidden": false}, {"_id": "697831d9026bdf0473116e74", "name": "Zhiheng Xi", "hidden": false}, {"_id": "697831d9026bdf0473116e75", "name": "Wenjie Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e76", "name": "Wenxuan Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e77", "name": "Wen Shen", "hidden": false}, {"_id": "697831d9026bdf0473116e78", "name": "Zhikai Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e79", "name": "Haoyu Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e7a", "name": "Jialing Tao", "hidden": false}, {"_id": "697831d9026bdf0473116e7b", "name": "Juntao Dai", "hidden": false}, {"_id": "697831d9026bdf0473116e7c", "name": "Jiaming Ji", "hidden": false}, {"_id": "697831d9026bdf0473116e7d", "name": "Zhongjie Ba", "hidden": false}, {"_id": "697831d9026bdf0473116e7e", "name": "Linfeng Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e7f", "name": "Yong Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e80", "name": "Quanshi Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e81", "name": "Lei Zhu", "hidden": false}, {"_id": "697831d9026bdf0473116e82", "name": "Zhihua Wei", "hidden": false}, {"_id": "697831d9026bdf0473116e83", "name": "Hui Xue", "hidden": false}, {"_id": "697831d9026bdf0473116e84", "name": "Chaochao Lu", "hidden": false}, {"_id": "697831d9026bdf0473116e85", "name": "Jing Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e86", "name": "Xia Hu", "hidden": false}], "publishedAt": "2026-01-26T13:45:41.000Z", "submittedOnDailyAt": "2026-01-28T01:26:49.833Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "submittedOnDailyBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "upvotes": 60, "discussionId": "697831d9026bdf0473116e87", "githubRepo": "https://github.com/AI45Lab/AgentDoG", "githubRepoAddedBy": "user", "ai_summary": "AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.", "ai_keywords": ["agentic guardrail", "three-dimensional taxonomy", "agentic safety benchmark", "Diagnostic Guardrail framework", "agent safety and security", "agent trajectories", "root cause diagnosis", "fine-grained monitoring", "model variants", "state-of-the-art performance"], "githubStars": 178, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "summary_zh": "<ul>\n    <li>AI\u4ee3\u7406\u7684\u5174\u8d77\u5e26\u6765\u4e86\u590d\u6742\u7684\u5b89\u5168\u548c\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u81ea\u4e3b\u5de5\u5177\u4f7f\u7528\u548c\u73af\u5883\u4e92\u52a8\u65b9\u9762\u3002</li>\n    <li>\u76ee\u524d\u7684\u76d1\u63a7\u6a21\u578b\u7f3a\u4e4f\u5bf9\u4ee3\u7406\u98ce\u9669\u7684\u610f\u8bc6\u548c\u900f\u660e\u5ea6\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u7ef4\u5206\u7c7b\u6cd5\u6765\u7cfb\u7edf\u5316\u4ee3\u7406\u98ce\u9669\uff0c\u5305\u62ec\u6765\u6e90\u3001\u5931\u8d25\u6a21\u5f0f\u548c\u540e\u679c\u3002</li>\n    <li>\u63a8\u51fa\u4e86\u65b0\u7684\u4ee3\u7406\u5b89\u5168\u57fa\u51c6\uff08ATBench\uff09\u548c\u8bca\u65ad\u76d1\u63a7\u6846\u67b6\uff08AgentDoG\uff09\uff0c\u53ef\u7ec6\u81f4\u76d1\u63a7\u4ee3\u7406\u7684\u884c\u4e3a\u8f68\u8ff9\u3002</li>\n    <li>AgentDoG\u80fd\u591f\u8bca\u65ad\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u4f9b\u8d85\u8d8a\u4e8c\u5143\u6807\u7b7e\u7684\u900f\u660e\u5ea6\uff0c\u4ee5\u4fc3\u8fdb\u6709\u6548\u7684\u4ee3\u7406\u5bf9\u9f50\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The rise of AI agents brings new safety and security challenges due to their ability to act independently and interact with their environment.</li>\n    <li>Existing safety models do not effectively understand or explain the risks associated with AI agents.</li>\n    <li>A new three-dimensional system is proposed to categorize risks based on their source, failure mode, and consequences.</li>\n    <li>A new safety benchmark called ATBench and a framework named AgentDoG are introduced to improve monitoring and diagnosing unsafe AI behaviors.</li>\n    <li>AgentDoG can identify the causes of unsafe actions and offers detailed insights for better alignment of AI agents, with models available in different sizes and open access to all resources.</li>\n</ul>"}, "publishedAt": "2026-01-26T08:45:41.000Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18491.png", "numComments": 6, "submittedBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "fullname": "Qihan Ren", "name": "jasonrqh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.19325", "authors": [{"_id": "69798298df44b75fa47e47a9", "name": "Zichen Wen", "hidden": false}, {"_id": "69798298df44b75fa47e47aa", "user": {"_id": "688c72c011ef3399b561dee7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/688c72c011ef3399b561dee7/puhgnTOAfZYetsC46hqGm.jpeg", "isPro": false, "fullname": "BoxueYang", "user": "Boxue", "type": "user"}, "name": "Boxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:12.982Z", "hidden": false}, {"_id": "69798298df44b75fa47e47ab", "name": "Shuang Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47ac", "name": "Yaojie Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ad", "name": "Yuhang Han", "hidden": false}, {"_id": "69798298df44b75fa47e47ae", "name": "Junlong Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47af", "name": "Cong Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b0", "name": "Yicheng Fu", "hidden": false}, {"_id": "69798298df44b75fa47e47b1", "name": "Jiawang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47b2", "name": "Jiangchao Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b3", "name": "Xi Fang", "hidden": false}, {"_id": "69798298df44b75fa47e47b4", "name": "Zhen Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b5", "name": "Henxing Cai", "hidden": false}, {"_id": "69798298df44b75fa47e47b6", "name": "Lin Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b7", "name": "Zhifeng Gao", "hidden": false}, {"_id": "69798298df44b75fa47e47b8", "name": "Yanhui Hong", "hidden": false}, {"_id": "69798298df44b75fa47e47b9", "name": "Nang Yuan", "hidden": false}, {"_id": "69798298df44b75fa47e47ba", "name": "Yixuan Li", "hidden": false}, {"_id": "69798298df44b75fa47e47bb", "name": "Guojiang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47bc", "name": "Haoyi Tao", "hidden": false}, {"_id": "69798298df44b75fa47e47bd", "name": "Nan Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47be", "name": "Han Lyu", "hidden": false}, {"_id": "69798298df44b75fa47e47bf", "name": "Guolin Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47c0", "name": "Ning Liao", "hidden": false}, {"_id": "69798298df44b75fa47e47c1", "name": "Xiaoxing Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c2", "name": "Kai Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c3", "name": "Zhiyu Li", "hidden": false}, {"_id": "69798298df44b75fa47e47c4", "name": "Feiyu Xiong", "hidden": false}, {"_id": "69798298df44b75fa47e47c5", "name": "Sihan Hu", "hidden": false}, {"_id": "69798298df44b75fa47e47c6", "name": "Kun Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c7", "name": "Yanfeng Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c8", "name": "Weinan E", "hidden": false}, {"_id": "69798298df44b75fa47e47c9", "name": "Linfeng Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ca", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-01-27T08:12:18.000Z", "submittedOnDailyAt": "2026-01-29T01:20:58.570Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "submittedOnDailyBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "upvotes": 53, "discussionId": "69798298df44b75fa47e47cb", "projectPage": "https://innovatorlm.github.io/Innovator-VL", "githubRepo": "https://github.com/InnovatorLM/Innovator-VL", "githubRepoAddedBy": "user", "ai_summary": "Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.", "ai_keywords": ["multimodal large language model", "scientific multimodal large language model", "end-to-end reproducible training pipeline", "supervised fine-tuning", "reinforcement learning", "scientific reasoning", "data efficiency", "principled data selection", "generalization", "scientific alignment"], "githubStars": 70, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n  <li>Innovator-VL\u662f\u4e00\u4e2a\u79d1\u5b66\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u5bf9\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7684\u7406\u89e3\u548c\u63a8\u7406\u3002</li>\n  <li>\u8be5\u6a21\u578b\u91c7\u7528\u900f\u660e\u7684\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u6570\u636e\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u5f3a\u5927\u7684\u79d1\u5b66\u667a\u80fd\u3002</li>\n  <li>\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u91cd\u590d\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u6e05\u6d17\u3001\u9884\u5904\u7406\u7b49\uff0c\u65b9\u4fbf\u793e\u533a\u6269\u5c55\u3002</li>\n  <li>Innovator-VL\u5728\u79d1\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u7528\u4e0d\u5230\u4e94\u767e\u4e07\u4e2a\u6837\u672c\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002</li>\n  <li>\u8be5\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u548c\u591a\u6a21\u6001\u63a8\u7406\u7b49\u65b9\u9762\u7684\u8868\u73b0\u4e5f\u5f88\u5f3a\uff0c\u8bc1\u660e\u79d1\u5b66\u5bf9\u9f50\u53ef\u4ee5\u4e0e\u901a\u7528\u80fd\u529b\u7ed3\u5408\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Innovator-VL is a new large language model that helps with understanding and reasoning in various scientific fields while also doing well in general vision tasks.</li>\n    <li>It uses a clear and reproducible training process, making it easy for others to follow and build upon.</li>\n    <li>The model is very efficient with data, performing well on scientific tasks with fewer than five million curated samples, without needing extensive pretraining.</li>\n    <li>Innovator-VL shows strong generalization abilities, performing well in both scientific tasks and general vision and reasoning challenges.</li>\n    <li>This work suggests that high-quality scientific models can be developed without needing large amounts of data, which can help future research efforts.</li>\n</ul>"}, "publishedAt": "2026-01-27T03:12:18.000Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19325.png", "numComments": 1, "submittedBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "fullname": "Zichen Wen", "name": "zichenwen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.17737", "authors": [{"_id": "6978310b026bdf0473116e44", "user": {"_id": "64545c77a7ce0a8fde809912", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg", "isPro": false, "fullname": "ChenYuMu", "user": "ChenYuMu", "type": "user"}, "name": "Chenyu Mu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:21.462Z", "hidden": false}, {"_id": "6978310b026bdf0473116e45", "user": {"_id": "6527a2df1eb78901534b0cc6", "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg", "isPro": false, "fullname": "Xin He", "user": "Kleinhe", "type": "user"}, "name": "Xin He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:20.414Z", "hidden": false}, {"_id": "6978310b026bdf0473116e46", "user": {"_id": "64300415b009240418dac70c", "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg", "isPro": false, "fullname": "Qu Yang", "user": "quyang22", "type": "user"}, "name": "Qu Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:22.475Z", "hidden": false}, {"_id": "6978310b026bdf0473116e47", "name": "Wanshun Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e48", "name": "Jiadi Yao", "hidden": false}, {"_id": "6978310b026bdf0473116e49", "name": "Huang Liu", "hidden": false}, {"_id": "6978310b026bdf0473116e4a", "name": "Zihao Yi", "hidden": false}, {"_id": "6978310b026bdf0473116e4b", "name": "Bo Zhao", "hidden": false}, {"_id": "6978310b026bdf0473116e4c", "name": "Xingyu Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e4d", "name": "Ruotian Ma", "hidden": false}, {"_id": "6978310b026bdf0473116e4e", "name": "Fanghua Ye", "hidden": false}, {"_id": "6978310b026bdf0473116e4f", "name": "Erkun Yang", "hidden": false}, {"_id": "6978310b026bdf0473116e50", "name": "Cheng Deng", "hidden": false}, {"_id": "6978310b026bdf0473116e51", "name": "Zhaopeng Tu", "hidden": false}, {"_id": "6978310b026bdf0473116e52", "name": "Xiaolong Li", "hidden": false}, {"_id": "6978310b026bdf0473116e53", "name": "Linus", "hidden": false}], "publishedAt": "2026-01-25T08:10:28.000Z", "submittedOnDailyAt": "2026-01-27T01:05:46.612Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "submittedOnDailyBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "isPro": false, "fullname": "Zhaopeng Tu", "user": "zptu", "type": "user"}, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "upvotes": 46, "discussionId": "6978310b026bdf0473116e54", "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/", "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent", "githubRepoAddedBy": "user", "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.", "ai_keywords": ["video generation", "dialogue-to-cinematic-video", "ScripterAgent", "DirectorAgent", "cross-scene continuous generation", "ScriptBench", "Visual-Script Alignment", "CriticAgent"], "githubStars": 228, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u89c6\u9891\u751f\u6210\u6280\u672f\u80fd\u591f\u6839\u636e\u7b80\u5355\u7684\u6587\u672c\u63d0\u793a\u5408\u6210\u51fa\u7cbe\u5f69\u7684\u89c6\u89c9\u5185\u5bb9\u3002</li>\n    <li>\u73b0\u6709\u6a21\u578b\u5728\u4ece\u9ad8\u5c42\u6b21\u6982\u5ff5\uff08\u5982\u5bf9\u8bdd\uff09\u751f\u6210\u8fde\u8d2f\u7684\u957f\u7bc7\u53d9\u4e8b\u65f6\u5b58\u5728\u201c\u8bed\u4e49\u5dee\u8ddd\u201d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u8bdd\u5230\u7535\u5f71\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u5305\u542bScripterAgent\u548cDirectorAgent\u4e24\u4e2a\u6a21\u578b\u3002</li>\n    <li>ScripterAgent\u5c06\u7c97\u7565\u5bf9\u8bdd\u8f6c\u6362\u4e3a\u8be6\u7ec6\u53ef\u6267\u884c\u7684\u7535\u5f71\u5267\u672c\uff0cDirectorAgent\u5219\u751f\u6210\u8fde\u8d2f\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u6211\u4eec\u7684\u6846\u67b6\u5728\u5267\u672c\u5fe0\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u6548\u679c\uff0c\u5e76\u63ed\u793a\u4e86\u89c6\u89c9\u6548\u679c\u548c\u5267\u672c\u9075\u5faa\u4e4b\u95f4\u7684\u91cd\u8981\u6743\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New video generation models can create impressive visuals from text but struggle with long, coherent stories.</li>\n    <li>To improve this, a new framework called ScripterAgent is introduced to turn dialogue into detailed cinematic scripts.</li>\n    <li>A large benchmark called ScriptBench was created to help train the model using expert guidance.</li>\n    <li>The framework uses another model, DirectorAgent, to ensure the video matches the script over longer scenes.</li>\n    <li>Evaluation shows this approach improves the connection between the script and video quality, highlighting challenges in balancing visual appeal with script accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-25T03:10:28.000Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17737.png", "numComments": 3, "submittedBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "fullname": "Zhaopeng Tu", "name": "zptu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18631", "authors": [{"_id": "6978a169026bdf0473117088", "user": {"_id": "66aca01e33f6b27979856f6f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg", "isPro": false, "fullname": "Mingyang Song", "user": "hitsmy", "type": "user"}, "name": "Mingyang Song", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:29:30.581Z", "hidden": false}, {"_id": "6978a169026bdf0473117089", "user": {"_id": "63a2a51ef30c464227924fc6", "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg", "isPro": false, "fullname": "Haoyu Sun", "user": "Mikivis", "type": "user"}, "name": "Haoyu Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:29:26.154Z", "hidden": false}, {"_id": "6978a169026bdf047311708a", "user": {"_id": "645b4819f9d4ec91fdd54852", "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg", "isPro": false, "fullname": "Jiawei Gu", "user": "kuvvi", "type": "user"}, "name": "Jiawei Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:29:28.392Z", "hidden": false}, {"_id": "6978a169026bdf047311708b", "name": "Linjie Li", "hidden": false}, {"_id": "6978a169026bdf047311708c", "name": "Luxin Xu", "hidden": false}, {"_id": "6978a169026bdf047311708d", "name": "Ranjay Krishna", "hidden": false}, {"_id": "6978a169026bdf047311708e", "name": "Yu Cheng", "hidden": false}], "publishedAt": "2026-01-26T16:04:43.000Z", "submittedOnDailyAt": "2026-01-28T01:52:20.218Z", "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "submittedOnDailyBy": {"_id": "66aca01e33f6b27979856f6f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg", "isPro": false, "fullname": "Mingyang Song", "user": "hitsmy", "type": "user"}, "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "upvotes": 38, "discussionId": "6978a16a026bdf047311708f", "projectPage": "https://adareasoner.github.io/", "githubRepo": "https://github.com/ssmisya/AdaReasoner", "githubRepoAddedBy": "user", "ai_summary": "AdaReasoner enables multimodal models to learn tool usage as a general reasoning skill through scalable data curation, reinforcement learning for tool selection, and adaptive learning mechanisms that improve performance on complex visual reasoning tasks.", "ai_keywords": ["multimodal large language models", "tool use", "reinforcement learning", "end-task success", "adaptive learning mechanism", "visual reasoning", "multimodal models", "tool selection", "tool sequencing", "long-horizon interactions"], "githubStars": 44, "organization": {"_id": "643cb0625fcffe09fb6ca688", "name": "Fudan-University", "fullname": "Fudan University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}, "summary_zh": "<ul>\n    <li>AdaReasoner \u662f\u4e00\u7cfb\u5217\u591a\u6a21\u6001\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u4f5c\u4e3a\u901a\u7528\u63a8\u7406\u6280\u80fd\uff0c\u800c\u975e\u7279\u5b9a\u5de5\u5177\u7684\u884c\u4e3a\u6765\u5b9e\u73b0\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u53ef\u6269\u5c55\u7684\u6570\u636e\u7b56\u5212\u6d41\u7a0b\u3001\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\u6765\u4f18\u5316\u5de5\u5177\u9009\u62e9\u548c\u4f7f\u7528\u3002</li>\n    <li>AdaReasoner \u80fd\u591f\u6839\u636e\u4efb\u52a1\u4e0a\u4e0b\u6587\u63a8\u65ad\u5de5\u5177\u7684\u6548\u7528\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u5de5\u5177\u4e0a\u8fdb\u884c\u63a8\u5e7f\u3002</li>\n    <li>\u7ecf\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u63d0\u5347\u4e867B\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd24.9%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AdaReasoner is a new type of model that helps improve visual reasoning by teaching machines to use tools effectively.</li>\n    <li>It learns how to choose and use tools based on the task at hand, without needing specific training for each tool.</li>\n    <li>The system includes a smart way to gather data, a reinforcement learning method for selecting tools, and a flexible learning approach for adjusting tool use.</li>\n    <li>AdaReasoner performs well in various tasks, showing it can adapt to new tools and situations effectively.</li>\n    <li>It achieves significant improvements over previous models, outpacing strong competitors like GPT-5 in several benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-26T11:04:43.000Z", "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18631.png", "numComments": 3, "submittedBy": {"_id": "66aca01e33f6b27979856f6f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg", "fullname": "Mingyang Song", "name": "hitsmy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "643cb0625fcffe09fb6ca688", "name": "Fudan-University", "fullname": "Fudan University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17367", "authors": [{"_id": "6978295b026bdf0473116db5", "user": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "isPro": false, "fullname": "Zecheng Tang", "user": "ZetangForward", "type": "user"}, "name": "Zecheng Tang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:42:48.253Z", "hidden": false}, {"_id": "6978295b026bdf0473116db6", "user": {"_id": "6732fb1d24b316be87acaafe", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732fb1d24b316be87acaafe/BzD8HL4vhh3mfeSF3rm_1.jpeg", "isPro": false, "fullname": "Quantong Qiu", "user": "QQTang1223", "type": "user"}, "name": "Quantong Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:08.590Z", "hidden": false}, {"_id": "6978295b026bdf0473116db7", "name": "Yi Yang", "hidden": false}, {"_id": "6978295b026bdf0473116db8", "name": "Zhiyi Hong", "hidden": false}, {"_id": "6978295b026bdf0473116db9", "name": "Haiya Xiang", "hidden": false}, {"_id": "6978295b026bdf0473116dba", "user": {"_id": "6875e95998c692330d72dc9a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wY7yjIix_IptruwlcG5Q4.png", "isPro": false, "fullname": "Kebin Liu", "user": "KebinLiu", "type": "user"}, "name": "Kebin Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:45.073Z", "hidden": false}, {"_id": "6978295b026bdf0473116dbb", "user": {"_id": "68cce9276e4618473d590342", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UqBWbWZ58EPwJ5iRUsJsB.png", "isPro": false, "fullname": "Qingqing Dang", "user": "DaisyGrace", "type": "user"}, "name": "Qingqing Dang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:50.734Z", "hidden": false}, {"_id": "6978295b026bdf0473116dbc", "user": {"_id": "6670e285b0c03c4e9d6e0985", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6670e285b0c03c4e9d6e0985/j9Zr-lOtrRmpFz5f4x420.jpeg", "isPro": false, "fullname": "Juntao Li", "user": "douvleplus", "type": "user"}, "name": "Juntao Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:56.908Z", "hidden": false}, {"_id": "6978295b026bdf0473116dbd", "name": "Min Zhang", "hidden": false}], "publishedAt": "2026-01-24T08:22:07.000Z", "submittedOnDailyAt": "2026-01-27T00:51:37.565Z", "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers", "submittedOnDailyBy": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "isPro": false, "fullname": "Zecheng Tang", "user": "ZetangForward", "type": "user"}, "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.", "upvotes": 26, "discussionId": "6978295b026bdf0473116dbe", "projectPage": "https://github.com/LCM-Lab/Elastic-Attention", "githubRepo": "https://github.com/LCM-Lab/Elastic-Attention", "githubRepoAddedBy": "user", "ai_summary": "Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.", "ai_keywords": ["standard attention mechanisms", "sparse attention", "full attention", "hybrid attention strategies", "attention router", "attention heads", "long-context scenarios", "pretrained models", "computational efficiency"], "githubStars": 11, "organization": {"_id": "61f8e653129c9ff1b911293d", "name": "SUDA", "fullname": "Soochow University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"}, "summary_zh": "<ul>\n    <li>\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u6027\u9650\u5236\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6269\u5c55\u6027\u3002</li>\n    <li>\u6df7\u5408\u6ce8\u610f\u529b\u7b56\u7565\u7ed3\u5408\u4e86\u7a00\u758f\u548c\u5b8c\u6574\u6ce8\u610f\u529b\uff0c\u4f46\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u8ba1\u7b97\u6bd4\u4f8b\uff0c\u65e0\u6cd5\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u6574\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u5f39\u6027\u6ce8\u610f\u529b\uff08Elastic Attention\uff09\uff0c\u5141\u8bb8\u6a21\u578b\u6839\u636e\u8f93\u5165\u52a8\u6001\u8c03\u6574\u7a00\u758f\u6027\u3002</li>\n    <li>\u901a\u8fc7\u96c6\u6210\u8f7b\u91cf\u7ea7\u7684\u6ce8\u610f\u529b\u8def\u7531\u5668\uff0c\u6a21\u578b\u53ef\u4ee5\u52a8\u6001\u5206\u914d\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u7684\u8ba1\u7b97\u6a21\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u77ed\u65f6\u95f4\u8bad\u7ec3\u540e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Standard attention mechanisms slow down large language models when dealing with long texts because they require a lot of calculations.</li>\n    <li>Hybrid attention strategies mix sparse and full attention but usually use fixed ratios, which don\u2019t adapt to different tasks.</li>\n    <li>Elastic Attention adjusts the level of attention dynamically based on the input, improving efficiency.</li>\n    <li>This method involves adding a lightweight Attention Router to the existing model, allowing for better performance in various contexts.</li>\n    <li>After 12 hours of training, our approach shows strong results on three benchmarks for long-context tasks.</li>\n</ul>"}, "publishedAt": "2026-01-24T03:22:07.000Z", "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers", "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17367.png", "numComments": 1, "submittedBy": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "fullname": "Zecheng Tang", "name": "ZetangForward", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "61f8e653129c9ff1b911293d", "name": "SUDA", "fullname": "Soochow University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\uff0c\u89c6\u9891\u63d0\u4f9b\u7684\u89c6\u89c9\u7ebf\u7d22\u6709\u9650\uff0c\u800c\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u79f0\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u7684\u5f00\u653e\u9886\u57df\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u8981\u6c42\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u4e92\u52a8\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793aAgentic\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8eWorkflow\uff0c\u4f46\u4f9d\u8d56\u4e8e\u6a21\u578b\u4fdd\u6301\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering needs to find clues in videos and gather information from the web for accurate answers.</li>\n    <li>The VideoDR benchmark was created to improve video question answering by combining video analysis with web searches and reasoning.</li>\n    <li>VideoDR includes high-quality video samples across six different topics, ensuring reliable data for testing.</li>\n    <li>The study found that two methods of processing (Workflow and Agentic) have different strengths, with Agentic's success depending on how well it tracks video information during searches.</li>\n    <li>Key challenges for future video research agents include keeping focus on goals and maintaining consistent information over longer searches.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\uff0c\u8868\u73b0\u8f83\u5f31\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u59823\u5c81\u7684\u4eba\u7c7b\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u72ec\u7acb\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u522b\uff0c\u8986\u76d6\u56db\u4e2a\u4e3b\u8981\u9886\u57df\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u5f97\u5206\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\uff0c\u8868\u660e\u5176\u57fa\u7840\u89c6\u89c9\u80fd\u529b\u4e0d\u8db3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills before they learn language, but modern Multimodal LLMs (MLLMs) still depend on language to understand visuals.</li>\n    <li>Current MLLMs struggle with basic visual tasks that even young children can easily perform.</li>\n    <li>BabyVision is a new benchmark created to test visual skills of MLLMs without relying on language knowledge.</li>\n    <li>It includes 388 tasks in 22 subclasses across four categories, showing that MLLMs score much lower than humans.</li>\n    <li>The findings highlight that MLLMs need improvement in basic visual understanding, and BabyVision aims to help make progress in this area.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u51c6\u786e\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u4e8e\u591a\u79cd\u5e94\u7528\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5148\u8fdb\u5206\u5272\u6a21\u578b\u80fd\u5206\u5272\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u540d\u4e3aSocioSeg\u7684\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\u6765\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u5177\u5907\u5f3a\u5927\u7684\u96f6-shot \u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas contain many types of information that are important for various uses.</li>\n    <li>Current technology can identify physical features well but struggles with social categories like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg that includes satellite images and detailed labels for social entities.</li>\n    <li>We developed a framework called SocioReasoner that mimics how humans identify and label social entities using advanced reasoning techniques.</li>\n    <li>Our method shows improvements over existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faLongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u5f3a\u5927\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u79cd\u81ea\u4e3b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u81ea\u4e3b\u641c\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u65b9\u9762\u3002</li>\n    <li>\u6a21\u578b\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u5de5\u5177\u4ea4\u4e92\u548c\u4e0d\u7a33\u5b9a\u7684\u73b0\u5b9e\u73af\u5883\u3002</li>\n    <li>\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\uff0c\u4f18\u5316\u4e86\u4ece\u9884\u8bad\u7ec3\u5230\u540e\u8bad\u7ec3\u7684\u6574\u4e2a\u8fc7\u7a0b\u3002</li>\n    <li>\u5f15\u5165\u91cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u8054\u5408\u6269\u5c55\u5b9e\u73b0\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source model with 560 billion parameters that excels in reasoning tasks.</li>\n    <li>It performs exceptionally well on various benchmarks, particularly in agentic search and tool use.</li>\n    <li>The model is designed to handle complex interactions and perform reliably in noisy, real-world situations.</li>\n    <li>Its effectiveness comes from a unique training approach that combines expert training and advanced data handling techniques.</li>\n    <li>To improve reasoning tasks, it features a Heavy Thinking mode that allows for deeper and broader thought processes during testing.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u5229\u75281.2\u4e07\u4ebf\u7684\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u91c7\u7528\u4e86\u5e73\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u63d0\u5347\u4e86\u611f\u77e5\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u4ec5\u4e3a100\u4ebf\u53c2\u6570\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u4e0e10\u81f320\u500d\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8fc7\u4e00\u4e9b\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u3002</li>\n    <li>STEP3-VL-10B\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u7684\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a lightweight, open-source model that balances efficiency and advanced intelligence across different types of data (multimodal).</li>\n    <li>It uses a special training method involving a large dataset (1.2 trillion tokens) to improve how it understands language and vision together.</li>\n    <li>The model features a unique reasoning approach called Parallel Coordinated Reasoning (PaCoRe), which helps it analyze and generate visual information effectively.</li>\n    <li>Despite being smaller than many other models, STEP3-VL-10B performs exceptionally well, achieving high scores on various benchmarks and outperforming much larger models.</li>\n    <li>The complete model is available for public use, offering a strong, efficient option for researchers and developers.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5b9a\u4f4d\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5ffd\u89c6\u4e86\u4eba\u7c7b\u4f7f\u7528\u5730\u56fe\u7684\u7b56\u7565\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u6dfb\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to identify where a photo was taken on Earth using visual clues.</li>\n    <li>Current large vision-language models don't use maps, a common strategy humans rely on.</li>\n    <li>This work introduces a new method called Thinking with Map, which treats map usage as an interactive process.</li>\n    <li>A two-stage approach is used: reinforcement learning improves the model's decision-making, and test-time scaling helps it evaluate multiple options before predicting.</li>\n    <li>The new method, tested on real-world images, shows significant improvements over existing models, raising accuracy from 8.0% to 22.1% at a 500m distance.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u6e05\u7406\u539f\u59cb\u6570\u636e\u3001\u53d1\u73b0\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u5173\u7cfb\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002</li>\n    <li>\u968f\u7740\u5e94\u7528\u5bf9\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0cLLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u6b63\u5728\u53d1\u751f\u91cd\u8981\u53d8\u5316\u3002</li>\n    <li>\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u4f7f\u7528LLM\u6280\u672f\u8fdb\u884c\u6570\u636e\u51c6\u5907\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5f3a\u8c03\u4ece\u4f20\u7edf\u7684\u89c4\u5219\u57fa\u7840\u65b9\u6cd5\u8f6c\u5411\u4ee5\u63d0\u793a\u4e3a\u9a71\u52a8\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u6570\u636e\u51c6\u5907\u88ab\u5206\u4e3a\u4e09\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u96c6\u6210\u548c\u6570\u636e\u4e30\u5bcc\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u5176\u4ee3\u8868\u6027\u6280\u672f\u53ca\u4f18\u7f3a\u70b9\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5f53\u524d\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6ce8\u91cd\u53ef\u6269\u5c55LLM\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation helps clean and analyze raw datasets for various applications like analytics and decision-making.</li>\n    <li>The paper reviews recent literature on using large language models (LLMs) to improve data preparation techniques.</li>\n    <li>It identifies a shift from traditional rule-based methods to more flexible, prompt-driven workflows for data handling.</li>\n    <li>The authors categorize data preparation tasks into three areas: cleaning, integration, and enrichment, discussing techniques and their pros and cons.</li>\n    <li>Lastly, the paper addresses research challenges and proposes future directions for developing scalable and reliable data systems.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u6f14\u53d8\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u672c\u8c03\u67e5\u603b\u7ed3\u4e86\u4ee3\u7406\u63a8\u7406\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u6311\u6218\u548c\u65b9\u5411\uff0c\u5982\u4e2a\u6027\u5316\u3001\u957f\u671f\u4e92\u52a8\u548c\u591a\u4ee3\u7406\u8bad\u7ec3\u7b49\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is important for making decisions and solving problems.</li>\n    <li>Large language models (LLMs) perform well in controlled situations but struggle in changing environments.</li>\n    <li>Agentic reasoning views LLMs as independent agents that can learn and adapt through interaction.</li>\n    <li>The survey categorizes agentic reasoning into three areas: basic skills, self-improvement, and teamwork among multiple agents.</li>\n    <li>It discusses various real-world applications and highlights future challenges like personalization and training multiple agents.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u9762\u4e34\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u662f\u7531\u4e8e\u8fc7\u4e8e\u5173\u6ce8\u5c40\u90e8\u884c\u4e3a\uff0c\u800c\u5ffd\u89c6\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u9ad8\u6c34\u5e73\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u805a\u7c7b\uff0c\u4ece\u800c\u63d0\u5347\u7a00\u6709\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models (LLMs) in complex reasoning tasks.</li>\n    <li>A common problem in RL is \"exploration collapse,\" where models focus too much on a few reasoning patterns, which limits diversity in solutions.</li>\n    <li>The proposed solution is called Uniqueness-Aware Reinforcement Learning, which rewards rare and high-level strategies rather than just common ones.</li>\n    <li>This method uses an LLM-based judge to group similar solutions and give more rewards to unique correct strategies.</li>\n    <li>The approach has shown to improve performance in various subjects, maintaining high accuracy while encouraging more diverse solutions.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18418", "authors": [{"_id": "69785315026bdf0473116f6a", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T08:31:51.245Z", "hidden": false}, {"_id": "69785315026bdf0473116f6b", "name": "Dayuan Fu", "hidden": false}, {"_id": "69785315026bdf0473116f6c", "name": "Tiantian Mi", "hidden": false}, {"_id": "69785315026bdf0473116f6d", "name": "Yumin Zhuang", "hidden": false}, {"_id": "69785315026bdf0473116f6e", "user": {"_id": "6865e6b362fc5689c5e67733", "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg", "isPro": false, "fullname": "Yaxing Huang", "user": "Rookie-Noob-Newbie", "type": "user"}, "name": "Yaxing Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:30.220Z", "hidden": false}, {"_id": "69785315026bdf0473116f6f", "user": {"_id": "67638cc0d63e4b348e8a5fa3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png", "isPro": false, "fullname": "Xuefeng Li", "user": "drxuefeng", "type": "user"}, "name": "Xuefeng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:37.248Z", "hidden": false}, {"_id": "69785315026bdf0473116f70", "name": "Lyumanshan Ye", "hidden": false}, {"_id": "69785315026bdf0473116f71", "name": "Muhang Xie", "hidden": false}, {"_id": "69785315026bdf0473116f72", "name": "Qishuo Hua", "hidden": false}, {"_id": "69785315026bdf0473116f73", "name": "Zhen Huang", "hidden": false}, {"_id": "69785315026bdf0473116f74", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:23.438Z", "hidden": false}, {"_id": "69785315026bdf0473116f75", "name": "Hanning Wang", "hidden": false}, {"_id": "69785315026bdf0473116f76", "user": {"_id": "66fa544c54f87b607fbffd2e", "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg", "isPro": false, "fullname": "Jifan Lin", "user": "evanlin2570", "type": "user"}, "name": "Jifan Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:57.029Z", "hidden": false}, {"_id": "69785315026bdf0473116f77", "name": "Yang Xiao", "hidden": false}, {"_id": "69785315026bdf0473116f78", "name": "Jie Sun", "hidden": false}, {"_id": "69785315026bdf0473116f79", "user": {"_id": "684faf712acd915b5afc055f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg", "isPro": false, "fullname": "Yunze Wu", "user": "wyzmike", "type": "user"}, "name": "Yunze Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:06.063Z", "hidden": false}, {"_id": "69785315026bdf0473116f7a", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-01-26T12:20:18.000Z", "submittedOnDailyAt": "2026-01-27T03:34:37.777Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "upvotes": 104, "discussionId": "69785315026bdf0473116f7b", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev", "githubRepoAddedBy": "user", "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.", "ai_keywords": ["Large Language Model", "agentic software engineering", "mid-training", "distribution mismatch", "agent-native data", "contextually-native trajectories", "environmentally-native trajectories", "SWE-Bench Verified", "Kimi-Dev", "resolution rates"], "githubStars": 22, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u6b63\u5728\u4ece\u5355\u6b21\u4ee3\u7801\u751f\u6210\u8f6c\u5411\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u3002</li>\n    <li>\u4e2d\u95f4\u8bad\u7ec3\uff08MT\uff09\u65b9\u6cd5\u5728\u5927\u578b\u6570\u636e\u4e0a\u5e94\u7528\uff0c\u80fd\u66f4\u6709\u6548\u5730\u57f9\u517b\u81ea\u4e3b\u884c\u4e3a\uff0c\u4f46\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u52a8\u6001\u5f00\u53d1\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u4e2d\u95f4\u8bad\u7ec3\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6709\u6548\u7684\u4ee3\u7406\u5f00\u53d1\u6570\u636e\u5408\u6210\u539f\u5219\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u901a\u8fc7\u4e0a\u4e0b\u6587\u548c\u73af\u5883\u539f\u751f\u8f68\u8ff9\u7684\u6570\u636e\u76d1\u7763\uff0c\u9a8c\u8bc1\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving to autonomously handle software engineering tasks like navigating and editing code.</li>\n    <li>Mid-training on large data sets that reflect real-world workflows is a promising but underused method due to high resource demands.</li>\n    <li>A key challenge is the mismatch between static training data and the dynamic nature of actual software development environments.</li>\n    <li>This study introduces a systematic approach for mid-training, focusing on two types of data: contextually-native and environmentally-native trajectories.</li>\n    <li>The new models show better performance in agentic capabilities compared to previous methods, achieving impressive resolution rates with fewer training tokens.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:20:18.000Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png", "numComments": 2, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 30, 2026";
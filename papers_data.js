window.trendingPapers = {
    "today": [{"paper": {"id": "2512.08269", "authors": [{"_id": "693bc3359874a2a5e4ffb3e3", "user": {"_id": "664df2176bc1025819f81caf", "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg", "isPro": false, "fullname": "taewoongkang", "user": "Keh0t0", "type": "user"}, "name": "Taewoong Kang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:18.431Z", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e4", "name": "Kinam Kim", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e5", "user": {"_id": "681afd78ba7048bcef707648", "avatarUrl": "/avatars/ef0a55c648634f8496e00e29b74f78bc.svg", "isPro": false, "fullname": "Dohyeon Kim", "user": "kdh8156", "type": "user"}, "name": "Dohyeon Kim", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:13:59.047Z", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e6", "name": "Minho Park", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e7", "name": "Junha Hyung", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e8", "name": "Jaegul Choo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/664df2176bc1025819f81caf/JNNjfUSkmFz_bX8SwSwOg.mp4"], "publishedAt": "2025-12-09T05:53:39.000Z", "submittedOnDailyAt": "2025-12-15T02:29:24.566Z", "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video", "submittedOnDailyBy": {"_id": "664df2176bc1025819f81caf", "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg", "isPro": false, "fullname": "taewoongkang", "user": "Keh0t0", "type": "user"}, "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.", "upvotes": 38, "discussionId": "693bc3369874a2a5e4ffb3e9", "projectPage": "https://keh0t0.github.io/EgoX/", "githubRepo": "https://github.com/KEH0T0/EgoX", "githubRepoAddedBy": "user", "ai_summary": "EgoX framework generates egocentric videos from exocentric inputs using video diffusion models with LoRA adaptation, unified conditioning, and geometry-guided self-attention for coherence and visual fidelity.", "ai_keywords": ["video diffusion models", "LoRA adaptation", "unified conditioning", "width and channel wise concatenation", "geometry-guided self-attention", "egocentric videos", "exocentric videos"], "githubStars": 17, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u4f7f\u4eba\u7c7b\u4ece\u81ea\u8eab\u89c6\u89d2\u76f4\u63a5\u4f53\u9a8c\u548c\u7406\u89e3\u4e16\u754c\u3002</li>\n    <li>\u5c06\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u8f6c\u6362\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u53ef\u4ee5\u589e\u5f3a\u6c89\u6d78\u611f\uff0c\u4f46\u9762\u4e34\u6311\u6218\uff0c\u5982\u76f8\u673a\u59ff\u6001\u53d8\u5316\u5927\u548c\u89c6\u89d2\u91cd\u53e0\u5c11\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51faEgoX\u6846\u67b6\uff0c\u4ece\u5355\u4e2a\u7b2c\u4e09\u4eba\u79f0\u8f93\u5165\u751f\u6210\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u3002</li>\n    <li>EgoX\u5229\u7528\u5927\u578b\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u65f6\u7a7a\u77e5\u8bc6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5b9e\u73b0\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5728\u672a\u89c1\u548c\u5b9e\u9645\u573a\u666f\u89c6\u9891\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5065\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Egocentric perception allows people to understand the world from their own viewpoint.</li>\n    <li>Transforming third-person videos into first-person videos is difficult due to different camera angles and limited overlapping views.</li>\n    <li>The new method, called EgoX, generates first-person videos from a single third-person video.</li>\n    <li>EgoX uses advanced video models and combines different types of video information for better results.</li>\n    <li>The approach ensures that the generated videos look realistic and maintain coherence, even with unseen footage.</li>\n</ul>"}, "publishedAt": "2025-12-09T00:53:39.000Z", "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video", "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/664df2176bc1025819f81caf/JNNjfUSkmFz_bX8SwSwOg.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08269.png", "numComments": 1, "submittedBy": {"_id": "664df2176bc1025819f81caf", "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg", "fullname": "taewoongkang", "name": "Keh0t0", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.11558", "authors": [{"_id": "693f7554f516c693246811d3", "name": "Zhenyang Cai", "hidden": false}, {"_id": "693f7554f516c693246811d4", "name": "Jiaming Zhang", "hidden": false}, {"_id": "693f7554f516c693246811d5", "name": "Junjie Zhao", "hidden": false}, {"_id": "693f7554f516c693246811d6", "user": {"_id": "660244de381ff94818335b67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660244de381ff94818335b67/Vwt4S739tpURyqgP3IDgd.jpeg", "isPro": false, "fullname": "ZiyiZENG", "user": "CocoNutZENG", "type": "user"}, "name": "Ziyi Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:39.120Z", "hidden": false}, {"_id": "693f7554f516c693246811d7", "name": "Yanchao Li", "hidden": false}, {"_id": "693f7554f516c693246811d8", "name": "Jingyi Liang", "hidden": false}, {"_id": "693f7554f516c693246811d9", "name": "Junying Chen", "hidden": false}, {"_id": "693f7554f516c693246811da", "name": "Yunjin Yang", "hidden": false}, {"_id": "693f7554f516c693246811db", "name": "Jiajun You", "hidden": false}, {"_id": "693f7554f516c693246811dc", "name": "Shuzhi Deng", "hidden": false}, {"_id": "693f7554f516c693246811dd", "name": "Tongfei Wang", "hidden": false}, {"_id": "693f7554f516c693246811de", "name": "Wanting Chen", "hidden": false}, {"_id": "693f7554f516c693246811df", "name": "Chunxiu Hao", "hidden": false}, {"_id": "693f7554f516c693246811e0", "name": "Ruiqi Xie", "hidden": false}, {"_id": "693f7554f516c693246811e1", "name": "Zhenwei Wen", "hidden": false}, {"_id": "693f7554f516c693246811e2", "name": "Xiangyi Feng", "hidden": false}, {"_id": "693f7554f516c693246811e3", "name": "Zou Ting", "hidden": false}, {"_id": "693f7554f516c693246811e4", "name": "Jin Zou Lin", "hidden": false}, {"_id": "693f7554f516c693246811e5", "name": "Jianquan Li", "hidden": false}, {"_id": "693f7554f516c693246811e6", "name": "Guangjun Yu", "hidden": false}, {"_id": "693f7554f516c693246811e7", "name": "Liangyi Chen", "hidden": false}, {"_id": "693f7554f516c693246811e8", "name": "Junwen Wang", "hidden": false}, {"_id": "693f7554f516c693246811e9", "name": "Shan Jiang", "hidden": false}, {"_id": "693f7554f516c693246811ea", "name": "Benyou Wang", "hidden": false}], "publishedAt": "2025-12-12T13:42:57.000Z", "submittedOnDailyAt": "2025-12-15T00:14:07.445Z", "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "submittedOnDailyBy": {"_id": "64f1a34f2c5c8b767916447e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg", "isPro": false, "fullname": "Zhenyang Cai", "user": "Eric3200", "type": "user"}, "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "upvotes": 37, "discussionId": "693f7555f516c693246811eb", "ai_summary": "DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.", "ai_keywords": ["multimodal large language models", "dentalGPT", "domain knowledge injection", "reinforcement learning", "annotated multimodal dataset", "dental images", "visual understanding", "multimodal complex reasoning", "intraoral benchmarks", "panoramic benchmarks", "medical VQA benchmarks", "disease classification", "dental VQA", "parameters"], "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86DentalGPT\uff0c\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u7259\u79d1\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u3002</li>\n    <li>\u521b\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u7259\u79d1\u6807\u6ce8\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc712\u4e07\u5f20\u7259\u79d1\u56fe\u50cf\u53ca\u8be6\u7ec6\u63cf\u8ff0\u3002</li>\n    <li>\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0cDentalGPT\u5728\u7259\u79d1\u72b6\u51b5\u7684\u89c6\u89c9\u7406\u89e3\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u591a\u6a21\u6001\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u7259\u79d1\u75be\u75c5\u5206\u7c7b\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u5148\u8fdb\u7684MLLM\uff0c\u5c3d\u7ba1\u53c2\u6570\u53ea\u670970\u4ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DentalGPT is a new language model designed specifically for dental care, improving how we interpret dental images and data.</li>\n    <li>It was created using a large dataset of over 120,000 dental images and detailed descriptions to enhance understanding of dental conditions.</li>\n    <li>The model underwent reinforcement learning to improve its reasoning abilities for better diagnosis.</li>\n    <li>DentalGPT outperformed many other advanced models in tasks related to disease classification and visual question answering in dentistry.</li>\n    <li>This shows that using high-quality dental data and tailored training can effectively improve dental AI models.</li>\n</ul>"}, "publishedAt": "2025-12-12T08:42:57.000Z", "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11558.png", "numComments": 2, "submittedBy": {"_id": "64f1a34f2c5c8b767916447e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg", "fullname": "Zhenyang Cai", "name": "Eric3200", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": true}, {"paper": {"id": "2512.11749", "authors": [{"_id": "693f754ef516c693246811c3", "user": {"_id": "662887715d246621f33d2ce6", "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg", "isPro": false, "fullname": "Shi Minglei", "user": "MingleiShi", "type": "user"}, "name": "Minglei Shi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:57.314Z", "hidden": false}, {"_id": "693f754ef516c693246811c4", "user": {"_id": "641d373d353524fe41f1d453", "avatarUrl": "/avatars/6fa9a0a4ba9818a221f835174a14be2d.svg", "isPro": false, "fullname": "Haolin Wang", "user": "howlin", "type": "user"}, "name": "Haolin Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:54.781Z", "hidden": false}, {"_id": "693f754ef516c693246811c5", "name": "Borui Zhang", "hidden": false}, {"_id": "693f754ef516c693246811c6", "name": "Wenzhao Zheng", "hidden": false}, {"_id": "693f754ef516c693246811c7", "name": "Bohan Zeng", "hidden": false}, {"_id": "693f754ef516c693246811c8", "name": "Ziyang Yuan", "hidden": false}, {"_id": "693f754ef516c693246811c9", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "693f754ef516c693246811ca", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "693f754ef516c693246811cb", "name": "Huan Yang", "hidden": false}, {"_id": "693f754ef516c693246811cc", "name": "Xintao Wang", "hidden": false}, {"_id": "693f754ef516c693246811cd", "name": "Pengfei Wan", "hidden": false}, {"_id": "693f754ef516c693246811ce", "name": "Kun Gai", "hidden": false}, {"_id": "693f754ef516c693246811cf", "name": "Jie Zhou", "hidden": false}, {"_id": "693f754ef516c693246811d0", "name": "Jiwen Lu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/V8NQYwKC-8n2SbplK5Ag7.png"], "publishedAt": "2025-12-12T17:45:03.000Z", "submittedOnDailyAt": "2025-12-15T00:19:12.400Z", "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.", "upvotes": 33, "discussionId": "693f754ff516c693246811d1", "githubRepo": "https://github.com/KlingTeam/SVG-T2I", "githubRepoAddedBy": "user", "ai_summary": "SVG-T2I, a scaled SVG framework, enables high-quality text-to-image synthesis directly in the Visual Foundation Model feature domain, achieving competitive performance in generative tasks.", "ai_keywords": ["Visual Foundation Model", "VFM", "SVG", "Self-supervised representations for Visual Generation", "SVG-T2I", "text-to-image diffusion", "GenEval", "DPG-Bench", "autoencoder", "generation model"], "githubStars": 40, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u751f\u6210\u4e0e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u6574\u5408\u89c6\u89c9\u7406\u89e3\u3001\u611f\u77e5\u548c\u751f\u6210\u7684\u7edf\u4e00\u8def\u5f84\u3002</li>\n    <li>\u76ee\u524d\uff0c\u5728VFM\u8868\u793a\u7a7a\u95f4\u5185\u8bad\u7ec3\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\u5c1a\u672a\u6df1\u5165\u3002</li>\n    <li>\u6211\u4eec\u6269\u5c55\u4e86SVG\u6846\u67b6\uff0c\u63d0\u51fa\u4e86SVG-T2I\uff0c\u4ee5\u652f\u6301\u5728VFM\u7279\u5f81\u57df\u5185\u8fdb\u884c\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u3002</li>\n    <li>\u901a\u8fc7\u6807\u51c6\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6d41\u7a0b\uff0cSVG-T2I\u5728GenEval\u4e0a\u8fbe\u52300.75\u7684\u5206\u6570\uff0c\u5728DPG-Bench\u4e0a\u8fbe\u523085.78\uff0c\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u9879\u76ee\u5b8c\u5168\u5f00\u6e90\uff0c\u5305\u62ec\u81ea\u52a8\u7f16\u7801\u5668\u3001\u751f\u6210\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u3001\u63a8\u7406\u3001\u8bc4\u4f30\u6d41\u7a0b\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u65b9\u4fbf\u8fdb\u4e00\u6b65\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visual Foundation Models (VFM) can improve how we understand and create images from text.</li>\n    <li>The SVG-T2I framework has been developed to enhance text-to-image generation within the VFM space.</li>\n    <li>SVG-T2I uses a standard text-to-image diffusion approach and achieved strong performance metrics (0.75 on GenEval and 85.78 on DPG-Bench).</li>\n    <li>This shows that VFMs are powerful for generating images from text.</li>\n    <li>The project is fully open-source, providing tools and models for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-12-12T12:45:03.000Z", "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/V8NQYwKC-8n2SbplK5Ag7.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11749.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 184}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.11799", "authors": [{"_id": "693f741df516c693246811ad", "name": "Ye Fang", "hidden": false}, {"_id": "693f741df516c693246811ae", "name": "Tong Wu", "hidden": false}, {"_id": "693f741df516c693246811af", "name": "Valentin Deschaintre", "hidden": false}, {"_id": "693f741df516c693246811b0", "name": "Duygu Ceylan", "hidden": false}, {"_id": "693f741df516c693246811b1", "name": "Iliyan Georgiev", "hidden": false}, {"_id": "693f741df516c693246811b2", "name": "Chun-Hao Paul Huang", "hidden": false}, {"_id": "693f741df516c693246811b3", "name": "Yiwei Hu", "hidden": false}, {"_id": "693f741df516c693246811b4", "name": "Xuelin Chen", "hidden": false}, {"_id": "693f741df516c693246811b5", "name": "Tuanfeng Yang Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IKbwcaT_GNej00AHtNUA1.mp4"], "publishedAt": "2025-12-12T18:59:54.000Z", "submittedOnDailyAt": "2025-12-15T00:06:37.373Z", "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.", "upvotes": 24, "discussionId": "693f741ef516c693246811b6", "projectPage": "https://aleafy.github.io/vrgbx/", "githubRepo": "https://github.com/Aleafy/V-RGBX", "githubRepoAddedBy": "user", "ai_summary": "V-RGBX is an end-to-end framework for intrinsic-aware video editing that combines video inverse rendering, photorealistic synthesis, and keyframe-based editing to produce consistent and physically plausible edits.", "ai_keywords": ["video inverse rendering", "intrinsic channels", "photorealistic video synthesis", "keyframe-based video editing", "interleaved conditioning mechanism", "object appearance editing", "scene-level relighting"], "githubStars": 40, "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "summary_zh": "<ul>\n    <li>V-RGBX\u662f\u9996\u4e2a\u652f\u6301\u5185\u5728\u5c5e\u6027\u89c6\u9891\u7f16\u8f91\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002</li>\n    <li>\u5b83\u80fd\u591f\u8fdb\u884c\u89c6\u9891\u9006\u6e32\u67d3\u3001\u751f\u6210\u771f\u5b9e\u611f\u89c6\u9891\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u7f16\u8f91\u3002</li>\n    <li>V-RGBX\u4f7f\u7528\u4e86\u4e00\u79cd\u4ea4\u9519\u6761\u4ef6\u673a\u5236\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u9009\u62e9\u5173\u952e\u5e27\u8fdb\u884c\u76f4\u89c2\u7684\u7269\u7406\u57fa\u7840\u89c6\u9891\u7f16\u8f91\u3002</li>\n    <li>\u8be5\u6846\u67b6\u751f\u6210\u7684\u89c6\u9891\u5728\u65f6\u95f4\u4e0a\u4fdd\u6301\u4e00\u81f4\uff0c\u4e14\u5728\u7269\u7406\u4e0a\u5408\u7406\u5730\u4f20\u64ad\u5173\u952e\u5e27\u7f16\u8f91\u3002</li>\n    <li>V-RGBX\u5728\u7269\u4f53\u5916\u89c2\u7f16\u8f91\u548c\u573a\u666f\u91cd\u5149\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>V-RGBX is a new framework for editing videos that understands important scene properties like color and materials.</li>\n    <li>It combines three main features: turning videos into intrinsic properties, creating realistic videos from these properties, and editing videos based on keyframes.</li>\n    <li>The framework allows users to easily edit videos while keeping everything looking realistic and consistent.</li>\n    <li>V-RGBX has been tested and shown to produce high-quality videos and supports various editing tasks like changing object appearances and lighting in scenes.</li>\n    <li>It performs better than previous methods in creating photorealistic and coherent video edits.</li>\n</ul>"}, "publishedAt": "2025-12-12T13:59:54.000Z", "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties", "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IKbwcaT_GNej00AHtNUA1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11799.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 184}, "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.10411", "authors": [{"_id": "693bca119874a2a5e4ffb3fb", "name": "Yijiong Yu", "hidden": false}, {"_id": "693bca119874a2a5e4ffb3fc", "name": "Jiale Liu", "hidden": false}, {"_id": "693bca119874a2a5e4ffb3fd", "name": "Qingyun Wu", "hidden": false}, {"_id": "693bca119874a2a5e4ffb3fe", "name": "Huazheng Wang", "hidden": false}, {"_id": "693bca119874a2a5e4ffb3ff", "name": "Ji Pei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6374c494958cd71fa7ea0a9d/uufd1O2gmYQsB47R_HezI.png"], "publishedAt": "2025-12-11T08:21:24.000Z", "submittedOnDailyAt": "2025-12-15T03:40:32.426Z", "title": "Sliding Window Attention Adaptation", "submittedOnDailyBy": {"_id": "6374c494958cd71fa7ea0a9d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/b2SjfvbjYqPCW38LzkzWl.jpeg", "isPro": false, "fullname": "yuyijiong", "user": "yuyijiong", "type": "user"}, "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation", "upvotes": 14, "discussionId": "693bca119874a2a5e4ffb400", "githubRepo": "https://github.com/yuyijiong/sliding-window-attention-adaptation", "githubRepoAddedBy": "user", "ai_summary": "Sliding Window Attention Adaptation (SWAA) enables Transformer-based Large Language Models (LLMs) to use sliding window attention without retraining, recovering long-context performance through a combination of adaptation techniques.", "ai_keywords": ["self-attention mechanism", "Transformer-based Large Language Models (LLMs)", "sliding window attention (SWA)", "full attention (FA)", "Sliding Window Attention Adaptation (SWAA)", "prefilling", "sink tokens", "chain-of-thought (CoT)"], "githubStars": 3, "organization": {"_id": "6897df91ad3033f4085e432c", "name": "OregonStateUniversity", "fullname": "Oregon State University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6897df118dbb78d2e8837335/ssdqjm2xjvu285uuDBZbd.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728Transformer\u6a21\u578b\u4e2d\uff0c\u5904\u7406\u957f\u8f93\u5165\u65f6\u6210\u672c\u5f88\u9ad8\u3002</li>\n    <li>\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b(SWA)\u53ef\u4ee5\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u9002\u5e94(SWAA)\uff0c\u7ed3\u5408\u4e94\u79cd\u65b9\u6cd5\u6765\u6539\u5584\u9002\u5e94\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSWA\u9002\u5e94\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u7279\u5b9a\u7684\u7ec4\u5408\u65b9\u6cd5\u624d\u80fd\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002</li>\n    <li>\u6211\u4eec\u5206\u6790\u4e86\u4e0d\u540cSWAA\u914d\u7f6e\u7684\u6027\u80fd\u4e0e\u6548\u7387\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a8\u8350\u7684\u4f7f\u7528\u65b9\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Transformer-based Large Language Models (LLMs) face high costs for processing long inputs due to their self-attention mechanism.</li>\n    <li>Sliding Window Attention (SWA) can lower these costs but may harm performance if applied directly to models trained with full attention.</li>\n    <li>The study introduces Sliding Window Attention Adaptation (SWAA), which includes five methods to improve adaptation without retraining the model.</li>\n    <li>Experiments show that combining these methods can restore long-context performance effectively.</li>\n    <li>The research provides insights on performance and efficiency trade-offs and offers practical recommendations for different use cases.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:21:24.000Z", "title": "Sliding Window Attention Adaptation", "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6374c494958cd71fa7ea0a9d/uufd1O2gmYQsB47R_HezI.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10411.png", "numComments": 1, "submittedBy": {"_id": "6374c494958cd71fa7ea0a9d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/b2SjfvbjYqPCW38LzkzWl.jpeg", "fullname": "yuyijiong", "name": "yuyijiong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 48}, "organization": {"_id": "6897df91ad3033f4085e432c", "name": "OregonStateUniversity", "fullname": "Oregon State University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6897df118dbb78d2e8837335/ssdqjm2xjvu285uuDBZbd.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.11253", "authors": [{"_id": "693f75c3f516c693246811ed", "name": "Zhiyuan Li", "hidden": false}, {"_id": "693f75c3f516c693246811ee", "name": "Chi-Man Pun", "hidden": false}, {"_id": "693f75c3f516c693246811ef", "name": "Chen Fang", "hidden": false}, {"_id": "693f75c3f516c693246811f0", "name": "Jue Wang", "hidden": false}, {"_id": "693f75c3f516c693246811f1", "user": {"_id": "63184c517ca1b876d99b7e0e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg", "isPro": false, "fullname": "Xiaodong Cun", "user": "vinthony", "type": "user"}, "name": "Xiaodong Cun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:36.497Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Dqr8Qb2QmiTS3fnJWMGgC.mp4"], "publishedAt": "2025-12-12T03:24:40.000Z", "submittedOnDailyAt": "2025-12-15T00:14:55.766Z", "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.", "upvotes": 10, "discussionId": "693f75c3f516c693246811f2", "githubRepo": "https://github.com/GVCLab/PersonaLive", "githubRepoAddedBy": "user", "ai_summary": "PersonaLive is a diffusion-based framework for real-time portrait animation that enhances speed and efficiency through multi-stage training, hybrid implicit signals, appearance distillation, and autoregressive micro-chunk streaming.", "ai_keywords": ["diffusion-based framework", "implicit facial representations", "3D implicit keypoints", "expressive image-level motion control", "appearance distillation", "autoregressive micro-chunk streaming", "sliding training strategy", "historical keyframe mechanism"], "githubStars": 206, "organization": {"_id": "69402e7fa7c562569cd809c2", "name": "GVCLab", "fullname": "GVC Lab at Great Bay University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63184c517ca1b876d99b7e0e/0mQ2Re10Y0HKnD878FGiU.png"}, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u7684\u8096\u50cf\u52a8\u753b\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\u548c\u8868\u60c5\u771f\u5b9e\u611f\uff0c\u4f46\u5ffd\u89c6\u4e86\u751f\u6210\u5ef6\u8fdf\u548c\u5b9e\u65f6\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86PersonaLive\uff0c\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5b9e\u65f6\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u4f7f\u7528\u6df7\u5408\u9690\u5f0f\u4fe1\u53f7\uff08\u9690\u5f0f\u9762\u90e8\u8868\u793a\u548c3D\u9690\u5f0f\u5173\u952e\u70b9\uff09\u6765\u5b9e\u73b0\u56fe\u50cf\u7ea7\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u5c11\u6b65\u9aa4\u5916\u89c2\u84b8\u998f\u7b56\u7565\uff0c\u51cf\u5c11\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002</li>\n    <li>PersonaLive\u5728\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u548c\u7a33\u5b9a\u6027\uff0c\u6027\u80fd\u6bd4\u4e4b\u524d\u7684\u6a21\u578b\u5feb7-22\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current portrait animation models focus on making images look better but are slow and not suitable for live streaming.</li>\n    <li>PersonaLive is a new system designed for real-time portrait animation during live streaming.</li>\n    <li>It uses special techniques to control facial movements and improve image quality efficiently.</li>\n    <li>PersonaLive has a method that speeds up the generation process, making it much faster than older models.</li>\n    <li>Tests show that PersonaLive is 7 to 22 times faster than previous models while still providing high-quality results.</li>\n</ul>"}, "publishedAt": "2025-12-11T22:24:40.000Z", "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming", "summary": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Dqr8Qb2QmiTS3fnJWMGgC.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11253.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 184}, "organization": {"_id": "69402e7fa7c562569cd809c2", "name": "GVCLab", "fullname": "GVC Lab at Great Bay University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63184c517ca1b876d99b7e0e/0mQ2Re10Y0HKnD878FGiU.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.11464", "authors": [{"_id": "693f7a62f516c6932468121f", "name": "Han Lin", "hidden": false}, {"_id": "693f7a62f516c69324681220", "name": "Xichen Pan", "hidden": false}, {"_id": "693f7a62f516c69324681221", "name": "Ziqi Huang", "hidden": false}, {"_id": "693f7a62f516c69324681222", "name": "Ji Hou", "hidden": false}, {"_id": "693f7a62f516c69324681223", "name": "Jialiang Wang", "hidden": false}, {"_id": "693f7a62f516c69324681224", "name": "Weifeng Chen", "hidden": false}, {"_id": "693f7a62f516c69324681225", "name": "Zecheng He", "hidden": false}, {"_id": "693f7a62f516c69324681226", "name": "Felix Juefei-Xu", "hidden": false}, {"_id": "693f7a62f516c69324681227", "name": "Junzhe Sun", "hidden": false}, {"_id": "693f7a62f516c69324681228", "name": "Zhipeng Fan", "hidden": false}, {"_id": "693f7a62f516c69324681229", "name": "Ali Thabet", "hidden": false}, {"_id": "693f7a62f516c6932468122a", "name": "Mohit Bansal", "hidden": false}, {"_id": "693f7a62f516c6932468122b", "name": "Chu Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/AXHUZPYF38Zi3K8XUNXs3.qt"], "publishedAt": "2025-12-12T11:07:11.000Z", "submittedOnDailyAt": "2025-12-15T00:34:31.893Z", "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.", "upvotes": 8, "discussionId": "693f7a62f516c6932468122c", "ai_summary": "MetaCanvas leverages multimodal large language models as latent-space planners to enhance precise and structured image and video generation, outperforming global-conditioning methods.", "ai_keywords": ["multimodal large language models", "diffusion models", "latent spaces", "text-to-image generation", "text/image-to-video generation", "image/video editing", "in-context video generation"], "organization": {"_id": "66b54027408752ae16404b05", "name": "metaresearch", "fullname": "Meta Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5b66\u4e60\u5feb\u901f\u63d0\u5347\u4e86\u89c6\u89c9\u7406\u89e3\uff0c\u4e3b\u8981\u901a\u8fc7\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u3002</li>\n    <li>\u5728\u89c6\u89c9\u751f\u6210\u4e2d\uff0c\u8fd9\u4e9b\u5f3a\u5927\u7684\u6a21\u578b\u901a\u5e38\u88ab\u7b80\u5316\u4e3a\u5168\u5c40\u6587\u672c\u7f16\u7801\u5668\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u76ee\u524d\u7684\u591a\u6a21\u6001LLMs\u53ef\u4ee5\u89e3\u6790\u590d\u6742\u5e03\u5c40\u548c\u77e5\u8bc6\u5bc6\u96c6\u573a\u666f\uff0c\u4f46\u751f\u6210\u7cbe\u786e\u63a7\u5236\u7684\u56fe\u50cf\u6216\u89c6\u9891\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MetaCanvas\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u8ba9MLLMs\u53ef\u4ee5\u76f4\u63a5\u5728\u7a7a\u95f4\u548c\u65f6\u7a7a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a8\u7406\u548c\u89c4\u5212\u3002</li>\n    <li>MetaCanvas\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u5c06MLLMs\u89c6\u4e3a\u6f5c\u5728\u7a7a\u95f4\u89c4\u5212\u8005\u662f\u7f29\u5c0f\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u5dee\u8ddd\u7684\u6709\u524d\u666f\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal learning has improved visual understanding using large language models (LLMs).</li>\n    <li>Current models mostly use LLMs for text encoding but miss out on their reasoning abilities for image and video generation.</li>\n    <li>MetaCanvas is a new framework that allows LLMs to think and plan in spaces related to images and videos.</li>\n    <li>It has been tested on three different models and performs better than traditional methods in generating and editing images and videos.</li>\n    <li>This approach shows promise in improving the connection between understanding and creating visual content.</li>\n</ul>"}, "publishedAt": "2025-12-12T06:07:11.000Z", "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas", "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/AXHUZPYF38Zi3K8XUNXs3.qt"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11464.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 184}, "organization": {"_id": "66b54027408752ae16404b05", "name": "metaresearch", "fullname": "Meta Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.06818", "authors": [{"_id": "693fb648f516c693246812ae", "name": "Jan Held", "hidden": false}, {"_id": "693fb648f516c693246812af", "name": "Sanghyun Son", "hidden": false}, {"_id": "693fb648f516c693246812b0", "name": "Renaud Vandeghen", "hidden": false}, {"_id": "693fb648f516c693246812b1", "name": "Daniel Rebain", "hidden": false}, {"_id": "693fb648f516c693246812b2", "name": "Matheus Gadelha", "hidden": false}, {"_id": "693fb648f516c693246812b3", "name": "Yi Zhou", "hidden": false}, {"_id": "693fb648f516c693246812b4", "name": "Anthony Cioppa", "hidden": false}, {"_id": "693fb648f516c693246812b5", "name": "Ming C. Lin", "hidden": false}, {"_id": "693fb648f516c693246812b6", "name": "Marc Van Droogenbroeck", "hidden": false}, {"_id": "693fb648f516c693246812b7", "name": "Andrea Tagliasacchi", "hidden": false}], "publishedAt": "2025-12-07T12:31:04.000Z", "submittedOnDailyAt": "2025-12-15T04:51:16.693Z", "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes", "submittedOnDailyBy": {"_id": "6675bb599be4300672be16f0", "avatarUrl": "/avatars/0bca4a89515a197ed624b670f704002e.svg", "isPro": false, "fullname": "Jan Held", "user": "janheld14", "type": "user"}, "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.", "upvotes": 6, "discussionId": "693fb648f516c693246812b8", "projectPage": "https://meshsplatting.github.io/", "githubRepo": "https://github.com/meshsplatting/mesh-splatting", "githubRepoAddedBy": "user", "ai_summary": "MeshSplatting, a mesh-based reconstruction method, enhances novel view synthesis by optimizing geometry and appearance through differentiable rendering, improving quality and efficiency over existing techniques.", "ai_keywords": ["3D Gaussian Splatting", "MeshSplatting", "differentiable rendering", "restricted Delaunay triangulation", "surface consistency", "PSNR", "MiLo", "Mip-NeRF360"], "githubStars": 272, "summary_zh": "<ul>\n    <li>MeshSplatting\u662f\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f18\u5316\u51e0\u4f55\u5f62\u72b6\u548c\u5916\u89c2\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u9650\u5236\u7684Delaunay\u4e09\u89d2\u5256\u5206\u6765\u786e\u4fdd\u7f51\u683c\u7684\u8fde\u63a5\u6027\u3002</li>\n    <li>MeshSplatting\u751f\u6210\u5e73\u6ed1\u4e14\u9ad8\u8d28\u91cf\u7684\u7f51\u683c\uff0c\u80fd\u591f\u5728\u5b9e\u65f63D\u5f15\u64ce\u4e2d\u9ad8\u6548\u6e32\u67d3\u3002</li>\n    <li>\u5728Mip-NeRF360\u6570\u636e\u96c6\u4e0a\uff0cMeshSplatting\u7684PSNR\u63d0\u9ad8\u4e860.69 dB\uff0c\u6bd4\u73b0\u6709\u6280\u672fMiLo\u66f4\u4f18\u79c0\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u8bad\u7ec3\u901f\u5ea6\u662f\u73b0\u6709\u6280\u672f\u7684\u4e24\u500d\uff0c\u5e76\u4e14\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e86\u4e00\u534a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MeshSplatting is a new method that combines geometry and appearance for better 3D mesh rendering.</li>\n    <li>It uses a technique called differentiable rendering and ensures smooth, high-quality meshes that work well in real-time 3D engines.</li>\n    <li>This method improves view synthesis quality, boosting performance by +0.69 dB compared to the best existing methods.</li>\n    <li>MeshSplatting is faster, training twice as quickly and using half the memory of previous methods.</li>\n    <li>The project website is available for more information: <a href=\"https://meshsplatting.github.io/\">meshsplatting.github.io</a>.</li>\n</ul>"}, "publishedAt": "2025-12-07T07:31:04.000Z", "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes", "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06818.png", "numComments": 1, "submittedBy": {"_id": "6675bb599be4300672be16f0", "avatarUrl": "/avatars/0bca4a89515a197ed624b670f704002e.svg", "fullname": "Jan Held", "name": "janheld14", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2512.11792", "authors": [{"_id": "693f78a8f516c69324681216", "user": {"_id": "648ffe95669191ccb6772a2e", "avatarUrl": "/avatars/025cac7dc64ea9ef2074754c92086baa.svg", "isPro": false, "fullname": "Yang Fei", "user": "sunfly", "type": "user"}, "name": "Yang Fei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T13:11:17.203Z", "hidden": false}, {"_id": "693f78a8f516c69324681217", "name": "George Stoica", "hidden": false}, {"_id": "693f78a8f516c69324681218", "name": "Jingyuan Liu", "hidden": false}, {"_id": "693f78a8f516c69324681219", "name": "Qifeng Chen", "hidden": false}, {"_id": "693f78a8f516c6932468121a", "name": "Ranjay Krishna", "hidden": false}, {"_id": "693f78a8f516c6932468121b", "name": "Xiaojuan Wang", "hidden": false}, {"_id": "693f78a8f516c6932468121c", "user": {"_id": "6326723c21c98f83d6ad128e", "avatarUrl": "/avatars/6c208ff815c037d0a6a2ada3198e7fd4.svg", "isPro": false, "fullname": "Benlin Liu", "user": "Tim666", "type": "user"}, "name": "Benlin Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:31.327Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/648ffe95669191ccb6772a2e/vHOdQzZdsRcbaNkkCf9b7.mp4"], "publishedAt": "2025-12-12T18:56:35.000Z", "submittedOnDailyAt": "2025-12-15T00:52:13.247Z", "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation", "submittedOnDailyBy": {"_id": "648ffe95669191ccb6772a2e", "avatarUrl": "/avatars/025cac7dc64ea9ef2074754c92086baa.svg", "isPro": false, "fullname": "Yang Fei", "user": "sunfly", "type": "user"}, "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .", "upvotes": 5, "discussionId": "693f78a8f516c6932468121d", "projectPage": "https://sam2videox.github.io/", "ai_summary": "SAM2VideoX improves realistic motion generation in video models by integrating structure-preserving priors from an autoregressive model into a bidirectional diffusion model with novel feature fusion and local alignment techniques.", "ai_keywords": ["diffusion models", "autoregressive video tracking model", "bidirectional video diffusion model", "bidirectional feature fusion module", "Local Gram Flow loss", "VBench", "FVD", "human preference", "REPA", "LoRA-finetuning"], "summary_zh": "<ul>\n    <li>\u73b0\u5b9e\u4e2d\u7684\u8fd0\u52a8\u9700\u8981\u5728\u4e25\u683c\u7ea6\u675f\u548c\u53ef\u53d8\u7ed3\u6784\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002</li>\n    <li>\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u6709\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u771f\u5b9e\u7684\u4fdd\u7559\u7ed3\u6784\u7684\u8fd0\u52a8\u4ecd\u7136\u5f88\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u50cf\u4eba\u7c7b\u548c\u52a8\u7269\u8fd9\u6837\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u53ef\u4ee5\u4ece\u81ea\u56de\u5f52\u89c6\u9891\u8ddf\u8e2a\u6a21\u578b\uff08SAM2\uff09\u63d0\u53d6\u4fdd\u7559\u7ed3\u6784\u7684\u8fd0\u52a8\u5148\u9a8c\u3002</li>\n    <li>\u65b0\u6a21\u578bSAM2VideoX\u5305\u542b\u4e24\u4e2a\u521b\u65b0\uff1a\u4e00\u79cd\u53cc\u5411\u7279\u5f81\u878d\u5408\u6a21\u5757\u548c\u5c40\u90e8\u683c\u62c9\u59c6\u6d41\u635f\u5931\u3002</li>\n    <li>\u5728VBench\u548c\u4eba\u7c7b\u7814\u7a76\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAM2VideoX\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on generating realistic motion for video models, especially for complex objects like humans and animals.</li>\n    <li>Current methods struggle to maintain realistic motion, often using noisy motion representations that don\u2019t work well.</li>\n    <li>The authors introduce a new algorithm that improves motion generation by using a model called SAM2 and integrating it into a diffusion model called CogVideoX.</li>\n    <li>The new model, SAM2VideoX, has two main features: a method to extract global motion patterns and a loss function to ensure local features move together correctly.</li>\n    <li>Tests show that SAM2VideoX performs better than previous models, achieving higher scores and better human preferences in motion generation.</li>\n</ul>"}, "publishedAt": "2025-12-12T13:56:35.000Z", "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation", "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/648ffe95669191ccb6772a2e/vHOdQzZdsRcbaNkkCf9b7.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11792.png", "numComments": 1, "submittedBy": {"_id": "648ffe95669191ccb6772a2e", "avatarUrl": "/avatars/025cac7dc64ea9ef2074754c92086baa.svg", "fullname": "Yang Fei", "name": "sunfly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10605", "authors": [{"_id": "693d1217f516c69324680d3d", "user": {"_id": "67a1e2c02bf092a76139a2af", "avatarUrl": "/avatars/502259c5eac07c62c16ee8c6ca10f7b6.svg", "isPro": false, "fullname": "leo chen", "user": "legendleochen", "type": "user"}, "name": "Lihuang Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:13:09.402Z", "hidden": false}, {"_id": "693d1217f516c69324680d3e", "name": "Xiangyu Luo", "hidden": false}, {"_id": "693d1217f516c69324680d3f", "name": "Jun Meng", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/Vc7thgYBevGRS0YvBrDMo.mp4", "https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/M3GvADYEEOLwtBGK60dz2.png", "https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/YOha4ye3IDHenmAWXm05l.png", "https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/4PEBajNfHOryZKH0iRv6t.png"], "publishedAt": "2025-12-11T12:58:36.000Z", "submittedOnDailyAt": "2025-12-15T06:27:26.973Z", "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator", "submittedOnDailyBy": {"_id": "67a1e2c02bf092a76139a2af", "avatarUrl": "/avatars/502259c5eac07c62c16ee8c6ca10f7b6.svg", "isPro": false, "fullname": "leo chen", "user": "legendleochen", "type": "user"}, "summary": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.", "upvotes": 4, "discussionId": "693d1218f516c69324680d40", "githubRepo": "https://github.com/LegendLeoChen/LEO-RobotAgent", "githubRepoAddedBy": "user", "ai_summary": "A general-purpose language-driven framework for robots, LEO-RobotAgent, enhances human-robot interaction and task planning using large language models across various robot types and tasks.", "ai_keywords": ["language-driven", "intelligent agent framework", "LLMs", "generalization", "robustness", "efficiency", "bidirectional intent understanding", "task planning", "modular toolset", "human-robot interaction", "unmanned aerial vehicles", "robotic arms", "wheeled robots"], "githubStars": 5, "organization": {"_id": "6345aadf5efccdc07f1365a5", "name": "ZhejiangUniversity", "fullname": "Zhejiang University"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86LEO-RobotAgent\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u7684\u8bed\u8a00\u9a71\u52a8\u667a\u80fd\u673a\u5668\u4eba\u6846\u67b6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u64cd\u4f5c\u4e0d\u540c\u7c7b\u578b\u7684\u673a\u5668\u4eba\uff0c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002</li>\n    <li>\u6846\u67b6\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u63d0\u5347\u4eba\u673a\u4e92\u52a8\u7684\u7406\u89e3\u3002</li>\n    <li>\u8bbe\u8ba1\u7b80\u5316\u7ed3\u6784\uff0c\u4f7f\u5927\u578b\u6a21\u578b\u80fd\u591f\u72ec\u7acb\u601d\u8003\u3001\u89c4\u5212\u548c\u884c\u52a8\u3002</li>\n    <li>\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u4e3b\u6d41\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5e76\u80fd\u9ad8\u6548\u6267\u884c\u591a\u79cd\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LEO-RobotAgent is a framework that allows robots to use language models to perform complex tasks in different situations.</li>\n    <li>The framework is designed for strong generalization, robustness, and efficiency in human-robot interactions.</li>\n    <li>It simplifies robot task planning, enabling large models to think, plan, and act independently.</li>\n    <li>LEO-RobotAgent includes a toolset that allows flexible use of various tools for different tasks.</li>\n    <li>Experiments show it works well with various types of robots, including drones and robotic arms, handling tasks of varying difficulty.</li>\n</ul>"}, "publishedAt": "2025-12-11T07:58:36.000Z", "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator", "summary": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/Vc7thgYBevGRS0YvBrDMo.mp4", "https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/M3GvADYEEOLwtBGK60dz2.png", "https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/YOha4ye3IDHenmAWXm05l.png", "https://cdn-uploads.huggingface.co/production/uploads/67a1e2c02bf092a76139a2af/4PEBajNfHOryZKH0iRv6t.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10605.png", "numComments": 2, "submittedBy": {"_id": "67a1e2c02bf092a76139a2af", "avatarUrl": "/avatars/502259c5eac07c62c16ee8c6ca10f7b6.svg", "fullname": "leo chen", "name": "legendleochen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "6345aadf5efccdc07f1365a5", "name": "ZhejiangUniversity", "fullname": "Zhejiang University"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u63a7\u5236\u7cbe\u5ea6\u4f4e\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u8f93\u51fa\u6548\u679c\u4e0d\u591f\u7406\u60f3\u3002</li>\n    <li>Wan-Move\u901a\u8fc7\u7cbe\u786e\u7684\u8fd0\u52a8\u63a7\u5236\u7f29\u5c0f\u4e86\u8fd9\u4e00\u5dee\u8ddd\uff0c\u91c7\u7528\u5bc6\u96c6\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5c06\u8f68\u8ff9\u6295\u5f71\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u751f\u6210\u5bf9\u573a\u666f\u5143\u7d20\u8fd0\u52a8\u7684\u8be6\u7ec6\u7279\u5f81\u56fe\u3002</li>\n    <li>\u901a\u8fc7\u5927\u89c4\u6a21\u8bad\u7ec3\uff0cWan-Move\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684480p\u89c6\u9891\uff0c\u8fd0\u52a8\u63a7\u5236\u6548\u679c\u5ab2\u7f8e\u5546\u4e1a\u4ea7\u54c1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new and easy-to-use framework for controlling motion in video generation models.</li>\n    <li>It improves upon existing methods by allowing precise and high-quality motion control, addressing issues of coarse control and scalability.</li>\n    <li>The framework uses dense point trajectories to represent object motions and creates a feature map for accurate scene movement.</li>\n    <li>Wan-Move can be integrated with existing models without changing their architecture and allows for easy scaling and fine-tuning.</li>\n    <li>It includes MoveBench, a comprehensive benchmark for evaluating motion quality, and the results show Wan-Move outperforms other methods.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.08478", "authors": [{"_id": "6938e00fdfc35938ba129f4f", "name": "Yuning Gong", "hidden": false}, {"_id": "6938e00fdfc35938ba129f50", "name": "Yifei Liu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f51", "name": "Yifan Zhan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f52", "name": "Muyao Niu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f53", "name": "Xueying Li", "hidden": false}, {"_id": "6938e00fdfc35938ba129f54", "name": "Yuanjun Liao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f55", "name": "Jiaming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f56", "name": "Yuanyuan Gao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f57", "name": "Jiaqi Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f58", "name": "Minming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f59", "name": "Li Zhou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5a", "name": "Yuning Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5b", "name": "Wei Wang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5c", "name": "Xiaoqing Hou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5d", "name": "Huaxi Huang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5e", "name": "Shixiang Tang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5f", "name": "Le Ma", "hidden": false}, {"_id": "6938e00fdfc35938ba129f60", "name": "Dingwen Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f61", "name": "Xue Yang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f62", "name": "Junchi Yan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f63", "name": "Yanchi Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f64", "name": "Yinqiang Zheng", "hidden": false}, {"_id": "6938e00fdfc35938ba129f65", "name": "Xiao Sun", "hidden": false}, {"_id": "6938e00fdfc35938ba129f66", "user": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "name": "Zhihang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:56:28.162Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "publishedAt": "2025-12-09T10:54:58.000Z", "submittedOnDailyAt": "2025-12-10T07:43:37.566Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "submittedOnDailyBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "upvotes": 64, "discussionId": "6938e00fdfc35938ba129f67", "projectPage": "https://visionary-laboratory.github.io/visionary/", "githubRepo": "https://github.com/Visionary-Laboratory/visionary", "githubRepoAddedBy": "user", "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.", "ai_keywords": ["Neural rendering", "3D Gaussian Splatting", "3DGS", "WebGPU", "ONNX inference", "Gaussian Generator contract", "three.js", "TypeScript API", "MLP-based 3DGS", "4DGS", "neural avatars", "style transformation", "GPU-based primitive sorting", "World Model Carrier"], "githubStars": 162, "summary_zh": "<ul>\n    <li>Visionary\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7f51\u9875\u5e73\u53f0\uff0c\u652f\u6301\u5b9e\u65f6\u7684\u9ad8\u65af\u70b9\u4e91\u548c\u7f51\u683c\u6e32\u67d3\u3002</li>\n    <li>\u5b83\u4f7f\u7528\u9ad8\u6548\u7684WebGPU\u6e32\u67d3\u5668\uff0c\u80fd\u591f\u5b9e\u73b0\u52a8\u6001\u7684\u795e\u7ecf\u5904\u7406\uff0c\u5e76\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u6d4f\u89c8\u5668\u4f53\u9a8c\u3002</li>\n    <li>\u5e73\u53f0\u5f15\u5165\u6807\u51c6\u5316\u7684\u9ad8\u65af\u751f\u6210\u5668\u5408\u7ea6\uff0c\u652f\u6301\u6807\u51c6\u76843D\u9ad8\u65af\u6e32\u67d3\uff0c\u8fd8\u5141\u8bb8\u63d2\u4ef6\u7b97\u6cd5\u751f\u6210\u6216\u66f4\u65b0\u6bcf\u4e00\u5e27\u7684\u9ad8\u65af\u3002</li>\n    <li>\u4e0e\u73b0\u6709\u7f51\u9875\u67e5\u770b\u5668\u76f8\u6bd4\uff0cVisionary\u5728\u6e32\u67d3\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u652f\u6301\u591a\u79cd\u53d8\u4f53\u548c\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u901a\u8fc7\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u7edf\u4e00\u63a8\u7406\u548c\u6e32\u67d3\uff0cVisionary\u964d\u4f4e\u4e863D\u9ad8\u65af\u65b9\u6cd5\u7684\u590d\u5236\u3001\u6bd4\u8f83\u548c\u90e8\u7f72\u7684\u96be\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visionary is a new web platform for easy and fast rendering of 3D models using 3D Gaussian Splatting (3DGS).</li>\n    <li>It allows real-time rendering in browsers with a simple \"click-to-run\" experience and requires minimal setup.</li>\n    <li>The platform supports dynamic content and can easily integrate with existing web applications using a TypeScript API.</li>\n    <li>Visionary improves rendering efficiency by using GPU-based sorting and supports various advanced rendering techniques.</li>\n    <li>It simplifies the process of using and sharing 3DGS methods, making it easier for developers and researchers to work with 3D models.</li>\n</ul>"}, "publishedAt": "2025-12-09T05:54:58.000Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08478.png", "numComments": 3, "submittedBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "fullname": "Zhihang Zhong", "name": "Zuica96", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10430", "authors": [{"_id": "693bba8d9874a2a5e4ffb3ab", "user": {"_id": "64f4c8739ee58d48e8507e0e", "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg", "isPro": false, "fullname": "Dmitrii Stoianov", "user": "heylimon", "type": "user"}, "name": "Dmitrii Stoianov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:40.198Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ac", "user": {"_id": "64fb054ebb362cbf2fe53159", "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg", "isPro": false, "fullname": "Danil Taranets", "user": "taranetsdan", "type": "user"}, "name": "Danil Taranets", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:29.281Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ad", "user": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "name": "Olga Tsymboi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:38.180Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ae", "user": {"_id": "6780dcd6acf8d824c03864da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png", "isPro": false, "fullname": "Ramil Latypov", "user": "kylecr4ne", "type": "user"}, "name": "Ramil Latypov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:23.437Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3af", "user": {"_id": "6513f03e86d74f32ed65e3b8", "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg", "isPro": false, "fullname": "Almaz Dautov", "user": "the-hir0", "type": "user"}, "name": "Almaz Dautov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:30.990Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b0", "user": {"_id": "621a8daf325b927e60fcef08", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg", "isPro": false, "fullname": "Vladislav Kruglikov", "user": "vladislavkruglikov", "type": "user"}, "name": "Vladislav Kruglikov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:21.170Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b1", "name": "Nikita Surkov", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b2", "user": {"_id": "63188c428d698d8c1642a0d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg", "isPro": false, "fullname": "German Abramov", "user": "germanjke", "type": "user"}, "name": "German Abramov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:28.579Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b3", "name": "Pavel Gein", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b4", "user": {"_id": "636a9a07e3ad78bc68b1a5a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg", "isPro": false, "fullname": "Dmitry Abulkhanov", "user": "mponty", "type": "user"}, "name": "Dmitry Abulkhanov", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:02:42.107Z", "hidden": true}, {"_id": "693bba8d9874a2a5e4ffb3b5", "user": {"_id": "658bc20cfdf2279d4721f218", "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg", "isPro": false, "fullname": "Mikhail Gashkov", "user": "MikeGashkov", "type": "user"}, "name": "Mikhail Gashkov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:32.809Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b6", "name": "Viktor Zelenkovskiy", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b7", "user": {"_id": "644f64bc17b6189cda54cae8", "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg", "isPro": false, "fullname": "Artem Batalov", "user": "batalovme", "type": "user"}, "name": "Artem Batalov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:27.497Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b8", "user": {"_id": "62609d224e6e4b84475eb8d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg", "isPro": false, "fullname": "Alex Medvedev", "user": "kenkaneki", "type": "user"}, "name": "Aleksandr Medvedev", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:36.035Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b9", "user": {"_id": "63f26358be95ed4c9a9b0583", "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg", "isPro": false, "fullname": "Anatoly Potapov", "user": "AnatoliiPotapov", "type": "user"}, "name": "Anatolii Potapov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:25.666Z", "hidden": false}], "publishedAt": "2025-12-11T08:40:10.000Z", "submittedOnDailyAt": "2025-12-12T08:33:32.798Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "submittedOnDailyBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "upvotes": 60, "discussionId": "693bba8e9874a2a5e4ffb3ba", "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.", "ai_keywords": ["Cyrillic-dense tokenizer", "EAGLE speculative-decoding pipeline", "hybrid reasoning", "efficient inference", "reasoning-trace generation"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86T-pro 2.0\uff0c\u4e00\u4e2a\u5f00\u653e\u6743\u91cd\u7684\u4fc4\u7f57\u65af\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u65ad\u3002</li>\n    <li>\u8be5\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u5e76\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4e86\u5bc6\u96c6\u7684\u897f\u91cc\u5c14\u7f16\u7801\u5668\u548c\u6539\u8fdb\u7684EAGLE\u89e3\u7801\u7ba1\u9053\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u4e86\u4fc3\u8fdb\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u7814\u7a76\uff0c\u6211\u4eec\u5728Hugging Face\u4e0a\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001T-Wix 500k\u6307\u4ee4\u8bed\u6599\u5e93\u3001T-Math\u63a8\u7406\u57fa\u51c6\u548cEAGLE\u6743\u91cd\u3002</li>\n    <li>\u8fd9\u4e9b\u8d44\u6e90\u4f7f\u7528\u6237\u80fd\u591f\u7814\u7a76\u4fc4\u8bed\u63a8\u7406\uff0c\u5e76\u6269\u5c55\u6216\u9002\u5e94\u6a21\u578b\u53ca\u63a8\u65ad\u7ba1\u9053\u3002</li>\n    <li>T-pro 2.0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5171\u7f51\u7edc\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u5f0f\u4ee5\u53ca\u63a8\u65ad\u901f\u5ea6\u63d0\u5347\uff0c\u4fbf\u4e8e\u6784\u5efa\u548c\u8bc4\u4f30\u9ad8\u6548\u7684\u4fc4\u8bedLLM\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>T-pro 2.0 is a new Russian language model designed for reasoning and fast responses.</li>\n    <li>The model can provide answers and show how it reaches those answers, using efficient technology to reduce waiting time.</li>\n    <li>Researchers can access the model, a large instruction dataset, and a reasoning test to further study and improve Russian language processing.</li>\n    <li>A public demo allows users to see the model in action and test its speed in different tasks.</li>\n    <li>T-pro 2.0 aims to make it easier to create and test practical applications for Russian language models.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:40:10.000Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png", "numComments": 1, "submittedBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "fullname": "Olga Tsymboi", "name": "oltsy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.09363", "authors": [{"_id": "693a379e74fced5bf9c32412", "user": {"_id": "6486ff6561053da6442fef1a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg", "isPro": false, "fullname": "KeXing", "user": "KXingLab", "type": "user"}, "name": "Ke Xing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:26.656Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32413", "name": "Longfei Li", "hidden": false}, {"_id": "693a379e74fced5bf9c32414", "user": {"_id": "64b7ab4c037d6452a31910eb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png", "isPro": false, "fullname": "yuyangyin", "user": "yuyangyin", "type": "user"}, "name": "Yuyang Yin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:43:30.256Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32415", "name": "Hanwen Liang", "hidden": false}, {"_id": "693a379e74fced5bf9c32416", "name": "Guixun Luo", "hidden": false}, {"_id": "693a379e74fced5bf9c32417", "name": "Chen Fang", "hidden": false}, {"_id": "693a379e74fced5bf9c32418", "name": "Jue Wang", "hidden": false}, {"_id": "693a379e74fced5bf9c32419", "name": "Konstantinos N. Plataniotis", "hidden": false}, {"_id": "693a379e74fced5bf9c3241a", "name": "Xiaojie Jin", "hidden": false}, {"_id": "693a379e74fced5bf9c3241b", "name": "Yao Zhao", "hidden": false}, {"_id": "693a379e74fced5bf9c3241c", "name": "Yunchao Wei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "publishedAt": "2025-12-10T06:50:16.000Z", "submittedOnDailyAt": "2025-12-11T00:46:52.612Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "upvotes": 44, "discussionId": "693a379e74fced5bf9c3241d", "projectPage": "https://ke-xing.github.io/StereoWorld/", "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.", "ai_keywords": ["stereo video", "monocular-to-stereo", "pretrained video generator", "geometry-aware regularization", "spatio-temporal tiling", "high-definition stereo video dataset", "natural human interpupillary distance (IPD)", "visual fidelity", "geometric consistency"], "summary_zh": "<ul>\n    <li>XR\u8bbe\u5907\u7684\u666e\u53ca\u589e\u52a0\u4e86\u5bf9\u9ad8\u8d28\u91cf\u7acb\u4f53\u89c6\u9891\u7684\u9700\u6c42\uff0c\u4f46\u5236\u4f5c\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u73b0\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86StereoWorld\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u4fdd\u771f\u7acb\u4f53\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u7ed3\u5408\u5355\u76ee\u89c6\u9891\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u76d1\u7763\u786e\u4fdd3D\u7ed3\u6784\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u6574\u5408\u4e86\u4e00\u79cd\u65f6\u7a7a\u5207\u7247\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc71100\u4e07\u5e27\u7684\u9ad8\u6e05\u7acb\u4f53\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4ee5\u4fbf\u8fdb\u884c\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>StereoWorld is a new framework for creating high-quality stereo videos from standard videos.</li>\n    <li>The framework uses a pretrained video generator and focuses on maintaining 3D structure accuracy.</li>\n    <li>It includes a method for efficiently creating high-resolution videos.</li>\n    <li>A large dataset with over 11 million frames was created to help train and test the framework.</li>\n    <li>StereoWorld produces better quality stereo videos compared to older methods.</li>\n</ul>"}, "publishedAt": "2025-12-10T01:50:16.000Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09363.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08269", "authors": [{"_id": "693bc3359874a2a5e4ffb3e3", "user": {"_id": "664df2176bc1025819f81caf", "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg", "isPro": false, "fullname": "taewoongkang", "user": "Keh0t0", "type": "user"}, "name": "Taewoong Kang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:18.431Z", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e4", "name": "Kinam Kim", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e5", "user": {"_id": "681afd78ba7048bcef707648", "avatarUrl": "/avatars/ef0a55c648634f8496e00e29b74f78bc.svg", "isPro": false, "fullname": "Dohyeon Kim", "user": "kdh8156", "type": "user"}, "name": "Dohyeon Kim", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:13:59.047Z", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e6", "name": "Minho Park", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e7", "name": "Junha Hyung", "hidden": false}, {"_id": "693bc3359874a2a5e4ffb3e8", "name": "Jaegul Choo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/664df2176bc1025819f81caf/JNNjfUSkmFz_bX8SwSwOg.mp4"], "publishedAt": "2025-12-09T05:53:39.000Z", "submittedOnDailyAt": "2025-12-15T02:29:24.566Z", "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video", "submittedOnDailyBy": {"_id": "664df2176bc1025819f81caf", "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg", "isPro": false, "fullname": "taewoongkang", "user": "Keh0t0", "type": "user"}, "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.", "upvotes": 38, "discussionId": "693bc3369874a2a5e4ffb3e9", "projectPage": "https://keh0t0.github.io/EgoX/", "githubRepo": "https://github.com/KEH0T0/EgoX", "githubRepoAddedBy": "user", "ai_summary": "EgoX framework generates egocentric videos from exocentric inputs using video diffusion models with LoRA adaptation, unified conditioning, and geometry-guided self-attention for coherence and visual fidelity.", "ai_keywords": ["video diffusion models", "LoRA adaptation", "unified conditioning", "width and channel wise concatenation", "geometry-guided self-attention", "egocentric videos", "exocentric videos"], "githubStars": 17, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u4f7f\u4eba\u7c7b\u4ece\u81ea\u8eab\u89c6\u89d2\u76f4\u63a5\u4f53\u9a8c\u548c\u7406\u89e3\u4e16\u754c\u3002</li>\n    <li>\u5c06\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u8f6c\u6362\u4e3a\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u53ef\u4ee5\u589e\u5f3a\u6c89\u6d78\u611f\uff0c\u4f46\u9762\u4e34\u6311\u6218\uff0c\u5982\u76f8\u673a\u59ff\u6001\u53d8\u5316\u5927\u548c\u89c6\u89d2\u91cd\u53e0\u5c11\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51faEgoX\u6846\u67b6\uff0c\u4ece\u5355\u4e2a\u7b2c\u4e09\u4eba\u79f0\u8f93\u5165\u751f\u6210\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u3002</li>\n    <li>EgoX\u5229\u7528\u5927\u578b\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u65f6\u7a7a\u77e5\u8bc6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5b9e\u73b0\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5728\u672a\u89c1\u548c\u5b9e\u9645\u573a\u666f\u89c6\u9891\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5065\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Egocentric perception allows people to understand the world from their own viewpoint.</li>\n    <li>Transforming third-person videos into first-person videos is difficult due to different camera angles and limited overlapping views.</li>\n    <li>The new method, called EgoX, generates first-person videos from a single third-person video.</li>\n    <li>EgoX uses advanced video models and combines different types of video information for better results.</li>\n    <li>The approach ensures that the generated videos look realistic and maintain coherence, even with unseen footage.</li>\n</ul>"}, "publishedAt": "2025-12-09T00:53:39.000Z", "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video", "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/664df2176bc1025819f81caf/JNNjfUSkmFz_bX8SwSwOg.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08269.png", "numComments": 1, "submittedBy": {"_id": "664df2176bc1025819f81caf", "avatarUrl": "/avatars/964e2e4612fe05fa0cbadf6139c63077.svg", "fullname": "taewoongkang", "name": "Keh0t0", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.11558", "authors": [{"_id": "693f7554f516c693246811d3", "name": "Zhenyang Cai", "hidden": false}, {"_id": "693f7554f516c693246811d4", "name": "Jiaming Zhang", "hidden": false}, {"_id": "693f7554f516c693246811d5", "name": "Junjie Zhao", "hidden": false}, {"_id": "693f7554f516c693246811d6", "user": {"_id": "660244de381ff94818335b67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660244de381ff94818335b67/Vwt4S739tpURyqgP3IDgd.jpeg", "isPro": false, "fullname": "ZiyiZENG", "user": "CocoNutZENG", "type": "user"}, "name": "Ziyi Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:39.120Z", "hidden": false}, {"_id": "693f7554f516c693246811d7", "name": "Yanchao Li", "hidden": false}, {"_id": "693f7554f516c693246811d8", "name": "Jingyi Liang", "hidden": false}, {"_id": "693f7554f516c693246811d9", "name": "Junying Chen", "hidden": false}, {"_id": "693f7554f516c693246811da", "name": "Yunjin Yang", "hidden": false}, {"_id": "693f7554f516c693246811db", "name": "Jiajun You", "hidden": false}, {"_id": "693f7554f516c693246811dc", "name": "Shuzhi Deng", "hidden": false}, {"_id": "693f7554f516c693246811dd", "name": "Tongfei Wang", "hidden": false}, {"_id": "693f7554f516c693246811de", "name": "Wanting Chen", "hidden": false}, {"_id": "693f7554f516c693246811df", "name": "Chunxiu Hao", "hidden": false}, {"_id": "693f7554f516c693246811e0", "name": "Ruiqi Xie", "hidden": false}, {"_id": "693f7554f516c693246811e1", "name": "Zhenwei Wen", "hidden": false}, {"_id": "693f7554f516c693246811e2", "name": "Xiangyi Feng", "hidden": false}, {"_id": "693f7554f516c693246811e3", "name": "Zou Ting", "hidden": false}, {"_id": "693f7554f516c693246811e4", "name": "Jin Zou Lin", "hidden": false}, {"_id": "693f7554f516c693246811e5", "name": "Jianquan Li", "hidden": false}, {"_id": "693f7554f516c693246811e6", "name": "Guangjun Yu", "hidden": false}, {"_id": "693f7554f516c693246811e7", "name": "Liangyi Chen", "hidden": false}, {"_id": "693f7554f516c693246811e8", "name": "Junwen Wang", "hidden": false}, {"_id": "693f7554f516c693246811e9", "name": "Shan Jiang", "hidden": false}, {"_id": "693f7554f516c693246811ea", "name": "Benyou Wang", "hidden": false}], "publishedAt": "2025-12-12T13:42:57.000Z", "submittedOnDailyAt": "2025-12-15T00:14:07.445Z", "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "submittedOnDailyBy": {"_id": "64f1a34f2c5c8b767916447e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg", "isPro": false, "fullname": "Zhenyang Cai", "user": "Eric3200", "type": "user"}, "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "upvotes": 37, "discussionId": "693f7555f516c693246811eb", "ai_summary": "DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.", "ai_keywords": ["multimodal large language models", "dentalGPT", "domain knowledge injection", "reinforcement learning", "annotated multimodal dataset", "dental images", "visual understanding", "multimodal complex reasoning", "intraoral benchmarks", "panoramic benchmarks", "medical VQA benchmarks", "disease classification", "dental VQA", "parameters"], "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86DentalGPT\uff0c\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u7259\u79d1\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u3002</li>\n    <li>\u521b\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u7259\u79d1\u6807\u6ce8\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc712\u4e07\u5f20\u7259\u79d1\u56fe\u50cf\u53ca\u8be6\u7ec6\u63cf\u8ff0\u3002</li>\n    <li>\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0cDentalGPT\u5728\u7259\u79d1\u72b6\u51b5\u7684\u89c6\u89c9\u7406\u89e3\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u591a\u6a21\u6001\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u7259\u79d1\u75be\u75c5\u5206\u7c7b\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u5148\u8fdb\u7684MLLM\uff0c\u5c3d\u7ba1\u53c2\u6570\u53ea\u670970\u4ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DentalGPT is a new language model designed specifically for dental care, improving how we interpret dental images and data.</li>\n    <li>It was created using a large dataset of over 120,000 dental images and detailed descriptions to enhance understanding of dental conditions.</li>\n    <li>The model underwent reinforcement learning to improve its reasoning abilities for better diagnosis.</li>\n    <li>DentalGPT outperformed many other advanced models in tasks related to disease classification and visual question answering in dentistry.</li>\n    <li>This shows that using high-quality dental data and tailored training can effectively improve dental AI models.</li>\n</ul>"}, "publishedAt": "2025-12-12T08:42:57.000Z", "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11558.png", "numComments": 2, "submittedBy": {"_id": "64f1a34f2c5c8b767916447e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg", "fullname": "Zhenyang Cai", "name": "Eric3200", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10739", "authors": [{"_id": "693b91d89874a2a5e4ffb329", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32a", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:56.632Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32b", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32c", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32d", "user": {"_id": "64e8505321540e1da3226b54", "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg", "isPro": false, "fullname": "Wenwei Zhang", "user": "ZwwWayne", "type": "user"}, "name": "Wenwei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:21.987Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32e", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32f", "name": "Fan Zheng", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb330", "user": {"_id": "670f8df2005a358fdc6c2fb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qw2yocepl2vhC5T2ae49b.png", "isPro": false, "fullname": "tianyou", "user": "matianyou", "type": "user"}, "name": "Tianyou Ma", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:57.205Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb331", "user": {"_id": "687f853bb39262ba84f3eeff", "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg", "isPro": false, "fullname": "Junhao Shen", "user": "shenjunhao", "type": "user"}, "name": "Junhao Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:00.496Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb332", "name": "Haiteng Zhao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb333", "user": {"_id": "6454b1073aaeff9f3d330ef6", "avatarUrl": "/avatars/331fcbbf18a72b0dc8419ca3a77299bb.svg", "isPro": false, "fullname": "Duanyang Zhang", "user": "KKKDaniel", "type": "user"}, "name": "Duanyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:12.911Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb334", "name": "Huilun Zhang", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb335", "user": {"_id": "63fd691794cc8f815d50c112", "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg", "isPro": false, "fullname": "liu", "user": "Harold-lkk", "type": "user"}, "name": "Kuikun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:58.650Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb336", "name": "Chengqi Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb337", "name": "Yanhui Duan", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb338", "name": "Chiyu Chen", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb339", "name": "Ningsheng Ma", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33a", "name": "Jianfei Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33b", "name": "Han Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33c", "user": {"_id": "636317ed80c1a705a6eff396", "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg", "isPro": false, "fullname": "Dahua Lin", "user": "lindahua", "type": "user"}, "name": "Dahua Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:20.346Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33d", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:26:28.000Z", "submittedOnDailyAt": "2025-12-12T01:27:55.307Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "upvotes": 37, "discussionId": "693b91d99874a2a5e4ffb33e", "ai_summary": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "long reasoning chains of thought (CoTs)", "iterative active learning", "Rejection Fine-Tuning (RFT)", "F1 score", "accuracy", "DeepSeek-R1-Distill-Qwen-32B", "AIME2025"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4f7f\u7528\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002</li>\n    <li>\u73b0\u6709\u7684\u7ed3\u679c\u57fa\u7840\u68c0\u9a8c\u5668\u65e0\u6cd5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u68c0\u9a8c\u5668\uff08OPV\uff09\uff0c\u901a\u8fc7\u603b\u7ed3\u7ed3\u679c\u9a8c\u8bc1\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002</li>\n    <li>OPV\u91c7\u7528\u8fed\u4ee3\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u51cf\u5c11\u6ce8\u91ca\u6210\u672c\uff0c\u63d0\u9ad8\u9a8c\u8bc1\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPV\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u4e0e\u7b56\u7565\u6a21\u578b\u5408\u4f5c\u65f6\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) have improved in solving complex problems using a method called Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>Current verifiers struggle to check unreliable steps in long reasoning processes and lack enough high-quality human annotations for accurate error detection.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) checks the reasoning process behind summarized outcomes to improve verification accuracy and efficiency.</li>\n    <li>OPV uses an active learning approach to reduce annotation costs by focusing on the most uncertain cases for expert review, enhancing its verification abilities over time.</li>\n    <li>Experiments show OPV outperforms larger models in accuracy and effectively identifies errors, improving performance when used with other models.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:26:28.000Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10739.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10949", "authors": [{"_id": "693b895d9874a2a5e4ffb302", "user": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "name": "Yiwen Tang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:55.504Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb303", "user": {"_id": "642a8302d651bae3c11b72b1", "avatarUrl": "/avatars/4d2d422613e274d80482fed9a7d3f785.svg", "isPro": false, "fullname": "Zoey Guo", "user": "Purple1288", "type": "user"}, "name": "Zoey Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:01.705Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb304", "user": {"_id": "6708920aeae29d1cd41a703b", "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg", "isPro": false, "fullname": "kaixin zhu", "user": "czkk566", "type": "user"}, "name": "Kaixin Zhu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:10.275Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb305", "name": "Ray Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb306", "user": {"_id": "6535045a910b844786a6642f", "avatarUrl": "/avatars/37a94864a7a348151837b421ea6d77e3.svg", "isPro": false, "fullname": "Qizhi Chen", "user": "Tavish9", "type": "user"}, "name": "Qizhi Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:11.060Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb307", "user": {"_id": "6349214f8146350b3a4c5cdf", "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg", "isPro": false, "fullname": "Dongzhi Jiang", "user": "CaraJ", "type": "user"}, "name": "Dongzhi Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:06:02.910Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb308", "name": "Junli Liu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb309", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:08.733Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30a", "user": {"_id": "6662d2c9de4c4e1f04bd29c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QnnO_KOyZjFd-iXuPnHqR.png", "isPro": false, "fullname": "HaomingSong", "user": "HaomingSong", "type": "user"}, "name": "Haoming Song", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:30.062Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30b", "user": {"_id": "64daecec888b7e9c400f59b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png", "isPro": false, "fullname": "Delin Qu", "user": "delinqu", "type": "user"}, "name": "Delin Qu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:34.085Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30c", "name": "Tianyi Bai", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30d", "name": "Dan Xu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30e", "name": "Wentao Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30f", "name": "Bin Zhao", "hidden": false}], "publishedAt": "2025-12-11T18:59:52.000Z", "submittedOnDailyAt": "2025-12-12T00:49:43.850Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "submittedOnDailyBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "upvotes": 36, "discussionId": "693b895d9874a2a5e4ffb310", "githubRepo": "https://github.com/Ivan-Tang-3D/3DGen-R1", "githubRepoAddedBy": "user", "ai_summary": "This study investigates reinforcement learning for text-to-3D generation, focusing on reward designs, RL algorithms, benchmarking, and hierarchical optimization, introducing AR3D-R1 as the first RL-enhanced model for 3D generation.", "ai_keywords": ["reinforcement learning", "text-to-3D generation", "reward designs", "GRPO variants", "token-level optimization", "MME-3DR", "Hi-GRPO", "hierarchical 3D generation", "AR3D-R1"], "githubStars": 40, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u57282D\u56fe\u50cf\u751f\u6210\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u57283D\u751f\u6210\u65b9\u9762\u4ecd\u672a\u5f97\u5230\u5e7f\u6cdb\u63a2\u7d22\u3002</li>\n    <li>3D\u751f\u6210\u9700\u8981\u5168\u5c40\u4e00\u81f4\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u7ec6\u81f4\u7684\u5c40\u90e8\u7eb9\u7406\uff0c\u5bf9\u5956\u52b1\u8bbe\u8ba1\u548cRL\u7b97\u6cd5\u975e\u5e38\u654f\u611f\u3002</li>\n    <li>\u6211\u4eec\u8fdb\u884c\u4e86\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u7814\u7a76\uff0c\u8bc4\u4f30\u5956\u52b1\u8bbe\u8ba1\u3001\u6a21\u578b\u9009\u62e9\u53ca\u5176\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002</li>\n    <li>\u63d0\u51fa\u4e86MME-3DR\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f303D\u751f\u6210\u6a21\u578b\u7684\u9690\u6027\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8eRL\u7684\u6587\u672c\u52303D\u6a21\u578bAR3D-R1\uff0c\u4ece\u7c97\u7565\u5f62\u72b6\u5230\u7eb9\u7406\u7ec6\u5316\u8fdb\u884c\u4f18\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) has been used successfully in 2D image generation but is still largely unexplored in 3D generation due to its complexity.</li>\n    <li>The study investigates effective reward designs and algorithms for improving text-to-3D generation, emphasizing the importance of aligning with human preferences.</li>\n    <li>It introduces a new benchmark called MME-3DR to better assess the reasoning abilities of 3D generation models.</li>\n    <li>The researchers propose a new method called Hi-GRPO to optimize 3D generation in a hierarchical manner, focusing on both global structure and local details.</li>\n    <li>The result of this work is AR3D-R1, the first RL-enhanced model for generating 3D objects from text, with a focus on refining shapes and textures.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:59:52.000Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10949.png", "numComments": 2, "submittedBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "fullname": "Ivan Tang", "name": "IvanTang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.11749", "authors": [{"_id": "693f754ef516c693246811c3", "user": {"_id": "662887715d246621f33d2ce6", "avatarUrl": "/avatars/3e0b1017b1e1bf284758ce840c174290.svg", "isPro": false, "fullname": "Shi Minglei", "user": "MingleiShi", "type": "user"}, "name": "Minglei Shi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:57.314Z", "hidden": false}, {"_id": "693f754ef516c693246811c4", "user": {"_id": "641d373d353524fe41f1d453", "avatarUrl": "/avatars/6fa9a0a4ba9818a221f835174a14be2d.svg", "isPro": false, "fullname": "Haolin Wang", "user": "howlin", "type": "user"}, "name": "Haolin Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:54.781Z", "hidden": false}, {"_id": "693f754ef516c693246811c5", "name": "Borui Zhang", "hidden": false}, {"_id": "693f754ef516c693246811c6", "name": "Wenzhao Zheng", "hidden": false}, {"_id": "693f754ef516c693246811c7", "name": "Bohan Zeng", "hidden": false}, {"_id": "693f754ef516c693246811c8", "name": "Ziyang Yuan", "hidden": false}, {"_id": "693f754ef516c693246811c9", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "693f754ef516c693246811ca", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "693f754ef516c693246811cb", "name": "Huan Yang", "hidden": false}, {"_id": "693f754ef516c693246811cc", "name": "Xintao Wang", "hidden": false}, {"_id": "693f754ef516c693246811cd", "name": "Pengfei Wan", "hidden": false}, {"_id": "693f754ef516c693246811ce", "name": "Kun Gai", "hidden": false}, {"_id": "693f754ef516c693246811cf", "name": "Jie Zhou", "hidden": false}, {"_id": "693f754ef516c693246811d0", "name": "Jiwen Lu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/V8NQYwKC-8n2SbplK5Ag7.png"], "publishedAt": "2025-12-12T17:45:03.000Z", "submittedOnDailyAt": "2025-12-15T00:19:12.400Z", "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.", "upvotes": 33, "discussionId": "693f754ff516c693246811d1", "githubRepo": "https://github.com/KlingTeam/SVG-T2I", "githubRepoAddedBy": "user", "ai_summary": "SVG-T2I, a scaled SVG framework, enables high-quality text-to-image synthesis directly in the Visual Foundation Model feature domain, achieving competitive performance in generative tasks.", "ai_keywords": ["Visual Foundation Model", "VFM", "SVG", "Self-supervised representations for Visual Generation", "SVG-T2I", "text-to-image diffusion", "GenEval", "DPG-Bench", "autoencoder", "generation model"], "githubStars": 40, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u751f\u6210\u4e0e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u6574\u5408\u89c6\u89c9\u7406\u89e3\u3001\u611f\u77e5\u548c\u751f\u6210\u7684\u7edf\u4e00\u8def\u5f84\u3002</li>\n    <li>\u76ee\u524d\uff0c\u5728VFM\u8868\u793a\u7a7a\u95f4\u5185\u8bad\u7ec3\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\u5c1a\u672a\u6df1\u5165\u3002</li>\n    <li>\u6211\u4eec\u6269\u5c55\u4e86SVG\u6846\u67b6\uff0c\u63d0\u51fa\u4e86SVG-T2I\uff0c\u4ee5\u652f\u6301\u5728VFM\u7279\u5f81\u57df\u5185\u8fdb\u884c\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u3002</li>\n    <li>\u901a\u8fc7\u6807\u51c6\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6d41\u7a0b\uff0cSVG-T2I\u5728GenEval\u4e0a\u8fbe\u52300.75\u7684\u5206\u6570\uff0c\u5728DPG-Bench\u4e0a\u8fbe\u523085.78\uff0c\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u9879\u76ee\u5b8c\u5168\u5f00\u6e90\uff0c\u5305\u62ec\u81ea\u52a8\u7f16\u7801\u5668\u3001\u751f\u6210\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u3001\u63a8\u7406\u3001\u8bc4\u4f30\u6d41\u7a0b\u548c\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u65b9\u4fbf\u8fdb\u4e00\u6b65\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visual Foundation Models (VFM) can improve how we understand and create images from text.</li>\n    <li>The SVG-T2I framework has been developed to enhance text-to-image generation within the VFM space.</li>\n    <li>SVG-T2I uses a standard text-to-image diffusion approach and achieved strong performance metrics (0.75 on GenEval and 85.78 on DPG-Bench).</li>\n    <li>This shows that VFMs are powerful for generating images from text.</li>\n    <li>The project is fully open-source, providing tools and models for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-12-12T12:45:03.000Z", "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/V8NQYwKC-8n2SbplK5Ag7.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11749.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 184}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10756", "authors": [{"_id": "693b91539874a2a5e4ffb318", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91539874a2a5e4ffb319", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31a", "name": "Wenwei Zhang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31b", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31c", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:02.946Z", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31d", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31e", "name": "Tianyou Ma", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31f", "name": "Yuhong Liu", "hidden": false}, {"_id": "693b91539874a2a5e4ffb320", "name": "Zhi Wang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb321", "name": "Runyuan Ma", "hidden": false}, {"_id": "693b91539874a2a5e4ffb322", "name": "Guangyu Wang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb323", "name": "Wei Li", "hidden": false}, {"_id": "693b91539874a2a5e4ffb324", "name": "Conghui He", "hidden": false}, {"_id": "693b91539874a2a5e4ffb325", "name": "Dahua Lin", "hidden": false}, {"_id": "693b91539874a2a5e4ffb326", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:47:38.000Z", "submittedOnDailyAt": "2025-12-12T01:23:10.732Z", "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "upvotes": 30, "discussionId": "693b91539874a2a5e4ffb327", "ai_summary": "The Outcome-based Process Verifier (OPV) improves the verification of complex reasoning chains in large language models by combining outcome-based and process-based verification with iterative active learning and Rejection Fine-Tuning, achieving state-of-the-art performance on various benchmarks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "verifiers", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "CoTs", "iterative active learning", "Rejection Fine-Tuning (RFT)", "OPV-Bench", "AIME2025", "DeepSeek-R1-Distill-Qwen-32B"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7684\u7ed3\u679c\u57fa\u7840\u9a8c\u8bc1\u5668\u65e0\u6cd5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\u3002</li>\n    <li>\u5f53\u524d\u7684\u8fc7\u7a0b\u57fa\u7840\u9a8c\u8bc1\u5668\u5728\u590d\u6742\u7684\u957f\u63a8\u7406\u94fe\u4e2d\u96be\u4ee5\u53ef\u9760\u5730\u68c0\u6d4b\u9519\u8bef\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u7a00\u7f3a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\uff0c\u5b83\u901a\u8fc7\u9a8c\u8bc1\u957f\u63a8\u7406\u94fe\u7684\u603b\u7ed3\u7ed3\u679c\u6765\u5b9e\u73b0\u51c6\u786e\u548c\u9ad8\u6548\u7684\u9a8c\u8bc1\u3002</li>\n    <li>OPV\u91c7\u7528\u8fed\u4ee3\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\u9010\u6b65\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\uff0c\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPV\u5728\u6027\u80fd\u548c\u9002\u7528\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u5927\u578b\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models have improved at solving complex reasoning tasks using a method called Reinforcement Learning with Verifiable Rewards.</li>\n    <li>Current verifiers struggle to check long reasoning processes due to unreliable steps and a lack of high-quality human annotations.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) checks the reasoning behind summarized outcomes to provide better verification with less annotation effort.</li>\n    <li>OPV uses an active learning approach to gradually enhance its verification abilities while minimizing costs by focusing on uncertain cases for expert annotation.</li>\n    <li>Experiments show that OPV performs better than larger models, achieving a higher F1 score and improving accuracy when used with policy models.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:47:38.000Z", "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10756.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u8f6c\u5316\u4e3a\u53ef\u8fd0\u884c\u7684\u4ee3\u7801\u3002</li>\n    <li>\u5de5\u5177\u5982Github Copilot\u548cCursor\u63a8\u52a8\u4e86LLMs\u7684\u5546\u4e1a\u5e94\u7528\uff0c\u6027\u80fd\u4ece\u5355\u4e00\u6570\u5b57\u63d0\u5347\u5230\u5728\u8bc4\u6d4b\u4e2d\u8d85\u8fc795%\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u7efc\u5408\u6307\u5357\uff0c\u6db5\u76d6\u4ece\u6570\u636e\u6574\u7406\u5230\u8bad\u7ec3\u540e\u7684\u5404\u4e2a\u9636\u6bb5\u3002</li>\n    <li>\u5206\u6790\u4e86\u901a\u7528LLMs\u548c\u4e13\u7528\u4ee3\u7801LLMs\u7684\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u6280\u672f\u3001\u8bbe\u8ba1\u51b3\u7b56\u548c\u6743\u8861\u3002</li>\n    <li>\u7814\u7a76\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\u5206\u6790\u4ee3\u7801\u9884\u8bad\u7ec3\u7b49\u8fc7\u7a0b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) have changed how software is developed by turning natural language descriptions into code.</li>\n    <li>Tools like GitHub Copilot and others have increased the use of LLMs in commercial software development.</li>\n    <li>This work examines the entire process of developing code LLMs, from data collection to training and deployment.</li>\n    <li>It analyzes both general-purpose LLMs and those specialized in coding, discussing their strengths and weaknesses.</li>\n    <li>Experiments are conducted to explore various aspects of training and improving code LLMs, focusing on real-world applications and challenges.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faDeepSeek-V3.2\u6a21\u578b\uff0c\u517c\u5177\u9ad8\u6548\u8ba1\u7b97\u548c\u51fa\u8272\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u5176\u9ad8\u8ba1\u7b97\u53d8\u4f53DeepSeek-V3.2-Speciale\u8d85\u8d8aGPT-5\uff0c\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u4e0eGemini-3.0-Pro\u76f8\u5ab2\u7f8e\u3002</li>\n    <li>\u5f00\u53d1\u5927\u578b\u667a\u80fd\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines high efficiency with strong performance in reasoning and tasks.</li>\n    <li>It features a new attention mechanism called DeepSeek Sparse Attention (DSA) which reduces the amount of computation needed while maintaining performance in long contexts.</li>\n    <li>The model uses a scalable reinforcement learning system, allowing it to perform similarly to GPT-5, and even better in some versions, achieving top results in major math and informatics competitions.</li>\n    <li>DeepSeek-V3.2 includes a unique pipeline for creating training data that enhances its ability to reason and follow instructions in complex tasks.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u9886\u5148\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u8fdb\u884c\u63a8\u7406\u548c\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faZ-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff08S3-DiT\uff09\u3002</li>\n    <li>Z-Image\u7684\u8bad\u7ec3\u6d41\u7a0b\u4ec5\u9700314K H800 GPU\u5c0f\u65f6\uff0c\u6210\u672c\u7ea6\u4e3a630K\u7f8e\u5143\uff0c\u4e14\u5728\u4f01\u4e1a\u7ea7\u548c\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5747\u53ef\u5feb\u901f\u63a8\u7406\u3002</li>\n    <li>Z-Image\u5728\u751f\u6210\u903c\u771f\u56fe\u50cf\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u80fd\u591f\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u7ade\u4e89\uff0c\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86\u76f8\u5173\u4ee3\u7801\u548c\u6f14\u793a\uff0c\u4ee5\u4fc3\u8fdb\u7ecf\u6d4e\u5b9e\u60e0\u7684\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The current image generation market is led by expensive proprietary models like Nano Banana Pro and Seedream 4.0.</li>\n    <li>Open-source models like Qwen-Image and Hunyuan-Image-3.0 are large (20B to 80B parameters), making them hard to use on regular computers.</li>\n    <li>We introduce Z-Image, a smaller 6B-parameter model using a new architecture that is more efficient and easier to run.</li>\n    <li>Z-Image is fast and can work on consumer-grade hardware, making high-quality image generation accessible.</li>\n    <li>Our model performs well in generating realistic images and handling bilingual text, matching or exceeding the performance of top commercial models.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u957f\u89c6\u9891\u63a8\u7406\u7684\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u8868\u73b0\u51fa\u5f88\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u901a\u8fc7\u5feb\u901f\u6d4f\u89c8\u548c\u7ec6\u81f4\u67e5\u770b\u76f8\u5173\u7247\u6bb5\u6765\u7406\u89e3\u957f\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u88c1\u526a\u5de5\u5177\u805a\u7126\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u9010\u6b65\u83b7\u53d6\u66f4\u7ec6\u81f4\u7684\u89c6\u9891\u5e27\u3002</li>\n    <li>\u4e3a\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\uff0c\u6211\u4eec\u5c06\u53d1\u5e03\u4e00\u4e2a\u5305\u542b247.9K\u6837\u672c\u7684\u8bad\u7ec3\u6570\u636e\u96c6VideoSIAH\u3002</li>\n    <li>LongVT\u5728\u56db\u4e2a\u957f\u671f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u53ef\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models can analyze videos and text but struggle with long videos due to hallucinations and scattered evidence.</li>\n    <li>LongVT is a new framework that helps in understanding long videos by mimicking human viewing habits: first getting an overall view and then focusing on specific parts.</li>\n    <li>The framework uses the model's ability to pinpoint specific video clips and analyze them in detail for better answers.</li>\n    <li>To support this task, a new dataset called VideoSIAH will be released, containing a large number of samples for training and evaluation.</li>\n    <li>LongVT shows better performance than existing methods on four difficult video reasoning tasks, and all resources are available online for public use.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u7531\u4e8e\u8ba1\u7b97\u987a\u5e8f\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u5934\u50cf\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cLive Avatar\u201d\uff0c\u4e00\u4e2a\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u5934\u50cf\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u80fd\u5728\u591a\u4e2aGPU\u4e0a\u9ad8\u6548\u5904\u7406\u566a\u58f0\u53bb\u9664\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u53c2\u8003\u56fe\u50cf\u6765\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u4ee5\u6bcf\u79d220\u5e27\u7684\u901f\u5ea6\u751f\u6210\u89c6\u9891\uff0c\u662f\u9996\u4e2a\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5b9e\u65f6\u5934\u50cf\u751f\u6210\u7684\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating realistic avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>The system uses a method called Timestep-forcing Pipeline Parallelism to speed up the video generation process across multiple GPUs.</li>\n    <li>To improve consistency and reduce errors in appearance, it employs the Rolling Sink Frame Mechanism, which adjusts visuals using a reference image.</li>\n    <li>Live Avatar achieves high-quality video generation at 20 frames per second on five high-performance GPUs.</li>\n    <li>This approach sets a new standard for real-time avatar creation in long video applications, making it more practical for various uses.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u771f\u5b9e\u4f01\u4e1a\u7684\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u6a21\u5f0f\u4e0a\u8fdb\u884c\u4ed3\u5e93\u7ea7\u7684\u5de5\u7a0b\uff0c\u5305\u62ec\u4ece\u96f6\u5f00\u59cb\u8bbe\u8ba1\u591a\u9636\u6bb5SQL\u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u9010\u6b65\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u667a\u80fd\u4f53\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u7684\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>\u901a\u8fc7\u8bc6\u522b\u8fd9\u4e9b\u5c40\u9650\u6027\uff0cDAComp\u4e3a\u5f00\u53d1\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark with 210 tasks that simulate complex data workflows in businesses.</li>\n    <li>It includes data engineering tasks that involve creating and modifying SQL pipelines and managing data systems.</li>\n    <li>Data analysis tasks focus on solving open-ended business problems through strategic planning and data exploration.</li>\n    <li>Performance on these tasks is low, with less than 20% success in engineering tasks and under 40% in analysis tasks.</li>\n    <li>DAComp helps identify weaknesses in current data processing capabilities, aiming to improve autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.13254", "authors": [{"_id": "691c1c836bfd5965c0fd39e4", "user": {"_id": "6419b34a9a27800807c34a63", "avatarUrl": "/avatars/ee1391a9a153bae0dd04323b1fa5b5d6.svg", "isPro": false, "fullname": "Shalini M", "user": "shalinimaiti", "type": "user"}, "name": "Shalini Maiti", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:47.486Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e5", "user": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "name": "Amar Budhiraja", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:49.892Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e6", "user": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "isPro": false, "fullname": "Bhavul Gauri", "user": "bhavul", "type": "user"}, "name": "Bhavul Gauri", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:31.124Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e7", "user": {"_id": "691c6b9b660c15d270b5838a", "avatarUrl": "/avatars/2dc40e079bd8b7af7ae4a4ac43acc552.svg", "isPro": false, "fullname": "Gaurav Chaurasia", "user": "gchauras", "type": "user"}, "name": "Gaurav Chaurasia", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:45.558Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e8", "name": "Anton Protopopov", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e9", "name": "Alexis Audran-Reiss", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ea", "name": "Michael Slater", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39eb", "name": "Despoina Magka", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ec", "name": "Tatiana Shavrina", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ed", "name": "Roberta Raileanu", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ee", "name": "Yoram Bachrach", "hidden": false}], "publishedAt": "2025-11-17T11:13:34.000Z", "submittedOnDailyAt": "2025-11-18T04:47:00.815Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "submittedOnDailyBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "upvotes": 131, "discussionId": "691c1c846bfd5965c0fd39fc", "githubRepo": "https://github.com/facebookresearch/llm_souping", "githubStars": 60, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>\u6a21\u578b\u201c\u6df7\u5408\u201d\uff08model souping\uff09\u901a\u8fc7\u5bf9\u591a\u4e2a\u76f8\u540c\u67b6\u6784\u7684\u6a21\u578b\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u80fd\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86\u201c\u7c7b\u522b\u4e13\u5bb6\u6df7\u5408\u201d\uff08SoCE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u51c6\u7ec4\u5408\u6765\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u6765\u4f18\u5316\u6027\u80fd\u3002</li>\n    <li>SoCE\u65b9\u6cd5\u5229\u7528\u57fa\u51c6\u7c7b\u522b\u4e4b\u95f4\u7684\u4f4e\u76f8\u5173\u6027\u6765\u8bc6\u522b\u201c\u4e13\u5bb6\u201d\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4f18\u5316\u52a0\u6743\u5408\u5e76\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u80fd\u529b\u3001\u5de5\u5177\u8c03\u7528\u548c\u6570\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are powerful but require a lot of resources and time to train.</li>\n    <li>Model souping, or averaging weights from multiple models, can improve performance without needing extensive retraining.</li>\n    <li>This paper presents Soup Of Category Experts (SoCE), a new method for model souping that selects the best models based on benchmark categories.</li>\n    <li>SoCE uses weighted averaging to combine models, focusing on those that perform well in related areas instead of using equal weights for all models.</li>\n    <li>The method shows better performance and reliability in various tasks, achieving top results in certain benchmarks.</li>\n</ul>"}, "publishedAt": "2025-11-17T06:13:34.000Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13254.png", "numComments": 4, "submittedBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "fullname": "Amar Budhiraja", "name": "ambud26", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Dec 16, 2025";
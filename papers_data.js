window.trendingPapers = {
    "today": [{"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 53, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u65f6\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8f85\u52a9\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u6765\u5f15\u5bfc\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u6709\u5c40\u9650\u6027\u4e14\u6210\u672c\u9ad8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u673a\u5236\uff0c\u5e2e\u52a9\u6a21\u578b\u81ea\u4e3b\u53d1\u73b0\u5e76\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u63d0\u53d6\u56fe\u50cf\u4e2d\u7684\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6848\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text and struggle with visual reasoning tasks.</li>\n    <li>Current solutions use helper images and other visual aids, but these methods have limitations and require a lot of manual work.</li>\n    <li>We propose a new method that helps LMMs learn to use visual reasoning on their own, without needing specific guidance.</li>\n    <li>This method allows the model to better understand and use visual information for different tasks.</li>\n    <li>Our approach shows better performance than traditional methods and works well across various vision-related challenges.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 4, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20605", "authors": [{"_id": "694e76e5746a34b55dd545eb", "name": "Seijin Kobayashi", "hidden": false}, {"_id": "694e76e5746a34b55dd545ec", "name": "Yanick Schimpf", "hidden": false}, {"_id": "694e76e5746a34b55dd545ed", "name": "Maximilian Schlegel", "hidden": false}, {"_id": "694e76e5746a34b55dd545ee", "name": "Angelika Steger", "hidden": false}, {"_id": "694e76e5746a34b55dd545ef", "name": "Maciej Wolczyk", "hidden": false}, {"_id": "694e76e5746a34b55dd545f0", "name": "Johannes von Oswald", "hidden": false}, {"_id": "694e76e5746a34b55dd545f1", "name": "Nino Scherrer", "hidden": false}, {"_id": "694e76e5746a34b55dd545f2", "name": "Kaitlin Maile", "hidden": false}, {"_id": "694e76e5746a34b55dd545f3", "name": "Guillaume Lajoie", "hidden": false}, {"_id": "694e76e5746a34b55dd545f4", "name": "Blake A. Richards", "hidden": false}, {"_id": "694e76e5746a34b55dd545f5", "name": "Rif A. Saurous", "hidden": false}, {"_id": "694e76e5746a34b55dd545f6", "name": "James Manyika", "hidden": false}, {"_id": "694e76e5746a34b55dd545f7", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "694e76e5746a34b55dd545f8", "name": "Alexander Meulemans", "hidden": false}, {"_id": "694e76e5746a34b55dd545f9", "name": "Jo\u00e3o Sacramento", "hidden": false}], "publishedAt": "2025-12-23T18:51:50.000Z", "submittedOnDailyAt": "2025-12-26T11:17:05.505Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "submittedOnDailyBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "upvotes": 48, "discussionId": "694e76e5746a34b55dd545fa", "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u4e0b\u4e00\u8bcd\u9884\u6d4b\u9884\u8bad\u7ec3\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\uff0c\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\u3002</li>\n    <li>\u5728RL\u8fc7\u7a0b\u4e2d\uff0c\u8fd9\u4e9b\u6a21\u578b\u9010\u4e2a\u751f\u6210\u65b0\u8f93\u51fa\uff0c\u4f46\u8fd9\u79cd\u9010\u4e2a\u91c7\u6837\u7684\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u7a00\u758f\u65f6\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u8fdb\u884c\u63a2\u7d22\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u9636\u975e\u56e0\u679c\u5e8f\u5217\u6a21\u578b\uff0c\u80fd\u591f\u63a7\u5236\u57fa\u7840\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6fc0\u6d3b\uff0c\u5e2e\u52a9\u53d1\u73b0\u65f6\u95f4\u62bd\u8c61\u884c\u4e3a\u3002</li>\n    <li>\u5185\u90e8\u63a7\u5236\u5668\u7684\u5f3a\u5316\u5b66\u4e60\uff08\u79f0\u4e3a\u201c\u5185\u90e8RL\u201d\uff09\u4f7f\u5f97\u5728\u6807\u51c6RL\u5fae\u8c03\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u7a00\u758f\u5956\u52b1\u4e2d\u5b66\u4e60\u6210\u4e3a\u53ef\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large autoregressive models have improved performance through next-token prediction and reinforcement learning (RL).</li>\n    <li>Generating outputs token-by-token can be slow and inefficient, especially with sparse rewards.</li>\n    <li>We propose a new method using internal representations of the model to find more effective actions.</li>\n    <li>This involves a higher-order model that helps control the base autoregressive model's outputs.</li>\n    <li>Our approach, called \"internal RL,\" allows better learning from sparse rewards and supports hierarchical reinforcement learning.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:51:50.000Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png", "numComments": 3, "submittedBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "fullname": "Maximilian Schlegel", "name": "schlegelm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16093", "authors": [{"_id": "6944be16fbf17e708e186002", "user": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "name": "Jintao Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:52:30.866Z", "hidden": false}, {"_id": "6944be16fbf17e708e186003", "name": "Kaiwen Zheng", "hidden": false}, {"_id": "6944be16fbf17e708e186004", "name": "Kai Jiang", "hidden": false}, {"_id": "6944be16fbf17e708e186005", "name": "Haoxu Wang", "hidden": false}, {"_id": "6944be16fbf17e708e186006", "name": "Ion Stoica", "hidden": false}, {"_id": "6944be16fbf17e708e186007", "name": "Joseph E. Gonzalez", "hidden": false}, {"_id": "6944be16fbf17e708e186008", "name": "Jianfei Chen", "hidden": false}, {"_id": "6944be16fbf17e708e186009", "name": "Jun Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "publishedAt": "2025-12-18T02:21:30.000Z", "submittedOnDailyAt": "2025-12-25T00:44:44.469Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "submittedOnDailyBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "upvotes": 48, "discussionId": "6944be16fbf17e708e18600a", "projectPage": "https://github.com/thu-ml/TurboDiffusion", "githubRepo": "https://github.com/thu-ml/TurboDiffusion", "githubRepoAddedBy": "user", "ai_summary": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.", "ai_keywords": ["SageAttention", "Sparse-Linear Attention", "rCM", "W8A8 quantization", "diffusion generation", "video generation", "RTX 5090 GPU"], "githubStars": 1958, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "summary_zh": "<ul>\n    <li>TurboDiffusion\u662f\u4e00\u4e2a\u89c6\u9891\u751f\u6210\u52a0\u901f\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u89c6\u9891\u751f\u6210\u901f\u5ea6\u63d0\u9ad8100-200\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\u3002</li>\n    <li>\u5176\u52a0\u901f\u4e3b\u8981\u4f9d\u8d56\u4e8e\u51e0\u4e2a\u7ec4\u4ef6\uff0c\u5305\u62ec\u4f4e\u4f4dSageAttention\u548c\u53ef\u8bad\u7ec3\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u3002</li>\n    <li>\u91c7\u7528rCM\u8fdb\u884c\u9ad8\u6548\u7684\u6b65\u9aa4\u84b8\u998f\uff0c\u4ee5\u53ca\u5c06\u6a21\u578b\u53c2\u6570\u548c\u6fc0\u6d3b\u91cf\u5316\u4e3a8\u4f4d\u6765\u52a0\u901f\u7ebf\u6027\u5c42\u5e76\u538b\u7f29\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u5728\u5355\u4e2aRTX 5090 GPU\u4e0a\uff0cTurboDiffusion\u4e5f\u80fd\u5b9e\u73b0\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n    <li>GitHub\u4e0a\u63d0\u4f9b\u4e86\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u6613\u4e8e\u4f7f\u7528\u7684\u4ee3\u7801\uff0c\u94fe\u63a5\u4e3ahttps://github.com/thu-ml/TurboDiffusion\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>TurboDiffusion is a new framework that speeds up video generation by 100-200 times without losing video quality.</li>\n    <li>It uses special techniques like low-bit SageAttention and trainable Sparse-Linear Attention to make calculations faster.</li>\n    <li>The framework also employs step distillation for efficiency and quantizes data to 8 bits to reduce model size and speed up processing.</li>\n    <li>Tests show TurboDiffusion can greatly increase speed even on a single powerful GPU, while still producing good quality videos.</li>\n    <li>The framework and its resources are available on GitHub for easy access and use.</li>\n</ul>"}, "publishedAt": "2025-12-17T21:21:30.000Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16093.png", "numComments": 2, "submittedBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "fullname": "Jintao Zhang", "name": "jt-zhang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4e00\u822c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u4e0a\u8f83\u5f31\u3002</li>\n    <li>\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86DSR Suite\uff0c\u5305\u62ec\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u95ee\u7b54\u751f\u6210\u7ba1\u9053\uff0c\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u751f\u6210\u591a\u9009\u9898\u3002</li>\n    <li>\u8be5\u7ba1\u9053\u63d0\u53d6\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u5982\u76f8\u673a\u59ff\u6001\u3001\u5c40\u90e8\u70b9\u4e91\u548c3D\u8f68\u8ff9\uff0c\u652f\u6301DSR-Train\u7684\u5b66\u4e60\u548cDSR-Bench\u7684\u8bc4\u4f30\u3002</li>\n    <li>\u65b0\u6570\u636e\u96c6\u5f3a\u8c03\u771f\u5b9e\u89c6\u9891\u6765\u6e90\u30013D\u9700\u6c42\u3001\u89c6\u89d2\u53d8\u5316\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u7ec6\u81f4\u7684\u7a0b\u5e8f\u6027\u7b54\u6848\u3002</li>\n    <li>\u901a\u8fc7\u8f7b\u91cf\u7ea7\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\u5c06\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u878d\u5165VLM\uff0c\u5b9e\u9a8c\u663e\u793a\u80fd\u663e\u8457\u63d0\u5347\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u822c\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) are good at general understanding but struggle with dynamic spatial reasoning (DSR).</li>\n    <li>The DSR Suite is introduced to improve DSR by creating question-answer pairs from real-world videos.</li>\n    <li>This suite uses advanced vision models to gather important geometric and motion information from videos.</li>\n    <li>It focuses on various aspects such as real-world videos, 3D object requirements, viewpoint changes, and interactions between multiple objects.</li>\n    <li>A new Geometry Selection Module (GSM) helps VLMs use relevant geometric information without being overloaded with unnecessary data.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u201c\u4e00\u955c\u5230\u5e95\u201d\u6280\u672f\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5177\u6709\u72ec\u7279\u7684\u7f8e\u5b66\uff0c\u4f46\u5b9e\u73b0\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002</li>\n    <li>DreaMontage\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7528\u6237\u8f93\u5165\u751f\u6210\u8fde\u8d2f\u4e14\u751f\u52a8\u7684\u957f\u65f6\u95f4\u201c\u4e00\u955c\u5230\u5e95\u201d\u89c6\u9891\u3002</li>\n    <li>\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u8c03\u8282\u673a\u5236\u548c\u81ea\u9002\u5e94\u8c03\u4f18\u7b56\u7565\u6765\u589e\u5f3a\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>\u4f7f\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u89c6\u89c9\u8868\u8fbe\u9636\u6bb5\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u53d9\u4e8b\u6d41\u7545\u6027\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6bb5\u843d\u81ea\u56de\u5f52\u63a8\u65ad\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" filmmaking technique is complex and expensive but offers a unique aesthetic.</li>\n    <li>DreaMontage is a new system that creates smooth, long one-shot videos from various user inputs.</li>\n    <li>The system uses a special method to control video frames and improve visual quality.</li>\n    <li>It addresses challenges like motion realism and smooth transitions to enhance the final output.</li>\n    <li>Experiments show that DreaMontage produces high-quality videos efficiently, allowing users to create cohesive cinematic experiences.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15716", "authors": [{"_id": "694b65e7746a34b55dd53dbe", "user": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "isPro": false, "fullname": "Jinjing Zhao", "user": "Jinjing713", "type": "user"}, "name": "Jinjing Zhao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:42.906Z", "hidden": false}, {"_id": "694b65e7746a34b55dd53dbf", "name": "Fangyun Wei", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc0", "name": "Zhening Liu", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc1", "name": "Hongyang Zhang", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc2", "name": "Chang Xu", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc3", "name": "Yan Lu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64f8962bce75bb0fb50bdbdb/eC4hVgIfk0MxzPgn6mvGC.mp4"], "publishedAt": "2025-12-17T18:59:59.000Z", "submittedOnDailyAt": "2025-12-26T03:15:40.222Z", "title": "Spatia: Video Generation with Updatable Spatial Memory", "submittedOnDailyBy": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "isPro": false, "fullname": "Jinjing Zhao", "user": "Jinjing713", "type": "user"}, "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.", "upvotes": 20, "discussionId": "694b65e8746a34b55dd53dc4", "projectPage": "https://zhaojingjing713.github.io/Spatia/", "githubRepo": "https://github.com/ZhaoJingjing713/Spatia", "githubRepoAddedBy": "user", "ai_summary": "Spatia, a spatial memory-aware video generation framework, maintains long-term spatial and temporal consistency by preserving and updating a 3D scene point cloud, enabling realistic video generation and interactive editing.", "ai_keywords": ["spatial memory-aware", "video generation framework", "3D scene point cloud", "dynamic-static disentanglement", "visual SLAM", "explicit camera control", "3D-aware interactive editing"], "githubStars": 83, "organization": {"_id": "670621bc820835bbf0d2b499", "name": "Sydney-Uni", "fullname": "The University of Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628dbd0f9ec8275172da853f/YSJ_DfLPaAywMvoIoM2J4.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Spatia\uff0c\u4e00\u4e2a\u5229\u7528\u7a7a\u95f4\u8bb0\u5fc6\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u4fdd\u5b583D\u573a\u666f\u70b9\u4e91\u4f5c\u4e3a\u6301\u4e45\u7684\u7a7a\u95f4\u8bb0\u5fc6\u3002</li>\n    <li>Spatia\u901a\u8fc7\u89c6\u89c9SLAM\u4e0d\u65ad\u66f4\u65b0\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u589e\u5f3a\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u771f\u5b9e\u7684\u52a8\u6001\u5b9e\u4f53\uff0c\u540c\u65f6\u652f\u6301\u660e\u786e\u7684\u76f8\u673a\u63a7\u5236\u548c3D\u4ea4\u4e92\u7f16\u8f91\u3002</li>\n    <li>Spatia\u4e3a\u53ef\u6269\u5c55\u3001\u57fa\u4e8e\u8bb0\u5fc6\u7684\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u51e0\u4f55\u57fa\u7840\u7684\u6846\u67b6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video generation models have trouble keeping video content consistent over time and space.</li>\n    <li>We introduce Spatia, a new framework that uses a 3D point cloud as a memory to improve video generation.</li>\n    <li>Spatia generates video clips while updating this memory to ensure better spatial consistency.</li>\n    <li>The framework allows for realistic movement and offers features like camera control and 3D editing.</li>\n    <li>Spatia provides a more reliable way to create videos by focusing on both memory and dynamic elements.</li>\n</ul>"}, "publishedAt": "2025-12-17T13:59:59.000Z", "title": "Spatia: Video Generation with Updatable Spatial Memory", "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64f8962bce75bb0fb50bdbdb/eC4hVgIfk0MxzPgn6mvGC.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15716.png", "numComments": 3, "submittedBy": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "fullname": "Jinjing Zhao", "name": "Jinjing713", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "670621bc820835bbf0d2b499", "name": "Sydney-Uni", "fullname": "The University of Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628dbd0f9ec8275172da853f/YSJ_DfLPaAywMvoIoM2J4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.19995", "authors": [{"_id": "694e1a8c746a34b55dd54540", "name": "Ming Li", "hidden": false}, {"_id": "694e1a8c746a34b55dd54541", "name": "Chenrui Fan", "hidden": false}, {"_id": "694e1a8c746a34b55dd54542", "name": "Yize Cheng", "hidden": false}, {"_id": "694e1a8c746a34b55dd54543", "name": "Soheil Feizi", "hidden": false}, {"_id": "694e1a8c746a34b55dd54544", "name": "Tianyi Zhou", "hidden": false}], "publishedAt": "2025-12-23T02:44:25.000Z", "submittedOnDailyAt": "2025-12-26T02:51:58.853Z", "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "submittedOnDailyBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "isPro": false, "fullname": "Tianyi Zhou", "user": "zhoutianyi", "type": "user"}, "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "upvotes": 12, "discussionId": "694e1a8d746a34b55dd54545", "githubRepo": "https://github.com/MingLiiii/ThinkARM", "githubRepoAddedBy": "user", "githubStars": 13, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u8bc6\u522b\u548c\u5206\u6790\uff0c\u901a\u5e38\u53ea\u505c\u7559\u5728\u8868\u9762\u7edf\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86ThinkARM\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u62bd\u8c61\u4e3a\u529f\u80fd\u6027\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5982\u5206\u6790\u3001\u63a2\u7d22\u3001\u5b9e\u65bd\u548c\u9a8c\u8bc1\u3002</li>\n    <li>\u5728\u4e0d\u540c\u6a21\u578b\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\uff0c\u8fd9\u79cd\u62bd\u8c61\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u548c\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u7684\u7ed3\u6784\u5dee\u5f02\u3002</li>\n    <li>\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u63a2\u7d22\u662f\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u7684\u5173\u952e\u5206\u652f\u6b65\u9aa4\uff0c\u6548\u7387\u5bfc\u5411\u7684\u65b9\u6cd5\u4f1a\u9009\u62e9\u6027\u5730\u6291\u5236\u8bc4\u4f30\u53cd\u9988\u6b65\u9aa4\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u60c5\u8282\u7ea7\u8868\u793a\u4f7f\u63a8\u7406\u6b65\u9aa4\u53d8\u5f97\u660e\u786e\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u5206\u6790\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u7ed3\u6784\u548c\u53d8\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models show reasoning processes, but understanding their thinking steps is challenging.</li>\n    <li>We developed a framework called ThinkARM to break down reasoning into clear steps like Analyze, Explore, Implement, and Verify.</li>\n    <li>Applying this framework to mathematical problem solving helps identify differences between models that reason and those that don\u2019t.</li>\n    <li>Case studies show that exploring solutions is crucial for correct outcomes, and efficient methods may limit feedback steps.</li>\n    <li>This approach helps clarify how reasoning works in language models, making it easier to analyze their thought processes.</li>\n</ul>"}, "publishedAt": "2025-12-22T21:44:25.000Z", "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19995.png", "numComments": 4, "submittedBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "fullname": "Tianyi Zhou", "name": "zhoutianyi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19949", "authors": [{"_id": "694e0a82746a34b55dd54516", "name": "Zixuan Huang", "hidden": false}, {"_id": "694e0a82746a34b55dd54517", "name": "Xiang Li", "hidden": false}, {"_id": "694e0a82746a34b55dd54518", "name": "Zhaoyang Lv", "hidden": false}, {"_id": "694e0a82746a34b55dd54519", "name": "James M. Rehg", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/629fe0dd198a9a5a755f3ce0/4F38iBHO42lBqgkLGfGka.png"], "publishedAt": "2025-12-23T00:38:52.000Z", "submittedOnDailyAt": "2025-12-26T02:44:58.632Z", "title": "How Much 3D Do Video Foundation Models Encode?", "submittedOnDailyBy": {"_id": "629fe0dd198a9a5a755f3ce0", "avatarUrl": "/avatars/e643d43f66c10729f155edca96aef1f8.svg", "isPro": false, "fullname": "Zixuan Huang", "user": "zxhuang1698", "type": "user"}, "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.", "upvotes": 7, "discussionId": "694e0a82746a34b55dd5451a", "projectPage": "https://vidfm-3d-probe.github.io/", "organization": {"_id": "60212a089f64108326fac7c2", "name": "illinois", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u662f\u4e09\u7ef4\u4e16\u754c\u7684\u8fde\u7eed\u4e8c\u7ef4\u6295\u5f71\u3002</li>\n    <li>\u7814\u7a76\u73b0\u6709\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff08VidFMs\uff09\u5728\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\u7684\u4e09\u7ef4\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\u6765\u6d4b\u91cfVidFMs\u7684\u4e09\u7ef4\u610f\u8bc6\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf9\u4e09\u7ef4\u7269\u4f53\u548c\u573a\u666f\u6709\u5f88\u5f3a\u7684\u7406\u89e3\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u8bad\u7ec3\u7684\u4e09\u7ef4\u6a21\u578b\u3002</li>\n    <li>\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u4e09\u7ef4\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c2\u5bdf\u7ed3\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study investigates whether video models (VidFMs) trained on large amounts of video data can understand 3D environments.</li>\n    <li>It introduces a new method to assess how well these models grasp 3D properties without being specifically designed for 3D tasks.</li>\n    <li>Findings reveal that advanced video generation models have a strong understanding of 3D objects and scenes.</li>\n    <li>These models can outperform some specialized models that are trained explicitly for 3D understanding.</li>\n    <li>The research provides insights that could help in developing better 3D models in the future.</li>\n</ul>"}, "publishedAt": "2025-12-22T19:38:52.000Z", "title": "How Much 3D Do Video Foundation Models Encode?", "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/629fe0dd198a9a5a755f3ce0/4F38iBHO42lBqgkLGfGka.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19949.png", "numComments": 2, "submittedBy": {"_id": "629fe0dd198a9a5a755f3ce0", "avatarUrl": "/avatars/e643d43f66c10729f155edca96aef1f8.svg", "fullname": "Zixuan Huang", "name": "zxhuang1698", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "60212a089f64108326fac7c2", "name": "illinois", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19680", "authors": [{"_id": "694b4e0c746a34b55dd53c34", "name": "Xinyao Liao", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c35", "name": "Qiyuan He", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c36", "name": "Kai Xu", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c37", "name": "Xiaoye Qu", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c38", "name": "Yicong Li", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c39", "name": "Wei Wei", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c3a", "name": "Angela Yao", "hidden": false}], "publishedAt": "2025-12-22T18:54:30.000Z", "submittedOnDailyAt": "2025-12-26T02:44:27.188Z", "title": "VA-\u03c0: Variational Policy Alignment for Pixel-Aware Autoregressive Generation", "submittedOnDailyBy": {"_id": "64cb54da1af278541d663708", "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg", "isPro": false, "fullname": "Xiaoye Qu", "user": "Xiaoye08", "type": "user"}, "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-\u03c0, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-\u03c0 formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-\u03c0 introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-\u03c0 enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.", "upvotes": 6, "discussionId": "694b4e0c746a34b55dd53c3b", "projectPage": "https://lil-shake.github.io/va-pi.github.io/", "githubRepo": "https://github.com/Lil-Shake/VA-Pi", "githubRepoAddedBy": "user", "ai_summary": "VA-$\\pi$ optimizes autoregressive visual generators using a pixel-space objective to improve image quality and performance without retraining tokenizers or using external rewards.", "ai_keywords": ["autoregressive (AR) visual generation", "tokenizers", "discrete sequences", "evidence lower bound (ELBO)", "reinforcement-based alignment", "policy", "intrinsic reward", "teacher forcing", "distributional consistency", "FID", "IS", "LlamaGen-XXL", "GenEval", "LlamaGen", "Janus-Pro"], "githubStars": 4, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVA-\u03c0\u7684\u8f7b\u91cf\u7ea7\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>VA-\u03c0\u901a\u8fc7\u53d8\u5206\u4f18\u5316\u65b9\u6cd5\u76f4\u63a5\u5bf9\u9f50\u751f\u6210\u5668\u548c\u5206\u8bcd\u5668\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u50cf\u7d20\u7a7a\u95f4\u91cd\u5efa\u8d28\u91cf\u4f5c\u4e3a\u5185\u90e8\u5956\u52b1\uff0c\u6307\u5bfc\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002</li>\n    <li>\u5728\u4ec5\u4f7f\u75281%\u7684ImageNet-1K\u6570\u636e\u548c25\u5206\u949f\u7684\u8c03\u4f18\u65f6\u95f4\u5185\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n    <li>VA-\u03c0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5206\u8bcd\u5668\u6216\u4f7f\u7528\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u5feb\u901f\u9002\u5e94\u73b0\u6709\u7684\u81ea\u56de\u5f52\u751f\u6210\u5668\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive visual generation uses tokenizers to convert images into sequences, but this can lead to low-quality images due to misalignment between tokens and image quality.</li>\n    <li>The proposed method, VA-\u03c0, improves AR models by directly optimizing them for image quality using principles from pixel-space objectives.</li>\n    <li>VA-\u03c0 aligns the generator and tokenizer through a reinforcement learning approach, rewarding the model based on how well it reconstructs images from token sequences.</li>\n    <li>This method allows for quick adaptation of existing AR generators without the need to retrain tokenizers or use external reward models.</li>\n    <li>VA-\u03c0 significantly enhances image generation performance, achieving notable improvements in quality metrics with minimal data and tuning time.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:54:30.000Z", "title": "VA-\u03c0: Variational Policy Alignment for Pixel-Aware Autoregressive Generation", "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-\u03c0, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-\u03c0 formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-\u03c0 introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-\u03c0 enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19680.png", "numComments": 2, "submittedBy": {"_id": "64cb54da1af278541d663708", "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg", "fullname": "Xiaoye Qu", "name": "Xiaoye08", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13043", "authors": [{"_id": "694df96e746a34b55dd544df", "name": "Tong Wei", "hidden": false}, {"_id": "694df96e746a34b55dd544e0", "name": "Yijun Yang", "hidden": false}, {"_id": "694df96e746a34b55dd544e1", "name": "Changhao Zhang", "hidden": false}, {"_id": "694df96e746a34b55dd544e2", "name": "Junliang Xing", "hidden": false}, {"_id": "694df96e746a34b55dd544e3", "name": "Yuanchun Shi", "hidden": false}, {"_id": "694df96e746a34b55dd544e4", "name": "Zongqing Lu", "hidden": false}, {"_id": "694df96e746a34b55dd544e5", "name": "Deheng Ye", "hidden": false}], "publishedAt": "2025-12-15T07:11:56.000Z", "submittedOnDailyAt": "2025-12-26T00:32:37.057Z", "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "submittedOnDailyBy": {"_id": "66c2e8b4a03b764ca9057e65", "avatarUrl": "/avatars/ce17d68f02a08c148ebb9df3170812c8.svg", "isPro": false, "fullname": "Tong Wei", "user": "weit123", "type": "user"}, "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "upvotes": 3, "discussionId": "694df96e746a34b55dd544e6", "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u591a\u56de\u5408\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u5956\u52b1\u7a00\u758f\u548c\u957f\u671f\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002</li>\n    <li>\u6700\u8fd1\u7684\u65b9\u6cd5\u901a\u8fc7\u8001\u5e08\u6a21\u578b\u63d0\u4f9b\u9010\u6b65\u53cd\u9988\u6765\u589e\u52a0\u5956\u52b1\u5bc6\u5ea6\uff0c\u4f46\u4f9d\u8d56\u6602\u8d35\u7684\u8001\u5e08\u6a21\u578b\uff0c\u5f71\u54cd\u5b9e\u7528\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GTR-Turbo\uff0c\u5b83\u5728\u4e0d\u9700\u8981\u6602\u8d35\u8001\u5e08\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347\u4e86GTR\u7684\u6027\u80fd\u3002</li>\n    <li>GTR-Turbo\u5408\u5e76\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u4ea7\u751f\u7684\u68c0\u67e5\u70b9\u6743\u91cd\uff0c\u4f5c\u4e3a\u201c\u514d\u8d39\u201d\u8001\u5e08\u8fdb\u884c\u540e\u7eed\u6307\u5bfc\u3002</li>\n    <li>\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0cGTR-Turbo\u5c06\u57fa\u7ebf\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e8610-30%\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8650%\uff0c\u8ba1\u7b97\u6210\u672c\u51cf\u5c11\u4e8660%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-turn reinforcement learning (RL) for agents that understand both vision and language faces challenges like sparse rewards.</li>\n    <li>Previous methods used expensive teacher models for feedback, making them hard to use and reproduce.</li>\n    <li>GTR-Turbo is a new method that improves performance without needing a costly teacher model.</li>\n    <li>It combines weights from different training checkpoints to create a \"free\" teacher model for guiding RL training.</li>\n    <li>GTR-Turbo boosts accuracy by 10-30% and cuts training time and costs significantly compared to earlier methods.</li>\n</ul>"}, "publishedAt": "2025-12-15T02:11:56.000Z", "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13043.png", "numComments": 2, "submittedBy": {"_id": "66c2e8b4a03b764ca9057e65", "avatarUrl": "/avatars/ce17d68f02a08c148ebb9df3170812c8.svg", "fullname": "Tong Wei", "name": "weit123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u4f7f\u7528VAE\u7a7a\u95f4\u5b66\u4e60\u89c6\u9891\u6f5c\u5728\u5206\u5e03\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u6548\u7387\u4f4e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SemanticGen\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new method for generating videos that addresses issues with speed and computational cost in existing models.</li>\n    <li>Instead of starting with detailed video tokens, it begins in a high-level semantic space for better overall planning.</li>\n    <li>The process has two stages: first, it creates a global layout of the video using a diffusion model, then it adds details with another diffusion model.</li>\n    <li>This approach leads to faster video generation and is more efficient for long videos compared to traditional methods.</li>\n    <li>Tests show that SemanticGen creates high-quality videos and performs better than current leading models.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 53, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u65f6\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8f85\u52a9\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u6765\u5f15\u5bfc\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u6709\u5c40\u9650\u6027\u4e14\u6210\u672c\u9ad8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u673a\u5236\uff0c\u5e2e\u52a9\u6a21\u578b\u81ea\u4e3b\u53d1\u73b0\u5e76\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u63d0\u53d6\u56fe\u50cf\u4e2d\u7684\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6848\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text and struggle with visual reasoning tasks.</li>\n    <li>Current solutions use helper images and other visual aids, but these methods have limitations and require a lot of manual work.</li>\n    <li>We propose a new method that helps LMMs learn to use visual reasoning on their own, without needing specific guidance.</li>\n    <li>This method allows the model to better understand and use visual information for different tasks.</li>\n    <li>Our approach shows better performance than traditional methods and works well across various vision-related challenges.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 4, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.19693", "authors": [{"_id": "694a0ffa335742716e93227d", "name": "Weichen Fan", "hidden": false}, {"_id": "694a0ffa335742716e93227e", "name": "Haiwen Diao", "hidden": false}, {"_id": "694a0ffa335742716e93227f", "name": "Quan Wang", "hidden": false}, {"_id": "694a0ffa335742716e932280", "name": "Dahua Lin", "hidden": false}, {"_id": "694a0ffa335742716e932281", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-22T18:59:57.000Z", "submittedOnDailyAt": "2025-12-23T01:15:14.379Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "upvotes": 51, "discussionId": "694a0ffa335742716e932282", "githubRepo": "https://github.com/WeichenFan/UAE", "githubRepoAddedBy": "user", "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.", "ai_keywords": ["spectral characteristics", "semantic encoders", "pixel encoders", "feature spectrum", "low-frequency components", "high-frequency information", "Prism Hypothesis", "Unified Autoencoding", "frequency-band modulator", "ImageNet", "MS-COCO", "latent space"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u4e49\u7f16\u7801\u5668\u548c\u50cf\u7d20\u7f16\u7801\u5668\u7684\u9891\u8c31\u7279\u5f81\u3002</li>\n    <li>\u53d1\u73b0\u8bed\u4e49\u7f16\u7801\u5668\u4e3b\u8981\u6355\u6349\u4f4e\u9891\u6210\u5206\uff0c\u8868\u793a\u62bd\u8c61\u610f\u4e49\uff1b\u800c\u50cf\u7d20\u7f16\u7801\u5668\u8fd8\u4fdd\u7559\u9ad8\u9891\u4fe1\u606f\uff0c\u4f20\u8fbe\u7ec6\u8282\u3002</li>\n    <li>\u63d0\u51fa\u201c\u68f1\u955c\u5047\u8bbe\u201d\uff0c\u8ba4\u4e3a\u6bcf\u79cd\u6570\u636e\u6a21\u6001\u53ef\u4ee5\u770b\u4f5c\u662f\u81ea\u7136\u754c\u5728\u5171\u4eab\u7279\u5f81\u9891\u8c31\u4e0a\u7684\u6295\u5f71\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u63d0\u51fa\u7edf\u4e00\u81ea\u7f16\u7801\u6a21\u578b\uff08UAE\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u9891\u5e26\u8c03\u5236\u5668\u7ed3\u5408\u8bed\u4e49\u7ed3\u6784\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n    <li>\u5728ImageNet\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUAE\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u6210\u529f\u7edf\u4e00\u4e86\u8bed\u4e49\u62bd\u8c61\u548c\u50cf\u7d20\u7ea7\u7ec6\u8282\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Different types of encoders (semantic and pixel) capture different types of information from data.</li>\n  <li>Semantic encoders focus on low-frequency information related to abstract meanings.</li>\n  <li>Pixel encoders capture both low and high-frequency information, allowing for detailed imagery.</li>\n  <li>The paper introduces the Prism Hypothesis, suggesting that data can be viewed as projections on a shared spectrum.</li>\n  <li>A new model called Unified Autoencoding (UAE) combines semantic and pixel information effectively, achieving high performance in tests.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:59:57.000Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19673", "authors": [{"_id": "694ac3ad746a34b55dd53b6c", "name": "Yuqiao Tan", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6d", "name": "Minzheng Wang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6e", "name": "Shizhu He", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6f", "name": "Huanxuan Liao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b70", "name": "Chengfeng Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b71", "name": "Qiunan Lu", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b72", "name": "Tian Liang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b73", "name": "Jun Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b74", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-22T18:51:48.000Z", "submittedOnDailyAt": "2025-12-24T00:28:45.252Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "submittedOnDailyBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "isPro": false, "fullname": "mz.w", "user": "iiiiwis", "type": "user"}, "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "upvotes": 49, "discussionId": "694ac3ad746a34b55dd53b75", "githubRepo": "https://github.com/Trae1ounG/BuPO", "githubRepoAddedBy": "user", "ai_summary": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.", "ai_keywords": ["reinforcement learning", "large language models", "Transformer residual stream", "unembedding matrix", "Internal Layer Policies", "Internal Modular Policies", "self-attention", "feed-forward network", "entropy", "Bottom-up Policy Optimization"], "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u7edf\u4e00\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u5176\u5185\u90e8\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u5206\u6790Transformer\u7684\u6b8b\u5dee\u6d41\uff0c\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u5185\u90e8\u5c42\u7b56\u7565\u548c\u5185\u90e8\u6a21\u5757\u7b56\u7565\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u65e9\u671f\u5c42\u4fdd\u6301\u9ad8\u71b5\u4ee5\u8fdb\u884c\u63a2\u7d22\uff0c\u800c\u9876\u90e8\u5c42\u8d8b\u5411\u4e8e\u96f6\u71b5\u4ee5\u8fdb\u884c\u7cbe\u7ec6\u8c03\u6574\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5Bottom-up Policy Optimization (BuPO)\uff0c\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u5c42\u7b56\u7565\u3002</li>\n    <li>\u901a\u8fc7\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Current reinforcement learning methods treat large language models as a single unit, missing the details of how they work internally.</li>\n  <li>This paper breaks down language model policies to understand contributions from different layers and components.</li>\n  <li>It identifies Internal Layer Policies and Internal Modular Policies based on how each layer and component behaves.</li>\n  <li>Findings show that early layers focus on exploration, while top layers refine outputs, with different models showing unique convergence patterns.</li>\n  <li>The authors introduce a new training method called Bottom-up Policy Optimization (BuPO) that improves performance by optimizing internal policies early in training.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:51:48.000Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19673.png", "numComments": 4, "submittedBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "fullname": "mz.w", "name": "iiiiwis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20605", "authors": [{"_id": "694e76e5746a34b55dd545eb", "name": "Seijin Kobayashi", "hidden": false}, {"_id": "694e76e5746a34b55dd545ec", "name": "Yanick Schimpf", "hidden": false}, {"_id": "694e76e5746a34b55dd545ed", "name": "Maximilian Schlegel", "hidden": false}, {"_id": "694e76e5746a34b55dd545ee", "name": "Angelika Steger", "hidden": false}, {"_id": "694e76e5746a34b55dd545ef", "name": "Maciej Wolczyk", "hidden": false}, {"_id": "694e76e5746a34b55dd545f0", "name": "Johannes von Oswald", "hidden": false}, {"_id": "694e76e5746a34b55dd545f1", "name": "Nino Scherrer", "hidden": false}, {"_id": "694e76e5746a34b55dd545f2", "name": "Kaitlin Maile", "hidden": false}, {"_id": "694e76e5746a34b55dd545f3", "name": "Guillaume Lajoie", "hidden": false}, {"_id": "694e76e5746a34b55dd545f4", "name": "Blake A. Richards", "hidden": false}, {"_id": "694e76e5746a34b55dd545f5", "name": "Rif A. Saurous", "hidden": false}, {"_id": "694e76e5746a34b55dd545f6", "name": "James Manyika", "hidden": false}, {"_id": "694e76e5746a34b55dd545f7", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "694e76e5746a34b55dd545f8", "name": "Alexander Meulemans", "hidden": false}, {"_id": "694e76e5746a34b55dd545f9", "name": "Jo\u00e3o Sacramento", "hidden": false}], "publishedAt": "2025-12-23T18:51:50.000Z", "submittedOnDailyAt": "2025-12-26T11:17:05.505Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "submittedOnDailyBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "upvotes": 48, "discussionId": "694e76e5746a34b55dd545fa", "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u4e0b\u4e00\u8bcd\u9884\u6d4b\u9884\u8bad\u7ec3\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\uff0c\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\u3002</li>\n    <li>\u5728RL\u8fc7\u7a0b\u4e2d\uff0c\u8fd9\u4e9b\u6a21\u578b\u9010\u4e2a\u751f\u6210\u65b0\u8f93\u51fa\uff0c\u4f46\u8fd9\u79cd\u9010\u4e2a\u91c7\u6837\u7684\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u7a00\u758f\u65f6\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u5728\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u8fdb\u884c\u63a2\u7d22\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u9636\u975e\u56e0\u679c\u5e8f\u5217\u6a21\u578b\uff0c\u80fd\u591f\u63a7\u5236\u57fa\u7840\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6fc0\u6d3b\uff0c\u5e2e\u52a9\u53d1\u73b0\u65f6\u95f4\u62bd\u8c61\u884c\u4e3a\u3002</li>\n    <li>\u5185\u90e8\u63a7\u5236\u5668\u7684\u5f3a\u5316\u5b66\u4e60\uff08\u79f0\u4e3a\u201c\u5185\u90e8RL\u201d\uff09\u4f7f\u5f97\u5728\u6807\u51c6RL\u5fae\u8c03\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u7a00\u758f\u5956\u52b1\u4e2d\u5b66\u4e60\u6210\u4e3a\u53ef\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large autoregressive models have improved performance through next-token prediction and reinforcement learning (RL).</li>\n    <li>Generating outputs token-by-token can be slow and inefficient, especially with sparse rewards.</li>\n    <li>We propose a new method using internal representations of the model to find more effective actions.</li>\n    <li>This involves a higher-order model that helps control the base autoregressive model's outputs.</li>\n    <li>Our approach, called \"internal RL,\" allows better learning from sparse rewards and supports hierarchical reinforcement learning.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:51:50.000Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png", "numComments": 3, "submittedBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "fullname": "Maximilian Schlegel", "name": "schlegelm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4e00\u822c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u4e0a\u8f83\u5f31\u3002</li>\n    <li>\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86DSR Suite\uff0c\u5305\u62ec\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u95ee\u7b54\u751f\u6210\u7ba1\u9053\uff0c\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u751f\u6210\u591a\u9009\u9898\u3002</li>\n    <li>\u8be5\u7ba1\u9053\u63d0\u53d6\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u5982\u76f8\u673a\u59ff\u6001\u3001\u5c40\u90e8\u70b9\u4e91\u548c3D\u8f68\u8ff9\uff0c\u652f\u6301DSR-Train\u7684\u5b66\u4e60\u548cDSR-Bench\u7684\u8bc4\u4f30\u3002</li>\n    <li>\u65b0\u6570\u636e\u96c6\u5f3a\u8c03\u771f\u5b9e\u89c6\u9891\u6765\u6e90\u30013D\u9700\u6c42\u3001\u89c6\u89d2\u53d8\u5316\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u7ec6\u81f4\u7684\u7a0b\u5e8f\u6027\u7b54\u6848\u3002</li>\n    <li>\u901a\u8fc7\u8f7b\u91cf\u7ea7\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\u5c06\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u878d\u5165VLM\uff0c\u5b9e\u9a8c\u663e\u793a\u80fd\u663e\u8457\u63d0\u5347\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u822c\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) are good at general understanding but struggle with dynamic spatial reasoning (DSR).</li>\n    <li>The DSR Suite is introduced to improve DSR by creating question-answer pairs from real-world videos.</li>\n    <li>This suite uses advanced vision models to gather important geometric and motion information from videos.</li>\n    <li>It focuses on various aspects such as real-world videos, 3D object requirements, viewpoint changes, and interactions between multiple objects.</li>\n    <li>A new Geometry Selection Module (GSM) helps VLMs use relevant geometric information without being overloaded with unnecessary data.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20618", "authors": [{"_id": "694ba02a746a34b55dd53e8b", "name": "Runtao Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8c", "name": "Ziyi Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8d", "name": "Jiaqi Tang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8e", "name": "Yue Ma", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8f", "name": "Renjie Pi", "hidden": false}, {"_id": "694ba02a746a34b55dd53e90", "name": "Jipeng Zhang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e91", "name": "Qifeng Chen", "hidden": false}], "publishedAt": "2025-12-23T18:59:49.000Z", "submittedOnDailyAt": "2025-12-24T05:57:23.776Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "submittedOnDailyBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "isPro": true, "fullname": "Jiaqi Tang", "user": "Jiaqi-hkust", "type": "user"}, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "upvotes": 38, "discussionId": "694ba02a746a34b55dd53e92", "ai_summary": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.", "ai_keywords": ["multimodal LLMs", "long-video QA", "multi-agent framework", "grounding agent", "vision agent", "reinforcement learning", "temporal grounding", "LongTVQA", "LongTVQA+"], "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7531\u4e00\u4e2a\u4e3b\u667a\u80fd\u4f53\u534f\u8c03\u5176\u4ed6\u667a\u80fd\u4f53\u6765\u5904\u7406\u957f\u89c6\u9891\u95ee\u7b54\u3002</li>\n    <li>\u4e3b\u667a\u80fd\u4f53\u8d1f\u8d23\u5b9a\u4f4d\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u7247\u6bb5\uff0c\u5e76\u63d0\u53d6\u5fc5\u8981\u7684\u6587\u672c\u4fe1\u606f\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u9f13\u52b1\u667a\u80fd\u4f53\u4e4b\u95f4\u9ad8\u6548\u5408\u4f5c\u5e76\u51c6\u786e\u805a\u7126\u4e8e\u76f8\u5173\u5185\u5bb9\u3002</li>\n    <li>\u5728LongTVQA\u548cLongTVQA+\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New multimodal LLMs can help answer questions about long videos, but many methods lose details or use limited tools.</li>\n    <li>We suggest a multi-agent system where a master agent coordinates other agents to find and analyze relevant video segments.</li>\n    <li>The master agent uses reinforcement learning to improve its decision-making and teamwork.</li>\n    <li>This approach helps focus on important video clips and enhances understanding with visual details.</li>\n    <li>Our system performs better than existing methods on new datasets and shows that learning improves its reasoning and planning skills.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:49.000Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png", "numComments": 1, "submittedBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "fullname": "Jiaqi Tang", "name": "Jiaqi-hkust", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20617", "authors": [{"_id": "694b58e3746a34b55dd53cff", "name": "Yuxi Xiao", "hidden": false}, {"_id": "694b58e3746a34b55dd53d00", "name": "Longfei Li", "hidden": false}, {"_id": "694b58e3746a34b55dd53d01", "name": "Shen Yan", "hidden": false}, {"_id": "694b58e3746a34b55dd53d02", "name": "Xinhang Liu", "hidden": false}, {"_id": "694b58e3746a34b55dd53d03", "name": "Sida Peng", "hidden": false}, {"_id": "694b58e3746a34b55dd53d04", "name": "Yunchao Wei", "hidden": false}, {"_id": "694b58e3746a34b55dd53d05", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "694b58e3746a34b55dd53d06", "name": "Bingyi Kang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "publishedAt": "2025-12-23T18:59:46.000Z", "submittedOnDailyAt": "2025-12-24T00:38:28.003Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "upvotes": 34, "discussionId": "694b58e4746a34b55dd53d07", "projectPage": "https://spatialtree.github.io/", "ai_summary": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.", "ai_keywords": ["SpatialTree", "low-level perception", "mental mapping", "simulation", "agentic competence", "capability-centric hierarchical benchmark", "targeted supervised fine-tuning", "negative transfer", "cross-level transfer", "naive RL", "auto-think strategy"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SpatialTree\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u7a7a\u95f4\u80fd\u529b\u5c42\u6b21\u7ed3\u6784\uff0c\u5305\u62ec\u56db\u4e2a\u7b49\u7ea7\uff1a\u4f4e\u7ea7\u611f\u77e5\uff08L1\uff09\u3001\u5fc3\u7406\u6620\u5c04\uff08L2\uff09\u3001\u6a21\u62df\uff08L3\uff09\u548c\u884c\u4e3a\u80fd\u529b\uff08L4\uff09\u3002</li>\n    <li>\u901a\u8fc7\u8fd9\u4e2a\u5c42\u6b21\u7ed3\u6784\uff0c\u6211\u4eec\u8bc4\u4f30\u4e8627\u79cd\u5b50\u80fd\u529b\uff0c\u53d1\u73b0L1\u6280\u80fd\u76f8\u4e92\u72ec\u7acb\uff0c\u800c\u66f4\u9ad8\u5c42\u6b21\u7684\u6280\u80fd\u4e4b\u95f4\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002</li>\n    <li>\u5728\u6709\u9488\u5bf9\u6027\u7684\u76d1\u7763\u5fae\u8c03\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0L1\u5b58\u5728\u8d1f\u8fc1\u79fb\uff0c\u4f46\u4f4e\u5c42\u6b21\u548c\u9ad8\u5c42\u6b21\u6280\u80fd\u4e4b\u95f4\u6709\u5f88\u5f3a\u7684\u6b63\u8fc1\u79fb\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u63d0\u9ad8\u6240\u6709\u5c42\u6b21\u7684\u8868\u73b0\uff0c\u4f46\u9700\u8981\u6291\u5236\u4e0d\u5fc5\u8981\u7684\u601d\u8003\u3002</li>\n    <li>SpatialTree\u4e3a\u7406\u89e3\u548c\u7cfb\u7edf\u6027\u63d0\u5347\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Cognitive science shows that spatial abilities grow from basic perception to complex reasoning.</li>\n    <li>The study introduces a framework called SpatialTree, which organizes spatial abilities into four levels: perception, mental mapping, simulation, and agentic competence.</li>\n    <li>They created a benchmark to evaluate mainstream multimodal large language models (MLLMs) across 27 different spatial skills.</li>\n    <li>The results showed that lower-level skills are independent, while higher-level skills depend on each other.</li>\n    <li>A new strategy called auto-think was proposed to improve performance across all levels of spatial abilities in MLLMs.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:46.000Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19134", "authors": [{"_id": "694a1765335742716e9322b7", "name": "Dehai Min", "hidden": false}, {"_id": "694a1765335742716e9322b8", "name": "Kailin Zhang", "hidden": false}, {"_id": "694a1765335742716e9322b9", "name": "Tongtong Wu", "hidden": false}, {"_id": "694a1765335742716e9322ba", "name": "Lu Cheng", "hidden": false}], "publishedAt": "2025-12-22T08:28:05.000Z", "submittedOnDailyAt": "2025-12-23T01:46:57.477Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "submittedOnDailyBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "isPro": false, "fullname": "Dehai Min", "user": "ZhishanQ", "type": "user"}, "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "upvotes": 25, "discussionId": "694a1765335742716e9322bb", "githubRepo": "https://github.com/ZhishanQ/QuCo-RAG", "githubRepoAddedBy": "user", "ai_summary": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.", "ai_keywords": ["dynamic retrieval-augmented generation", "large language models", "hallucinations", "model-internal signals", "logits", "entropy", "pre-training data", "uncertainty quantification", "low-frequency entities", "entity co-occurrence", "Infini-gram", "multi-hop QA", "EM gains", "OLMo-2", "Llama", "Qwen", "GPT", "biomedical QA", "domain generalization", "corpus-grounded verification"], "githubStars": 6, "summary_zh": "<ul>\n    <li>QuCo-RAG\u662f\u4e00\u79cd\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u4f55\u65f6\u8fdb\u884c\u68c0\u7d22\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u73b0\u8c61\u3002</li>\n    <li>\u4e0e\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\u4e0d\u540c\uff0cQuCo-RAG\u4f7f\u7528\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u8ba1\u7b97\u7684\u5ba2\u89c2\u7edf\u8ba1\u6570\u636e\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u8bc6\u522b\u4e0d\u786e\u5b9a\u6027\uff1a\u5728\u751f\u6210\u524d\u8bc6\u522b\u4f4e\u9891\u5b9e\u4f53\uff0c\u751f\u6210\u8fc7\u7a0b\u4e2d\u9a8c\u8bc1\u5b9e\u4f53\u5171\u73b0\u60c5\u51b5\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQuCo-RAG\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e865\u523012\u5206\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u4e5f\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9886\u57df\u7684\u9886\u57df\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u7a33\u5065\u6027\uff0c\u4e14\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QuCo-RAG is a new method that improves how large language models (LLMs) decide when to look up information while generating text to reduce errors.</li>\n    <li>Instead of relying on the model's own confidence, which can be unreliable, QuCo-RAG uses objective data from its training to measure uncertainty.</li>\n    <li>The method checks for rare entities before generating text and verifies their presence in training data during generation to identify potential errors.</li>\n    <li>Tests show that QuCo-RAG significantly outperforms existing methods, improving accuracy for various models by up to 14 points in certain tasks.</li>\n    <li>This approach is robust and works well across different domains, including biomedical questions, and the code is available for public use.</li>\n</ul>"}, "publishedAt": "2025-12-22T03:28:05.000Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19134.png", "numComments": 2, "submittedBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "fullname": "Dehai Min", "name": "ZhishanQ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u201c\u4e00\u955c\u5230\u5e95\u201d\u6280\u672f\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5177\u6709\u72ec\u7279\u7684\u7f8e\u5b66\uff0c\u4f46\u5b9e\u73b0\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002</li>\n    <li>DreaMontage\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7528\u6237\u8f93\u5165\u751f\u6210\u8fde\u8d2f\u4e14\u751f\u52a8\u7684\u957f\u65f6\u95f4\u201c\u4e00\u955c\u5230\u5e95\u201d\u89c6\u9891\u3002</li>\n    <li>\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u8c03\u8282\u673a\u5236\u548c\u81ea\u9002\u5e94\u8c03\u4f18\u7b56\u7565\u6765\u589e\u5f3a\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>\u4f7f\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u89c6\u89c9\u8868\u8fbe\u9636\u6bb5\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u53d9\u4e8b\u6d41\u7545\u6027\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6bb5\u843d\u81ea\u56de\u5f52\u63a8\u65ad\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" filmmaking technique is complex and expensive but offers a unique aesthetic.</li>\n    <li>DreaMontage is a new system that creates smooth, long one-shot videos from various user inputs.</li>\n    <li>The system uses a special method to control video frames and improve visual quality.</li>\n    <li>It addresses challenges like motion realism and smooth transitions to enhance the final output.</li>\n    <li>Experiments show that DreaMontage produces high-quality videos efficiently, allowing users to create cohesive cinematic experiences.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u667a\u80fd\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u6587\u672c\u5904\u7406\u7684\u6548\u679c\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cDeepSeek-V3.2\u5728\u6027\u80fd\u4e0a\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u672c\u8d85\u8d8a\u4e86GPT-5\uff0c\u63a8\u7406\u80fd\u529b\u4e0eGemini-3.0-Pro\u76f8\u5f53\u3002</li>\n    <li>\u8be5\u6a21\u578b\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\uff08IMO\uff09\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\uff08IOI\uff09\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u63d0\u5347\u4e86\u5728\u590d\u6742\u4e92\u52a8\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines fast computing with strong reasoning and performance for agents.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which reduces computing needs while maintaining performance, especially with long texts.</li>\n    <li>The model uses a scalable reinforcement learning system, allowing it to perform similarly to GPT-5 and even better with its high-compute version.</li>\n    <li>DeepSeek-V3.2-Speciale outperforms GPT-5 and shows excellent reasoning skills, winning gold medals in major international math and informatics competitions.</li>\n    <li>It includes a new pipeline for creating training data, improving the model's ability to reason and follow instructions in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u7531\u4e8e\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u5b58\u5728\u8bb8\u591a\u95ee\u9898\uff0c\u5982\u7f3a\u4e4f\u89c4\u8303\u548c\u53ef\u91cd\u590d\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u4e86\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7ba1\u9053\uff0c\u6db5\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u591a\u4e2a\u9886\u57df\u3002</li>\n    <li>DataFlow-Agent\u53ef\u4ee5\u81ea\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u8bf4\u660e\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\uff0c\u63d0\u9ad8\u4e86\u7528\u6237\u53cb\u597d\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4f7f\u7528\u6848\u4f8b\u4e2d\uff0cDataFlow\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The demand for high-quality data for Large Language Models (LLMs) is increasing, but current data preparation methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create better data preparation processes, allowing for reusable and flexible data transformations.</li>\n    <li>It offers a user-friendly interface similar to PyTorch, making it easier to build and debug data pipelines.</li>\n    <li>DataFlow includes around 200 reusable tools and supports various tasks, such as text processing and code generation.</li>\n    <li>Tests show that DataFlow improves LLM performance significantly, outperforming existing datasets and providing a strong foundation for future AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u8ba1\u7b97\u901f\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLive Avatar\u7684\u7b97\u6cd5\u7cfb\u7edf\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u3002</li>\n    <li>\u91c7\u7528\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u6d41\u6c34\u7ebf\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u63d0\u5347\u4e86\u591aGPU\u5904\u7406\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002</li>\n    <li>\u5f15\u5165\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\u4ee5\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u63a8\u8fdb\u4e86\u5de5\u4e1a\u89c6\u9891\u5408\u6210\u7684\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system that allows for real-time, high-quality video generation of avatars using advanced technology.</li>\n    <li>It uses a special method called Timestep-forcing Pipeline Parallelism (TPP) to speed up the process by working across multiple GPUs.</li>\n    <li>To improve video consistency and reduce errors, it features a technique called the Rolling Sink Frame Mechanism (RSFM) that adjusts the avatar's look using a reference image.</li>\n    <li>The system can produce 20 frames per second using powerful GPUs, achieving high-quality results quickly.</li>\n    <li>Live Avatar sets a new standard for using advanced models in creating long videos in various industries.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u73b0\u5b9e\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\uff08DE\uff09\u4efb\u52a1\u6d89\u53ca\u6784\u5efa\u590d\u6742\u7684SQL\u7ba1\u9053\u548c\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\uff08DA\uff09\u4efb\u52a1\u8981\u6c42\u89e3\u51b3\u5f00\u653e\u6027\u5546\u4e1a\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u53ef\u884c\u7684\u5efa\u8bae\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u7684\u6280\u672f\u5728DAComp\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u4e3a\u5f00\u53d1\u771f\u6b63\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u5e76\u6539\u5584\u73b0\u6709\u7684\u5c40\u9650\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks that simulate real-world data intelligence workflows in businesses.</li>\n    <li>It includes data engineering tasks that involve creating and improving SQL pipelines, and data analysis tasks that require solving complex business problems.</li>\n    <li>Performance on these tasks is low, especially for data engineering with success rates below 20%, showing a major challenge in managing data processes.</li>\n    <li>Data analysis tasks also scored below 40%, indicating issues with open-ended problem-solving and reasoning skills.</li>\n    <li>DAComp helps identify these challenges, aiming to improve the development of autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u5206\u5f00\u7684\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u7247\u548c\u89c6\u9891\u80cc\u666f\uff0c\u80fd\u521b\u5efa\u7535\u5f71\u7ea7\u522b\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u3002</li>\n    <li>Kling-Omni \u5728\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Kling-Omni is a new framework for creating high-quality videos from various visual and language inputs.</li>\n  <li>It combines video generation, editing, and reasoning into one system, unlike other methods that treat these tasks separately.</li>\n  <li>The framework accepts different types of user inputs, like text, images, and videos, to create unified video content.</li>\n  <li>Kling-Omni uses advanced data systems and efficient training methods to enhance its performance in video creation.</li>\n  <li>It shows strong abilities in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u5e26\u6765\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7cbe\u786e\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7c97\u7cd9\u63a7\u5236\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u4f7f\u573a\u666f\u63a7\u5236\u66f4\u52a0\u7ec6\u81f4\u3002</li>\n    <li>Wan-Move\u80fd\u591f\u751f\u62105\u79d2\u7684480p\u89c6\u9891\uff0c\u5176\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0e\u5546\u4e1a\u8f6f\u4ef6Kling 1.5 Pro\u7684Motion Brush\u76f8\u5f53\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540d\u4e3aMoveBench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u8fd0\u52a8\u8d28\u91cf\uff0c\u5e76\u4e14\u6240\u6709\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u90fd\u5df2\u516c\u5f00\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over how objects move in videos, addressing limitations of existing methods.</li>\n    <li>The framework uses dense point trajectories to represent object motions and update video features accordingly.</li>\n    <li>Wan-Move can be easily integrated into existing video models without changing their architecture and allows for scalable fine-tuning.</li>\n    <li>A new benchmark called MoveBench has been created to evaluate motion quality, showing that Wan-Move produces high-quality videos.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01816", "authors": [{"_id": "692e5c0537312eaa83fd87b8", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:43.760Z", "hidden": false}, {"_id": "692e5c0537312eaa83fd87b9", "name": "Siyuan Li", "hidden": false}, {"_id": "692e5c0537312eaa83fd87ba", "name": "Conghui He", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bb", "name": "Lijun Wu", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bc", "user": {"_id": "64be296a46cc3cdfbb057f7e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg", "isPro": false, "fullname": "Cheng Tan", "user": "chengtan9907", "type": "user"}, "name": "Cheng Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:41.755Z", "hidden": false}], "publishedAt": "2025-12-01T15:52:31.000Z", "submittedOnDailyAt": "2025-12-02T01:31:46.625Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "submittedOnDailyBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "upvotes": 87, "discussionId": "692e5c0537312eaa83fd87bd", "projectPage": "https://opendatalab-raiser.github.io/Envision/", "githubRepo": "https://github.com/opendatalab-raiser/Envision", "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.", "ai_keywords": ["multimodal models", "text-to-image (T2I)", "causal event progression", "spatiotemporal causality", "Envision-a", "Envision-Score", "multi-dimensional consistency", "physicality", "aesthetics", "causal narrative coherence", "spatiotemporal consistency", "multi-frame reasoning", "dynamic world modeling"], "githubStars": 27, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bd5\u56fe\u901a\u8fc7\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6765\u514b\u670d\u5355\u4e00\u6a21\u6001\u8868\u793a\u7684\u5c40\u9650\u6027\uff0c\u901a\u5e38\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u7684\u4efb\u52a1\u6765\u6821\u51c6\u8bed\u4e49\u4e00\u81f4\u6027\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u4f9d\u8d56\u9759\u6001\u5355\u56fe\u50cf\u751f\u6210\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u968f\u65f6\u95f4\u53d8\u5316\u7684\u52a8\u6001\u8fc7\u7a0b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Envision\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u94fe\u5f0f\u6587\u672c\u5230\u591a\u56fe\u50cf\u751f\u6210\uff0c\u5305\u542b1000\u4e2a\u8de8\u516d\u4e2a\u9886\u57df\u7684\u56db\u9636\u6bb5\u63d0\u793a\u3002</li>\n    <li>\u5f15\u5165Envision-Score\u4f5c\u4e3a\u5168\u9762\u6307\u6807\uff0c\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e16\u754c\u77e5\u8bc6\uff0c\u5e76\u9075\u5faa\u56e0\u679c\u65f6\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u4e13\u95e8\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7f8e\u5b66\u8868\u73b0\u4e0a\u4f18\u79c0\uff0c\u4f46\u7f3a\u4e4f\u5185\u5728\u7684\u4e16\u754c\u77e5\u8bc6\uff1b\u800c\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u8ff0\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u9762\u4e34\u65f6\u7a7a\u4e00\u81f4\u6027\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current multimodal models combine text and images but struggle with dynamic processes because they focus on single images.</li>\n    <li>To improve this, a new benchmark called Envision is proposed for generating multiple images from text, based on cause-and-effect relationships.</li>\n    <li>Envision includes 1,000 prompts from various fields and measures how well models understand and generate sequences of images.</li>\n    <li>The study introduces Envision-Score, a new metric to evaluate models on consistency, realism, and aesthetics.</li>\n    <li>Results show that while specialized models excel in aesthetics, unified models are better at understanding narratives, but all still face challenges with consistency over time.</li>\n</ul>"}, "publishedAt": "2025-12-01T10:52:31.000Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png", "numComments": 4, "submittedBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "fullname": "Juanxi Tian", "name": "Juanxi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13}, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u89c6\u89c9\u4e0a\u771f\u5b9e\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u9075\u5faa\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5982FVD\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u63a8\u7406\u5931\u8d25\uff0c\u4f8b\u5982\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u89c4\u5f8b\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165MMGR\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u6db5\u76d6\u4e09\u4e2a\u9886\u57df\u7684\u751f\u6210\u63a8\u7406\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u4f7f\u7528\u7ec6\u81f4\u7684\u6307\u6807\u8bc4\u4f30\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u5bf9\u9886\u5148\u89c6\u9891\u548c\u56fe\u50cf\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u663e\u8457\u7684\u8868\u73b0\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic content, but their accuracy in simulating the world is uncertain.</li>\n    <li>Current evaluation methods mainly focus on visual quality and ignore logical mistakes like violating physics or causality.</li>\n    <li>MMGR is a new evaluation framework that assesses reasoning abilities in five areas: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>It tests generative reasoning in three fields: Abstract Reasoning, Embodied Navigation, and Physical Commonsense.</li>\n    <li>Benchmarking shows that while models perform moderately well in Physical Commonsense, they struggle significantly in Abstract Reasoning and long-term spatial tasks.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.01374", "authors": [{"_id": "692e6bf937312eaa83fd8890", "user": {"_id": "610b70452719facd4ea85e28", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg", "isPro": false, "fullname": "Chujie Zheng", "user": "chujiezheng", "type": "user"}, "name": "Chujie Zheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:27.206Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8891", "name": "Kai Dang", "hidden": false}, {"_id": "692e6bf937312eaa83fd8892", "name": "Bowen Yu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8893", "name": "Mingze Li", "hidden": false}, {"_id": "692e6bf937312eaa83fd8894", "user": {"_id": "6278bd42541f3d2dfa77ea70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg", "isPro": false, "fullname": "Huiqiang Jiang", "user": "iofu728", "type": "user"}, "name": "Huiqiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:41.367Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8895", "name": "Junrong Lin", "hidden": false}, {"_id": "692e6bf937312eaa83fd8896", "name": "Yuqiong Liu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8897", "user": {"_id": "62088594a5943c8a8fc94560", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png", "isPro": false, "fullname": "An Yang", "user": "yangapku", "type": "user"}, "name": "An Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:25.208Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8898", "name": "Jingren Zhou", "hidden": false}, {"_id": "692e6bf937312eaa83fd8899", "name": "Junyang Lin", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "publishedAt": "2025-12-01T07:45:39.000Z", "submittedOnDailyAt": "2025-12-02T02:47:49.367Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "submittedOnDailyBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "isPro": false, "fullname": "Bowen Yu", "user": "Tigerph", "type": "user"}, "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "upvotes": 78, "discussionId": "692e6bfa37312eaa83fd889a", "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.", "ai_keywords": ["reinforcement learning", "large language models", "sequence-level reward", "token-level objective", "policy gradient methods", "REINFORCE", "first-order approximation", "training-inference discrepancy", "policy staleness", "importance sampling correction", "clipping", "Routing Replay", "Mixture-of-Experts", "on-policy training", "off-policy updates"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u5e8f\u5217\u7ea7\u5956\u52b1\u3002</li>\n    <li>\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5f53\u8bad\u7ec3\u4e0e\u63a8\u7406\u5dee\u5f02\u548c\u7b56\u7565\u9648\u65e7\u6027\u6700\u5c0f\u5316\u65f6\uff0c\u66ff\u4ee3\u76ee\u6807\u7684\u6709\u6548\u6027\u63d0\u9ad8\u3002</li>\n    <li>\u5f3a\u8c03\u4e86\u51e0\u79cd\u6280\u672f\u5728\u7a33\u5b9aRL\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5305\u62ec\u91cd\u8981\u6027\u91c7\u6837\u4fee\u6b63\u3001\u526a\u5207\u548cMixture-of-Experts\uff08MoE\uff09\u6a21\u578b\u7684Routing Replay\u3002</li>\n    <li>\u572830B MoE\u6a21\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u4fee\u6b63\u7684\u57fa\u7840\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u6700\u4f73\u3002</li>\n    <li>\u4e00\u65e6\u8bad\u7ec3\u7a33\u5b9a\uff0c\u5ef6\u957f\u4f18\u5316\u8fc7\u7a0b\u53ef\u83b7\u5f97\u76f8\u4f3c\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u4e0e\u521d\u59cb\u51b7\u542f\u52a8\u65e0\u5173\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper presents a new way to use reinforcement learning (RL) with large language models.</li>\n    <li>It explains how to optimize rewards at the sequence level by using a simpler token-level approach, especially in policy gradient methods.</li>\n    <li>The authors found that minimizing training-inference differences and policy staleness makes this approach more effective.</li>\n    <li>They conducted extensive experiments with a large model and found that using importance sampling correction leads to better training stability.</li>\n    <li>The paper suggests methods to stabilize RL training, which could help future research in this area.</li>\n</ul>"}, "publishedAt": "2025-12-01T02:45:39.000Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png", "numComments": 2, "submittedBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "fullname": "Bowen Yu", "name": "Tigerph", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u662f\u6307\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u81ea\u4e3b\u6784\u601d\u3001\u8c03\u67e5\u548c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4efb\u52a1\uff08\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72\u6e7f\u5b9e\u9a8c\u548c\u5b9e\u9a8c\u63a8\u7406\uff09\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u4e13\u5bb6\u7cbe\u5fc3\u7b56\u5212\u7684\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u4e2d\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\uff0c\u6e7f\u5b9e\u9a8c\u7684\u6b65\u9aa4\u4e00\u81f4\u6027\u5dee\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002\u6b64\u9879\u7814\u7a76\u4e3aAI\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which means AI's ability to think and work like a scientist is still undeveloped.</li>\n    <li>The authors propose a new SGI definition based on a model that includes key steps like thinking, planning, acting, and observing.</li>\n    <li>They introduce SGI-Bench, a testing tool with over 1,000 examples to evaluate AI performance in scientific tasks.</li>\n    <li>Results show AI struggles with deep research accuracy, idea feasibility, and executing experiments correctly.</li>\n    <li>A new method called Test-Time Reinforcement Learning (TTRL) is suggested to improve AI's ability to generate innovative scientific ideas during evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 29, 2025";
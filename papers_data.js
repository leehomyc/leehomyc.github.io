window.trendingPapers = {
    "today": [{"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u8bc6\u522b\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u5728\u8bc6\u522b\u7269\u7406\u5c5e\u6027\uff08\u5982\u5efa\u7b51\u7269\u3001\u6c34\u4f53\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u5f88\u5f3a\u7684\u96f6-shot \u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces have many different social and physical features that need to be identified from satellite images.</li>\n    <li>Current models can identify physical features well, but not social categories like schools and parks.</li>\n    <li>The authors created a new dataset called SocioSeg, which includes satellite images and detailed labels for social entities.</li>\n    <li>They developed a new framework called SocioReasoner that uses vision-language reasoning to help identify social entities more effectively.</li>\n    <li>Tests show their method outperforms existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u7684\u5e73\u8861\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002</li>\n    <li>\u91c7\u7528\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6765\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u89c6\u89c9\u5047\u8bbe\u63a2\u7d22\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u4ec5\u4e3a10\u4ebf\u53c2\u6570\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u80fd\u4e0e10\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8d8a\u4e00\u4e9b\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u793e\u533a\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>The model uses a unique training method with 1.2 trillion multimodal data points, combining a perception encoder and a decoder for better understanding of language and vision.</li>\n    <li>It employs a post-training process with over 1,000 rounds of reinforcement learning to improve its performance.</li>\n    <li>Despite being smaller than many competing models, STEP3-VL-10B performs exceptionally well, achieving high scores on various benchmarks.</li>\n    <li>The full model is available for the community to use, promoting efficient and reproducible research.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u540e\u671f\u8bad\u7ec3\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u51fa\u73b0\u201c\u63a2\u7d22\u5d29\u6e83\u201d\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u63d0\u524d\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u867d\u7136\u63d0\u5347\u4e86\u901a\u8fc7\u7387\uff08pass@1\uff09\uff0c\u4f46\u9650\u5236\u4e86\u591a\u6837\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e13\u6ce8\u4e8e\u5956\u52b1\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u7ed3\u679c\u8fdb\u884c\u805a\u7c7b\uff0c\u4ece\u800c\u63d0\u5347\u7a00\u6709\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u591a\u6837\u6027\u548cpass@k\uff0c\u800c\u4e0d\u727a\u7272pass@1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models (LLMs) in complex reasoning tasks.</li>\n    <li>A common problem is \"exploration collapse,\" where models focus too much on a few successful reasoning patterns.</li>\n    <li>This focus limits diversity in solutions and improvements in performance on larger problem sets.</li>\n    <li>The proposed solution, Uniqueness-Aware Reinforcement Learning, rewards unique and correct solutions with rare strategies.</li>\n    <li>This method improves overall performance and exploration across various subjects without losing effectiveness in simpler tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09667", "authors": [{"_id": "6969b0f732f0333869ff9476", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:48.445Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9477", "user": {"_id": "662b4e3bc709a61df840fda1", "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg", "isPro": false, "fullname": "Hu Yunhai", "user": "AlexCCtop", "type": "user"}, "name": "Yunhai Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:37:06.706Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9478", "user": {"_id": "650026d30339dae3dba2cec5", "avatarUrl": "/avatars/fcc9ea4336f8d4bb177e5c9eacdd05c9.svg", "isPro": false, "fullname": "Juncheng Liu", "user": "juncliu", "type": "user"}, "name": "Juncheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:33.401Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9479", "name": "Shuyue Stella Li", "hidden": false}, {"_id": "6969b0f732f0333869ff947a", "name": "Yucheng Wang", "hidden": false}, {"_id": "6969b0f732f0333869ff947b", "user": {"_id": "638e40d450a4e4beef98196b", "avatarUrl": "/avatars/fe27e019baf48caeb44e19b7289db9fb.svg", "isPro": false, "fullname": "Zhen Xu", "user": "zhenxu", "type": "user"}, "name": "Zhen Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:04.868Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947c", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0f732f0333869ff947d", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:15.855Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947e", "name": "Xinxing Xu", "hidden": false}, {"_id": "6969b0f732f0333869ff947f", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:25.577Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9480", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:31.289Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9481", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:36.481Z", "hidden": false}], "publishedAt": "2026-01-14T17:57:43.000Z", "submittedOnDailyAt": "2026-01-16T01:01:32.343Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "upvotes": 63, "discussionId": "6969b0f832f0333869ff9482", "ai_summary": "Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.", "ai_keywords": ["multi-agent systems", "reinforcement learning", "test-time reinforcement learning", "multi-agent reinforcement learning", "credit assignment", "multi-expert teams", "dialogue systems", "distribution-shift-robust reasoning"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5df2\u7ecf\u6210\u4e3a\u8bb8\u591a\u5e94\u7528\u4e2d\u7684\u5408\u4f5c\u4f19\u4f34\uff0c\u5177\u5907\u591a\u6837\u6027\u548c\u4ea4\u53c9\u68c0\u67e5\u7684\u4f18\u52bf\u3002</li>\n    <li>\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u961f\u53cb\u4e4b\u95f4\u7684\u5171\u540c\u9002\u5e94\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\uff0c\u5956\u52b1\u901a\u5e38\u7a00\u758f\u4e14\u53d8\u5316\u5927\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08MATTRL\uff09\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u7ed3\u6784\u5316\u6587\u672c\u7ecf\u9a8c\u6ce8\u5165\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u4e2d\u3002</li>\n    <li>MATTRL \u5f62\u6210\u4e00\u4e2a\u591a\u4e13\u5bb6\u56e2\u961f\uff0c\u8fdb\u884c\u591a\u8f6e\u8ba8\u8bba\uff0c\u5e76\u5728\u6700\u7ec8\u51b3\u7b56\u4e2d\u8fbe\u6210\u5171\u8bc6\u3002</li>\n    <li>\u5728\u533b\u5b66\u3001\u6570\u5b66\u548c\u6559\u80b2\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATTRL \u7684\u51c6\u786e\u6027\u6bd4\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u9ad8\u4e86\u5e73\u5747 3.67%\uff0c\u6bd4\u76f8\u4f3c\u7684\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u9ad8\u4e86 8.67%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems using large language models (LLMs) are becoming useful collaborators in various fields, but training them can be resource-heavy and unstable.</li>\n    <li>To address this, a new framework called Multi-Agent Test-Time Reinforcement Learning (MATTRL) is introduced, which enhances decision-making by using structured experiences during inference.</li>\n    <li>MATTRL creates a team of specialists that discuss and reach consensus during multi-turn interactions, improving the decision-making process.</li>\n    <li>The framework shows better performance, increasing accuracy by an average of 3.67% compared to existing multi-agent systems, and 8.67% over single-agent systems.</li>\n    <li>The study also explores how different credit-assignment methods affect training results, ensuring a stable and effective approach to multi-agent reasoning without the need for tuning.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:57:43.000Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09667.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02242", "authors": [{"_id": "695cafde6aa73bc11f091566", "user": {"_id": "65e7151ef7c2e46887e225b1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e7151ef7c2e46887e225b1/NXj_CrUkzwdUT8T3a-H8N.jpeg", "isPro": false, "fullname": "Grigorii Alekseenko", "user": "Riko0", "type": "user"}, "name": "Grigorii Alekseenko", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:36:29.469Z", "hidden": false}, {"_id": "695cafde6aa73bc11f091567", "user": {"_id": "65ae526111a0a3ff61d7d726", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ae526111a0a3ff61d7d726/x6OVUbowh-eXH9bxWCERB.jpeg", "isPro": false, "fullname": "Aleksandr", "user": "grac20101", "type": "user"}, "name": "Aleksandr Gordeev", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T14:16:49.474Z", "hidden": false}, {"_id": "695cafde6aa73bc11f091568", "user": {"_id": "6498095fce9190ebb8699113", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png", "isPro": true, "fullname": "Irina Tolstykh", "user": "iitolstykh", "type": "user"}, "name": "Irina Tolstykh", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T12:42:37.939Z", "hidden": false}, {"_id": "695cafde6aa73bc11f091569", "name": "Bulat Suleimanov", "hidden": false}, {"_id": "695cafde6aa73bc11f09156a", "name": "Vladimir Dokholyan", "hidden": false}, {"_id": "695cafde6aa73bc11f09156b", "name": "Georgii Fedorov", "hidden": false}, {"_id": "695cafde6aa73bc11f09156c", "name": "Sergey Yakubson", "hidden": false}, {"_id": "695cafde6aa73bc11f09156d", "name": "Aleksandra Tsybina", "hidden": false}, {"_id": "695cafde6aa73bc11f09156e", "name": "Mikhail Chernyshov", "hidden": false}, {"_id": "695cafde6aa73bc11f09156f", "user": {"_id": "6416d8ef8f689506e70dd2e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679218871593-noauth.jpeg", "isPro": false, "fullname": "Maksim Kuprashevich", "user": "WildChlamydia", "type": "user"}, "name": "Maksim Kuprashevich", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:34:25.729Z", "hidden": false}], "publishedAt": "2026-01-05T16:17:20.000Z", "submittedOnDailyAt": "2026-01-16T08:00:29.947Z", "title": "VIBE: Visual Instruction Based Editor", "submittedOnDailyBy": {"_id": "6498095fce9190ebb8699113", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png", "isPro": true, "fullname": "Irina Tolstykh", "user": "iitolstykh", "type": "user"}, "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "upvotes": 46, "discussionId": "695cafde6aa73bc11f091570", "projectPage": "https://riko0.github.io/VIBE/", "githubRepo": "https://github.com/ai-forever/vibe", "githubRepoAddedBy": "user", "ai_summary": "A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.", "ai_keywords": ["diffusion models", "Qwen3-VL", "Sana1.5", "instruction-based image editing", "image generation", "source consistency", "inference efficiency", "parameter-efficient models", "ImgEdit benchmark", "GEdit benchmark"], "githubStars": 22, "summary_zh": "<ul>\n    <li>\u6307\u4ee4\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u662f\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e2d\u53d1\u5c55\u6700\u5feb\u7684\u9886\u57df\u4e4b\u4e00\uff0c\u8fc7\u53bb\u4e00\u5e74\u8fdb\u5c55\u663e\u8457\u3002</li>\n    <li>\u76ee\u524d\u53ea\u6709\u5c11\u6570\u5f00\u6e90\u65b9\u6cd5\u80fd\u8fbe\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8d28\u91cf\uff0c\u5927\u591a\u6570\u4f7f\u7528\u7684\u6269\u6563\u6a21\u578b\u53c2\u6570\u91cf\u5927\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u4e14\u9ad8\u6548\u7684\u6307\u4ee4\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u6d41\u7a0b\uff0c\u4f7f\u75282B\u53c2\u6570\u7684Qwen3-VL\u6a21\u578b\u548c1.6B\u53c2\u6570\u7684Sana1.5\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728ImgEdit\u548cGEdit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u5728\u4f4e\u6210\u672c\u63a8\u65ad\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002</li>\n    <li>\u6a21\u578b\u572824GB GPU\u5185\u5b58\u4e0b\u8fd0\u884c\uff0c\u80fd\u591f\u5728\u7ea64\u79d2\u5185\u751f\u6210\u9ad8\u8fbe2K\u5206\u8fa8\u7387\u7684\u7f16\u8f91\u56fe\u50cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Instruction-based image editing is rapidly advancing, with many new models and systems released recently.</li>\n  <li>Current open-source models often lack real-world quality, and popular models are large and costly to run.</li>\n  <li>This paper introduces a smaller, efficient image editing pipeline using a 2B-parameter model for guidance and a 1.6B-parameter model for generating images.</li>\n  <li>The new method provides high-quality edits while being cheaper to run and maintaining source consistency.</li>\n  <li>It performs well on tasks like attribute adjustments and object removals, fitting within 24 GB of GPU memory and generating images quickly.</li>\n</ul>"}, "publishedAt": "2026-01-05T11:17:20.000Z", "title": "VIBE: Visual Instruction Based Editor", "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02242.png", "numComments": 2, "submittedBy": {"_id": "6498095fce9190ebb8699113", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png", "fullname": "Irina Tolstykh", "name": "iitolstykh", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 27, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07641", "authors": [{"_id": "696745d2c5e371f6b235d1f1", "user": {"_id": "67e655f7d6b8333a8f78eadf", "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg", "isPro": false, "fullname": "Jiaxuan Lu", "user": "Blue-Giant", "type": "user"}, "name": "Jiaxuan Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:53.789Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f2", "user": {"_id": "65617bf9f5532ac1bde64d07", "avatarUrl": "/avatars/6659fe26eecfce8ac699caa73b823fe1.svg", "isPro": false, "fullname": "ZIYU KONG", "user": "ziyukong", "type": "user"}, "name": "Ziyu Kong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:59.511Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f3", "name": "Yemin Wang", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f4", "name": "Rong Fu", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f5", "user": {"_id": "691b0f528411a45dc9ee9de8", "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg", "isPro": false, "fullname": "Haiyuan Wan", "user": "HY-Wan", "type": "user"}, "name": "Haiyuan Wan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:47.426Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f6", "user": {"_id": "67c443afb753bd020f9c97d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png", "isPro": false, "fullname": "Cheng", "user": "YangC777", "type": "user"}, "name": "Cheng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:18.927Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f7", "name": "Wenjie Lou", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f8", "name": "Haoran Sun", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f9", "user": {"_id": "67fc7887864dfcbd93ce6322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1ecRk5ZWDXALss7e4fjtQ.png", "isPro": false, "fullname": "Lilong Wang", "user": "Eason2025", "type": "user"}, "name": "Lilong Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:20.390Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fa", "user": {"_id": "671280b9a895018bbc281dea", "avatarUrl": "/avatars/55f3c2d3698011bc718ec18295519caa.svg", "isPro": false, "fullname": "Yankai Jiang", "user": "yankaijiang", "type": "user"}, "name": "Yankai Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:26.741Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fb", "name": "Xiaosong Wang", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fc", "name": "Xiao Sun", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fd", "user": {"_id": "6538b861613fe158bd581e35", "avatarUrl": "/avatars/6817dbfe903675721fd227058b0a91ac.svg", "isPro": false, "fullname": "Dongzhan Zhou", "user": "schrodingers-tiger", "type": "user"}, "name": "Dongzhan Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:36.629Z", "hidden": false}], "publishedAt": "2026-01-12T15:22:51.000Z", "submittedOnDailyAt": "2026-01-16T05:19:21.135Z", "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning", "submittedOnDailyBy": {"_id": "67e655f7d6b8333a8f78eadf", "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg", "isPro": false, "fullname": "Jiaxuan Lu", "user": "Blue-Giant", "type": "user"}, "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.", "upvotes": 35, "discussionId": "696745d2c5e371f6b235d1fe", "githubRepo": "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol", "githubRepoAddedBy": "user", "ai_summary": "Test-Time Tool Evolution enables AI agents to dynamically create and refine computational tools during inference, overcoming limitations of static tool libraries in scientific applications.", "ai_keywords": ["LLM-based agents", "tool libraries", "scientific reasoning", "computational methods", "test-time tool evolution", "SciEvo benchmark", "tool synthesis", "tool verification", "tool evolution", "cross-domain adaptation"], "githubStars": 34, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>AI\u5728\u79d1\u5b66\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\u662f\u521b\u9020\u8ba1\u7b97\u65b9\u6cd5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63a8\u7406\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u4f9d\u8d56\u4e8e\u9759\u6001\u7684\u5de5\u5177\u5e93\uff0c\u8fd9\u5728\u79d1\u5b66\u9886\u57df\u5e76\u4e0d\u9002\u7528\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u6d4b\u8bd5\u65f6\u5de5\u5177\u8fdb\u5316\uff08TTE\uff09\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5408\u6210\u3001\u9a8c\u8bc1\u548c\u8fdb\u5316\u53ef\u6267\u884c\u5de5\u5177\u3002</li>\n    <li>TTE\u901a\u8fc7\u5c06\u5de5\u5177\u8f6c\u53d8\u4e3a\u95ee\u9898\u9a71\u52a8\u7684\u4ea7\u7269\uff0c\u514b\u670d\u4e86\u9759\u6001\u5de5\u5177\u5e93\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86SciEvo\u57fa\u51c6\uff0c\u5305\u542b1590\u4e2a\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u548c925\u4e2a\u81ea\u52a8\u8fdb\u5316\u5de5\u5177\uff0cTTE\u5728\u51c6\u786e\u6027\u548c\u5de5\u5177\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The main issue in AI for Science is not just reasoning, but also developing flexible methods for scientific exploration.</li>\n    <li>Current AI agents use fixed tool libraries, which are not suitable for scientific tasks where tools can be varied and incomplete.</li>\n    <li>This paper introduces a method called Test-Time Tool Evolution (TTE) that allows AI to create and adapt tools during problem-solving.</li>\n    <li>TTE addresses the limitations of static tool libraries by making tools dynamic and relevant to specific scientific challenges.</li>\n    <li>The authors also created a benchmark called SciEvo, which includes many scientific tasks and tools, showing that TTE performs well in accuracy and efficiency.</li>\n</ul>"}, "publishedAt": "2026-01-12T10:22:51.000Z", "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning", "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07641.png", "numComments": 1, "submittedBy": {"_id": "67e655f7d6b8333a8f78eadf", "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg", "fullname": "Jiaxuan Lu", "name": "Blue-Giant", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10305", "authors": [{"_id": "6969a0b932f0333869ff9381", "user": {"_id": "67e289aea1e569cd0a41db1d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/w7b_6u7nZDH9Lp6NJKdVJ.png", "isPro": false, "fullname": "shen hengyu", "user": "dewecho", "type": "user"}, "name": "Hengyu Shen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:54.646Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9382", "user": {"_id": "641030c77a15af878ae5bd8f", "avatarUrl": "/avatars/8a5037edf55c78ebc317c8b191343671.svg", "isPro": false, "fullname": "TianchengGu", "user": "TianchengGu", "type": "user"}, "name": "Tiancheng Gu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:00.059Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9383", "name": "Bin Qin", "hidden": false}, {"_id": "6969a0b932f0333869ff9384", "name": "Lan Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff9385", "name": "Yuling Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff9386", "name": "Shuo Tan", "hidden": false}, {"_id": "6969a0b932f0333869ff9387", "user": {"_id": "63dfc05342591dda0b945e58", "avatarUrl": "/avatars/3fd796035c2243d6b03cc361bc06e64e.svg", "isPro": false, "fullname": "Zelong Sun", "user": "dfgdgh", "type": "user"}, "name": "Zelong Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:29.711Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9388", "name": "Jun Wang", "hidden": false}, {"_id": "6969a0b932f0333869ff9389", "name": "Nan Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff938a", "name": "Xiang An", "hidden": false}, {"_id": "6969a0b932f0333869ff938b", "user": {"_id": "6760a8f5e4b55ba1b2b0a7b4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NddUMmwmZFbS25v1q8KyS.png", "isPro": false, "fullname": "Weidong Cai", "user": "SeriousBro", "type": "user"}, "name": "Weidong Cai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:16.748Z", "hidden": false}, {"_id": "6969a0b932f0333869ff938c", "user": {"_id": "694d00c3ece16a65e2b84774", "avatarUrl": "/avatars/72bfeec4602ba4069faf0dba02c2be96.svg", "isPro": false, "fullname": "Ziyong Feng", "user": "fengziyong", "type": "user"}, "name": "Ziyong Feng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:10.787Z", "hidden": false}, {"_id": "6969a0b932f0333869ff938d", "user": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "isPro": false, "fullname": "Kaicheng Yang", "user": "Kaichengalex", "type": "user"}, "name": "Kaicheng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:30:51.089Z", "hidden": false}], "publishedAt": "2026-01-15T11:28:58.000Z", "submittedOnDailyAt": "2026-01-16T00:37:46.383Z", "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset", "submittedOnDailyBy": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "isPro": false, "fullname": "Kaicheng Yang", "user": "Kaichengalex", "type": "user"}, "summary": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.", "upvotes": 29, "discussionId": "6969a0b932f0333869ff938e", "projectPage": "https://deepglint.github.io/DanQing/", "githubRepo": "https://github.com/deepglint/DanQing", "githubRepoAddedBy": "user", "ai_summary": "A large-scale Chinese image-text dataset called DanQing is introduced to advance vision-language pretraining, demonstrating superior performance in various downstream tasks through continual pretraining of the SigLIP2 model.", "ai_keywords": ["Vision-Language Pre-training", "contrastive pretraining", "cross-modal retrieval", "image captioning", "SigLIP2", "continual pre-training"], "githubStars": 12, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u540d\u4e3aDanQing\u7684\u9ad8\u8d28\u91cf\u4e2d\u6587\u56fe\u6587\u6570\u636e\u96c6\uff0c\u5305\u542b1\u4ebf\u5bf9\u56fe\u50cf\u548c\u6587\u672c\u3002</li>\n    <li>DanQing\u901a\u8fc7\u4e25\u683c\u7684\u7b5b\u9009\u8fc7\u7a0b\u6784\u5efa\uff0c\u6570\u636e\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u7684\u6570\u636e\u96c6\u3002</li>\n    <li>\u8fd9\u4e2a\u6570\u636e\u96c6\u4e3b\u8981\u57fa\u4e8e2024-2025\u5e74\u7684\u7f51\u7edc\u6570\u636e\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u8bed\u4e49\u8d8b\u52bf\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDanQing\u5728\u591a\u9879\u4e2d\u6587\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u96f6\u6837\u672c\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u3002</li>\n    <li>\u6211\u4eec\u5c06\u4ee5CC-BY 4.0\u8bb8\u53ef\u8bc1\u5f00\u6e90DanQing\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u4e2d\u6587\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language Pre-training (VLP) models are effective for tasks like cross-modal retrieval and image captioning using large image-text datasets.</li>\n    <li>Chinese VLP models have not advanced as quickly due to a lack of quality Chinese image-text data.</li>\n    <li>A new dataset called DanQing has been created, consisting of 100 million image-text pairs from Common Crawl, focusing on high-quality data.</li>\n    <li>DanQing uses more recent web data from 2024-2025, helping models understand current trends better.</li>\n    <li>Tests show DanQing outperforms existing datasets in various Chinese tasks, and it will be open-sourced for further research.</li>\n</ul>"}, "publishedAt": "2026-01-15T06:28:58.000Z", "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset", "summary": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10305.png", "numComments": 2, "submittedBy": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "fullname": "Kaicheng Yang", "name": "Kaichengalex", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10402", "authors": [{"_id": "6969f54c844a787c4fdea404", "user": {"_id": "65352517fca2c10e43035e32", "avatarUrl": "/avatars/f404cef8a79f27435865ac4d85ef0927.svg", "isPro": false, "fullname": "xinyuzhu", "user": "xinyuzhu", "type": "user"}, "name": "Xinyu Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:28:31.884Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea405", "user": {"_id": "6614ed4809c63bfbddc53ddf", "avatarUrl": "/avatars/018bfc168bb4010cf6018e42148e0f51.svg", "isPro": false, "fullname": "Yuzhu Cai", "user": "Ethical-Lens", "type": "user"}, "name": "Yuzhu Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:36:59.231Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea406", "user": {"_id": "651e1f5522890326303fb1f4", "avatarUrl": "/avatars/4e47b8d7669c3aafcab4e1438ed386e7.svg", "isPro": false, "fullname": "Zexi Liu", "user": "ZeroXLeo", "type": "user"}, "name": "Zexi Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:28:33.861Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea407", "user": {"_id": "690320c2eba76a9b99854b4d", "avatarUrl": "/avatars/a612962c3627bdb7986a5ab5f14af2eb.svg", "isPro": false, "fullname": "Bingyang Zheng", "user": "bulibuliyang", "type": "user"}, "name": "Bingyang Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:38:51.672Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea408", "name": "Cheng Wang", "hidden": false}, {"_id": "6969f54c844a787c4fdea409", "name": "Rui Ye", "hidden": false}, {"_id": "6969f54c844a787c4fdea40a", "user": {"_id": "64f793d4dcd7b028c15bbe50", "avatarUrl": "/avatars/fb5d7392736309dd5c80ac32750d164b.svg", "isPro": false, "fullname": "Jiaao Chen", "user": "Jiaaoc", "type": "user"}, "name": "Jiaao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:08.324Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea40b", "user": {"_id": "677e237c70bd7b5431f88450", "avatarUrl": "/avatars/cc64b0af06588e43c94c185900362751.svg", "isPro": false, "fullname": "Hanrui Wang", "user": "azrealwang", "type": "user"}, "name": "Hanrui Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:14.281Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea40c", "user": {"_id": "68c08c9da619a2f12eae4913", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FYHEbHaFSOcWScBmeHQcF.png", "isPro": false, "fullname": "Wei Chen Wang", "user": "bumbigby", "type": "user"}, "name": "Wei-Chen Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:21.035Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea40d", "name": "Yuzhi Zhang", "hidden": false}, {"_id": "6969f54c844a787c4fdea40e", "name": "Linfeng Zhang", "hidden": false}, {"_id": "6969f54c844a787c4fdea40f", "name": "Weinan E", "hidden": false}, {"_id": "6969f54c844a787c4fdea410", "name": "Di Jin", "hidden": false}, {"_id": "6969f54c844a787c4fdea411", "user": {"_id": "65257545b017be1fc1915364", "avatarUrl": "/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg", "isPro": false, "fullname": "Siheng Chen", "user": "sihengchen", "type": "user"}, "name": "Siheng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:39.159Z", "hidden": false}], "publishedAt": "2026-01-15T13:52:04.000Z", "submittedOnDailyAt": "2026-01-16T06:18:42.647Z", "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "submittedOnDailyBy": {"_id": "6614ed4809c63bfbddc53ddf", "avatarUrl": "/avatars/018bfc168bb4010cf6018e42148e0f51.svg", "isPro": false, "fullname": "Yuzhu Cai", "user": "Ethical-Lens", "type": "user"}, "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "upvotes": 26, "discussionId": "6969f54c844a787c4fdea412", "projectPage": "https://sjtu-sai-agents.github.io/ML-Master/", "githubRepo": "https://github.com/sjtu-sai-agents/ML-Master", "githubRepoAddedBy": "user", "ai_summary": "ML-Master 2.0 enables long-term autonomous machine learning engineering through hierarchical cognitive caching that manages extended context and learns from execution traces.", "ai_keywords": ["Large Language Models", "ultra-long-horizon autonomy", "machine learning engineering", "Hierarchical Cognitive Caching", "cognitive accumulation", "context management", "experimental strategy", "execution traces", "cross-task wisdom"], "githubStars": 332, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u4eba\u5de5\u667a\u80fd\u76ee\u524d\u5728\u8d85\u957f\u65f6\u95f4\u81ea\u4e3b\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u957f\u671f\u6218\u7565\u4e00\u81f4\u6027\u548c\u53cd\u9988\u4fee\u6b63\u7684\u5b9e\u9a8c\u4e2d\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u671f\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u7684\u771f\u5b9e\u7814\u7a76\u73af\u5883\u4e2d\u5bb9\u6613\u88ab\u7ec6\u8282\u6df9\u6ca1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ML-Master 2.0\uff0c\u4e00\u4e2a\u80fd\u591f\u638c\u63e1\u8d85\u957f\u65f6\u95f4\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u4ee3\u8868\u79d1\u5b66\u53d1\u73b0\u7684\u4e00\u4e2a\u7f29\u5f71\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165\u5c42\u6b21\u8ba4\u77e5\u7f13\u5b58\uff08HCC\uff09\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u533a\u5206\u65f6\u95f4\u4e0a\u7684\u7ecf\u9a8c\uff0c\u4f7f\u5f97\u4ee3\u7406\u80fd\u591f\u5728\u6267\u884c\u548c\u957f\u671f\u6218\u7565\u4e4b\u95f4\u89e3\u8026\u3002</li>\n    <li>\u5728OpenAI\u7684MLE-Bench\u8bc4\u4f30\u4e2d\uff0cML-Master 2.0\u572824\u5c0f\u65f6\u9884\u7b97\u5185\u8fbe\u5230\u4e8656.44%\u7684\u6700\u4f73\u6210\u7ee9\uff0c\u5c55\u793a\u4e86\u5176\u8d85\u957f\u65f6\u95f4\u81ea\u4e3b\u6027\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>AI development is struggling with tasks that require long-term planning and learning over days or weeks.</li>\n  <li>Current AI models, like Large Language Models, work well for short tasks but struggle with complex, real-world challenges.</li>\n  <li>ML-Master 2.0 is a new AI agent designed to handle long-term machine learning tasks effectively.</li>\n  <li>The agent uses a method called Hierarchical Cognitive Caching to manage and store knowledge over time.</li>\n  <li>In tests, ML-Master 2.0 achieved a high success rate, showing its potential for advanced autonomous exploration in science.</li>\n</ul>"}, "publishedAt": "2026-01-15T08:52:04.000Z", "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10402.png", "numComments": 1, "submittedBy": {"_id": "6614ed4809c63bfbddc53ddf", "avatarUrl": "/avatars/018bfc168bb4010cf6018e42148e0f51.svg", "fullname": "Yuzhu Cai", "name": "Ethical-Lens", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10061", "authors": [{"_id": "6969aeaf32f0333869ff9459", "name": "Chengzhuo Tong", "hidden": false}, {"_id": "6969aeaf32f0333869ff945a", "user": {"_id": "662db438137b72821671db2f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hLMBYuvyF6sfkS0sSrDt-.jpeg", "isPro": false, "fullname": "Mingkun Chang", "user": "D4isyC", "type": "user"}, "name": "Mingkun Chang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:17.228Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945b", "user": {"_id": "67c14a89f85f9a6c361226ba", "avatarUrl": "/avatars/538eede44205f49fe5a562dcce992d7c.svg", "isPro": false, "fullname": "shenglong", "user": "zhangshenglong", "type": "user"}, "name": "Shenglong Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:31.154Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945c", "user": {"_id": "65e71ef39cf349af2940b317", "avatarUrl": "/avatars/fc1cd8d3510946fc947d67b16b51834b.svg", "isPro": false, "fullname": "Yuran Wang", "user": "Ryann829", "type": "user"}, "name": "Yuran Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:37.205Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945d", "name": "Cheng Liang", "hidden": false}, {"_id": "6969aeaf32f0333869ff945e", "user": {"_id": "6713a71e7dfe714b425cccfb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/95YYcbv_f6J8yWTunwn4z.png", "isPro": false, "fullname": "zhizhengzhao", "user": "zhizhengzhao", "type": "user"}, "name": "Zhizheng Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:42.205Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945f", "name": "Ruichuan An", "hidden": false}, {"_id": "6969aeaf32f0333869ff9460", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:03.210Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff9461", "user": {"_id": "673c7319d11b1c2e246ead9c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg", "isPro": false, "fullname": "Yang Shi", "user": "DogNeverSleep", "type": "user"}, "name": "Yang Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:39.706Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff9462", "name": "Yifan Dai", "hidden": false}, {"_id": "6969aeaf32f0333869ff9463", "user": {"_id": "68418019d777f13c594ffe5f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a2yvPF9IoCcXKFz7TkADe.png", "isPro": false, "fullname": "Ziming Zhao", "user": "ZimingZhao", "type": "user"}, "name": "Ziming Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:38:15.698Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff9464", "name": "Guanbin Li", "hidden": false}, {"_id": "6969aeaf32f0333869ff9465", "name": "Pengfei Wan", "hidden": false}, {"_id": "6969aeaf32f0333869ff9466", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6969aeaf32f0333869ff9467", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2026-01-15T04:33:06.000Z", "submittedOnDailyAt": "2026-01-16T00:52:07.049Z", "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.", "upvotes": 25, "discussionId": "6969aeaf32f0333869ff9468", "projectPage": "https://cof-t2i.github.io/", "githubRepo": "https://github.com/VisionChengzhuo/CoF-T2I", "githubRepoAddedBy": "user", "ai_summary": "Chain-of-Frame reasoning is integrated into text-to-image generation through progressive visual refinement with explicit intermediate steps, achieving superior performance on benchmark datasets.", "ai_keywords": ["Chain-of-Frame reasoning", "text-to-image generation", "progressive visual refinement", "CoF trajectories", "GenEval", "Imagine-Bench"], "githubStars": 18, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5c55\u793a\u4e86\u9010\u5e27\u63a8\u7406\u7684\u80fd\u529b\uff0c\u79f0\u4e3a\u94fe\u6846\u63a8\u7406\uff08CoF\uff09\u3002</li>\n    <li>\u8fd9\u79cd\u80fd\u529b\u4f7f\u89c6\u9891\u6a21\u578b\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86CoF-T2I\u6a21\u578b\uff0c\u5c06\u94fe\u6846\u63a8\u7406\u6574\u5408\u5230\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u3002</li>\n    <li>CoF-T2I\u901a\u8fc7\u9010\u6b65\u89c6\u89c9\u4f18\u5316\uff0c\u4f7f\u7528\u4e2d\u95f4\u5e27\u4f5c\u4e3a\u63a8\u7406\u6b65\u9aa4\u6765\u751f\u6210\u6700\u7ec8\u56fe\u50cf\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCoF-T2I\u663e\u8457\u8d85\u8fc7\u57fa\u7840\u89c6\u9891\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New video models can perform Chain-of-Frame (CoF) reasoning, allowing them to analyze videos frame by frame.</li>\n    <li>This ability has been useful in tasks like solving mazes and visual puzzles, but not much has been done for text-to-image (T2I) generation.</li>\n    <li>The CoF-T2I model combines CoF reasoning with T2I generation, using intermediate frames to improve the creation process.</li>\n    <li>A new dataset called CoF-Evol-Instruct helps to guide the generation from basic ideas to final visuals.</li>\n    <li>Tests show CoF-T2I performs better than standard video models and does well on tough benchmarks, showing its potential for producing high-quality images from text.</li>\n</ul>"}, "publishedAt": "2026-01-14T23:33:06.000Z", "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation", "summary": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10061.png", "numComments": 1, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10332", "authors": [{"_id": "6969a7d132f0333869ff93bc", "name": "Siqi Kou", "hidden": false}, {"_id": "6969a7d132f0333869ff93bd", "name": "Jiachun Jin", "hidden": false}, {"_id": "6969a7d132f0333869ff93be", "name": "Zetong Zhou", "hidden": false}, {"_id": "6969a7d132f0333869ff93bf", "name": "Ye Ma", "hidden": false}, {"_id": "6969a7d132f0333869ff93c0", "name": "Yugang Wang", "hidden": false}, {"_id": "6969a7d132f0333869ff93c1", "name": "Quan Chen", "hidden": false}, {"_id": "6969a7d132f0333869ff93c2", "name": "Peng Jiang", "hidden": false}, {"_id": "6969a7d132f0333869ff93c3", "name": "Xiao Yang", "hidden": false}, {"_id": "6969a7d132f0333869ff93c4", "name": "Jun Zhu", "hidden": false}, {"_id": "6969a7d132f0333869ff93c5", "name": "Kai Yu", "hidden": false}, {"_id": "6969a7d132f0333869ff93c6", "name": "Zhijie Deng", "hidden": false}], "publishedAt": "2026-01-15T12:19:05.000Z", "submittedOnDailyAt": "2026-01-16T00:29:52.118Z", "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders", "submittedOnDailyBy": {"_id": "654e330f350abceb30a1390b", "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg", "isPro": false, "fullname": "KouSiqi", "user": "karrykkk", "type": "user"}, "summary": "Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.", "upvotes": 20, "discussionId": "6969a7d132f0333869ff93c7", "githubRepo": "https://github.com/zhijie-group/Think-Then-Generate", "githubRepoAddedBy": "user", "ai_summary": "Text-to-image diffusion models enhanced with language model reasoning capabilities achieve improved factual consistency and semantic alignment through a think-then-generate paradigm with dual-gradient reinforcement optimization.", "ai_keywords": ["text-to-image diffusion models", "language model-based text encoders", "think-then-generate paradigm", "supervised fine-tuning", "dual-gradient reinforcement optimization", "image-grounded rewards", "semantic alignment", "factual consistency", "visual realism"], "githubStars": 37, "organization": {"_id": "673d5fe8d031224e947dc235", "name": "SJTU-DENG-Lab", "fullname": "DENG Lab @ SJTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"}, "summary_zh": "<ul>\n    <li>\u6700\u65b0\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f7f\u5f97\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u6210\u4e3a\u53ef\u80fd\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u591a\u6570\u6a21\u578b\u4ec5\u628a\u6587\u672c\u6620\u5c04\u5230\u50cf\u7d20\uff0c\u6ca1\u6709\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\"\u5148\u601d\u8003\u518d\u751f\u6210\"\u7684\u601d\u8def\uff0c\u9f13\u52b1\u8bed\u8a00\u6a21\u578b\u5bf9\u7528\u6237\u7684\u539f\u59cb\u63d0\u793a\u8fdb\u884c\u63a8\u7406\u548c\u91cd\u5199\u3002</li>\n    <li>\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u6fc0\u6d3b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e0e\u6269\u6563\u6a21\u578b\u5171\u540c\u4f18\u5316\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u751f\u6210\u56fe\u50cf\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u6709\u663e\u8457\u63d0\u9ad8\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in text-to-image models allow for high-quality images from text descriptions.</li>\n    <li>Most models treat text as simple input, not using the reasoning abilities of language models to improve image quality.</li>\n    <li>The proposed \"think-then-generate\" approach encourages language models to analyze and rewrite user prompts for better image generation.</li>\n    <li>A fine-tuning process helps the language model reason about context, while image generation is improved through a co-optimization technique.</li>\n    <li>Experiments show significant improvements in factual consistency and image quality, achieving high scores in testing benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-15T07:19:05.000Z", "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders", "summary": "Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10332.png", "numComments": 2, "submittedBy": {"_id": "654e330f350abceb30a1390b", "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg", "fullname": "KouSiqi", "name": "karrykkk", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "673d5fe8d031224e947dc235", "name": "SJTU-DENG-Lab", "fullname": "DENG Lab @ SJTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u8bc6\u522b\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u5728\u8bc6\u522b\u7269\u7406\u5c5e\u6027\uff08\u5982\u5efa\u7b51\u7269\u3001\u6c34\u4f53\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u5f88\u5f3a\u7684\u96f6-shot \u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces have many different social and physical features that need to be identified from satellite images.</li>\n    <li>Current models can identify physical features well, but not social categories like schools and parks.</li>\n    <li>The authors created a new dataset called SocioSeg, which includes satellite images and detailed labels for social entities.</li>\n    <li>They developed a new framework called SocioReasoner that uses vision-language reasoning to help identify social entities more effectively.</li>\n    <li>Tests show their method outperforms existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u7684\u5e73\u8861\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002</li>\n    <li>\u91c7\u7528\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6765\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u89c6\u89c9\u5047\u8bbe\u63a2\u7d22\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u4ec5\u4e3a10\u4ebf\u53c2\u6570\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u80fd\u4e0e10\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8d8a\u4e00\u4e9b\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u793e\u533a\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>The model uses a unique training method with 1.2 trillion multimodal data points, combining a perception encoder and a decoder for better understanding of language and vision.</li>\n    <li>It employs a post-training process with over 1,000 rounds of reinforcement learning to improve its performance.</li>\n    <li>Despite being smaller than many competing models, STEP3-VL-10B performs exceptionally well, achieving high scores on various benchmarks.</li>\n    <li>The full model is available for the community to use, promoting efficient and reproducible research.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u540e\u671f\u8bad\u7ec3\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u51fa\u73b0\u201c\u63a2\u7d22\u5d29\u6e83\u201d\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u63d0\u524d\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u867d\u7136\u63d0\u5347\u4e86\u901a\u8fc7\u7387\uff08pass@1\uff09\uff0c\u4f46\u9650\u5236\u4e86\u591a\u6837\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e13\u6ce8\u4e8e\u5956\u52b1\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u7ed3\u679c\u8fdb\u884c\u805a\u7c7b\uff0c\u4ece\u800c\u63d0\u5347\u7a00\u6709\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u591a\u6837\u6027\u548cpass@k\uff0c\u800c\u4e0d\u727a\u7272pass@1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models (LLMs) in complex reasoning tasks.</li>\n    <li>A common problem is \"exploration collapse,\" where models focus too much on a few successful reasoning patterns.</li>\n    <li>This focus limits diversity in solutions and improvements in performance on larger problem sets.</li>\n    <li>The proposed solution, Uniqueness-Aware Reinforcement Learning, rewards unique and correct solutions with rare strategies.</li>\n    <li>This method improves overall performance and exploration across various subjects without losing effectiveness in simpler tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u53cd\u590d\u7684\u201c\u751f\u6210-\u9a8c\u8bc1-\u6539\u8fdb\u201d\u5468\u671f\u6765\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5728\u6709\u9650\u9884\u7b97\u5185\u627e\u5230\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8fd9\u79cd\u4f4e\u6548\u7387\u6e90\u4e8e\u521d\u59cb\u5316\u504f\u5dee\u3001\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u968f\u673a\u64cd\u4f5c\u4ee5\u53ca\u8de8\u4efb\u52a1\u7684\u7ecf\u9a8c\u5229\u7528\u4e0d\u8db3\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53d7\u63a7\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\uff0c\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u57fa\u4e8e\u53cd\u9988\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5206\u5c42\u8fdb\u5316\u8bb0\u5fc6\u3002</li>\n    <li>CSE\u5728EffiBench-X\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\uff0c\u4e14\u5728\u65e9\u671f\u751f\u6210\u4e2d\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u5728\u6574\u4e2a\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\uff0c\u5730\u5740\u662f https://github.com/QuantaAlpha/EvoControl\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods help improve code generation but often struggle to find better solutions efficiently.</li>\n    <li>The main issues are poor starting points, random operations without guidance, and not making use of past experiences.</li>\n    <li>Controlled Self-Evolution (CSE) addresses these issues with three main features: diverse planning for better strategy coverage, feedback-based evolution methods, and a memory system to track successes and failures.</li>\n    <li>Tests show that CSE performs better than other methods and improves efficiency early on while continuing to get better.</li>\n    <li>The CSE code is available for public use at https://github.com/QuantaAlpha/EvoControl.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09688", "authors": [{"_id": "696864c90ac10a06522f6a4a", "name": "Yibo Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4b", "name": "Lei Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4c", "name": "Yue Deng", "hidden": false}, {"_id": "696864c90ac10a06522f6a4d", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:02:22.232Z", "hidden": false}, {"_id": "696864c90ac10a06522f6a4e", "name": "Yao Xiao", "hidden": false}, {"_id": "696864c90ac10a06522f6a4f", "name": "Huanjin Yao", "hidden": false}, {"_id": "696864c90ac10a06522f6a50", "name": "Liwei Kang", "hidden": false}, {"_id": "696864c90ac10a06522f6a51", "name": "Hai Ye", "hidden": false}, {"_id": "696864c90ac10a06522f6a52", "name": "Yongcheng Jing", "hidden": false}, {"_id": "696864c90ac10a06522f6a53", "name": "Lidong Bing", "hidden": false}], "publishedAt": "2026-01-14T18:38:31.000Z", "submittedOnDailyAt": "2026-01-15T01:33:59.520Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "submittedOnDailyBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "upvotes": 90, "discussionId": "696864c90ac10a06522f6a54", "githubRepo": "https://github.com/Infinity-AILab/DeepResearchEval", "githubRepoAddedBy": "user", "ai_summary": "DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.", "ai_keywords": ["automated framework", "deep research task construction", "agentic evaluation", "persona-driven pipeline", "task qualification", "search necessity", "adaptive point-wise quality evaluation", "active fact-checking", "web search", "multi-source evidence integration"], "githubStars": 67, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7528\u4e8e\u591a\u6b65\u9aa4\u7684\u7f51\u7edc\u7814\u7a76\u548c\u5206\u6790\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u4ecd\u7136\u5f88\u56f0\u96be\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u9700\u8981\u5927\u91cf\u6807\u6ce8\uff0c\u4f9d\u8d56\u9759\u6001\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6216\u8005\u5728\u7f3a\u5c11\u5f15\u7528\u65f6\u65e0\u6cd5\u53ef\u9760\u9a8c\u8bc1\u4e8b\u5b9e\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86DeepResearchEval\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u6784\u5efa\u548c\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002</li>\n    <li>\u4efb\u52a1\u6784\u5efa\u91c7\u7528\u57fa\u4e8e\u7528\u6237\u6863\u6848\u7684\u6d41\u7a0b\uff0c\u751f\u6210\u73b0\u5b9e\u4e14\u590d\u6742\u7684\u7814\u7a76\u4efb\u52a1\uff0c\u786e\u4fdd\u53ea\u4fdd\u7559\u9700\u8981\u591a\u6e90\u8bc1\u636e\u6574\u5408\u7684\u4efb\u52a1\u3002</li>\n    <li>\u8bc4\u4f30\u90e8\u5206\u5305\u62ec\u52a8\u6001\u8bc4\u4f30\u548c\u4e3b\u52a8\u4e8b\u5b9e\u6838\u67e5\uff0c\u80fd\u591f\u5728\u7f3a\u4e4f\u5f15\u7528\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u548c\u9a8c\u8bc1\u62a5\u544a\u58f0\u660e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep research systems help with complex web research and analysis, but evaluating them is difficult.</li>\n    <li>Current benchmarks need a lot of manual work and often do not check facts properly, especially without citations.</li>\n    <li>We introduce DeepResearchEval, a new automated framework for creating and evaluating research tasks.</li>\n    <li>The framework generates realistic research tasks based on different user profiles and focuses on tasks needing multiple sources.</li>\n    <li>It includes a system for evaluating tasks that adapts to each task's needs and checks facts automatically using web searches.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:38:31.000Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png", "numComments": 1, "submittedBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "fullname": "Keming Wu", "name": "wukeming11", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09259", "authors": [{"_id": "696856230ac10a06522f69dd", "name": "Jian Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69de", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:12.229Z", "hidden": false}, {"_id": "696856230ac10a06522f69df", "name": "Zhangqi Wang", "hidden": false}, {"_id": "696856230ac10a06522f69e0", "name": "Yu He", "hidden": false}, {"_id": "696856230ac10a06522f69e1", "name": "Haoran Luo", "hidden": false}, {"_id": "696856230ac10a06522f69e2", "name": "li yuan", "hidden": false}, {"_id": "696856230ac10a06522f69e3", "name": "Lingling Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69e4", "name": "Rui Mao", "hidden": false}, {"_id": "696856230ac10a06522f69e5", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:14.086Z", "hidden": false}, {"_id": "696856230ac10a06522f69e6", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T07:48:00.000Z", "submittedOnDailyAt": "2026-01-15T00:22:01.292Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "upvotes": 81, "discussionId": "696856230ac10a06522f69e7", "githubRepo": "https://github.com/exoskeletonzj/MAXS", "githubRepoAddedBy": "user", "ai_summary": "MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.", "ai_keywords": ["LLM agents", "tool execution", "reasoning planning", "lookahead strategy", "advantage value", "step consistency variance", "inter-step trend slopes", "trajectory convergence", "multi-tool reasoning", "inference efficiency"], "githubStars": 5, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5e38\u9047\u5230\u5c40\u9650\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAXS\u7684\u5143\u81ea\u9002\u5e94\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7075\u6d3b\u6574\u5408\u5de5\u5177\u6267\u884c\u548c\u63a8\u7406\u89c4\u5212\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>MAXS\u91c7\u7528\u524d\u77bb\u7b56\u7565\uff0c\u63d0\u524d\u4f30\u8ba1\u5de5\u5177\u4f7f\u7528\u7684\u4f18\u52bf\u4ef7\u503c\uff0c\u5e76\u9009\u62e9\u7a33\u5b9a\u4e14\u9ad8\u4ef7\u503c\u7684\u63a8\u7406\u6b65\u9aa4\u3002</li>\n    <li>\u5f15\u5165\u8f68\u8ff9\u6536\u655b\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u8def\u5f84\u4e00\u81f4\u6027\u8fbe\u5230\u540e\u505c\u6b62\u8fdb\u4e00\u6b65\u8ba1\u7b97\uff0c\u4ece\u800c\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u5b9e\u8bc1\u7814\u7a76\uff0cMAXS\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model (LLM) Agents can reason well by using different tools together, but they face issues like short-sighted predictions and unstable reasoning paths.</li>\n    <li>The proposed solution, MAXS, is a new framework that improves LLM agents by allowing them to plan their reasoning and tool use more effectively.</li>\n    <li>MAXS uses a lookahead approach to predict the best reasoning steps a few steps ahead and chooses stable and valuable actions based on this information.</li>\n    <li>It also has a mechanism to stop unnecessary calculations once the reasoning path is consistent, which helps save resources while maintaining effectiveness.</li>\n    <li>Tests show that MAXS works better than current methods in terms of both performance and efficiency across various models and datasets.</li>\n</ul>"}, "publishedAt": "2026-01-14T02:48:00.000Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09259.png", "numComments": 3, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09274", "authors": [{"_id": "6968568f0ac10a06522f69e9", "name": "Jian Zhang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ea", "name": "Yu He", "hidden": false}, {"_id": "6968568f0ac10a06522f69eb", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:02.764Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69ec", "name": "Zhangqi Wang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ed", "name": "Kai He", "hidden": false}, {"_id": "6968568f0ac10a06522f69ee", "name": "Fangzhi Xu", "hidden": false}, {"_id": "6968568f0ac10a06522f69ef", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:05.035Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69f0", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T08:17:41.000Z", "submittedOnDailyAt": "2026-01-15T00:23:45.077Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "upvotes": 74, "discussionId": "6968568f0ac10a06522f69f1", "projectPage": "https://a3-bench.github.io/", "githubRepo": "https://github.com/exoskeletonzj/A3-Bench", "githubRepoAddedBy": "user", "githubStars": 0, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u63a8\u7406\u4e0d\u4ec5\u4f9d\u8d56\u903b\u8f91\u63a8\u7406\uff0c\u8fd8\u9700\u8981\u6fc0\u6d3b\u5148\u524d\u77e5\u8bc6\u548c\u7ecf\u9a8c\u7ed3\u6784\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u8bb0\u5fc6\u9a71\u52a8\u7684\u4eba\u7c7b\u63a8\u7406\u673a\u5236\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86A^3-Bench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u8bb0\u5fc6\u9a71\u52a8\u7684\u6fc0\u6d3b\u673a\u5236\u3002</li>\n    <li>\u6211\u4eec\u6807\u6ce8\u4e862198\u4e2a\u79d1\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u53cc\u5c3a\u5ea6\u8bb0\u5fc6\u8bc4\u4f30\u6846\u67b6\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1A^3-Bench\uff0c\u5e76\u5206\u6790\u8bb0\u5fc6\u6fc0\u6d3b\u5bf9\u63a8\u7406\u8868\u73b0\u7684\u5f71\u54cd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Scientific reasoning relies on both logical thinking and using prior knowledge and experiences.</li>\n    <li>Current tests focus on final answers rather than the memory processes that help in reasoning.</li>\n    <li>A^3-Bench is a new benchmark created to assess scientific reasoning based on memory activation.</li>\n    <li>It includes 2,198 science reasoning problems and a new way to evaluate memory use, called the AAUI metric.</li>\n    <li>Experiments show how memory activation affects reasoning performance, offering new insights into scientific reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-14T03:17:41.000Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09274.png", "numComments": 2, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09667", "authors": [{"_id": "6969b0f732f0333869ff9476", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:48.445Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9477", "user": {"_id": "662b4e3bc709a61df840fda1", "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg", "isPro": false, "fullname": "Hu Yunhai", "user": "AlexCCtop", "type": "user"}, "name": "Yunhai Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:37:06.706Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9478", "user": {"_id": "650026d30339dae3dba2cec5", "avatarUrl": "/avatars/fcc9ea4336f8d4bb177e5c9eacdd05c9.svg", "isPro": false, "fullname": "Juncheng Liu", "user": "juncliu", "type": "user"}, "name": "Juncheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:33.401Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9479", "name": "Shuyue Stella Li", "hidden": false}, {"_id": "6969b0f732f0333869ff947a", "name": "Yucheng Wang", "hidden": false}, {"_id": "6969b0f732f0333869ff947b", "user": {"_id": "638e40d450a4e4beef98196b", "avatarUrl": "/avatars/fe27e019baf48caeb44e19b7289db9fb.svg", "isPro": false, "fullname": "Zhen Xu", "user": "zhenxu", "type": "user"}, "name": "Zhen Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:04.868Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947c", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0f732f0333869ff947d", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:15.855Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947e", "name": "Xinxing Xu", "hidden": false}, {"_id": "6969b0f732f0333869ff947f", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:25.577Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9480", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:31.289Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9481", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:36.481Z", "hidden": false}], "publishedAt": "2026-01-14T17:57:43.000Z", "submittedOnDailyAt": "2026-01-16T01:01:32.343Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "upvotes": 63, "discussionId": "6969b0f832f0333869ff9482", "ai_summary": "Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.", "ai_keywords": ["multi-agent systems", "reinforcement learning", "test-time reinforcement learning", "multi-agent reinforcement learning", "credit assignment", "multi-expert teams", "dialogue systems", "distribution-shift-robust reasoning"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5df2\u7ecf\u6210\u4e3a\u8bb8\u591a\u5e94\u7528\u4e2d\u7684\u5408\u4f5c\u4f19\u4f34\uff0c\u5177\u5907\u591a\u6837\u6027\u548c\u4ea4\u53c9\u68c0\u67e5\u7684\u4f18\u52bf\u3002</li>\n    <li>\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u961f\u53cb\u4e4b\u95f4\u7684\u5171\u540c\u9002\u5e94\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\uff0c\u5956\u52b1\u901a\u5e38\u7a00\u758f\u4e14\u53d8\u5316\u5927\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08MATTRL\uff09\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u7ed3\u6784\u5316\u6587\u672c\u7ecf\u9a8c\u6ce8\u5165\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u4e2d\u3002</li>\n    <li>MATTRL \u5f62\u6210\u4e00\u4e2a\u591a\u4e13\u5bb6\u56e2\u961f\uff0c\u8fdb\u884c\u591a\u8f6e\u8ba8\u8bba\uff0c\u5e76\u5728\u6700\u7ec8\u51b3\u7b56\u4e2d\u8fbe\u6210\u5171\u8bc6\u3002</li>\n    <li>\u5728\u533b\u5b66\u3001\u6570\u5b66\u548c\u6559\u80b2\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATTRL \u7684\u51c6\u786e\u6027\u6bd4\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u9ad8\u4e86\u5e73\u5747 3.67%\uff0c\u6bd4\u76f8\u4f3c\u7684\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u9ad8\u4e86 8.67%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems using large language models (LLMs) are becoming useful collaborators in various fields, but training them can be resource-heavy and unstable.</li>\n    <li>To address this, a new framework called Multi-Agent Test-Time Reinforcement Learning (MATTRL) is introduced, which enhances decision-making by using structured experiences during inference.</li>\n    <li>MATTRL creates a team of specialists that discuss and reach consensus during multi-turn interactions, improving the decision-making process.</li>\n    <li>The framework shows better performance, increasing accuracy by an average of 3.67% compared to existing multi-agent systems, and 8.67% over single-agent systems.</li>\n    <li>The study also explores how different credit-assignment methods affect training results, ensuring a stable and effective approach to multi-agent reasoning without the need for tuning.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:57:43.000Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09667.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09088", "authors": [{"_id": "69688bbc0ac10a06522f6aeb", "user": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "name": "Shaotian Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:27.639Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aec", "name": "Kaiyuan Liu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aed", "user": {"_id": "64b73e3830a0b8ff60145a29", "avatarUrl": "/avatars/297469812b57b2ddf7d52b9391d80bde.svg", "isPro": false, "fullname": "Chen Shen", "user": "zjushenchen", "type": "user"}, "name": "Chen Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:19.260Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aee", "user": {"_id": "6225b0d87f5fba1007d62fae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6225b0d87f5fba1007d62fae/clONu5C-lkoSswcJjcG0u.jpeg", "isPro": false, "fullname": "Bing Wang", "user": "wangbing1416", "type": "user"}, "name": "Bing Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:17.091Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aef", "user": {"_id": "694a226980a37f293a4ce7c0", "avatarUrl": "/avatars/7a48f4eeb80a1b5688cbfb10a59765a0.svg", "isPro": false, "fullname": "Sinan Fan", "user": "sinan25", "type": "user"}, "name": "Sinan Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:14.748Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af0", "name": "Jun Zhang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af1", "name": "Yue Wu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af2", "name": "Zheng Wang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af3", "name": "Jieping Ye", "hidden": false}], "publishedAt": "2026-01-14T02:43:17.000Z", "submittedOnDailyAt": "2026-01-15T07:24:58.461Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "submittedOnDailyBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "upvotes": 43, "discussionId": "69688bbc0ac10a06522f6af4", "projectPage": "https://github.com/D2I-ai/dasd-thinking", "githubRepo": "https://github.com/D2I-ai/dasd-thinking", "githubRepoAddedBy": "user", "ai_summary": "A lightweight open-source reasoning model achieves state-of-the-art performance through enhanced sequence-level distillation that addresses limitations in current teacher-student knowledge transfer methods.", "ai_keywords": ["sequence-level distillation", "teacher-student distillation", "SFT", "heuristic rules", "output distribution", "generalization capability", "exposure bias", "teacher-forced training", "autoregressive inference"], "githubStars": 16, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aDASD-4B-Thinking\u7684\u8f7b\u91cf\u7ea7\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u6570\u5b66\u3001\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u5927\u578b\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u6307\u51fa\u5176\u5728\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u4e4b\u95f4\u7684\u4e92\u52a8\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e09\u5927\u5c40\u9650\uff0c\u5305\u62ec\u6559\u5e08\u5e8f\u5217\u5206\u5e03\u8868\u793a\u4e0d\u8db3\u548c\u5b66\u4e60\u80fd\u529b\u4e0d\u5339\u914d\u3002</li>\n    <li>DASD-4B-Thinking\u53ea\u4f7f\u7528448K\u8bad\u7ec3\u6837\u672c\uff0c\u663e\u8457\u5c11\u4e8e\u5927\u591a\u6570\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u6a21\u578b\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DASD-4B-Thinking is a new open-source reasoning model that performs well in math, science, and coding tasks.</li>\n    <li>It outperforms larger models while using fewer training samples, making it efficient.</li>\n    <li>The report critiques the common distillation method used in model training, highlighting its limitations.</li>\n    <li>Three main issues were identified: poor representation of teacher outputs, misalignment in learning, and training biases.</li>\n    <li>The authors propose new methods to improve the training process and share their models and data with the community.</li>\n</ul>"}, "publishedAt": "2026-01-13T21:43:17.000Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09088.png", "numComments": 4, "submittedBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "fullname": "Shaotian", "name": "ystluffy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08225", "authors": [{"_id": "6967202cc5e371f6b235d1cc", "name": "Jungho Cho", "hidden": false}, {"_id": "6967202cc5e371f6b235d1cd", "name": "Minbyul Jeong", "hidden": false}, {"_id": "6967202cc5e371f6b235d1ce", "user": {"_id": "65446c938737c799e9ad6f83", "avatarUrl": "/avatars/6ade251e01442b14cbf8cd7888358fd1.svg", "isPro": false, "fullname": "Sungrae Park", "user": "sungrae-park", "type": "user"}, "name": "Sungrae Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:29.339Z", "hidden": false}], "publishedAt": "2026-01-13T05:14:09.000Z", "submittedOnDailyAt": "2026-01-14T02:19:55.431Z", "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale", "submittedOnDailyBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "isPro": false, "fullname": "Minbyul Jeong", "user": "Minbyul", "type": "user"}, "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.", "upvotes": 40, "discussionId": "6967202dc5e371f6b235d1cf", "ai_summary": "Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.", "ai_keywords": ["large reasoning models", "multi-turn dialogue generation", "task-oriented simulation", "user simulator", "behavioral rules", "turn-by-turn feedback", "automated task generation", "high-density dataset", "human-agent interaction"], "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u9700\u6c42\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u3002</li>\n    <li>\u73b0\u6709\u7684\u6570\u636e\u96c6\u548c\u751f\u6210\u65b9\u6cd5\u6709\u5c40\u9650\uff0c\u65e0\u6cd5\u5e94\u5bf9\u4eba\u673a\u534f\u4f5c\u7684\u590d\u6742\u6027\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u591a\u8f6e\u5bf9\u8bdd\uff0c\u4ee5\u52a8\u6001\u751f\u6210\u9ad8\u4ef7\u503c\u7684\u9886\u57df\u7279\u5b9a\u5de5\u5177\u3002</li>\n    <li>\u521d\u59cb\u8bbe\u8ba1\u8fc7\u4e8e\u4e13\u6ce8\u4e8e\u4efb\u52a1\u89e3\u51b3\uff0c\u5bfc\u81f4\u5bf9\u8bdd\u4e92\u52a8\u8f83\u5c11\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u3002</li>\n    <li>\u901a\u8fc7\u7528\u6237\u5bfc\u5411\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u4e86\u66f4\u771f\u5b9e\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u751f\u6210\u7684\u6570\u636e\u96c6\u80fd\u4f53\u73b0\u4eba\u673a\u4e92\u52a8\u7684\u590d\u6742\u9700\u6c42\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large reasoning models (LRMs) are becoming popular as autonomous agents that need advanced tool-use skills.</li>\n    <li>Current datasets are limited by fixed tools and don't adapt well to complex human-agent interactions.</li>\n    <li>The authors created a system to automatically generate dialogues that involve using tools to complete tasks.</li>\n    <li>They found that focusing only on tasks leads to limited conversations, so they developed a system that simulates human-like interactions.</li>\n    <li>This new approach allows for more realistic and longer dialogues, producing a rich dataset that captures real-world problem-solving interactions.</li>\n</ul>"}, "publishedAt": "2026-01-13T00:14:09.000Z", "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale", "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08225.png", "numComments": 2, "submittedBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "fullname": "Minbyul Jeong", "name": "Minbyul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0cVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u7684\u5f00\u653e\u9886\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u8981\u6c42\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u63d0\u53d6\u3001\u4e92\u52a8\u7f51\u9875\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u201cAgentic\u201d\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5e76\u4e0d\u4f18\u4e8e\u201cWorkflow\u201d\u65b9\u6cd5\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u4e0b\u4e00\u4ee3\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7684\u4e3b\u8981\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often relies on visual clues from videos and requires retrieving information from the web.</li>\n    <li>We created a new benchmark called VideoDR for video-based question answering that includes tasks like visual clue extraction and multi-hop reasoning.</li>\n    <li>VideoDR includes high-quality samples across six different topics, thanks to careful human annotation.</li>\n    <li>We tested various models and found that the effectiveness of different approaches depends on how well they keep track of visual information during retrieval.</li>\n    <li>The study highlights important challenges for future video research agents, such as maintaining consistency over long tasks.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\uff0c\u8868\u73b0\u8f83\u5f31\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aBabyVision\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30MLLMs\u7684\u57fa\u672c\u89c6\u89c9\u80fd\u529b\uff0c\u4e0d\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\uff0c\u6db5\u76d6\u56db\u4e2a\u5173\u952e\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u513f\u7ae5\uff0cGemini3-Pro-Preview\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c\u6210\u4eba\u7684\u5e73\u5747\u5f97\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u4e3a\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop basic visual skills early, but modern Multimodal LLMs (MLLMs) still depend heavily on language to understand visuals.</li>\n    <li>Many top MLLMs struggle with simple visual tasks that even young children can do easily.</li>\n    <li>To study this issue, the researchers created BabyVision, a benchmark that tests visual skills without relying on language.</li>\n    <li>BabyVision includes 388 tasks across four main categories, showing that MLLMs perform much worse than humans on these tests.</li>\n    <li>The research aims to improve MLLMs' visual understanding and has tools available for others to use and replicate the findings.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u8bc6\u522b\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u5728\u8bc6\u522b\u7269\u7406\u5c5e\u6027\uff08\u5982\u5efa\u7b51\u7269\u3001\u6c34\u4f53\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u5f88\u5f3a\u7684\u96f6-shot \u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces have many different social and physical features that need to be identified from satellite images.</li>\n    <li>Current models can identify physical features well, but not social categories like schools and parks.</li>\n    <li>The authors created a new dataset called SocioSeg, which includes satellite images and detailed labels for social entities.</li>\n    <li>They developed a new framework called SocioReasoner that uses vision-language reasoning to help identify social entities more effectively.</li>\n    <li>Tests show their method outperforms existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u7684\u5e73\u8861\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002</li>\n    <li>\u91c7\u7528\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6765\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u89c6\u89c9\u5047\u8bbe\u63a2\u7d22\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u4ec5\u4e3a10\u4ebf\u53c2\u6570\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u80fd\u4e0e10\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8d8a\u4e00\u4e9b\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u793e\u533a\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>The model uses a unique training method with 1.2 trillion multimodal data points, combining a perception encoder and a decoder for better understanding of language and vision.</li>\n    <li>It employs a post-training process with over 1,000 rounds of reinforcement learning to improve its performance.</li>\n    <li>Despite being smaller than many competing models, STEP3-VL-10B performs exceptionally well, achieving high scores on various benchmarks.</li>\n    <li>The full model is available for the community to use, promoting efficient and reproducible research.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4f7f\u7528\u5730\u56fe\u8fd9\u4e00\u4eba\u7c7b\u5e38\u7528\u7b56\u7565\u3002</li>\n    <li>\u6211\u4eec\u4e3a\u6a21\u578b\u52a0\u5165\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u5faa\u73af\u201d\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002</li>\n    <li>\u63d0\u51fa\u4e86MAPBench\uff0c\u4e00\u4e2a\u5b8c\u5168\u7531\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7ec4\u6210\u7684\u5730\u7406\u5b9a\u4f4d\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728500\u7c73\u51c6\u786e\u7387\u4e0a\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to determine where a photo was taken on Earth using visual clues.</li>\n    <li>Current models focus on world knowledge and reasoning but often ignore the use of maps, which is a common human strategy.</li>\n    <li>This study introduces a new model that incorporates map thinking and uses a two-step optimization process.</li>\n    <li>The model uses reinforcement learning to improve its decision-making and parallel testing to explore several options before deciding.</li>\n    <li>Results show this new approach performs better than existing models, significantly increasing accuracy in predicting locations.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u540e\u671f\u8bad\u7ec3\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u51fa\u73b0\u201c\u63a2\u7d22\u5d29\u6e83\u201d\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u63d0\u524d\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u867d\u7136\u63d0\u5347\u4e86\u901a\u8fc7\u7387\uff08pass@1\uff09\uff0c\u4f46\u9650\u5236\u4e86\u591a\u6837\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e13\u6ce8\u4e8e\u5956\u52b1\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u7ed3\u679c\u8fdb\u884c\u805a\u7c7b\uff0c\u4ece\u800c\u63d0\u5347\u7a00\u6709\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u591a\u6837\u6027\u548cpass@k\uff0c\u800c\u4e0d\u727a\u7272pass@1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models (LLMs) in complex reasoning tasks.</li>\n    <li>A common problem is \"exploration collapse,\" where models focus too much on a few successful reasoning patterns.</li>\n    <li>This focus limits diversity in solutions and improvements in performance on larger problem sets.</li>\n    <li>The proposed solution, Uniqueness-Aware Reinforcement Learning, rewards unique and correct solutions with rare strategies.</li>\n    <li>This method improves overall performance and exploration across various subjects without losing effectiveness in simpler tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 103, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 93, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5df2\u6210\u4e3a\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u7684\u91cd\u8981\u65b9\u6cd5\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u5730\u652f\u6301\u590d\u6742\u63a8\u7406\u548c\u6574\u4f53\u7406\u89e3\u3002</li>\n    <li>HGMem\u7684\u8bb0\u5fc6\u4ee5\u8d85\u56fe\u5f62\u5f0f\u8868\u793a\uff0c\u5141\u8bb8\u8bb0\u5fc6\u5355\u5143\u4e4b\u95f4\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cHGMem\u5728\u591a\u4e2a\u5168\u7403\u6027\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models think better and reason more effectively.</li>\n    <li>Current memory systems only store facts passively and do not connect them well, limiting their usefulness for reasoning.</li>\n    <li>HGMem is a new memory system that uses hypergraphs to create dynamic connections between facts, improving reasoning and understanding.</li>\n    <li>This new memory structure allows for better organization of knowledge and stronger reasoning in complex tasks.</li>\n    <li>Tests show that HGMem significantly enhances performance in multi-step RAG compared to existing systems.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u6a21\u578b\u4e0d\u4ec5\u80fd\u7ed9\u51fa\u51c6\u786e\u7684\u56de\u7b54\uff0c\u8fd8\u80fd\u8868\u73b0\u51fa\u7b26\u5408\u591a\u6837\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u4f7f\u7528\u591a\u79cd\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u671d\u5411\u8fd9\u4e9b\u671f\u671b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u4f7f\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u5f52\u4e00\u5316\uff0c\u4f7f\u5f97\u5b83\u4eec\u7684\u4f18\u52bf\u503c\u76f8\u540c\uff0c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u5956\u52b1\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u90fd\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate answers and behave according to various human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models toward desired behaviors, but the method used (GRPO) may cause issues.</li>\n    <li>Applying GRPO can lead to similar reward values, making training less effective and sometimes causing it to fail early.</li>\n    <li>A new method called GDPO helps by keeping individual rewards separate, improving training accuracy and stability.</li>\n    <li>GDPO outperforms GRPO in three tasks: tool calling, math reasoning, and coding reasoning, showing it works better for multi-reward optimization.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u53cd\u590d\u7684\u201c\u751f\u6210-\u9a8c\u8bc1-\u6539\u8fdb\u201d\u5468\u671f\u6765\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5728\u6709\u9650\u9884\u7b97\u5185\u627e\u5230\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8fd9\u79cd\u4f4e\u6548\u7387\u6e90\u4e8e\u521d\u59cb\u5316\u504f\u5dee\u3001\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u968f\u673a\u64cd\u4f5c\u4ee5\u53ca\u8de8\u4efb\u52a1\u7684\u7ecf\u9a8c\u5229\u7528\u4e0d\u8db3\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53d7\u63a7\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\uff0c\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u57fa\u4e8e\u53cd\u9988\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5206\u5c42\u8fdb\u5316\u8bb0\u5fc6\u3002</li>\n    <li>CSE\u5728EffiBench-X\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\uff0c\u4e14\u5728\u65e9\u671f\u751f\u6210\u4e2d\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u5728\u6574\u4e2a\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\uff0c\u5730\u5740\u662f https://github.com/QuantaAlpha/EvoControl\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods help improve code generation but often struggle to find better solutions efficiently.</li>\n    <li>The main issues are poor starting points, random operations without guidance, and not making use of past experiences.</li>\n    <li>Controlled Self-Evolution (CSE) addresses these issues with three main features: diverse planning for better strategy coverage, feedback-based evolution methods, and a memory system to track successes and failures.</li>\n    <li>Tests show that CSE performs better than other methods and improves efficiency early on while continuing to get better.</li>\n    <li>The CSE code is available for public use at https://github.com/QuantaAlpha/EvoControl.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>\u81ea\u52a8\u5f62\u5f0f\u5316\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u7ffb\u8bd1\u4e3a\u6b63\u5f0f\u8bed\u53e5\uff0c\u4ee5\u4fbf\u673a\u5668\u63a8\u7406\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faMMFormalizer\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6570\u5b66\u548c\u7269\u7406\u5b9e\u4f53\uff0c\u6269\u5c55\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u3002</li>\n    <li>MMFormalizer\u901a\u8fc7\u9012\u5f52\u6784\u5efa\u5f62\u5f0f\u547d\u9898\uff0c\u786e\u4fdd\u6bcf\u4e2a\u62bd\u8c61\u90fd\u6709\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5PhyX-AF\u4e0a\u8bc4\u4f30MMFormalizer\uff0c\u7ed3\u679c\u663e\u793aGPT-5\u5728\u7269\u7406\u63a8\u7406\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002</li>\n    <li>\u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization helps convert natural language math into formal statements for machine reasoning but faces challenges due to the complex physical world.</li>\n    <li>MMFormalizer improves this process by connecting text with real-world mathematical and physical concepts using visual elements.</li>\n    <li>It builds formal propositions from visual and perceptual information, ensuring all abstractions are backed by evidence.</li>\n    <li>MMFormalizer was tested on a new benchmark called PhyX-AF, showing strong performance, especially with GPT-5 in physical reasoning.</li>\n    <li>This is the first method that effectively addresses various fields like classical mechanics, relativity, quantum mechanics, and thermodynamics in multimodal autoformalization.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 19, 2026";
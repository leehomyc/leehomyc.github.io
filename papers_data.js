window.trendingPapers = {
    "today": [{"paper": {"id": "2512.16093", "authors": [{"_id": "6944be16fbf17e708e186002", "user": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "name": "Jintao Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:52:30.866Z", "hidden": false}, {"_id": "6944be16fbf17e708e186003", "name": "Kaiwen Zheng", "hidden": false}, {"_id": "6944be16fbf17e708e186004", "name": "Kai Jiang", "hidden": false}, {"_id": "6944be16fbf17e708e186005", "name": "Haoxu Wang", "hidden": false}, {"_id": "6944be16fbf17e708e186006", "name": "Ion Stoica", "hidden": false}, {"_id": "6944be16fbf17e708e186007", "name": "Joseph E. Gonzalez", "hidden": false}, {"_id": "6944be16fbf17e708e186008", "name": "Jianfei Chen", "hidden": false}, {"_id": "6944be16fbf17e708e186009", "name": "Jun Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "publishedAt": "2025-12-18T02:21:30.000Z", "submittedOnDailyAt": "2025-12-25T00:44:44.469Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "submittedOnDailyBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "upvotes": 48, "discussionId": "6944be16fbf17e708e18600a", "projectPage": "https://github.com/thu-ml/TurboDiffusion", "githubRepo": "https://github.com/thu-ml/TurboDiffusion", "githubRepoAddedBy": "user", "ai_summary": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.", "ai_keywords": ["SageAttention", "Sparse-Linear Attention", "rCM", "W8A8 quantization", "diffusion generation", "video generation", "RTX 5090 GPU"], "githubStars": 1958, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "summary_zh": "<ul>\n    <li>TurboDiffusion\u662f\u4e00\u79cd\u89c6\u9891\u751f\u6210\u52a0\u901f\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u89c6\u9891\u751f\u6210\u901f\u5ea6\u63d0\u9ad8100-200\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u51e0\u4e2a\u7ec4\u4ef6\u5b9e\u73b0\u52a0\u901f\uff0c\u5305\u62ec\u4f4e\u4f4dSageAttention\u548c\u53ef\u8bad\u7ec3\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\uff08SLA\uff09\u6765\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u3002</li>\n    <li>\u91c7\u7528rCM\u8fdb\u884c\u9ad8\u6548\u7684\u6b65\u9aa4\u84b8\u998f\uff0c\u4ee5\u53ca\u5c06\u6a21\u578b\u53c2\u6570\u548c\u6fc0\u6d3b\u91cf\u5316\u4e3a8\u4f4d\uff0c\u4ee5\u52a0\u901f\u7ebf\u6027\u5c42\u5e76\u538b\u7f29\u6a21\u578b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5355\u4e2aRTX 5090 GPU\u4e0a\uff0cTurboDiffusion\u4e5f\u80fd\u5b9e\u73b0100-200\u500d\u7684\u89c6\u9891\u751f\u6210\u52a0\u901f\u3002</li>\n    <li>\u8be5\u9879\u76ee\u7684\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u53ef\u4ee5\u5728GitHub\u4e0a\u627e\u5230\uff0c\u94fe\u63a5\u4e3ahttps://github.com/thu-ml/TurboDiffusion\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>TurboDiffusion is a new framework that speeds up video generation by 100-200 times without losing quality.</li>\n    <li>It uses special techniques like low-bit SageAttention and Sparse-Linear Attention to make attention computation faster.</li>\n    <li>The framework also employs step distillation for efficiency and 8-bit quantization to compress and speed up the model.</li>\n    <li>Tests showed that TurboDiffusion works well even on a single RTX 5090 GPU, maintaining good video quality.</li>\n    <li>The project is available on GitHub with model checkpoints and user-friendly code.</li>\n</ul>"}, "publishedAt": "2025-12-17T21:21:30.000Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16093.png", "numComments": 2, "submittedBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "fullname": "Jintao Zhang", "name": "jt-zhang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4e00\u822c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u4e0a\u4ecd\u7136\u8f83\u5f31\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63a8\u51fa\u4e86DSR Suite\uff0c\u901a\u8fc7\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\u4ece\u89c6\u9891\u4e2d\u751f\u6210\u591a\u9009\u9898\u548c\u7b54\u6848\u3002</li>\n    <li>\u8be5\u6d41\u7a0b\u63d0\u53d6\u4e86\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u6784\u5efa\u4e86\u7528\u4e8e\u5b66\u4e60\u7684DSR-Train\u548c\u7528\u4e8e\u8bc4\u4f30\u7684DSR-Bench\u3002</li>\n    <li>\u6570\u636e\u96c6\u5f3a\u8c03\u4e86\u771f\u5b9e\u89c6\u9891\u6765\u6e90\u30013D\u9700\u6c42\u3001\u89c6\u89d2\u53d8\u5316\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u7ec6\u81f4\u7684\u7b54\u6848\u3002</li>\n    <li>\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\uff0c\u4ee5\u5c06\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u6709\u6548\u6574\u5408\u5230VLM\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) are good at general understanding but struggle with dynamic spatial reasoning (DSR), which involves understanding how objects change in 3D space over time.</li>\n    <li>To improve DSR, the authors introduced the DSR Suite, which includes a system to create multiple-choice questions from real-world videos.</li>\n    <li>This system uses advanced vision models to gather detailed information about objects, such as their shapes, positions, and movements.</li>\n    <li>Key features of the dataset include diverse video sources, 3D object and scene requirements, and the ability to analyze interactions between multiple objects.</li>\n    <li>The authors developed a Geometry Selection Module (GSM) that helps VLMs use relevant geometric information effectively, resulting in better DSR performance without losing accuracy in general video understanding.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 33, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u901a\u8fc7\u8f85\u52a9\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u6216\u56fe\u50cf\u526a\u88c1\u6765\u76d1\u7763\u89c6\u89c9\u6b65\u9aa4\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u6709\u5f88\u591a\u9650\u5236\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4efb\u52a1\u7279\u5b9a\u673a\u5236\uff0c\u5141\u8bb8\u6a21\u578b\u81ea\u4e3b\u53d1\u73b0\u548c\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\uff0c\u65e0\u9700\u660e\u786e\u76d1\u7763\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u63d0\u53d6\u76f8\u5173\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u8d85\u8d8a\u4e86\u76f4\u63a5\u5fae\u8c03\u7684\u6548\u679c\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u80fd\u5f88\u597d\u5730\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text, which limits their ability to perform visual reasoning tasks.</li>\n    <li>Current methods use extra images or annotations to help LMMs with visual tasks, but these methods have drawbacks like high costs and limited flexibility.</li>\n    <li>The new approach we propose allows LMMs to learn visual reasoning on their own, without needing explicit guidance.</li>\n    <li>This method helps models adaptively process images and extract useful visual information, improving performance on various vision-related tasks.</li>\n    <li>Our approach achieves better results than traditional fine-tuning and can be applied to multiple tasks effectively.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 2, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20605", "authors": [{"_id": "694e76e5746a34b55dd545eb", "name": "Seijin Kobayashi", "hidden": false}, {"_id": "694e76e5746a34b55dd545ec", "name": "Yanick Schimpf", "hidden": false}, {"_id": "694e76e5746a34b55dd545ed", "name": "Maximilian Schlegel", "hidden": false}, {"_id": "694e76e5746a34b55dd545ee", "name": "Angelika Steger", "hidden": false}, {"_id": "694e76e5746a34b55dd545ef", "name": "Maciej Wolczyk", "hidden": false}, {"_id": "694e76e5746a34b55dd545f0", "name": "Johannes von Oswald", "hidden": false}, {"_id": "694e76e5746a34b55dd545f1", "name": "Nino Scherrer", "hidden": false}, {"_id": "694e76e5746a34b55dd545f2", "name": "Kaitlin Maile", "hidden": false}, {"_id": "694e76e5746a34b55dd545f3", "name": "Guillaume Lajoie", "hidden": false}, {"_id": "694e76e5746a34b55dd545f4", "name": "Blake A. Richards", "hidden": false}, {"_id": "694e76e5746a34b55dd545f5", "name": "Rif A. Saurous", "hidden": false}, {"_id": "694e76e5746a34b55dd545f6", "name": "James Manyika", "hidden": false}, {"_id": "694e76e5746a34b55dd545f7", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "694e76e5746a34b55dd545f8", "name": "Alexander Meulemans", "hidden": false}, {"_id": "694e76e5746a34b55dd545f9", "name": "Jo\u00e3o Sacramento", "hidden": false}], "publishedAt": "2025-12-23T18:51:50.000Z", "submittedOnDailyAt": "2025-12-26T11:17:05.505Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "submittedOnDailyBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "upvotes": 25, "discussionId": "694e76e5746a34b55dd545fa", "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\u9884\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\uff0c\u5df2\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u6210\u529f\u3002</li>\n    <li>\u5728RL\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u9010\u4e2a\u751f\u6210\u65b0\u8f93\u51fa\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\uff0c\u5c24\u5176\u662f\u5728\u5956\u52b1\u7a00\u5c11\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u8fdb\u884c\u63a2\u7d22\uff0c\u514b\u670d\u8fd9\u4e00\u95ee\u9898\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e00\u79cd\u66f4\u9ad8\u9636\u7684\u975e\u56e0\u679c\u5e8f\u5217\u6a21\u578b\uff0c\u80fd\u591f\u63a7\u5236\u57fa\u7840\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6fc0\u6d3b\u6d41\u3002</li>\n    <li>\u8fd9\u79cd\u5185\u90e8\u63a7\u5236\u5668\u7684\u5f3a\u5316\u5b66\u4e60\uff08\u79f0\u4e3a\u201c\u5185\u90e8RL\u201d\uff09\u80fd\u6709\u6548\u5b66\u4e60\u7a00\u758f\u5956\u52b1\u4efb\u52a1\uff0c\u663e\u793a\u51fa\u5c42\u6b21\u5316RL\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large autoregressive models are great at predicting the next token and can be improved with reinforcement learning (RL).</li>\n    <li>Generating outputs one token at a time can be inefficient, especially when rewards are rare.</li>\n    <li>We propose using a higher-order model to explore actions within the internal workings of a base autoregressive model.</li>\n    <li>This new model learns to control sequences of actions, leading to better exploration and learning from sparse rewards.</li>\n    <li>Our findings suggest that using internal reinforcement learning could enhance hierarchical learning in large models.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:51:50.000Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png", "numComments": 2, "submittedBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "fullname": "Maximilian Schlegel", "name": "schlegelm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u201c\u4e00\u955c\u5230\u5e95\u201d\u6280\u672f\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5177\u6709\u72ec\u7279\u7684\u7f8e\u5b66\uff0c\u4f46\u5b9e\u65bd\u6210\u672c\u9ad8\u3001\u73b0\u5b9e\u9650\u5236\u590d\u6742\u3002</li>\n    <li>DreaMontage\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7528\u6237\u63d0\u4f9b\u7684\u8f93\u5165\u5408\u6210\u6d41\u7545\u3001\u8868\u73b0\u4e30\u5bcc\u7684\u957f\u65f6\u95f4\u4e00\u955c\u89c6\u9891\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u4e09\u79cd\u4e3b\u8981\u65b9\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5305\u62ec\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u6761\u4ef6\u673a\u5236\u548c\u81ea\u9002\u5e94\u8c03\u4f18\u7b56\u7565\u3002</li>\n    <li>\u4e3a\u4e86\u589e\u5f3a\u89c6\u89c9\u6548\u679c\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u65bd\u4e86\u89c6\u89c9\u8868\u8fbe\u7684\u7ec6\u5316\u9636\u6bb5\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u5206\u6bb5\u81ea\u56de\u5f52\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u9ad8\u6548\u5730\u751f\u4ea7\u6269\u5c55\u5e8f\u5217\uff0c\u5b9e\u73b0\u89c6\u89c9\u6548\u679c\u7684\u8fde\u8d2f\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" technique in filmmaking is unique but often expensive and complicated to implement.</li>\n    <li>Current video generation models struggle with smoothness and coherence when combining clips.</li>\n    <li>DreaMontage is a new framework that creates long, seamless one-shot videos from various user inputs.</li>\n    <li>The framework improves control over video generation, enhances visual quality, and ensures smooth transitions.</li>\n    <li>It uses efficient strategies for generating extended sequences while keeping computational demands low.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15716", "authors": [{"_id": "694b65e7746a34b55dd53dbe", "user": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "isPro": false, "fullname": "Jinjing Zhao", "user": "Jinjing713", "type": "user"}, "name": "Jinjing Zhao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:42.906Z", "hidden": false}, {"_id": "694b65e7746a34b55dd53dbf", "name": "Fangyun Wei", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc0", "name": "Zhening Liu", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc1", "name": "Hongyang Zhang", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc2", "name": "Chang Xu", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc3", "name": "Yan Lu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64f8962bce75bb0fb50bdbdb/eC4hVgIfk0MxzPgn6mvGC.mp4"], "publishedAt": "2025-12-17T18:59:59.000Z", "submittedOnDailyAt": "2025-12-26T03:15:40.222Z", "title": "Spatia: Video Generation with Updatable Spatial Memory", "submittedOnDailyBy": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "isPro": false, "fullname": "Jinjing Zhao", "user": "Jinjing713", "type": "user"}, "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.", "upvotes": 13, "discussionId": "694b65e8746a34b55dd53dc4", "projectPage": "https://zhaojingjing713.github.io/Spatia/", "githubRepo": "https://github.com/ZhaoJingjing713/Spatia", "githubRepoAddedBy": "user", "ai_summary": "Spatia, a spatial memory-aware video generation framework, maintains long-term spatial and temporal consistency by preserving and updating a 3D scene point cloud, enabling realistic video generation and interactive editing.", "ai_keywords": ["spatial memory-aware", "video generation framework", "3D scene point cloud", "dynamic-static disentanglement", "visual SLAM", "explicit camera control", "3D-aware interactive editing"], "githubStars": 50, "organization": {"_id": "670621bc820835bbf0d2b499", "name": "Sydney-Uni", "fullname": "The University of Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628dbd0f9ec8275172da853f/YSJ_DfLPaAywMvoIoM2J4.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u4fdd\u6301\u957f\u671f\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Spatia\uff0c\u4e00\u4e2a\u7a7a\u95f4\u8bb0\u5fc6\u610f\u8bc6\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u4fdd\u5b583D\u573a\u666f\u70b9\u4e91\u4f5c\u4e3a\u6301\u4e45\u7684\u7a7a\u95f4\u8bb0\u5fc6\u3002</li>\n    <li>Spatia\u901a\u8fc7\u89c6\u89c9SLAM\u52a8\u6001\u66f4\u65b0\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u8fed\u4ee3\u751f\u6210\u89c6\u9891\u7247\u6bb5\u3002</li>\n    <li>\u8be5\u8bbe\u8ba1\u589e\u5f3a\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u903c\u771f\u52a8\u6001\u5b9e\u4f53\u7684\u80fd\u529b\u3002</li>\n    <li>Spatia\u652f\u6301\u663e\u5f0f\u76f8\u673a\u63a7\u5236\u548c3D\u4ea4\u4e92\u7f16\u8f91\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u57fa\u4e8e\u8bb0\u5fc6\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video generation models have trouble keeping consistent visuals and motion over time.</li>\n    <li>Spatia is a new framework that uses a 3D point cloud as a memory to improve video quality.</li>\n    <li>It generates video clips step-by-step while updating this memory using visual SLAM technology.</li>\n    <li>This approach helps maintain a stable background while still allowing for realistic moving objects.</li>\n    <li>Spatia also supports features like camera control and interactive 3D editing for more flexible video creation.</li>\n</ul>"}, "publishedAt": "2025-12-17T13:59:59.000Z", "title": "Spatia: Video Generation with Updatable Spatial Memory", "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64f8962bce75bb0fb50bdbdb/eC4hVgIfk0MxzPgn6mvGC.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15716.png", "numComments": 2, "submittedBy": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "fullname": "Jinjing Zhao", "name": "Jinjing713", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "670621bc820835bbf0d2b499", "name": "Sydney-Uni", "fullname": "The University of Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628dbd0f9ec8275172da853f/YSJ_DfLPaAywMvoIoM2J4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.19995", "authors": [{"_id": "694e1a8c746a34b55dd54540", "name": "Ming Li", "hidden": false}, {"_id": "694e1a8c746a34b55dd54541", "name": "Chenrui Fan", "hidden": false}, {"_id": "694e1a8c746a34b55dd54542", "name": "Yize Cheng", "hidden": false}, {"_id": "694e1a8c746a34b55dd54543", "name": "Soheil Feizi", "hidden": false}, {"_id": "694e1a8c746a34b55dd54544", "name": "Tianyi Zhou", "hidden": false}], "publishedAt": "2025-12-23T02:44:25.000Z", "submittedOnDailyAt": "2025-12-26T02:51:58.853Z", "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "submittedOnDailyBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "isPro": false, "fullname": "Tianyi Zhou", "user": "zhoutianyi", "type": "user"}, "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "upvotes": 4, "discussionId": "694e1a8d746a34b55dd54545", "githubRepo": "https://github.com/MingLiiii/ThinkARM", "githubRepoAddedBy": "user", "githubStars": 2, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u8bc6\u522b\u548c\u5206\u6790\uff0c\u901a\u5e38\u53ea\u80fd\u770b\u5230\u8868\u9762\u7edf\u8ba1\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86ThinkARM\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u62bd\u8c61\u4e3a\u529f\u80fd\u6027\u63a8\u7406\u6b65\u9aa4\uff0c\u5982\u5206\u6790\u3001\u63a2\u7d22\u3001\u5b9e\u65bd\u548c\u9a8c\u8bc1\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u4e0d\u540c\u6a21\u578b\u7684\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u5e94\u7528\u6b64\u6846\u67b6\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u548c\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u7684\u7ed3\u6784\u5dee\u5f02\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u63a2\u7d22\u662f\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u7684\u91cd\u8981\u5206\u652f\u6b65\u9aa4\uff0c\u800c\u6548\u7387\u5bfc\u5411\u7684\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u6291\u5236\u8bc4\u4f30\u53cd\u9988\u6b65\u9aa4\u3002</li>\n    <li>\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u60c5\u8282\u7ea7\u522b\u7684\u8868\u793a\u4f7f\u63a8\u7406\u6b65\u9aa4\u660e\u786e\u5316\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u5206\u6790\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u7ed3\u6784\u53ca\u5176\u53d8\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models show reasoning patterns, but understanding their thought processes is challenging.</li>\n    <li>We use Schoenfeld's Episode Theory to create ThinkARM, a framework that breaks down reasoning into clear steps like Analysis and Verification.</li>\n    <li>This framework helps to identify differences in thinking between models that reason well and those that do not.</li>\n    <li>Two case studies highlight that exploration is a key step for correct reasoning and that some methods reduce feedback during reasoning.</li>\n    <li>Our findings show that analyzing reasoning at a detailed level helps us understand how reasoning works in language models.</li>\n</ul>"}, "publishedAt": "2025-12-22T21:44:25.000Z", "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19995.png", "numComments": 3, "submittedBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "fullname": "Tianyi Zhou", "name": "zhoutianyi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19949", "authors": [{"_id": "694e0a82746a34b55dd54516", "name": "Zixuan Huang", "hidden": false}, {"_id": "694e0a82746a34b55dd54517", "name": "Xiang Li", "hidden": false}, {"_id": "694e0a82746a34b55dd54518", "name": "Zhaoyang Lv", "hidden": false}, {"_id": "694e0a82746a34b55dd54519", "name": "James M. Rehg", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/629fe0dd198a9a5a755f3ce0/4F38iBHO42lBqgkLGfGka.png"], "publishedAt": "2025-12-23T00:38:52.000Z", "submittedOnDailyAt": "2025-12-26T02:44:58.632Z", "title": "How Much 3D Do Video Foundation Models Encode?", "submittedOnDailyBy": {"_id": "629fe0dd198a9a5a755f3ce0", "avatarUrl": "/avatars/e643d43f66c10729f155edca96aef1f8.svg", "isPro": false, "fullname": "Zixuan Huang", "user": "zxhuang1698", "type": "user"}, "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.", "upvotes": 4, "discussionId": "694e0a82746a34b55dd5451a", "projectPage": "https://vidfm-3d-probe.github.io/", "organization": {"_id": "60212a089f64108326fac7c2", "name": "illinois", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u662f\u4e09\u7ef4\u4e16\u754c\u7684\u8fde\u7eed\u4e8c\u7ef4\u6295\u5f71\uff0c\u7814\u7a76\u89c6\u9891\u6a21\u578b\u7684\u4e09\u7ef4\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6d45\u5c42\u8bfb\u53d6\u8bc4\u4f30\u591a\u4e2a\u89c6\u9891\u6a21\u578b\u7684\u4e09\u7ef4\u610f\u8bc6\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf9\u4e09\u7ef4\u7269\u4f53\u548c\u573a\u666f\u6709\u5f88\u5f3a\u7684\u7406\u89e3\uff0c\u5c3d\u7ba1\u6ca1\u6709\u63a5\u53d7\u4e09\u7ef4\u6570\u636e\u7684\u8bad\u7ec3\u3002</li>\n    <li>\u8fd9\u4e9b\u89c6\u9891\u6a21\u578b\u5728\u4e09\u7ef4\u7406\u89e3\u65b9\u9762\u7684\u8868\u73b0\u751a\u81f3\u8d85\u8fc7\u4e86\u4e00\u4e9b\u4e13\u95e8\u4e3a\u4e09\u7ef4\u4efb\u52a1\u8bad\u7ec3\u7684\u5927\u578b\u6a21\u578b\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u4e09\u7ef4\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c2\u5bdf\u548c\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Videos show 3D worlds in 2D, and researchers are exploring if video models can understand 3D by training on large video datasets.</li>\n    <li>A new framework has been created to measure how well different video models understand 3D features.</li>\n    <li>The study finds that advanced video generation models have a strong grasp of 3D objects and scenes without needing 3D training data.</li>\n    <li>These video models sometimes perform better at 3D understanding than models specifically made for 3D tasks.</li>\n    <li>The research provides insights that could help develop better 3D models in the future.</li>\n</ul>"}, "publishedAt": "2025-12-22T19:38:52.000Z", "title": "How Much 3D Do Video Foundation Models Encode?", "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/629fe0dd198a9a5a755f3ce0/4F38iBHO42lBqgkLGfGka.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19949.png", "numComments": 2, "submittedBy": {"_id": "629fe0dd198a9a5a755f3ce0", "avatarUrl": "/avatars/e643d43f66c10729f155edca96aef1f8.svg", "fullname": "Zixuan Huang", "name": "zxhuang1698", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "60212a089f64108326fac7c2", "name": "illinois", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19680", "authors": [{"_id": "694b4e0c746a34b55dd53c34", "name": "Xinyao Liao", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c35", "name": "Qiyuan He", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c36", "name": "Kai Xu", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c37", "name": "Xiaoye Qu", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c38", "name": "Yicong Li", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c39", "name": "Wei Wei", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c3a", "name": "Angela Yao", "hidden": false}], "publishedAt": "2025-12-22T18:54:30.000Z", "submittedOnDailyAt": "2025-12-26T02:44:27.188Z", "title": "VA-\u03c0: Variational Policy Alignment for Pixel-Aware Autoregressive Generation", "submittedOnDailyBy": {"_id": "64cb54da1af278541d663708", "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg", "isPro": false, "fullname": "Xiaoye Qu", "user": "Xiaoye08", "type": "user"}, "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-\u03c0, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-\u03c0 formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-\u03c0 introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-\u03c0 enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.", "upvotes": 3, "discussionId": "694b4e0c746a34b55dd53c3b", "projectPage": "https://lil-shake.github.io/va-pi.github.io/", "githubRepo": "https://github.com/Lil-Shake/VA-Pi", "githubRepoAddedBy": "user", "ai_summary": "VA-$\\pi$ optimizes autoregressive visual generators using a pixel-space objective to improve image quality and performance without retraining tokenizers or using external rewards.", "ai_keywords": ["autoregressive (AR) visual generation", "tokenizers", "discrete sequences", "evidence lower bound (ELBO)", "reinforcement-based alignment", "policy", "intrinsic reward", "teacher forcing", "distributional consistency", "FID", "IS", "LlamaGen-XXL", "GenEval", "LlamaGen", "Janus-Pro"], "githubStars": 4, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u4f9d\u8d56\u4e8e\u5c06\u56fe\u50cf\u6620\u5c04\u4e3a\u79bb\u6563\u5e8f\u5217\u7684\u5206\u8bcd\u5668\uff0c\u4f46\u73b0\u6709\u7684\u5206\u8bcd\u5668\u4e0e\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86VA-\u03c0\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u76f4\u63a5\u4f18\u5316\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u91c7\u7528\u50cf\u7d20\u7a7a\u95f4\u76ee\u6807\u8fdb\u884c\u5bf9\u9f50\u3002</li>\n    <li>VA-\u03c0\u901a\u8fc7\u53d8\u5206\u4f18\u5316\u6765\u7edf\u4e00\u50cf\u7d20\u91cd\u5efa\u548c\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u4f7f\u7528\u50cf\u7d20\u7a7a\u95f4\u91cd\u5efa\u8d28\u91cf\u4f5c\u4e3a\u5956\u52b1\u6765\u4f18\u5316\u751f\u6210\u5668\u3002</li>\n    <li>\u8be5\u6846\u67b6\u65e0\u987b\u91cd\u65b0\u8bad\u7ec3\u5206\u8bcd\u5668\u6216\u4f7f\u7528\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u4ec5\u75281%\u7684ImageNet-1K\u6570\u636e\u548c25\u5206\u949f\u7684\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u3002</li>\n    <li>VA-\u03c0\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u83b7\u5f97\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AR visual generation uses tokenizers to convert images into sequences, but there can be issues with image quality when generating tokens.</li>\n    <li>VA-\u03c0 is a new method that improves AR models by focusing on directly optimizing image quality using pixel-level objectives.</li>\n    <li>It uses a variational optimization approach to align the generator and tokenizer, enhancing image reconstruction quality.</li>\n    <li>VA-\u03c0 works quickly with minimal data (1% of ImageNet-1K) and tuning time (25 minutes), significantly improving image quality metrics.</li>\n    <li>The method can be applied to existing models without needing to retrain tokenizers or use external rewards, and the code is publicly available.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:54:30.000Z", "title": "VA-\u03c0: Variational Policy Alignment for Pixel-Aware Autoregressive Generation", "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-\u03c0, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-\u03c0 formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-\u03c0 introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-\u03c0 enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19680.png", "numComments": 2, "submittedBy": {"_id": "64cb54da1af278541d663708", "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg", "fullname": "Xiaoye Qu", "name": "Xiaoye08", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13043", "authors": [{"_id": "694df96e746a34b55dd544df", "name": "Tong Wei", "hidden": false}, {"_id": "694df96e746a34b55dd544e0", "name": "Yijun Yang", "hidden": false}, {"_id": "694df96e746a34b55dd544e1", "name": "Changhao Zhang", "hidden": false}, {"_id": "694df96e746a34b55dd544e2", "name": "Junliang Xing", "hidden": false}, {"_id": "694df96e746a34b55dd544e3", "name": "Yuanchun Shi", "hidden": false}, {"_id": "694df96e746a34b55dd544e4", "name": "Zongqing Lu", "hidden": false}, {"_id": "694df96e746a34b55dd544e5", "name": "Deheng Ye", "hidden": false}], "publishedAt": "2025-12-15T07:11:56.000Z", "submittedOnDailyAt": "2025-12-26T00:32:37.057Z", "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "submittedOnDailyBy": {"_id": "66c2e8b4a03b764ca9057e65", "avatarUrl": "/avatars/ce17d68f02a08c148ebb9df3170812c8.svg", "isPro": false, "fullname": "Tong Wei", "user": "weit123", "type": "user"}, "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "upvotes": 3, "discussionId": "694df96e746a34b55dd544e6", "summary_zh": "<ul>\n    <li>\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u57fa\u7840\u4e0a\u5b58\u5728\u7a00\u758f\u5956\u52b1\u548c\u957f\u65f6\u95f4\u4fe1\u7528\u5206\u914d\u7684\u95ee\u9898\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u9010\u6b65\u53cd\u9988\uff0c\u4f46\u9650\u5236\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002</li>\n    <li>GTR-Turbo\u662fGTR\u7684\u9ad8\u6548\u5347\u7ea7\uff0c\u4e0d\u9700\u8981\u8bad\u7ec3\u6216\u67e5\u8be2\u6602\u8d35\u7684\u6559\u5e08\u6a21\u578b\u3002</li>\n    <li>GTR-Turbo\u901a\u8fc7\u5408\u5e76\u8bad\u7ec3\u4e2d\u7684\u68c0\u67e5\u70b9\u6743\u91cd\uff0c\u4f7f\u7528\u5408\u5e76\u6a21\u578b\u4f5c\u4e3a\u201c\u514d\u8d39\u201d\u6559\u5e08\u8fdb\u884c\u6307\u5bfc\u3002</li>\n    <li>\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0cGTR-Turbo\u63d0\u9ad8\u4e86\u57fa\u7ebf\u6a21\u578b\u7684\u51c6\u786e\u602710-30%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8650%\u7684\u8bad\u7ec3\u65f6\u95f4\u548c60%\u7684\u8ba1\u7b97\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-turn reinforcement learning for vision-language agents struggles with sparse rewards and delayed feedback.</li>\n    <li>Current methods use expensive teacher models for step-level feedback, which makes them hard to use and reproduce.</li>\n    <li>GTR-Turbo is a new method that improves upon existing techniques without needing costly teacher models.</li>\n    <li>It combines weights from different training checkpoints to create a free model that guides further training.</li>\n    <li>GTR-Turbo enhances the accuracy of models by 10-30% and reduces training time and costs significantly compared to previous methods.</li>\n</ul>"}, "publishedAt": "2025-12-15T02:11:56.000Z", "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13043.png", "numComments": 2, "submittedBy": {"_id": "66c2e8b4a03b764ca9057e65", "avatarUrl": "/avatars/ce17d68f02a08c148ebb9df3170812c8.svg", "fullname": "Tong Wei", "name": "weit123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u6700\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u5728VAE\u7a7a\u95f4\u4e2d\u5b66\u4e60\u89c6\u9891\u6f5c\u5728\u5206\u5e03\uff0c\u5e76\u901a\u8fc7VAE\u89e3\u7801\u5668\u6620\u5c04\u5230\u50cf\u7d20\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f46\u6536\u655b\u6162\u4e14\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\u3002</li>\n    <li>\u672c\u6587\u63d0\u51faSemanticGen\uff0c\u5229\u7528\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\uff0c\u4ece\u800c\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210VAE\u6f5c\u5728\u53d8\u91cf\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u66f4\u5feb\uff0c\u4e14\u5728\u957f\u89c6\u9891\u751f\u6210\u65f6\u4e5f\u6709\u6548\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new video generation method that improves on traditional models by working in a simpler, high-level semantic space.</li>\n    <li>It uses a two-step process: first, it creates a basic layout of the video, and then it adds detailed features.</li>\n    <li>This approach is faster and less resource-intensive, especially for long videos, compared to older methods that use complex low-level details.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and performs better than existing leading video generation methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19693", "authors": [{"_id": "694a0ffa335742716e93227d", "name": "Weichen Fan", "hidden": false}, {"_id": "694a0ffa335742716e93227e", "name": "Haiwen Diao", "hidden": false}, {"_id": "694a0ffa335742716e93227f", "name": "Quan Wang", "hidden": false}, {"_id": "694a0ffa335742716e932280", "name": "Dahua Lin", "hidden": false}, {"_id": "694a0ffa335742716e932281", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-22T18:59:57.000Z", "submittedOnDailyAt": "2025-12-23T01:15:14.379Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "upvotes": 51, "discussionId": "694a0ffa335742716e932282", "githubRepo": "https://github.com/WeichenFan/UAE", "githubRepoAddedBy": "user", "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.", "ai_keywords": ["spectral characteristics", "semantic encoders", "pixel encoders", "feature spectrum", "low-frequency components", "high-frequency information", "Prism Hypothesis", "Unified Autoencoding", "frequency-band modulator", "ImageNet", "MS-COCO", "latent space"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4e0d\u540c\u7f16\u7801\u5668\u7684\u9891\u8c31\u7279\u6027\uff0c\u53d1\u73b0\u8bed\u4e49\u7f16\u7801\u5668\u4e3b\u8981\u6355\u6349\u4f4e\u9891\u6210\u5206\uff0c\u50cf\u662f\u62bd\u8c61\u610f\u4e49\u3002</li>\n    <li>\u50cf\u7d20\u7f16\u7801\u5668\u5219\u4fdd\u7559\u9ad8\u9891\u4fe1\u606f\uff0c\u4f20\u8fbe\u7ec6\u8282\u3002</li>\n    <li>\u63d0\u51fa\u201c\u68f1\u955c\u5047\u8bbe\u201d\uff0c\u8ba4\u4e3a\u4e0d\u540c\u6570\u636e\u6a21\u6001\u662f\u81ea\u7136\u4e16\u754c\u7684\u5171\u4eab\u7279\u5f81\u9891\u8c31\u7684\u6295\u5f71\u3002</li>\n    <li>\u63d0\u51fa\u7edf\u4e00\u81ea\u7f16\u7801\uff08UAE\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u9891\u7387\u8c03\u5236\u5668\u7ed3\u5408\u8bed\u4e49\u7ed3\u6784\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n    <li>\u5728ImageNet\u548cMS-COCO\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1UAE\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study explores how different types of data encoders (semantic and pixel) relate to each other and their characteristics.</li>\n    <li>Semantic encoders focus on low-frequency features that represent abstract meanings, while pixel encoders capture high-frequency details for finer images.</li>\n    <li>This relationship is described as the \"Prism Hypothesis,\" suggesting that different data types reflect the natural world through a shared feature spectrum.</li>\n    <li>The authors introduce a new model called Unified Autoencoding (UAE) that combines semantic understanding and pixel details using a frequency-band modulator.</li>\n    <li>Tests on ImageNet and MS-COCO show that UAE performs excellently by merging abstract meaning with detailed image information.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:59:57.000Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19673", "authors": [{"_id": "694ac3ad746a34b55dd53b6c", "name": "Yuqiao Tan", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6d", "name": "Minzheng Wang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6e", "name": "Shizhu He", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6f", "name": "Huanxuan Liao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b70", "name": "Chengfeng Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b71", "name": "Qiunan Lu", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b72", "name": "Tian Liang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b73", "name": "Jun Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b74", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-22T18:51:48.000Z", "submittedOnDailyAt": "2025-12-24T00:28:45.252Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "submittedOnDailyBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "isPro": false, "fullname": "mz.w", "user": "iiiiwis", "type": "user"}, "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "upvotes": 49, "discussionId": "694ac3ad746a34b55dd53b75", "githubRepo": "https://github.com/Trae1ounG/BuPO", "githubRepoAddedBy": "user", "ai_summary": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.", "ai_keywords": ["reinforcement learning", "large language models", "Transformer residual stream", "unembedding matrix", "Internal Layer Policies", "Internal Modular Policies", "self-attention", "feed-forward network", "entropy", "Bottom-up Policy Optimization"], "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u5176\u5185\u90e8\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u5206\u6790Transformer\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\uff0c\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u5185\u90e8\u5c42\u7b56\u7565\u548c\u6a21\u5757\u7b56\u7565\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u65e9\u671f\u5c42\u4fdd\u6301\u9ad8\u71b5\u4ee5\u63a2\u7d22\uff0c\u800c\u9876\u90e8\u5c42\u7684\u71b5\u63a5\u8fd1\u96f6\u4ee5\u8fdb\u884c\u4f18\u5316\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u6536\u655b\u6a21\u5f0f\u6709\u6240\u4e0d\u540c\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u2014\u2014\u81ea\u4e0b\u800c\u4e0a\u7684\u7b56\u7565\u4f18\u5316\uff08BuPO\uff09\uff0c\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u5c42\u7b56\u7565\u3002</li>\n    <li>\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBuPO\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current reinforcement learning methods view large language models as a single unit, missing the details of how they work internally.</li>\n    <li>This paper breaks down the language model's policy to understand how different layers and components contribute to its performance.</li>\n    <li>It identifies two types of internal policies: Internal Layer Policies from each layer and Internal Modular Policies from specific components like self-attention.</li>\n    <li>Analysis shows that early layers explore a lot, while top layers focus on refining predictions, with differences observed among various model types.</li>\n    <li>The authors introduce a new training method called Bottom-up Policy Optimization (BuPO) that improves reasoning abilities in language models and shows better results in tests.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:51:48.000Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19673.png", "numComments": 4, "submittedBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "fullname": "mz.w", "name": "iiiiwis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4e00\u822c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u4e0a\u4ecd\u7136\u8f83\u5f31\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63a8\u51fa\u4e86DSR Suite\uff0c\u901a\u8fc7\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\u4ece\u89c6\u9891\u4e2d\u751f\u6210\u591a\u9009\u9898\u548c\u7b54\u6848\u3002</li>\n    <li>\u8be5\u6d41\u7a0b\u63d0\u53d6\u4e86\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u6784\u5efa\u4e86\u7528\u4e8e\u5b66\u4e60\u7684DSR-Train\u548c\u7528\u4e8e\u8bc4\u4f30\u7684DSR-Bench\u3002</li>\n    <li>\u6570\u636e\u96c6\u5f3a\u8c03\u4e86\u771f\u5b9e\u89c6\u9891\u6765\u6e90\u30013D\u9700\u6c42\u3001\u89c6\u89d2\u53d8\u5316\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u7ec6\u81f4\u7684\u7b54\u6848\u3002</li>\n    <li>\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\uff0c\u4ee5\u5c06\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u6709\u6548\u6574\u5408\u5230VLM\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) are good at general understanding but struggle with dynamic spatial reasoning (DSR), which involves understanding how objects change in 3D space over time.</li>\n    <li>To improve DSR, the authors introduced the DSR Suite, which includes a system to create multiple-choice questions from real-world videos.</li>\n    <li>This system uses advanced vision models to gather detailed information about objects, such as their shapes, positions, and movements.</li>\n    <li>Key features of the dataset include diverse video sources, 3D object and scene requirements, and the ability to analyze interactions between multiple objects.</li>\n    <li>The authors developed a Geometry Selection Module (GSM) that helps VLMs use relevant geometric information effectively, resulting in better DSR performance without losing accuracy in general video understanding.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20618", "authors": [{"_id": "694ba02a746a34b55dd53e8b", "name": "Runtao Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8c", "name": "Ziyi Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8d", "name": "Jiaqi Tang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8e", "name": "Yue Ma", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8f", "name": "Renjie Pi", "hidden": false}, {"_id": "694ba02a746a34b55dd53e90", "name": "Jipeng Zhang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e91", "name": "Qifeng Chen", "hidden": false}], "publishedAt": "2025-12-23T18:59:49.000Z", "submittedOnDailyAt": "2025-12-24T05:57:23.776Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "submittedOnDailyBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "isPro": true, "fullname": "Jiaqi Tang", "user": "Jiaqi-hkust", "type": "user"}, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "upvotes": 38, "discussionId": "694ba02a746a34b55dd53e92", "ai_summary": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.", "ai_keywords": ["multimodal LLMs", "long-video QA", "multi-agent framework", "grounding agent", "vision agent", "reinforcement learning", "temporal grounding", "LongTVQA", "LongTVQA+"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u95ee\u7b54\uff0c\u534f\u8c03\u4e3b\u667a\u80fd\u4f53\u3001\u5b9a\u4f4d\u667a\u80fd\u4f53\u548c\u89c6\u89c9\u667a\u80fd\u4f53\u3002</li>\n    <li>\u4e3b\u667a\u80fd\u4f53\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u7684\u5408\u4f5c\u6548\u7387\u548c\u51c6\u786e\u6027\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u51c6\u786e\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u7684\u7247\u6bb5\uff0c\u5e76\u63d0\u53d6\u5fc5\u8981\u7684\u6587\u672c\u4fe1\u606f\u3002</li>\n    <li>\u5728LongTVQA\u548cLongTVQA+\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u8d85\u8d8a\u4e86\u5176\u4ed6\u975e\u667a\u80fd\u4f53\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in multimodal large language models (LLMs) show potential for understanding long videos.</li>\n    <li>Many current methods lose important details by summarizing content or using limited tools.</li>\n    <li>We suggest a system where a main LLM works with other agents to identify relevant video segments and extract important text.</li>\n    <li>This system helps the main agent focus on important clips and enhances understanding with visual details.</li>\n    <li>Our method outperforms existing systems and shows better reasoning through reinforced learning.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:49.000Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png", "numComments": 1, "submittedBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "fullname": "Jiaqi Tang", "name": "Jiaqi-hkust", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20617", "authors": [{"_id": "694b58e3746a34b55dd53cff", "name": "Yuxi Xiao", "hidden": false}, {"_id": "694b58e3746a34b55dd53d00", "name": "Longfei Li", "hidden": false}, {"_id": "694b58e3746a34b55dd53d01", "name": "Shen Yan", "hidden": false}, {"_id": "694b58e3746a34b55dd53d02", "name": "Xinhang Liu", "hidden": false}, {"_id": "694b58e3746a34b55dd53d03", "name": "Sida Peng", "hidden": false}, {"_id": "694b58e3746a34b55dd53d04", "name": "Yunchao Wei", "hidden": false}, {"_id": "694b58e3746a34b55dd53d05", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "694b58e3746a34b55dd53d06", "name": "Bingyi Kang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "publishedAt": "2025-12-23T18:59:46.000Z", "submittedOnDailyAt": "2025-12-24T00:38:28.003Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "upvotes": 34, "discussionId": "694b58e4746a34b55dd53d07", "projectPage": "https://spatialtree.github.io/", "ai_summary": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.", "ai_keywords": ["SpatialTree", "low-level perception", "mental mapping", "simulation", "agentic competence", "capability-centric hierarchical benchmark", "targeted supervised fine-tuning", "negative transfer", "cross-level transfer", "naive RL", "auto-think strategy"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SpatialTree\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u7a7a\u95f4\u80fd\u529b\u5c42\u7ea7\u4f53\u7cfb\uff0c\u5206\u4e3a\u56db\u4e2a\u7b49\u7ea7\uff1a\u4f4e\u7ea7\u611f\u77e5\uff08L1\uff09\u3001\u5fc3\u7406\u6620\u5c04\uff08L2\uff09\u3001\u6a21\u62df\uff08L3\uff09\u548c\u4ee3\u7406\u80fd\u529b\uff08L4\uff09\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u4ee5\u80fd\u529b\u4e3a\u4e2d\u5fc3\u7684\u5c42\u7ea7\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u572827\u4e2a\u5b50\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cL1\u6280\u80fd\u4e4b\u95f4\u76f8\u5bf9\u72ec\u7acb\uff0c\u800c\u66f4\u9ad8\u7ea7\u7684\u6280\u80fd\u4e4b\u95f4\u76f8\u4e92\u5173\u8054\uff0c\u8868\u660e\u80fd\u529b\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u589e\u52a0\u3002</li>\n    <li>\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u6211\u4eec\u53d1\u73b0L1\u5b58\u5728\u8d1f\u8fc1\u79fb\u73b0\u8c61\uff0c\u4f46\u4f4e\u7ea7\u80fd\u529b\u5411\u9ad8\u7ea7\u80fd\u529b\u7684\u8fc1\u79fb\u6548\u679c\u663e\u8457\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u81ea\u6211\u601d\u8003\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u6240\u6709\u5c42\u7ea7\u4e0a\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u601d\u8003\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Spatial ability develops in stages, from basic perception to complex reasoning and interaction.</li>\n    <li>We created a framework called SpatialTree that organizes spatial abilities into four levels: perception, mental mapping, simulation, and competence.</li>\n    <li>We tested various multimodal large language models (MLLMs) using this framework, revealing that lower-level skills are independent while higher-level skills are more connected.</li>\n    <li>Targeted training showed that while lower-level skills can negatively affect each other, improving low-level skills can significantly enhance higher-level abilities.</li>\n    <li>We found that a new strategy called auto-think improves performance across all skill levels by reducing unnecessary thinking during training.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:46.000Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 33, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u901a\u8fc7\u8f85\u52a9\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u6216\u56fe\u50cf\u526a\u88c1\u6765\u76d1\u7763\u89c6\u89c9\u6b65\u9aa4\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u6709\u5f88\u591a\u9650\u5236\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4efb\u52a1\u7279\u5b9a\u673a\u5236\uff0c\u5141\u8bb8\u6a21\u578b\u81ea\u4e3b\u53d1\u73b0\u548c\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\uff0c\u65e0\u9700\u660e\u786e\u76d1\u7763\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u5730\u63d0\u53d6\u76f8\u5173\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u8d85\u8d8a\u4e86\u76f4\u63a5\u5fae\u8c03\u7684\u6548\u679c\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u80fd\u5f88\u597d\u5730\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text, which limits their ability to perform visual reasoning tasks.</li>\n    <li>Current methods use extra images or annotations to help LMMs with visual tasks, but these methods have drawbacks like high costs and limited flexibility.</li>\n    <li>The new approach we propose allows LMMs to learn visual reasoning on their own, without needing explicit guidance.</li>\n    <li>This method helps models adaptively process images and extract useful visual information, improving performance on various vision-related tasks.</li>\n    <li>Our approach achieves better results than traditional fine-tuning and can be applied to multiple tasks effectively.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 2, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20605", "authors": [{"_id": "694e76e5746a34b55dd545eb", "name": "Seijin Kobayashi", "hidden": false}, {"_id": "694e76e5746a34b55dd545ec", "name": "Yanick Schimpf", "hidden": false}, {"_id": "694e76e5746a34b55dd545ed", "name": "Maximilian Schlegel", "hidden": false}, {"_id": "694e76e5746a34b55dd545ee", "name": "Angelika Steger", "hidden": false}, {"_id": "694e76e5746a34b55dd545ef", "name": "Maciej Wolczyk", "hidden": false}, {"_id": "694e76e5746a34b55dd545f0", "name": "Johannes von Oswald", "hidden": false}, {"_id": "694e76e5746a34b55dd545f1", "name": "Nino Scherrer", "hidden": false}, {"_id": "694e76e5746a34b55dd545f2", "name": "Kaitlin Maile", "hidden": false}, {"_id": "694e76e5746a34b55dd545f3", "name": "Guillaume Lajoie", "hidden": false}, {"_id": "694e76e5746a34b55dd545f4", "name": "Blake A. Richards", "hidden": false}, {"_id": "694e76e5746a34b55dd545f5", "name": "Rif A. Saurous", "hidden": false}, {"_id": "694e76e5746a34b55dd545f6", "name": "James Manyika", "hidden": false}, {"_id": "694e76e5746a34b55dd545f7", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "694e76e5746a34b55dd545f8", "name": "Alexander Meulemans", "hidden": false}, {"_id": "694e76e5746a34b55dd545f9", "name": "Jo\u00e3o Sacramento", "hidden": false}], "publishedAt": "2025-12-23T18:51:50.000Z", "submittedOnDailyAt": "2025-12-26T11:17:05.505Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "submittedOnDailyBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "upvotes": 25, "discussionId": "694e76e5746a34b55dd545fa", "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\u9884\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\uff0c\u5df2\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u6210\u529f\u3002</li>\n    <li>\u5728RL\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u9010\u4e2a\u751f\u6210\u65b0\u8f93\u51fa\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\uff0c\u5c24\u5176\u662f\u5728\u5956\u52b1\u7a00\u5c11\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u8fdb\u884c\u63a2\u7d22\uff0c\u514b\u670d\u8fd9\u4e00\u95ee\u9898\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e00\u79cd\u66f4\u9ad8\u9636\u7684\u975e\u56e0\u679c\u5e8f\u5217\u6a21\u578b\uff0c\u80fd\u591f\u63a7\u5236\u57fa\u7840\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6fc0\u6d3b\u6d41\u3002</li>\n    <li>\u8fd9\u79cd\u5185\u90e8\u63a7\u5236\u5668\u7684\u5f3a\u5316\u5b66\u4e60\uff08\u79f0\u4e3a\u201c\u5185\u90e8RL\u201d\uff09\u80fd\u6709\u6548\u5b66\u4e60\u7a00\u758f\u5956\u52b1\u4efb\u52a1\uff0c\u663e\u793a\u51fa\u5c42\u6b21\u5316RL\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large autoregressive models are great at predicting the next token and can be improved with reinforcement learning (RL).</li>\n    <li>Generating outputs one token at a time can be inefficient, especially when rewards are rare.</li>\n    <li>We propose using a higher-order model to explore actions within the internal workings of a base autoregressive model.</li>\n    <li>This new model learns to control sequences of actions, leading to better exploration and learning from sparse rewards.</li>\n    <li>Our findings suggest that using internal reinforcement learning could enhance hierarchical learning in large models.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:51:50.000Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png", "numComments": 2, "submittedBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "fullname": "Maximilian Schlegel", "name": "schlegelm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19134", "authors": [{"_id": "694a1765335742716e9322b7", "name": "Dehai Min", "hidden": false}, {"_id": "694a1765335742716e9322b8", "name": "Kailin Zhang", "hidden": false}, {"_id": "694a1765335742716e9322b9", "name": "Tongtong Wu", "hidden": false}, {"_id": "694a1765335742716e9322ba", "name": "Lu Cheng", "hidden": false}], "publishedAt": "2025-12-22T08:28:05.000Z", "submittedOnDailyAt": "2025-12-23T01:46:57.477Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "submittedOnDailyBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "isPro": false, "fullname": "Dehai Min", "user": "ZhishanQ", "type": "user"}, "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "upvotes": 25, "discussionId": "694a1765335742716e9322bb", "githubRepo": "https://github.com/ZhishanQ/QuCo-RAG", "githubRepoAddedBy": "user", "ai_summary": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.", "ai_keywords": ["dynamic retrieval-augmented generation", "large language models", "hallucinations", "model-internal signals", "logits", "entropy", "pre-training data", "uncertainty quantification", "low-frequency entities", "entity co-occurrence", "Infini-gram", "multi-hop QA", "EM gains", "OLMo-2", "Llama", "Qwen", "GPT", "biomedical QA", "domain generalization", "corpus-grounded verification"], "githubStars": 6, "summary_zh": "<ul>\n    <li>QuCo-RAG \u662f\u4e00\u79cd\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u51b3\u5b9a\u4f55\u65f6\u68c0\u7d22\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u73b0\u8c61\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u4fe1\u5fc3\u8fc7\u9ad8\uff0c\u5373\u4f7f\u8f93\u51fa\u9519\u8bef\u3002</li>\n    <li>QuCo-RAG \u4ece\u5ba2\u89c2\u7edf\u8ba1\u6570\u636e\u51fa\u53d1\uff0c\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff1a\u751f\u6210\u524d\u8bc6\u522b\u4f4e\u9891\u5b9e\u4f53\uff0c\u751f\u6210\u65f6\u9a8c\u8bc1\u5b9e\u4f53\u5171\u73b0\u3002</li>\n    <li>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQuCo-RAG \u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e86 5-12 \u5206\uff0c\u4e14\u5728\u5176\u4ed6\u6a21\u578b\u4e0a\u4e5f\u6709\u6548\u3002</li>\n    <li>\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u7a33\u5065\u6027\uff0c\u4ee3\u7801\u53ef\u5728 GitHub \u4e0a\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QuCo-RAG is a new method that helps large language models (LLMs) avoid errors by deciding when to retrieve information during text generation.</li>\n    <li>Unlike previous methods that rely on unreliable internal signals from the model, QuCo-RAG uses objective statistics from pre-training data to measure uncertainty.</li>\n    <li>The method identifies knowledge gaps before generating text and checks for the presence of entities during generation to assess the risk of making mistakes.</li>\n    <li>Experiments show that QuCo-RAG significantly improves accuracy in answering questions compared to current best methods, even when applied to different models.</li>\n    <li>It also proves effective in specialized areas like biomedical QA, demonstrating its broad applicability and reliability.</li>\n</ul>"}, "publishedAt": "2025-12-22T03:28:05.000Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19134.png", "numComments": 2, "submittedBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "fullname": "Dehai Min", "name": "ZhishanQ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u201c\u4e00\u955c\u5230\u5e95\u201d\u6280\u672f\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5177\u6709\u72ec\u7279\u7684\u7f8e\u5b66\uff0c\u4f46\u5b9e\u65bd\u6210\u672c\u9ad8\u3001\u73b0\u5b9e\u9650\u5236\u590d\u6742\u3002</li>\n    <li>DreaMontage\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7528\u6237\u63d0\u4f9b\u7684\u8f93\u5165\u5408\u6210\u6d41\u7545\u3001\u8868\u73b0\u4e30\u5bcc\u7684\u957f\u65f6\u95f4\u4e00\u955c\u89c6\u9891\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u4e09\u79cd\u4e3b\u8981\u65b9\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5305\u62ec\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u6761\u4ef6\u673a\u5236\u548c\u81ea\u9002\u5e94\u8c03\u4f18\u7b56\u7565\u3002</li>\n    <li>\u4e3a\u4e86\u589e\u5f3a\u89c6\u89c9\u6548\u679c\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u65bd\u4e86\u89c6\u89c9\u8868\u8fbe\u7684\u7ec6\u5316\u9636\u6bb5\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u5206\u6bb5\u81ea\u56de\u5f52\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u9ad8\u6548\u5730\u751f\u4ea7\u6269\u5c55\u5e8f\u5217\uff0c\u5b9e\u73b0\u89c6\u89c9\u6548\u679c\u7684\u8fde\u8d2f\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" technique in filmmaking is unique but often expensive and complicated to implement.</li>\n    <li>Current video generation models struggle with smoothness and coherence when combining clips.</li>\n    <li>DreaMontage is a new framework that creates long, seamless one-shot videos from various user inputs.</li>\n    <li>The framework improves control over video generation, enhances visual quality, and ensures smooth transitions.</li>\n    <li>It uses efficient strategies for generating extended sequences while keeping computational demands low.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faDeepSeek-V3.2\u6a21\u578b\uff0c\u517c\u5177\u9ad8\u6548\u8ba1\u7b97\u548c\u5353\u8d8a\u63a8\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u957f\u6587\u672c\u573a\u666f\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002</li>\n    <li>\u6784\u5efa\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u53ef\u4e0eGPT-5\u76f8\u5ab2\u7f8e\uff0c\u5e76\u5728\u67d0\u4e9b\u65b9\u9762\u8d85\u8d8a\u5176\u8868\u73b0\u3002</li>\n    <li>\u5f00\u53d1\u5927\u89c4\u6a21\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines great performance with efficient computing.</li>\n    <li>It features a new attention mechanism called DeepSeek Sparse Attention (DSA) that reduces computing needs while maintaining performance for long contexts.</li>\n    <li>The model uses a strong reinforcement learning approach, allowing it to compete with and even surpass GPT-5 in certain tasks.</li>\n    <li>DeepSeek-V3.2-Speciale, a high-compute version, shows excellent reasoning skills and has won top awards in major mathematics and informatics competitions.</li>\n    <li>It includes a new system for generating training data that enhances the model's ability to follow instructions and perform well in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u76ee\u524d\u4e3b\u8981\u7531\u79c1\u4eba\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5f00\u6e90\u66ff\u4ee3\u54c1\u5982Qwen-Image\u548cHunyuan-Image-3.0\u53c2\u6570\u6570\u91cf\u5e9e\u5927\uff0820\u4ebf\u523080\u4ebf\uff09\uff0c\u4e0d\u9002\u5408\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u63a8\u7406\u548c\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846\u4ebf\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u53ef\u6269\u5c55\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u6311\u6218\u201c\u89c4\u6a21\u81f3\u4e0a\u201d\u7684\u7406\u5ff5\u3002</li>\n    <li>Z-Image\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5bb9\u666e\u901a\u786c\u4ef6\uff0c\u5e76\u80fd\u5feb\u901f\u751f\u6210\u56fe\u50cf\u548c\u6587\u672c\u3002</li>\n    <li>\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u3001\u6a21\u578b\u6743\u91cd\u548c\u5728\u7ebf\u6f14\u793a\uff0c\u4fc3\u8fdb\u66f4\u6613\u8bbf\u95ee\u3001\u66f4\u7ecf\u6d4e\u5b9e\u60e0\u7684\u524d\u6cbf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-performance image generation is currently led by proprietary models, but they are often too complex for everyday use.</li>\n    <li>Z-Image is a new, efficient generative model with only 6 billion parameters, designed to be practical for standard hardware.</li>\n    <li>The model was developed using a unique training method and completed its training in about 314,000 GPU hours, costing around $630,000.</li>\n    <li>Z-Image offers fast performance, working well on both high-end and consumer-grade hardware, and includes an editing model called Z-Image-Edit.</li>\n    <li>The model shows excellent results in creating realistic images and managing bilingual text, proving high-quality outcomes are possible with lower resource use.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002</li>\n    <li>DataFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86200\u591a\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u6570\u636e\u7ba1\u9053\uff0c\u6db5\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>DataFlow-Agent\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\uff0cDataFlow\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u7279\u5b9a\u5408\u6210\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DataFlow is a new framework designed to improve how data is prepared for Large Language Models (LLMs).</li>\n    <li>It offers a modular system with reusable data transformation tools and an easy-to-use pipeline construction API.</li>\n    <li>DataFlow includes about 200 operators and supports various tasks like text processing, code generation, and mathematical reasoning.</li>\n    <li>The framework enhances LLM performance, showing better accuracy than existing datasets in several tests.</li>\n    <li>DataFlow also features an agent that can create data pipelines from natural language instructions, making it user-friendly.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5728\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u4e2d\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u7b97\u6cd5\u4e0e\u7cfb\u7edf\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u901a\u8fc7\u591aGPU\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u6253\u7834\u81ea\u56de\u5f52\u74f6\u9888\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u6d41\u5f0f\u5904\u7406\u3002</li>\n    <li>\u91c7\u7528\u6eda\u52a8\u6c34\u69fd\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u52a8\u6001\u8c03\u6574\u5916\u89c2\uff0c\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u8272\u5f69\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u9996\u6b21\u5728\u6b64\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u7528\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new framework for generating realistic avatars in real-time using a powerful 14-billion-parameter diffusion model.</li>\n    <li>The framework uses Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by dividing tasks across multiple GPUs, allowing for stable streaming.</li>\n    <li>To improve video quality, it employs the Rolling Sink Frame Mechanism (RSFM), which keeps the avatar's appearance consistent by using a cached image.</li>\n    <li>It also applies Self-Forcing Distribution Matching Distillation to adapt large models for real-time use without losing visual quality.</li>\n    <li>Live Avatar achieves impressive performance, generating videos at 20 frames per second on five GPUs, setting a new standard for high-quality avatar generation.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u771f\u5b9e\u7684\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\uff08DE\uff09\u4efb\u52a1\u6d89\u53ca\u5bf9\u5de5\u4e1a\u6a21\u5f0f\u7684\u5de5\u7a0b\u8bbe\u8ba1\u548c\u591a\u9636\u6bb5SQL\u7ba1\u9053\u7684\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\uff08DA\uff09\u4efb\u52a1\u8981\u6c42\u89e3\u51b3\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u6218\u7565\u89c4\u5212\u548c\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u7684\u667a\u80fd\u4f53\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u5de5\u7a0b\u4e0e\u5206\u6790\u80fd\u529b\u7684\u533a\u522b\uff0c\u4fc3\u8fdb\u771f\u6b63\u80fd\u529b\u5f3a\u5927\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark with 210 tasks designed to reflect real-world data workflows in businesses.</li>\n    <li>The benchmark includes data engineering tasks that involve creating and modifying complex data systems, and data analysis tasks that focus on solving open-ended business problems.</li>\n    <li>Performance on these tasks is low, with data engineering tasks scoring under 20% and data analysis tasks averaging below 40%.</li>\n    <li>This highlights significant challenges in creating effective automated data agents, as engineering and analysis require different skills.</li>\n    <li>DAComp aims to help improve the development of autonomous data agents for better enterprise data management.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u521b\u4f5c\u7684\u80fd\u529b\u3002</li>\n    <li>Kling-Omni \u4e0d\u4ec5\u662f\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\uff0c\u4e5f\u6709\u52a9\u4e8e\u53d1\u5c55\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\uff0c\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u3001\u751f\u6210\u548c\u4e0e\u590d\u6742\u4e16\u754c\u4e92\u52a8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a framework that creates high-quality videos from different types of inputs like text, images, and videos.</li>\n    <li>It combines various tasks such as video generation, editing, and reasoning into one complete system.</li>\n    <li>The framework processes inputs into a unified format to produce cinematic and intelligent videos.</li>\n    <li>A strong data system and advanced training methods support its video creation abilities.</li>\n    <li>Kling-Omni is seen as a major step towards creating advanced systems that can understand and interact with complex environments.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cWan-Move\u63d0\u4f9b\u66f4\u7cbe\u786e\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u5bc6\u96c6\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u5141\u8bb8\u5bf9\u573a\u666f\u8fdb\u884c\u7ec6\u81f4\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u800c\u65e0\u9700\u6539\u53d8\u67b6\u6784\u3002</li>\n    <li>\u901a\u8fc7\u5728MoveBench\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cWan-Move\u5c55\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u8fd0\u52a8\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework for controlling motion in video generation models.</li>\n    <li>It improves upon existing methods by providing precise and high-quality motion control.</li>\n    <li>The framework uses dense point trajectories to accurately represent object movements.</li>\n    <li>Wan-Move integrates seamlessly with existing image-to-video models without changing their architecture.</li>\n    <li>It has been tested and shown to generate high-quality videos, and a new benchmark called MoveBench has been created for evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01816", "authors": [{"_id": "692e5c0537312eaa83fd87b8", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:43.760Z", "hidden": false}, {"_id": "692e5c0537312eaa83fd87b9", "name": "Siyuan Li", "hidden": false}, {"_id": "692e5c0537312eaa83fd87ba", "name": "Conghui He", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bb", "name": "Lijun Wu", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bc", "user": {"_id": "64be296a46cc3cdfbb057f7e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg", "isPro": false, "fullname": "Cheng Tan", "user": "chengtan9907", "type": "user"}, "name": "Cheng Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:41.755Z", "hidden": false}], "publishedAt": "2025-12-01T15:52:31.000Z", "submittedOnDailyAt": "2025-12-02T01:31:46.625Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "submittedOnDailyBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "upvotes": 87, "discussionId": "692e5c0537312eaa83fd87bd", "projectPage": "https://opendatalab-raiser.github.io/Envision/", "githubRepo": "https://github.com/opendatalab-raiser/Envision", "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.", "ai_keywords": ["multimodal models", "text-to-image (T2I)", "causal event progression", "spatiotemporal causality", "Envision-a", "Envision-Score", "multi-dimensional consistency", "physicality", "aesthetics", "causal narrative coherence", "spatiotemporal consistency", "multi-frame reasoning", "dynamic world modeling"], "githubStars": 27, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bd5\u56fe\u8d85\u8d8a\u5355\u4e00\u6a21\u6001\u7684\u5c40\u9650\u6027\uff0c\u4f46\u4f9d\u8d56\u9759\u6001\u5355\u56fe\u751f\u6210\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u52a8\u6001\u8fc7\u7a0b\u5efa\u6a21\u7684\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cEnvision\u201d\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u94fe\u5f0f\u6587\u672c\u5230\u591a\u56fe\u50cf\u751f\u6210\u6765\u8bc4\u4f30\u56e0\u679c\u4e8b\u4ef6\u8fdb\u5c55\uff0c\u5305\u542b1000\u4e2a\u8de8\u516d\u4e2a\u9886\u57df\u7684\u56db\u9636\u6bb5\u63d0\u793a\u3002</li>\n    <li>\u4e3a\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u5185\u5316\u4e86\u4e16\u754c\u77e5\u8bc6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u201cEnvision-Score\u201d\u8fd9\u4e00\u7efc\u5408\u6307\u6807\uff0c\u8bc4\u4f30\u591a\u7ef4\u4e00\u81f4\u6027\u3001\u7269\u7406\u6027\u548c\u7f8e\u5b66\u3002</li>\n    <li>\u5bf915\u4e2a\u6a21\u578b\u7684\u5168\u9762\u8bc4\u4f30\u53d1\u73b0\uff0c\u4e13\u95e8\u7684T2I\u6a21\u578b\u5728\u7f8e\u5b66\u6e32\u67d3\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u7f3a\u4e4f\u5185\u5728\u7684\u4e16\u754c\u77e5\u8bc6\u3002</li>\n    <li>\u5c3d\u7ba1\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u4e8b\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u7136\u96be\u4ee5\u514b\u670d\u65f6\u7a7a\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u52a8\u6001\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current models combine text and images but struggle with dynamic processes because they focus on generating single images.</li>\n    <li>To improve this, the authors propose a new benchmark called Envision, which evaluates how well models can generate sequences of images based on text prompts.</li>\n    <li>Envision includes 1,000 prompts from various scientific and humanities fields and aims to test models' understanding of cause-and-effect over time.</li>\n    <li>The study introduces Envision-Score, a new metric that assesses models on consistency, realism, and aesthetic quality.</li>\n    <li>Results show that while specialized models are good at creating appealing images, unified models perform better in understanding and generating coherent narratives, though all models still struggle with maintaining consistency over time.</li>\n</ul>"}, "publishedAt": "2025-12-01T10:52:31.000Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png", "numComments": 4, "submittedBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "fullname": "Juanxi Tian", "name": "Juanxi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13}, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u80fd\u751f\u6210\u89c6\u89c9\u771f\u5b9e\u548c\u65f6\u95f4\u4e00\u81f4\u7684\u5185\u5bb9\uff0c\u4f46\u5176\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u6307\u6807\u5982Frechet Video Distance\uff08FVD\uff09\u4fa7\u91cd\u611f\u77e5\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u53ca\u5168\u7403\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faMMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u548c\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8bc4\u4f30\u751f\u6210\u63a8\u7406\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u4f7f\u7528\u7ec6\u81f4\u7684\u6307\u6807\u8981\u6c42\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5982\u8fc7\u4e8e\u4f9d\u8d56\u611f\u77e5\u6570\u636e\u3001\u5168\u7403\u72b6\u6001\u4e00\u81f4\u6027\u5dee\uff0c\u4ee5\u53ca\u5956\u52b1\u89c6\u89c9\u5408\u7406\u6027\u800c\u975e\u56e0\u679c\u6b63\u786e\u6027\u7684\u76ee\u6807\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models create realistic content, but their ability to simulate the world depends on understanding physical and logical rules.</li>\n    <li>Current evaluation methods focus on visual quality and miss issues like causality and consistency.</li>\n    <li>MMGR is a new evaluation framework that assesses reasoning abilities in five areas: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR tests models in three areas: Abstract Reasoning, Embodied Navigation, and Physical Commonsense.</li>\n    <li>Benchmarking reveals that while models perform well in Physical Commonsense, they struggle significantly with Abstract Reasoning and long-term planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.01374", "authors": [{"_id": "692e6bf937312eaa83fd8890", "user": {"_id": "610b70452719facd4ea85e28", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg", "isPro": false, "fullname": "Chujie Zheng", "user": "chujiezheng", "type": "user"}, "name": "Chujie Zheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:27.206Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8891", "name": "Kai Dang", "hidden": false}, {"_id": "692e6bf937312eaa83fd8892", "name": "Bowen Yu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8893", "name": "Mingze Li", "hidden": false}, {"_id": "692e6bf937312eaa83fd8894", "user": {"_id": "6278bd42541f3d2dfa77ea70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg", "isPro": false, "fullname": "Huiqiang Jiang", "user": "iofu728", "type": "user"}, "name": "Huiqiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:41.367Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8895", "name": "Junrong Lin", "hidden": false}, {"_id": "692e6bf937312eaa83fd8896", "name": "Yuqiong Liu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8897", "user": {"_id": "62088594a5943c8a8fc94560", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png", "isPro": false, "fullname": "An Yang", "user": "yangapku", "type": "user"}, "name": "An Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:25.208Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8898", "name": "Jingren Zhou", "hidden": false}, {"_id": "692e6bf937312eaa83fd8899", "name": "Junyang Lin", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "publishedAt": "2025-12-01T07:45:39.000Z", "submittedOnDailyAt": "2025-12-02T02:47:49.367Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "submittedOnDailyBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "isPro": false, "fullname": "Bowen Yu", "user": "Tigerph", "type": "user"}, "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "upvotes": 78, "discussionId": "692e6bfa37312eaa83fd889a", "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.", "ai_keywords": ["reinforcement learning", "large language models", "sequence-level reward", "token-level objective", "policy gradient methods", "REINFORCE", "first-order approximation", "training-inference discrepancy", "policy staleness", "importance sampling correction", "clipping", "Routing Replay", "Mixture-of-Experts", "on-policy training", "off-policy updates"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u89e3\u91ca\u4e86\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u53ef\u4ee5\u901a\u8fc7\u4ee3\u7406\u7684\u5355\u4e2a\u6807\u8bb0\u76ee\u6807\u6765\u4f18\u5316\u771f\u5b9e\u7684\u5e8f\u5217\u7ea7\u5956\u52b1\u3002</li>\n    <li>\u5f53\u8bad\u7ec3\u4e0e\u63a8\u7406\u5dee\u5f02\u548c\u7b56\u7565\u6ede\u540e\u6700\u5c0f\u5316\u65f6\uff0c\u4ee3\u7406\u76ee\u6807\u7684\u6709\u6548\u6027\u4f1a\u589e\u5f3a\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\u7684\u57fa\u672c\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5728\u5728\u7ebf\u8bad\u7ec3\u4e2d\u5177\u6709\u6700\u9ad8\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5f15\u5165\u79bb\u7ebf\u66f4\u65b0\u540e\uff0c\u7ed3\u5408\u526a\u5207\u548c\u8def\u7531\u91cd\u653e\u5bf9\u4e8e\u51cf\u5c11\u7b56\u7565\u6ede\u540e\u5f15\u8d77\u7684\u4e0d\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper presents a new approach for using reinforcement learning with large language models.</li>\n    <li>It explains how a token-level objective can help optimize sequence-level rewards under certain conditions.</li>\n    <li>The effectiveness of this approach depends on minimizing discrepancies in training and inference, as well as reducing policy staleness.</li>\n    <li>Key techniques like importance sampling correction, clipping, and Routing Replay are shown to stabilize training.</li>\n    <li>Experiments demonstrate that stable training leads to consistent performance, regardless of how training starts.</li>\n</ul>"}, "publishedAt": "2025-12-01T02:45:39.000Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png", "numComments": 2, "submittedBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "fullname": "Bowen Yu", "name": "Tigerph", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Dec 27, 2025";
window.trendingPapers = {
    "today": [{"paper": {"id": "2512.17504", "authors": [{"_id": "694c0ede746a34b55dd53fa2", "user": {"_id": "6726857c88f2f9df27225d48", "avatarUrl": "/avatars/6a4e09d1759f1c2fa241e51ad85f9f00.svg", "isPro": false, "fullname": "Hoiyeong Jin", "user": "myyzzzoooo", "type": "user"}, "name": "Hoiyeong Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:44:16.181Z", "hidden": false}, {"_id": "694c0ede746a34b55dd53fa3", "user": {"_id": "669e3d80f433fc42bebe2ff0", "avatarUrl": "/avatars/2122a3288ca017922a966361aec1fda4.svg", "isPro": false, "fullname": "Jang hyojin", "user": "Whit3Snow", "type": "user"}, "name": "Hyojin Jang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:44:20.849Z", "hidden": false}, {"_id": "694c0ede746a34b55dd53fa4", "name": "Jeongho Kim", "hidden": false}, {"_id": "694c0ede746a34b55dd53fa5", "name": "Junha Hyung", "hidden": false}, {"_id": "694c0ede746a34b55dd53fa6", "name": "Kinam Kim", "hidden": false}, {"_id": "694c0ede746a34b55dd53fa7", "name": "Dongjin Kim", "hidden": false}, {"_id": "694c0ede746a34b55dd53fa8", "name": "Huijin Choi", "hidden": false}, {"_id": "694c0ede746a34b55dd53fa9", "name": "Hyeonji Kim", "hidden": false}, {"_id": "694c0ede746a34b55dd53faa", "name": "Jaegul Choo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6726857c88f2f9df27225d48/ZFmCv5gVUK7FY4ivZnTJY.mp4"], "publishedAt": "2025-12-19T12:14:36.000Z", "submittedOnDailyAt": "2025-12-29T00:07:19.171Z", "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion", "submittedOnDailyBy": {"_id": "6726857c88f2f9df27225d48", "avatarUrl": "/avatars/6a4e09d1759f1c2fa241e51ad85f9f00.svg", "isPro": false, "fullname": "Hoiyeong Jin", "user": "myyzzzoooo", "type": "user"}, "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.", "upvotes": 70, "discussionId": "694c0ede746a34b55dd53fab", "projectPage": "https://myyzzzoooo.github.io/InsertAnywhere/", "githubRepo": "https://github.com/myyzzzoooo/InsertAnywhere", "githubRepoAddedBy": "user", "ai_summary": "InsertAnywhere framework enhances video object insertion by generating geometrically consistent and visually coherent scenarios through 4D aware mask generation and diffusion-based synthesis.", "ai_keywords": ["diffusion-based video generation", "realistic video object insertion", "4D scene understanding", "occlusion effects", "geometrically consistent object placement", "appearance-faithful video synthesis", "4D aware mask generation", "diffusion based video generation model", "ROSE++", "illumination aware synthetic dataset", "object removal dataset", "VLM generated reference image", "geometrically plausible", "visually coherent object insertions"], "githubStars": 27, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "summary_zh": "<ul>\n    <li>InsertAnywhere\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u5bf9\u8c61\u63d2\u5165\u6846\u67b6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u51e0\u4f55\u4e00\u81f4\u7684\u5bf9\u8c61\u653e\u7f6e\u548c\u771f\u5b9e\u611f\u89c6\u9891\u5408\u6210\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u75284D\u611f\u77e5\u7684\u63a9\u6a21\u751f\u6210\u6a21\u5757\uff0c\u91cd\u5efa\u573a\u666f\u51e0\u4f55\u5e76\u5728\u591a\u4e2a\u5e27\u4e2d\u4fdd\u6301\u7528\u6237\u6307\u5b9a\u7684\u5bf9\u8c61\u653e\u7f6e\u3002</li>\n    <li>\u901a\u8fc7\u6269\u5c55\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5408\u6210\u63d2\u5165\u5bf9\u8c61\u53ca\u5176\u5468\u56f4\u73af\u5883\u7684\u5c40\u90e8\u53d8\u5316\uff0c\u5982\u5149\u7167\u548c\u9634\u5f71\u3002</li>\n    <li>\u5f15\u5165ROSE++\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8f6c\u6362ROSE\u5bf9\u8c61\u53bb\u9664\u6570\u636e\u96c6\uff0c\u652f\u6301\u76d1\u7763\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5404\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u51e0\u4f55\u5408\u7406\u548c\u89c6\u89c9\u4e00\u81f4\u7684\u5bf9\u8c61\u63d2\u5165\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7814\u7a76\u548c\u5546\u4e1a\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>InsertAnywhere is a new system for inserting objects into videos while keeping them realistic and visually appealing.</li>\n    <li>It uses a special method to understand the 4D scene and ensures objects are placed correctly over time, even with changes in lighting and occlusion.</li>\n    <li>The system combines this understanding with a video generation model to create natural-looking objects and their surroundings.</li>\n    <li>To train the model, a new dataset called ROSE++ was created, which includes videos with and without objects.</li>\n    <li>Tests show that InsertAnywhere works better than other current methods for adding objects to videos in various real-life situations.</li>\n</ul>"}, "publishedAt": "2025-12-19T07:14:36.000Z", "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion", "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6726857c88f2f9df27225d48/ZFmCv5gVUK7FY4ivZnTJY.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17504.png", "numComments": 1, "submittedBy": {"_id": "6726857c88f2f9df27225d48", "avatarUrl": "/avatars/6a4e09d1759f1c2fa241e51ad85f9f00.svg", "fullname": "Hoiyeong Jin", "name": "myyzzzoooo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.17220", "authors": [{"_id": "69493ea1d2000e944d383ab5", "name": "Yuqing Li", "hidden": false}, {"_id": "69493ea1d2000e944d383ab6", "name": "Jiangnan Li", "hidden": false}, {"_id": "69493ea1d2000e944d383ab7", "name": "Zheng Lin", "hidden": false}, {"_id": "69493ea1d2000e944d383ab8", "name": "Ziyan Zhou", "hidden": false}, {"_id": "69493ea1d2000e944d383ab9", "name": "Junjie Wu", "hidden": false}, {"_id": "69493ea1d2000e944d383aba", "name": "Weiping Wang", "hidden": false}, {"_id": "69493ea1d2000e944d383abb", "name": "Jie Zhou", "hidden": false}, {"_id": "69493ea1d2000e944d383abc", "name": "Mo Yu", "hidden": false}], "publishedAt": "2025-12-19T04:08:29.000Z", "submittedOnDailyAt": "2025-12-29T00:41:08.445Z", "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.", "upvotes": 69, "discussionId": "69493ea1d2000e944d383abd", "ai_summary": "MiA-RAG, a Mindscape-Aware Retrieval-Augmented Generation system, enhances LLM-based RAG with global context awareness through hierarchical summarization, improving long-context tasks and evidence-based understanding.", "ai_keywords": ["Mindscape-Aware Capability", "Retrieval-Augmented Generation (RAG)", "MiA-RAG", "LLM-based RAG systems", "hierarchical summarization", "query embeddings", "global semantic representation", "evidence-based understanding", "global sense-making"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u901a\u8fc7\u6574\u4f53\u8bed\u4e49\u7406\u89e3\u957f\u6587\u672c\uff0c\u8fd9\u6709\u52a9\u4e8e\u7ec4\u7ec7\u77e5\u8bc6\u548c\u89e3\u91ca\u65b0\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff08RAG\uff09\u7f3a\u4e4f\u8fd9\u6837\u7684\u5168\u5c40\u89c6\u89d2\uff0c\u56e0\u6b64\u5728\u5904\u7406\u957f\u6587\u672c\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014Mindscape-Aware RAG\uff08MiA-RAG\uff09\uff0c\u5b83\u4f7fRAG\u7cfb\u7edf\u5177\u5907\u5168\u5c40\u4e0a\u4e0b\u6587\u610f\u8bc6\u3002</li>\n    <li>MiA-RAG\u901a\u8fc7\u5206\u5c42\u603b\u7ed3\u6784\u5efa\u601d\u7ef4\u56fe\u666f\uff0c\u5e76\u5728\u68c0\u7d22\u548c\u751f\u6210\u65f6\u4f9d\u8d56\u8fd9\u79cd\u5168\u5c40\u8bed\u4e49\u8868\u793a\u3002</li>\n    <li>\u5728\u5404\u79cd\u957f\u6587\u672c\u548c\u53cc\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMiA-RAG\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8fdb\u884c\u957f\u6587\u672c\u7684\u68c0\u7d22\u548c\u63a8\u7406\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans use a broad understanding of text to make sense of complex information and connect new ideas with what they already know.</li>\n    <li>Current systems for generating text struggle with long documents because they lack this overall understanding.</li>\n    <li>This paper introduces MiA-RAG, a new method that helps text generation systems understand the big picture by organizing information hierarchically.</li>\n    <li>MiA-RAG improves how the system retrieves and generates text by using a global context, leading to better reasoning and understanding.</li>\n    <li>Tests show that MiA-RAG performs better than existing methods on long and bilingual texts, making it more effective in understanding and reasoning like humans.</li>\n</ul>"}, "publishedAt": "2025-12-18T23:08:29.000Z", "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding", "summary": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17220.png", "numComments": 1, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22047", "authors": [{"_id": "6951e7d4746a34b55dd548a7", "name": "Hanzhang Zhou", "hidden": false}, {"_id": "6951e7d4746a34b55dd548a8", "name": "Xu Zhang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548a9", "name": "Panrong Tong", "hidden": false}, {"_id": "6951e7d4746a34b55dd548aa", "name": "Jianan Zhang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ab", "name": "Liangyu Chen", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ac", "name": "Quyu Kong", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ad", "name": "Chenglin Cai", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ae", "name": "Chen Liu", "hidden": false}, {"_id": "6951e7d4746a34b55dd548af", "name": "Yue Wang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548b0", "name": "Jingren Zhou", "hidden": false}, {"_id": "6951e7d4746a34b55dd548b1", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-26T14:51:52.000Z", "submittedOnDailyAt": "2025-12-29T00:01:11.405Z", "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.", "upvotes": 19, "discussionId": "6951e7d4746a34b55dd548b2", "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>MAI-UI\u662f\u4e00\u79cd\u65b0\u578b\u7684GUI\u667a\u80fd\u4ee3\u7406\uff0c\u65e8\u5728\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002</li>\n    <li>\u5b83\u6709\u4e0d\u540c\u89c4\u6a21\u7684\u7248\u672c\uff0c\u5305\u62ec2B\u30018B\u300132B\u548c235B-A22B\u3002</li>\n    <li>MAI-UI\u89e3\u51b3\u4e86\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7528\u6237\u4ea4\u4e92\u4e0d\u8db3\u3001\u4ec5\u9650UI\u64cd\u4f5c\u3001\u7f3a\u4e4f\u5b9e\u7528\u7684\u90e8\u7f72\u67b6\u6784\uff0c\u4ee5\u53ca\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8106\u5f31\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMAI-UI\u8fbe\u5230\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u73b0\u6709\u6a21\u578b\u3002</li>\n    <li>\u5176\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u663e\u793a\uff0c\u901a\u8fc7\u6269\u5c55\u5e76\u884c\u73af\u5883\u548c\u589e\u52a0\u6b65\u9aa4\u9884\u7b97\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u672c\u5730\u4e91\u534f\u4f5c\u7cfb\u7edf\u63d0\u9ad8\u4e86\u8bbe\u5907\u6027\u80fd\u548c\u7528\u6237\u9690\u79c1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MAI-UI is a new type of GUI agent that aims to improve how people interact with computers.</li>\n    <li>It includes various sizes of agents and tackles four main challenges for better usability.</li>\n    <li>MAI-UI uses a smart system to enhance data collection and improve how tasks are managed between devices and the cloud.</li>\n    <li>It achieves impressive performance on several benchmarks, surpassing previous models in GUI tasks and mobile navigation.</li>\n    <li>The system enhances on-device performance and user privacy, while reducing the need for cloud resources.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:52.000Z", "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents", "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22047.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 196}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21675", "authors": [{"_id": "6951e354746a34b55dd5487a", "name": "Shuo Cao", "hidden": false}, {"_id": "6951e354746a34b55dd5487b", "name": "Jiayang Li", "hidden": false}, {"_id": "6951e354746a34b55dd5487c", "name": "Xiaohui Li", "hidden": false}, {"_id": "6951e354746a34b55dd5487d", "user": {"_id": "625d5b9f0bec31f086e04cd9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg", "isPro": false, "fullname": "YuandongPu", "user": "Andrew613", "type": "user"}, "name": "Yuandong Pu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-29T14:23:05.871Z", "hidden": false}, {"_id": "6951e354746a34b55dd5487e", "name": "Kaiwen Zhu", "hidden": false}, {"_id": "6951e354746a34b55dd5487f", "name": "Yuanting Gao", "hidden": false}, {"_id": "6951e354746a34b55dd54880", "name": "Siqi Luo", "hidden": false}, {"_id": "6951e354746a34b55dd54881", "name": "Yi Xin", "hidden": false}, {"_id": "6951e354746a34b55dd54882", "name": "Qi Qin", "hidden": false}, {"_id": "6951e354746a34b55dd54883", "name": "Yu Zhou", "hidden": false}, {"_id": "6951e354746a34b55dd54884", "name": "Xiangyu Chen", "hidden": false}, {"_id": "6951e354746a34b55dd54885", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6951e354746a34b55dd54886", "name": "Bin Fu", "hidden": false}, {"_id": "6951e354746a34b55dd54887", "name": "Yu Qiao", "hidden": false}, {"_id": "6951e354746a34b55dd54888", "name": "Yihao Liu", "hidden": false}], "publishedAt": "2025-12-25T13:35:52.000Z", "submittedOnDailyAt": "2025-12-29T00:40:27.598Z", "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture", "submittedOnDailyBy": {"_id": "625d5b9f0bec31f086e04cd9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg", "isPro": false, "fullname": "YuandongPu", "user": "Andrew613", "type": "user"}, "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.", "upvotes": 19, "discussionId": "6951e354746a34b55dd54889", "projectPage": "https://thunderbolt215.github.io/Unipercept-project/", "githubRepo": "https://github.com/thunderbolt215/UniPercept", "githubRepoAddedBy": "user", "githubStars": 27, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u611f\u77e5\u7ea7\u56fe\u50cf\u7279\u5f81\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86UniPercept-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6db5\u76d6\u7f8e\u5b66\u3001\u8d28\u91cf\u3001\u7ed3\u6784\u548c\u7eb9\u7406\u4e09\u4e2a\u5173\u952e\u9886\u57df\u7684\u611f\u77e5\u7ea7\u56fe\u50cf\u7406\u89e3\u3002</li>\n    <li>\u5efa\u7acb\u4e86\u5206\u5c42\u5b9a\u4e49\u7cfb\u7edf\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u611f\u77e5\u7ea7\u56fe\u50cf\u7406\u89e3\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578bUniPercept\uff0c\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u4efb\u52a1\u5bf9\u9f50\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u5728\u89c6\u89c9\u8bc4\u5206\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u3002</li>\n    <li>UniPercept\u5728\u611f\u77e5\u7ea7\u56fe\u50cf\u7406\u89e3\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684MLLMs\uff0c\u5e76\u53ef\u4f5c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u53ef\u63d2\u62d4\u5956\u52b1\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UniPercept-Bench is a new framework for understanding images at a perceptual level, focusing on aesthetics, quality, structure, and texture.</li>\n    <li>The framework includes a hierarchical definition system and large datasets for evaluating image understanding.</li>\n    <li>UniPercept, a model developed within this framework, uses advanced training methods to improve performance on tasks like Visual Rating and Visual Question Answering.</li>\n    <li>This model outperforms existing multimodal large language models in understanding image features.</li>\n    <li>The work sets a foundation for future research in perceptual-level image understanding with a comprehensive benchmark and a strong model.</li>\n</ul>"}, "publishedAt": "2025-12-25T08:35:52.000Z", "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21675.png", "numComments": 2, "submittedBy": {"_id": "625d5b9f0bec31f086e04cd9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg", "fullname": "YuandongPu", "name": "Andrew613", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": true}, {"paper": {"id": "2512.22118", "authors": [{"_id": "6951e89b746a34b55dd548bd", "name": "Zhi Ouyang", "hidden": false}, {"_id": "6951e89b746a34b55dd548be", "user": {"_id": "67e60ae6ac37824273d74389", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png", "isPro": false, "fullname": "Dian Zheng", "user": "zhengli1013", "type": "user"}, "name": "Dian Zheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-29T14:23:01.346Z", "hidden": false}, {"_id": "6951e89b746a34b55dd548bf", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6951e89b746a34b55dd548c0", "name": "Jian-Jian Jiang", "hidden": false}, {"_id": "6951e89b746a34b55dd548c1", "name": "Kun-Yu Lin", "hidden": false}, {"_id": "6951e89b746a34b55dd548c2", "name": "Jingke Meng", "hidden": false}, {"_id": "6951e89b746a34b55dd548c3", "name": "Wei-Shi Zheng", "hidden": false}], "publishedAt": "2025-12-26T18:59:14.000Z", "submittedOnDailyAt": "2025-12-29T00:07:11.474Z", "title": "ProEdit: Inversion-based Editing From Prompts Done Right", "submittedOnDailyBy": {"_id": "67e60ae6ac37824273d74389", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png", "isPro": false, "fullname": "Dian Zheng", "user": "zhengli1013", "type": "user"}, "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.", "upvotes": 12, "discussionId": "6951e89b746a34b55dd548c4", "projectPage": "https://isee-laboratory.github.io/ProEdit/", "githubRepo": "https://github.com/iSEE-Laboratory/ProEdit", "githubRepoAddedBy": "user", "githubStars": 24, "summary_zh": "<ul>\n    <li>\u5012\u7f6e\u57fa\u7840\u7684\u89c6\u89c9\u7f16\u8f91\u53ef\u4ee5\u6839\u636e\u7528\u6237\u6307\u793a\u6709\u6548\u7f16\u8f91\u56fe\u50cf\u6216\u89c6\u9891\uff0c\u65e0\u9700\u8bad\u7ec3\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u8fc7\u4e8e\u4f9d\u8d56\u6e90\u56fe\u50cf\u4fe1\u606f\uff0c\u5bfc\u81f4\u76ee\u6807\u56fe\u50cf\u7f16\u8f91\u4e0d\u4e00\u81f4\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faProEdit\uff0c\u901a\u8fc7\u4e24\u65b9\u9762\u7684\u6539\u8fdb\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff1a\u6ce8\u610f\u529b\u65b9\u9762\u548c\u6f5c\u5728\u7a7a\u95f4\u65b9\u9762\u3002</li>\n    <li>\u5728\u6ce8\u610f\u529b\u65b9\u9762\uff0c\u6211\u4eec\u5f15\u5165KV-mix\uff0c\u6df7\u5408\u6e90\u56fe\u50cf\u548c\u76ee\u6807\u56fe\u50cf\u7684\u7279\u5f81\uff0c\u4fdd\u6301\u80cc\u666f\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5728\u6f5c\u5728\u7a7a\u95f4\u65b9\u9762\uff0c\u6211\u4eec\u63d0\u51faLatents-Shift\uff0c\u6270\u52a8\u6e90\u56fe\u50cf\u7684\u7f16\u8f91\u533a\u57df\uff0c\u6d88\u9664\u9006\u5411\u6f5c\u5728\u5f71\u54cd\uff0c\u5b9e\u9a8c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6848\u6027\u80fd\u4f18\u8d8a\u4e14\u6613\u4e8e\u4e0e\u73b0\u6709\u65b9\u6cd5\u6574\u5408\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Inversion-based visual editing allows users to edit images and videos without training.</li>\n    <li>Current methods depend too much on the source image, making it hard to change specific details like pose or color.</li>\n    <li>The new method, ProEdit, improves editing by mixing features from both source and target images and adjusting the source's latent region.</li>\n    <li>ProEdit maintains background consistency while allowing more freedom in editing the subject.</li>\n    <li>This method works well with existing editing tools and has shown superior performance in tests.</li>\n</ul>"}, "publishedAt": "2025-12-26T13:59:14.000Z", "title": "ProEdit: Inversion-based Editing From Prompts Done Right", "summary": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22118.png", "numComments": 1, "submittedBy": {"_id": "67e60ae6ac37824273d74389", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/YvPKZ_0gyJnvNwM1zK3JS.png", "fullname": "Dian Zheng", "name": "zhengli1013", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21859", "authors": [{"_id": "6951f746746a34b55dd548cb", "user": {"_id": "64b8a72952b7353d8c669086", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg", "isPro": false, "fullname": "Qi Fan", "user": "fanqiNO1", "type": "user"}, "name": "Qi Fan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-29T14:22:57.776Z", "hidden": false}, {"_id": "6951f746746a34b55dd548cc", "name": "An Zou", "hidden": false}, {"_id": "6951f746746a34b55dd548cd", "name": "Yehan Ma", "hidden": false}], "publishedAt": "2025-12-26T04:49:35.000Z", "submittedOnDailyAt": "2025-12-29T01:09:22.323Z", "title": "TimeBill: Time-Budgeted Inference for Large Language Models", "submittedOnDailyBy": {"_id": "64b8a72952b7353d8c669086", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg", "isPro": false, "fullname": "Qi Fan", "user": "fanqiNO1", "type": "user"}, "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.", "upvotes": 12, "discussionId": "6951f746746a34b55dd548ce", "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65f6\u95f4\u654f\u611f\u7cfb\u7edf\u4e2d\u5e94\u7528\uff0c\u51c6\u786e\u54cd\u5e94\u65f6\u95f4\u5bf9\u51b3\u7b56\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>LLMs\u7684\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4f7f\u5f97\u9884\u6d4b\u6574\u4f53\u6267\u884c\u65f6\u95f4\u53d8\u5f97\u56f0\u96be\u3002</li>\n    <li>\u73b0\u6709\u7684\u63a8\u7406\u65b9\u6cd5\u5728\u56fa\u5b9a\u7684\u7f13\u5b58\u6e05\u9664\u6bd4\u7387\u4e0b\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u9884\u7b97\u7684\u4efb\u52a1\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86TimeBill\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u7ec6\u7684\u54cd\u5e94\u957f\u5ea6\u9884\u6d4b\u5668\u548c\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u5668\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u548c\u54cd\u5e94\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\uff0cTimeBill\u5728\u4e0d\u540c\u7b56\u7565\u4e0b\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u5e76\u4fdd\u6301\u4e86\u54cd\u5e94\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are used in important areas like robotics and autonomous driving, where quick and accurate responses are vital.</li>\n    <li>LLMs have a generation process that makes it hard to predict how long their tasks will take.</li>\n    <li>Current methods for efficient inference struggle with changing tasks and time limits, which can lead to incomplete results.</li>\n    <li>The paper introduces TimeBill, a new framework that improves response times and efficiency for LLMs.</li>\n    <li>TimeBill uses tools to predict execution time and adjusts cache management to better fit time budgets, showing improved task completion rates in tests.</li>\n</ul>"}, "publishedAt": "2025-12-25T23:49:35.000Z", "title": "TimeBill: Time-Budgeted Inference for Large Language Models", "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21859.png", "numComments": 2, "submittedBy": {"_id": "64b8a72952b7353d8c669086", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg", "fullname": "Qi Fan", "name": "fanqiNO1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}, {"paper": {"id": "2512.22120", "authors": [{"_id": "6951e5dc746a34b55dd5488b", "name": "Shuoshuo Zhang", "hidden": false}, {"_id": "6951e5dc746a34b55dd5488c", "user": {"_id": "66cd7496e3a1071d56cd80ca", "avatarUrl": "/avatars/9d5b46fc76a46d6d2e0a5bcb98cf4ae9.svg", "isPro": false, "fullname": "Yizhen Zhang", "user": "alchemistyzz", "type": "user"}, "name": "Yizhen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-29T14:23:03.719Z", "hidden": false}, {"_id": "6951e5dc746a34b55dd5488d", "name": "Jingjing Fu", "hidden": false}, {"_id": "6951e5dc746a34b55dd5488e", "name": "Lei Song", "hidden": false}, {"_id": "6951e5dc746a34b55dd5488f", "name": "Jiang Bian", "hidden": false}, {"_id": "6951e5dc746a34b55dd54890", "name": "Yujiu Yang", "hidden": false}, {"_id": "6951e5dc746a34b55dd54891", "name": "Rui Wang", "hidden": false}], "publishedAt": "2025-12-26T18:59:47.000Z", "submittedOnDailyAt": "2025-12-29T00:26:41.152Z", "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning", "submittedOnDailyBy": {"_id": "641c6c51dad24840739667ed", "avatarUrl": "/avatars/916bf79bd0cb2e3c3214edf5cba25784.svg", "isPro": true, "fullname": "Shuoshuo Zhang", "user": "zss01", "type": "user"}, "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.", "upvotes": 9, "discussionId": "6951e5dc746a34b55dd54892", "githubRepo": "https://github.com/zss02/BiPS", "githubRepoAddedBy": "user", "githubStars": 5, "organization": {"_id": "66f55d53853f0506904d1922", "name": "IIGroup", "fullname": "Tsinghua IIGroup", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u53ef\u4ee5\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u6539\u5584\u6027\u80fd\uff0c\u4f46\u76ee\u524d\u7684\u673a\u5236\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u53cc\u5411\u611f\u77e5\u5851\u9020\uff08BiPS\uff09\uff0c\u53ef\u4ee5\u6539\u5584\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u3002</li>\n    <li>BiPS\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u56fe\u50cf\u533a\u57df\uff0c\u6765\u589e\u5f3a\u89c6\u89c9\u4f9d\u8d56\u6027\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u9ad8\u4e868.2%\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u5730\u5e94\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u548c\u56fe\u50cf\u7c7b\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Large vision-language models can improve with visual cues, but often miss detailed visual information and can be expensive to run.</li>\n  <li>The paper introduces Bi-directional Perceptual Shaping (BiPS), which enhances how models focus on relevant visual information during training.</li>\n  <li>BiPS uses two constraints: one encourages models to consider important areas of an image, and the other prevents them from relying only on text for answers.</li>\n  <li>This method shows significant improvement in performance on various tests, increasing accuracy by 8.2% on average.</li>\n  <li>BiPS also demonstrates good performance on new and different types of datasets that the model hasn't seen before.</li>\n</ul>"}, "publishedAt": "2025-12-26T13:59:47.000Z", "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning", "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22120.png", "numComments": 1, "submittedBy": {"_id": "641c6c51dad24840739667ed", "avatarUrl": "/avatars/916bf79bd0cb2e3c3214edf5cba25784.svg", "fullname": "Shuoshuo Zhang", "name": "zss01", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "66f55d53853f0506904d1922", "name": "IIGroup", "fullname": "Tsinghua IIGroup", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21643", "authors": [{"_id": "695217d6746a34b55dd548ee", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "695217d6746a34b55dd548ef", "user": {"_id": "625d5b9f0bec31f086e04cd9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg", "isPro": false, "fullname": "YuandongPu", "user": "Andrew613", "type": "user"}, "name": "Yuandong Pu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-29T14:22:54.030Z", "hidden": false}, {"_id": "695217d6746a34b55dd548f0", "name": "Xuming He", "hidden": false}, {"_id": "695217d6746a34b55dd548f1", "name": "Yidi Liu", "hidden": false}, {"_id": "695217d6746a34b55dd548f2", "name": "Yixin Chen", "hidden": false}, {"_id": "695217d6746a34b55dd548f3", "name": "Junchao Gong", "hidden": false}, {"_id": "695217d6746a34b55dd548f4", "name": "Xiang Zhuang", "hidden": false}, {"_id": "695217d6746a34b55dd548f5", "name": "Wanghan Xu", "hidden": false}, {"_id": "695217d6746a34b55dd548f6", "name": "Qinglong Cao", "hidden": false}, {"_id": "695217d6746a34b55dd548f7", "name": "Shixiang Tang", "hidden": false}, {"_id": "695217d6746a34b55dd548f8", "name": "Yihao Liu", "hidden": false}, {"_id": "695217d6746a34b55dd548f9", "name": "Wenlong Zhang", "hidden": false}, {"_id": "695217d6746a34b55dd548fa", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-25T12:08:09.000Z", "submittedOnDailyAt": "2025-12-29T03:26:08.562Z", "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding", "submittedOnDailyBy": {"_id": "625d5b9f0bec31f086e04cd9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg", "isPro": false, "fullname": "YuandongPu", "user": "Andrew613", "type": "user"}, "summary": "Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.", "upvotes": 8, "discussionId": "695217d6746a34b55dd548fb", "ai_summary": "Omni-Weather is a multimodal foundation model that integrates weather generation and understanding using a shared self-attention mechanism and a Chain-of-Thought dataset to enable interpretable, high-quality outputs.", "ai_keywords": ["multimodal foundation model", "radar encoder", "shared self-attention mechanism", "Chain-of-Thought dataset"], "summary_zh": "<ul>\n    <li>\u5929\u6c14\u5efa\u6a21\u9700\u8981\u51c6\u786e\u7684\u9884\u6d4b\u548c\u673a\u5236\u89e3\u91ca\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5c06\u8fd9\u4e24\u4e2a\u76ee\u6807\u5206\u5f00\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Omni-Weather\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5c06\u5929\u6c14\u751f\u6210\u548c\u7406\u89e3\u7edf\u4e00\u5728\u4e00\u4e2a\u67b6\u6784\u4e2d\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3002</li>\n    <li>Omni-Weather\u4f7f\u7528\u96f7\u8fbe\u7f16\u7801\u5668\u8fdb\u884c\u5929\u6c14\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u7edf\u4e00\u5904\u7406\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5929\u6c14\u751f\u6210\u4e2d\u7684\u56e0\u679c\u63a8\u7406\uff0c\u63d0\u9ad8\u8f93\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u548c\u611f\u77e5\u8d28\u91cf\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOmni-Weather\u5728\u5929\u6c14\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u8fd9\u4e24\u8005\u53ef\u4ee5\u76f8\u4e92\u589e\u5f3a\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Omni-Weather is a new model that combines weather prediction and understanding in one system.</li>\n  <li>It uses a radar encoder for generating weather data and a shared attention mechanism for processing.</li>\n  <li>The model includes a special dataset to help with reasoning about weather events, making the results easier to interpret.</li>\n  <li>Tests show that Omni-Weather performs better than other methods in both predicting and understanding weather.</li>\n  <li>The model shows that generating and understanding weather data can help each other improve.</li>\n</ul>"}, "publishedAt": "2025-12-25T07:08:09.000Z", "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding", "summary": "Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21643.png", "numComments": 1, "submittedBy": {"_id": "625d5b9f0bec31f086e04cd9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650285458447-noauth.jpeg", "fullname": "YuandongPu", "name": "Andrew613", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": true}, {"paper": {"id": "2512.18745", "authors": [{"_id": "694a4fe4335742716e9323ce", "user": {"_id": "65d5b967eeb590ea7435ad07", "avatarUrl": "/avatars/ca0a5e123d5da5aca97cfd8a2d07e60e.svg", "isPro": false, "fullname": "Kaican Li", "user": "m-Just", "type": "user"}, "name": "Kaican Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:47:40.670Z", "hidden": false}, {"_id": "694a4fe4335742716e9323cf", "name": "Lewei Yao", "hidden": false}, {"_id": "694a4fe4335742716e9323d0", "name": "Jiannan Wu", "hidden": false}, {"_id": "694a4fe4335742716e9323d1", "name": "Tiezheng Yu", "hidden": false}, {"_id": "694a4fe4335742716e9323d2", "name": "Jierun Chen", "hidden": false}, {"_id": "694a4fe4335742716e9323d3", "name": "Haoli Bai", "hidden": false}, {"_id": "694a4fe4335742716e9323d4", "name": "Lu Hou", "hidden": false}, {"_id": "694a4fe4335742716e9323d5", "name": "Lanqing Hong", "hidden": false}, {"_id": "694a4fe4335742716e9323d6", "name": "Wei Zhang", "hidden": false}, {"_id": "694a4fe4335742716e9323d7", "name": "Nevin L. Zhang", "hidden": false}], "publishedAt": "2025-12-21T14:23:07.000Z", "submittedOnDailyAt": "2025-12-29T04:36:37.110Z", "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search", "submittedOnDailyBy": {"_id": "65d5b967eeb590ea7435ad07", "avatarUrl": "/avatars/ca0a5e123d5da5aca97cfd8a2d07e60e.svg", "isPro": false, "fullname": "Kaican Li", "user": "m-Just", "type": "user"}, "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .", "upvotes": 6, "discussionId": "694a4fe4335742716e9323d8", "githubRepo": "https://github.com/m-Just/InSight-o3", "githubRepoAddedBy": "user", "ai_summary": "O3-Bench evaluates multimodal reasoning with interleaved attention to visual details, while InSight-o3 uses a multi-agent framework to improve performance through specialized visual search and reasoning tasks.", "ai_keywords": ["multimodal agents", "reasoning", "perception", "O3-Bench", "interleaved attention", "visual reasoning", "visual search", "vReasoner", "vSearcher", "generalized visual search", "multimodal LLM", "reinforcement learning"], "githubStars": 3, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001AI\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u5982\u5206\u6790\u56fe\u8868\u548c\u5730\u56fe\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86O3-Bench\uff0c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u89c6\u89c9\u7ec6\u8282\u7684\u6ce8\u610f\u529b\u3002</li>\n    <li>O3-Bench\u5305\u542b\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u9ad8\u96be\u5ea6\u95ee\u9898\uff0c\u8fde\u5148\u8fdb\u7cfb\u7edf\u7684\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a40.8%\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86InSight-o3\uff0c\u4e00\u4e2a\u7531\u89c6\u89c9\u63a8\u7406\u4ee3\u7406\u548c\u89c6\u89c9\u641c\u7d22\u4ee3\u7406\u7ec4\u6210\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u5e7f\u4e49\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u591a\u6a21\u6001LLM\uff0c\u4f7f\u5f97\u6211\u4eec\u7684\u89c6\u89c9\u641c\u7d22\u4ee3\u7406\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AI agents need to combine reasoning and perception to think with images, but current systems struggle with reasoning tasks.</li>\n    <li>O3-Bench is a new benchmark that tests multimodal reasoning by requiring agents to analyze complex visual information across different parts of images.</li>\n    <li>Even advanced systems, like OpenAI's o3, only achieve about 40.8% accuracy on O3-Bench, indicating the challenges involved.</li>\n    <li>InSight-o3 is a proposed solution that includes two agents: a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) that can find specific visual information based on free-form language descriptions.</li>\n    <li>The vSearcher enhances the performance of multimodal models, representing a step forward in the development of advanced AI systems.</li>\n</ul>"}, "publishedAt": "2025-12-21T09:23:07.000Z", "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search", "summary": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18745.png", "numComments": 1, "submittedBy": {"_id": "65d5b967eeb590ea7435ad07", "avatarUrl": "/avatars/ca0a5e123d5da5aca97cfd8a2d07e60e.svg", "fullname": "Kaican Li", "name": "m-Just", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21919", "authors": [{"_id": "6951e7bc746a34b55dd5489c", "name": "KaShun Shum", "hidden": false}, {"_id": "6951e7bc746a34b55dd5489d", "name": "Binyuan Hui", "hidden": false}, {"_id": "6951e7bc746a34b55dd5489e", "name": "Jiawei Chen", "hidden": false}, {"_id": "6951e7bc746a34b55dd5489f", "name": "Lei Zhang", "hidden": false}, {"_id": "6951e7bc746a34b55dd548a0", "name": "X. W.", "hidden": false}, {"_id": "6951e7bc746a34b55dd548a1", "name": "Jiaxi Yang", "hidden": false}, {"_id": "6951e7bc746a34b55dd548a2", "name": "Yuzhen Huang", "hidden": false}, {"_id": "6951e7bc746a34b55dd548a3", "name": "Junyang Lin", "hidden": false}, {"_id": "6951e7bc746a34b55dd548a4", "name": "Junxian He", "hidden": false}], "publishedAt": "2025-12-26T08:26:18.000Z", "submittedOnDailyAt": "2025-12-29T00:00:24.492Z", "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.", "upvotes": 5, "discussionId": "6951e7bd746a34b55dd548a5", "summary_zh": "<ul>\n    <li>\u6267\u884c\u53cd\u9988\uff08\u5982\u5355\u5143\u6d4b\u8bd5\uff09\u5728\u7f16\u7801\u4ee3\u7406\u7684\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u53cd\u9988\u5f80\u5f80\u7a00\u758f\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6210\u529f\u548c\u5931\u8d25\u7684\u8f68\u8ff9\u3002</li>\n    <li>\u65e0\u6267\u884c\u53cd\u9988\u7684\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u4fe1\u53f7\uff0c\u4f46\u5728\u73b0\u5b9e\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u4e24\u4e2a\u5728TTS\u4e0a\u8868\u73b0\u76f8\u4f3c\u7684\u9a8c\u8bc1\u5668\u5728RL\u4e2d\u7684\u7ed3\u679c\u5374\u622a\u7136\u4e0d\u540c\uff0c\u8fd9\u8bf4\u660eTTS\u7684\u80fd\u529b\u4e0d\u4e00\u5b9a\u9002\u7528\u4e8eRL\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u786e\u5b9a\u4e86RL\u8bad\u7ec3\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u4e24\u4e2a\u65b9\u9762\uff1a\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6821\u51c6\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e9b\u7814\u7a76\uff0c\u63d0\u51fa\u4e86SWE-RM\uff0c\u4e00\u4e2a\u5177\u670930\u4ebf\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u7684\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SWE\u4ee3\u7406\u5728TTS\u548cRL\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Execution-based feedback, like unit testing, is common for training coding agents but can be limited in providing clear distinctions between successful and unsuccessful outcomes.</li>\n    <li>Execution-free feedback from reward models can offer better signals without relying on unit tests, but it's not well-explored for software engineering agents.</li>\n    <li>Two reward models can have similar performance in test-time scaling (TTS) but yield different results in reinforcement learning (RL), highlighting the need for better evaluation metrics.</li>\n    <li>Key factors for improving RL training include classification accuracy and calibration, which were investigated through controlled experiments.</li>\n    <li>The new SWE-RM model significantly enhances coding agent performance in both TTS and RL, achieving state-of-the-art results among open-source models.</li>\n</ul>"}, "publishedAt": "2025-12-26T03:26:18.000Z", "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents", "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21919.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 196}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n  <li>\u4f20\u7edf\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u89c6\u9891\u5206\u5e03\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n  <li>SemanticGen\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n  <li>\u8be5\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u89c6\u9891\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n  <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u66f4\u5feb\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002</li>\n  <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u548c\u57fa\u7ebf\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new video generation method that improves upon traditional models that use VAE (Variational Autoencoder) techniques.</li>\n    <li>It starts by generating a simple, high-level outline of the video in a \"semantic space\" before adding detailed features.</li>\n    <li>This two-stage process uses diffusion models: the first stage creates the overall layout, and the second stage adds finer details.</li>\n    <li>Generating videos in the semantic space helps the model learn faster and works better for longer videos.</li>\n    <li>Tests show that SemanticGen creates high-quality videos and performs better than existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 57, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u8f85\u52a9\u56fe\u50cf\u7b49\u8fdb\u884c\u76d1\u7763\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4efb\u52a1\u4f9d\u8d56\u7684\u673a\u5236\uff0c\u8ba9LMMs\u81ea\u4e3b\u53d1\u73b0\u548c\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u673a\u5236\u80fd\u591f\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u7f16\u7801\u56fe\u50cf\uff0c\u63d0\u53d6\u76f8\u5173\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u800c\u65e0\u9700\u624b\u52a8\u76d1\u7763\u3002</li>\n    <li>\u6211\u4eec\u7684\u505a\u6cd5\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u80fd\u591f\u5f88\u597d\u5730\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text and struggle with visual reasoning tasks.</li>\n    <li>Current methods use helper images and annotations but are limited and costly.</li>\n    <li>The proposed method allows LMMs to learn visual reasoning on their own without explicit guidance.</li>\n    <li>This new approach adapts to different tasks and improves performance on various vision-related challenges.</li>\n    <li>It also shows better adaptability to multi-task learning compared to previous methods.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 5, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20605", "authors": [{"_id": "694e76e5746a34b55dd545eb", "name": "Seijin Kobayashi", "hidden": false}, {"_id": "694e76e5746a34b55dd545ec", "user": {"_id": "68288c78b4d8c06744817b87", "avatarUrl": "/avatars/c46824aa788cbe358b322dfce04a59ed.svg", "isPro": false, "fullname": "Yanick Schimpf", "user": "yschimpf", "type": "user"}, "name": "Yanick Schimpf", "status": "claimed_verified", "statusLastChangedAt": "2025-12-29T14:23:35.366Z", "hidden": false}, {"_id": "694e76e5746a34b55dd545ed", "user": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "name": "Maximilian Schlegel", "status": "claimed_verified", "statusLastChangedAt": "2025-12-29T14:23:37.525Z", "hidden": false}, {"_id": "694e76e5746a34b55dd545ee", "name": "Angelika Steger", "hidden": false}, {"_id": "694e76e5746a34b55dd545ef", "name": "Maciej Wolczyk", "hidden": false}, {"_id": "694e76e5746a34b55dd545f0", "name": "Johannes von Oswald", "hidden": false}, {"_id": "694e76e5746a34b55dd545f1", "name": "Nino Scherrer", "hidden": false}, {"_id": "694e76e5746a34b55dd545f2", "name": "Kaitlin Maile", "hidden": false}, {"_id": "694e76e5746a34b55dd545f3", "name": "Guillaume Lajoie", "hidden": false}, {"_id": "694e76e5746a34b55dd545f4", "name": "Blake A. Richards", "hidden": false}, {"_id": "694e76e5746a34b55dd545f5", "name": "Rif A. Saurous", "hidden": false}, {"_id": "694e76e5746a34b55dd545f6", "name": "James Manyika", "hidden": false}, {"_id": "694e76e5746a34b55dd545f7", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "694e76e5746a34b55dd545f8", "name": "Alexander Meulemans", "hidden": false}, {"_id": "694e76e5746a34b55dd545f9", "name": "Jo\u00e3o Sacramento", "hidden": false}], "publishedAt": "2025-12-23T18:51:50.000Z", "submittedOnDailyAt": "2025-12-26T11:17:05.505Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "submittedOnDailyBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "upvotes": 54, "discussionId": "694e76e5746a34b55dd545fa", "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n    <li>\u4f20\u7edf\u7684\u9010\u4e2a\u6807\u8bb0\u751f\u6210\u8f93\u51fa\u7684\u63a2\u7d22\u65b9\u5f0f\u6548\u7387\u4f4e\uff0c\u7279\u522b\u662f\u5728\u5956\u52b1\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u9636\u7684\u975e\u56e0\u679c\u5e8f\u5217\u6a21\u578b\uff0c\u80fd\u5728\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u8fdb\u884c\u63a2\u7d22\u548c\u64cd\u4f5c\u3002</li>\n    <li>\u8be5\u6a21\u578b\u80fd\u538b\u7f29\u957f\u6fc0\u6d3b\u5e8f\u5217\uff0c\u63a7\u5236\u5185\u90e8\u63a7\u5236\u5668\uff0c\u6267\u884c\u6709\u610f\u4e49\u7684\u884c\u4e3a\u5e8f\u5217\u3002</li>\n    <li>\u5185\u90e8\u5f3a\u5316\u5b66\u4e60\uff08internal RL\uff09\u80fd\u5728\u6807\u51c6RL\u5fae\u8c03\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u7a00\u758f\u5956\u52b1\u4e2d\u5b66\u4e60\uff0c\u663e\u793a\u4e86\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u5b9e\u73b0\u5206\u5c42RL\u7684\u5e0c\u671b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large autoregressive models are trained to predict the next token and fine-tuned using reinforcement learning (RL) for better performance.</li>\n    <li>Standard token-by-token exploration can be slow and inefficient, especially when rewards are not frequent.</li>\n    <li>The study introduces a new higher-order model that uses the internal workings of the autoregressive model to discover complex actions.</li>\n    <li>This approach allows the model to create meaningful actions that can take place over longer periods and helps with learning from sparse rewards.</li>\n    <li>The findings suggest that using internal reinforcement learning can improve exploration and performance in hierarchical tasks within these models.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:51:50.000Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png", "numComments": 5, "submittedBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "fullname": "Maximilian Schlegel", "name": "schlegelm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u8f83\u5f31\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u63a8\u51fa\u4e86DSR Suite\uff0c\u5305\u62ec\u4ece\u89c6\u9891\u4e2d\u81ea\u52a8\u751f\u6210\u591a\u9009\u62e9\u9898\u548c\u7b54\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u63d0\u53d6\u4e86\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u4ee5\u6784\u5efa\u7528\u4e8e\u5b66\u4e60\u7684DSR-Train\u548c\u7528\u4e8e\u8bc4\u4f30\u7684DSR-Bench\u3002</li>\n    <li>\u6570\u636e\u5f3a\u8c03\u4e86\u91ce\u5916\u89c6\u9891\u6e90\u30013D\u9700\u6c42\u3001\u89c6\u89d2\u53d8\u5316\u548c\u591a\u7269\u4f53\u4ea4\u4e92\u7b49\u65b9\u9762\u3002</li>\n    <li>\u8fd8\u63d0\u51fa\u4e86\u8f7b\u91cf\u5316\u7684\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\uff0c\u4ee5\u4fbf\u5c06\u51e0\u4f55\u4fe1\u606f\u6709\u6548\u6574\u5408\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLM) are good at understanding but struggle with dynamic spatial reasoning (DSR) in 3D space.</li>\n    <li>To improve DSR, researchers created a tool called DSR Suite that generates question-answer pairs from real-world videos.</li>\n    <li>This tool uses advanced vision models to gather important geometric and motion data from the videos.</li>\n    <li>The new data focuses on real videos, 3D object requirements, viewpoint changes, multi-object interactions, and detailed answers.</li>\n    <li>A new Geometry Selection Module (GSM) helps VLMs use geometric information effectively without overwhelming them with unnecessary details.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20618", "authors": [{"_id": "694ba02a746a34b55dd53e8b", "name": "Runtao Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8c", "name": "Ziyi Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8d", "name": "Jiaqi Tang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8e", "name": "Yue Ma", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8f", "name": "Renjie Pi", "hidden": false}, {"_id": "694ba02a746a34b55dd53e90", "name": "Jipeng Zhang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e91", "name": "Qifeng Chen", "hidden": false}], "publishedAt": "2025-12-23T18:59:49.000Z", "submittedOnDailyAt": "2025-12-24T05:57:23.776Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "submittedOnDailyBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "isPro": true, "fullname": "Jiaqi Tang", "user": "Jiaqi-hkust", "type": "user"}, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "upvotes": 38, "discussionId": "694ba02a746a34b55dd53e92", "ai_summary": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.", "ai_keywords": ["multimodal LLMs", "long-video QA", "multi-agent framework", "grounding agent", "vision agent", "reinforcement learning", "temporal grounding", "LongTVQA", "LongTVQA+"], "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u95ee\u7b54\uff0c\u80fd\u66f4\u597d\u5730\u5904\u7406\u4e00\u5c0f\u65f6\u7684\u5185\u5bb9\u3002</li>\n    <li>\u6846\u67b6\u4e2d\u5305\u62ec\u4e00\u4e2a\u4e3b\u667a\u80fd\u4f53\u3001\u4e00\u4e2a\u5b9a\u4f4d\u4ee3\u7406\u548c\u4e00\u4e2a\u89c6\u89c9\u4ee3\u7406\uff0c\u534f\u540c\u5de5\u4f5c\u4ee5\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\u3002</li>\n    <li>\u4e3b\u667a\u80fd\u4f53\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002</li>\n    <li>\u5728\u6211\u4eec\u521b\u5efa\u7684LongTVQA\u548cLongTVQA+\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u4e86\u8bad\u7ec3\u4ee3\u7406\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods in video question-answering (QA) can analyze long videos but often lose important details.</li>\n    <li>We suggest a multi-agent system where a master agent works with other agents to find relevant video parts and extract information.</li>\n    <li>The master agent uses reinforcement learning to improve its ability to plan and cooperate effectively.</li>\n    <li>This system improves the understanding of long videos compared to traditional methods.</li>\n    <li>Our experiments show that this approach leads to better reasoning and planning in video analysis.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:49.000Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png", "numComments": 1, "submittedBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "fullname": "Jiaqi Tang", "name": "Jiaqi-hkust", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20617", "authors": [{"_id": "694b58e3746a34b55dd53cff", "name": "Yuxi Xiao", "hidden": false}, {"_id": "694b58e3746a34b55dd53d00", "name": "Longfei Li", "hidden": false}, {"_id": "694b58e3746a34b55dd53d01", "name": "Shen Yan", "hidden": false}, {"_id": "694b58e3746a34b55dd53d02", "name": "Xinhang Liu", "hidden": false}, {"_id": "694b58e3746a34b55dd53d03", "name": "Sida Peng", "hidden": false}, {"_id": "694b58e3746a34b55dd53d04", "name": "Yunchao Wei", "hidden": false}, {"_id": "694b58e3746a34b55dd53d05", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "694b58e3746a34b55dd53d06", "name": "Bingyi Kang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "publishedAt": "2025-12-23T18:59:46.000Z", "submittedOnDailyAt": "2025-12-24T00:38:28.003Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "upvotes": 34, "discussionId": "694b58e4746a34b55dd53d07", "projectPage": "https://spatialtree.github.io/", "ai_summary": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.", "ai_keywords": ["SpatialTree", "low-level perception", "mental mapping", "simulation", "agentic competence", "capability-centric hierarchical benchmark", "targeted supervised fine-tuning", "negative transfer", "cross-level transfer", "naive RL", "auto-think strategy"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u7a7a\u95f4\u80fd\u529b\u7684\u53d1\u5c55\u662f\u4e00\u4e2a\u9010\u6b65\u8fc7\u7a0b\uff0c\u5305\u62ec\u611f\u77e5\u3001\u5fc3\u7406\u6620\u5c04\u3001\u6a21\u62df\u548c\u4e3b\u52a8\u80fd\u529b\u56db\u4e2a\u5c42\u6b21\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86SpatialTree\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u7a7a\u95f4\u80fd\u529b\u5c42\u7ea7\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8627\u79cd\u5b50\u80fd\u529b\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u4f4e\u7ea7\u6280\u80fd\uff08L1\uff09\u76f8\u4e92\u72ec\u7acb\uff0c\u800c\u9ad8\u7ea7\u6280\u80fd\u4e4b\u95f4\u6709\u5f3a\u76f8\u5173\u6027\uff0c\u8868\u660e\u5b83\u4eec\u4e4b\u95f4\u7684\u4f9d\u8d56\u6027\u589e\u52a0\u3002</li>\n    <li>\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u6211\u4eec\u53d1\u73b0L1\u5185\u5b58\u5728\u8d1f\u8fc1\u79fb\uff0c\u800c\u4ece\u4f4e\u7ea7\u5230\u9ad8\u7ea7\u80fd\u529b\u6709\u5f3a\u70c8\u7684\u6b63\u8fc1\u79fb\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u81ea\u6211\u601d\u8003\u7b56\u7565\uff0c\u53ef\u4ee5\u6539\u5584\u6240\u6709\u5c42\u7ea7\u7684\u8868\u73b0\uff0c\u907f\u514d\u8fc7\u5ea6\u601d\u8003\u9020\u6210\u7684\u5e72\u6270\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Spatial abilities develop in stages, from basic perception to complex reasoning.</li>\n    <li>Introducing SpatialTree, a new way to categorize spatial skills into four levels: perception, mental mapping, simulation, and agentic competence.</li>\n    <li>We evaluated popular multimodal language models (MLLMs) on 27 different spatial abilities, revealing a clear structure in their skills.</li>\n    <li>Found that basic skills (L1) are less connected, while higher skills (L2-L4) are more interdependent.</li>\n    <li>Proposed an auto-think strategy to improve performance across all skill levels in MLLMs by reducing unnecessary thinking.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:46.000Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u201c\u4e00\u955c\u5230\u5e95\u201d\u6280\u672f\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5177\u6709\u72ec\u7279\u800c\u590d\u6742\u7684\u7f8e\u5b66\uff0c\u4f46\u5b9e\u9645\u5b9e\u73b0\u53d7\u5230\u9ad8\u6210\u672c\u548c\u590d\u6742\u73b0\u5b9e\u6761\u4ef6\u7684\u9650\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u526a\u8f91\u62fc\u63a5\uff0c\u96be\u4ee5\u4fdd\u6301\u89c6\u89c9\u6d41\u7545\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86DreaMontage\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7528\u6237\u63d0\u4f9b\u7684\u591a\u6837\u8f93\u5165\u4e2d\u751f\u6210\u65e0\u7f1d\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u957f\u65f6\u95f4\u4e00\u955c\u89c6\u9891\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u96c6\u6210\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u6761\u4ef6\u673a\u5236\u3001\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8bbe\u8ba1\u6bb5\u843d\u81ea\u56de\u5f52\u63a8\u7406\u7b56\u7565\u6765\u89e3\u51b3\u5173\u952e\u6311\u6218\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u6548\u679c\u548c\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u5e2e\u52a9\u7528\u6237\u5c06\u5206\u6563\u7684\u89c6\u89c9\u6750\u6599\u8f6c\u5316\u4e3a\u751f\u52a8\u7684\u7535\u5f71\u4f53\u9a8c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" technique in filmmaking is visually appealing but expensive and complicated to execute.</li>\n    <li>Current video generation models often produce poor results due to simple clip joining that lacks smoothness.</li>\n    <li>DreaMontage is a new framework that creates long, smooth one-shot videos from user inputs.</li>\n    <li>It improves video quality by using a special conditioning method and a curated dataset for better motion and transitions.</li>\n    <li>The system is designed to efficiently produce longer sequences, allowing users to create cohesive cinematic experiences from shorter clips.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21094", "authors": [{"_id": "694ca45e746a34b55dd542de", "name": "Zhe Cao", "hidden": false}, {"_id": "694ca45e746a34b55dd542df", "name": "Tao Wang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e0", "user": {"_id": "68355c5ec0003bc40230b3f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WZQ5f8oqpni0i-D3a5R-P.png", "isPro": false, "fullname": "jasmineWang", "user": "Jessamine", "type": "user"}, "name": "Jiaming Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:43.024Z", "hidden": false}, {"_id": "694ca45e746a34b55dd542e1", "name": "Yanghai Wang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e2", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e3", "name": "Jialu Chen", "hidden": false}, {"_id": "694ca45e746a34b55dd542e4", "name": "Miao Deng", "hidden": false}, {"_id": "694ca45e746a34b55dd542e5", "user": {"_id": "68abfd1ba1f07af43fbbf3f1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ivRfEWMAo1GQWw3x06LQp.png", "isPro": false, "fullname": "jiahaowang", "user": "wang-jiahao", "type": "user"}, "name": "Jiahao Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:40.267Z", "hidden": false}, {"_id": "694ca45e746a34b55dd542e6", "name": "Yubin Guo", "hidden": false}, {"_id": "694ca45e746a34b55dd542e7", "name": "Chenxi Liao", "hidden": false}, {"_id": "694ca45e746a34b55dd542e8", "name": "Yize Zhang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e9", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "694ca45e746a34b55dd542ea", "name": "Jiaheng Liu", "hidden": false}], "publishedAt": "2025-12-24T10:30:35.000Z", "submittedOnDailyAt": "2025-12-25T00:12:49.258Z", "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation", "submittedOnDailyBy": {"_id": "65377c30e48353201e6fdda0", "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg", "isPro": false, "fullname": "Jiaheng Liu", "user": "CheeryLJH", "type": "user"}, "summary": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.", "upvotes": 22, "discussionId": "694ca45e746a34b55dd542eb", "projectPage": "https://nju-link.github.io/T2AV-Compass/", "githubRepo": "https://github.com/NJU-LINK/T2AV-Compass/", "githubRepoAddedBy": "user", "githubStars": 5, "organization": {"_id": "68edc767abe005ac1b354573", "name": "NJU-LINK", "fullname": "NJU-LINK Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u5230\u97f3\u9891\u89c6\u9891\u751f\u6210\uff08T2AV\uff09\u65e8\u5728\u4ece\u81ea\u7136\u8bed\u8a00\u5408\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u548c\u8bed\u4e49\u540c\u6b65\u7684\u97f3\u9891\uff0c\u4f46\u8bc4\u4f30\u65b9\u6cd5\u5f88\u5206\u6563\u3002</li>\n    <li>\u63d0\u51fa\u4e86T2AV-Compass\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30T2AV\u7cfb\u7edf\uff0c\u5305\u542b500\u4e2a\u591a\u6837\u4e14\u590d\u6742\u7684\u63d0\u793a\u3002</li>\n    <li>T2AV-Compass\u5f15\u5165\u4e86\u53cc\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5ba2\u89c2\u7684\u4fe1\u53f7\u7ea7\u6307\u6807\u548c\u4e3b\u89c2\u7684\u8bc4\u4f30\u534f\u8bae\u3002</li>\n    <li>\u5bf911\u4e2a\u4ee3\u8868\u6027T2AV\u7cfb\u7edf\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5f3a\u5927\u7684\u6a21\u578b\u5728\u4eba\u7c7b\u6c34\u5e73\u7684\u73b0\u5b9e\u611f\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u4e0a\u4e5f\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002</li>\n    <li>\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u672a\u6765\u6a21\u578b\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u5e76\u5f3a\u8c03\u4e86T2AV-Compass\u4f5c\u4e3a\u6311\u6218\u6027\u6d4b\u8bd5\u5e73\u53f0\u7684\u91cd\u8981\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text-to-Audio-Video (T2AV) generation creates videos and audio from written text, but its evaluation methods are not very effective.</li>\n    <li>T2AV-Compass is a new benchmark that helps in better evaluating T2AV systems, with 500 complex prompts designed for thorough testing.</li>\n    <li>The evaluation framework includes both objective measures (like video and audio quality) and subjective assessments (like how well the output follows instructions and its realism).</li>\n    <li>Tests on 11 T2AV systems show that even the best models do not match human-level quality, especially in audio realism and following instructions.</li>\n    <li>T2AV-Compass is important for identifying areas where T2AV technology can improve and helps advance the field.</li>\n</ul>"}, "publishedAt": "2025-12-24T05:30:35.000Z", "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation", "summary": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21094.png", "numComments": 1, "submittedBy": {"_id": "65377c30e48353201e6fdda0", "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg", "fullname": "Jiaheng Liu", "name": "CheeryLJH", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 25}, "organization": {"_id": "68edc767abe005ac1b354573", "name": "NJU-LINK", "fullname": "NJU-LINK Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21337", "authors": [{"_id": "694ccc03746a34b55dd54372", "name": "Li-Zhong Szu-Tu", "hidden": false}, {"_id": "694ccc03746a34b55dd54373", "name": "Ting-Lin Wu", "hidden": false}, {"_id": "694ccc03746a34b55dd54374", "name": "Chia-Jui Chang", "hidden": false}, {"_id": "694ccc03746a34b55dd54375", "name": "He Syu", "hidden": false}, {"_id": "694ccc03746a34b55dd54376", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KzxafmNkf4IHasqHjp3DV.jpeg"], "publishedAt": "2025-12-24T18:59:54.000Z", "submittedOnDailyAt": "2025-12-25T03:01:52.321Z", "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/", "upvotes": 20, "discussionId": "694ccc03746a34b55dd54377", "projectPage": "https://sytwu.github.io/BeyondMemo/", "githubRepo": "https://github.com/Sytwu/BeyondMemo", "githubRepoAddedBy": "user", "githubStars": 3, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8457\u540d\u5efa\u7b51\u4e0a\u7684\u51c6\u786e\u7387\u6bd4\u666e\u901a\u5efa\u7b51\u9ad8\u51fa34%\uff0c\u663e\u793a\u51fa\u5b83\u4eec\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u901a\u7528\u7406\u89e3\u3002</li>\n    <li>\u4e3a\u7cfb\u7edf\u5730\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\uff0c\u6211\u4eec\u63a8\u51fa\u4e86\u6700\u5927\u7684\u5f00\u653e\u57fa\u51c6\u6570\u636e\u96c6\uff1aYearGuessr\uff0c\u6536\u5f55\u4e86\u6765\u81ea157\u4e2a\u56fd\u5bb6\u768455,546\u5f20\u5efa\u7b51\u56fe\u7247\u53ca\u5176\u591a\u79cd\u5c5e\u6027\u3002</li>\n    <li>\u8be5\u6570\u636e\u96c6\u5305\u542b\u5efa\u7b51\u7684\u5efa\u8bbe\u5e74\u4efd\uff081001-2024\uff09\u3001GPS\u6570\u636e\u548c\u9875\u9762\u6d4f\u89c8\u91cf\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u6d41\u884c\u5ea6\u7684\u4ee3\u7406\u3002</li>\n    <li>\u6211\u4eec\u5c06\u5efa\u7b51\u5e74\u4efd\u9884\u6d4b\u4efb\u52a1\u89c6\u4e3a\u6709\u5e8f\u56de\u5f52\uff0c\u5e76\u5f15\u5165\u4e86\u8003\u8651\u6d41\u884c\u5ea6\u7684\u533a\u95f4\u51c6\u786e\u5ea6\u6307\u6807\u6765\u91cf\u5316\u504f\u5dee\u3002</li>\n    <li>\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5c3d\u7ba1VLMs\u5728\u6d41\u884c\u4e14\u7ecf\u8fc7\u8bb0\u5fc6\u7684\u9879\u76ee\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0d\u88ab\u8bc6\u522b\u7684\u5bf9\u8c61\u4e0a\u5374\u663e\u8457\u6323\u624e\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u7f3a\u9677\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) show a bias towards famous buildings, achieving up to 34% higher accuracy on them compared to ordinary buildings.</li>\n    <li>The research introduces the YearGuessr dataset, which contains 55,546 building images from 157 countries, annotated with construction years, GPS data, and page-view counts.</li>\n    <li>The dataset allows for the prediction of construction years using a method called ordinal regression, with new metrics to measure popularity bias.</li>\n    <li>Tests with over 30 models, including the YearCLIP model, reveal that VLMs perform well on popular buildings but struggle with less recognized ones, indicating a flaw in their reasoning.</li>\n    <li>More information is available on the project page: <a href=\"https://sytwu.github.io/BeyondMemo/\">https://sytwu.github.io/BeyondMemo/</a>.</li>\n</ul>"}, "publishedAt": "2025-12-24T13:59:54.000Z", "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models", "summary": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KzxafmNkf4IHasqHjp3DV.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21337.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22047", "authors": [{"_id": "6951e7d4746a34b55dd548a7", "name": "Hanzhang Zhou", "hidden": false}, {"_id": "6951e7d4746a34b55dd548a8", "name": "Xu Zhang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548a9", "name": "Panrong Tong", "hidden": false}, {"_id": "6951e7d4746a34b55dd548aa", "name": "Jianan Zhang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ab", "name": "Liangyu Chen", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ac", "name": "Quyu Kong", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ad", "name": "Chenglin Cai", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ae", "name": "Chen Liu", "hidden": false}, {"_id": "6951e7d4746a34b55dd548af", "name": "Yue Wang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548b0", "name": "Jingren Zhou", "hidden": false}, {"_id": "6951e7d4746a34b55dd548b1", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-26T14:51:52.000Z", "submittedOnDailyAt": "2025-12-29T00:01:11.405Z", "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.", "upvotes": 19, "discussionId": "6951e7d4746a34b55dd548b2", "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>MAI-UI\u662f\u4e00\u79cd\u65b0\u578b\u7684GUI\u667a\u80fd\u4ee3\u7406\uff0c\u65e8\u5728\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002</li>\n    <li>\u5b83\u6709\u4e0d\u540c\u89c4\u6a21\u7684\u7248\u672c\uff0c\u5305\u62ec2B\u30018B\u300132B\u548c235B-A22B\u3002</li>\n    <li>MAI-UI\u89e3\u51b3\u4e86\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7528\u6237\u4ea4\u4e92\u4e0d\u8db3\u3001\u4ec5\u9650UI\u64cd\u4f5c\u3001\u7f3a\u4e4f\u5b9e\u7528\u7684\u90e8\u7f72\u67b6\u6784\uff0c\u4ee5\u53ca\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8106\u5f31\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMAI-UI\u8fbe\u5230\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u73b0\u6709\u6a21\u578b\u3002</li>\n    <li>\u5176\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u663e\u793a\uff0c\u901a\u8fc7\u6269\u5c55\u5e76\u884c\u73af\u5883\u548c\u589e\u52a0\u6b65\u9aa4\u9884\u7b97\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u672c\u5730\u4e91\u534f\u4f5c\u7cfb\u7edf\u63d0\u9ad8\u4e86\u8bbe\u5907\u6027\u80fd\u548c\u7528\u6237\u9690\u79c1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MAI-UI is a new type of GUI agent that aims to improve how people interact with computers.</li>\n    <li>It includes various sizes of agents and tackles four main challenges for better usability.</li>\n    <li>MAI-UI uses a smart system to enhance data collection and improve how tasks are managed between devices and the cloud.</li>\n    <li>It achieves impressive performance on several benchmarks, surpassing previous models in GUI tasks and mobile navigation.</li>\n    <li>The system enhances on-device performance and user privacy, while reducing the need for cloud resources.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:52.000Z", "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents", "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22047.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 196}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u6b3e\u517c\u5177\u9ad8\u6548\u8ba1\u7b97\u548c\u51fa\u8272\u63a8\u7406\u8868\u73b0\u7684\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86DeepSeek\u7a00\u758f\u6ce8\u610f\u529b\uff08DSA\uff09\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u4e14\u5176\u9ad8\u8ba1\u7b97\u7248\u672c\u8d85\u8d8a\u4e86GPT-5\uff0c\u63a8\u7406\u80fd\u529b\u4e0eGemini-3.0-Pro\u76f8\u5f53\u3002</li>\n    <li>\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\uff08IMO\uff09\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\uff08IOI\uff09\u4e2d\uff0cDeepSeek-V3.2\u8868\u73b0\u4f18\u5f02\uff0c\u83b7\u5f97\u91d1\u724c\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u667a\u80fd\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u6027\u5730\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e86\u5728\u590d\u6742\u4e92\u52a8\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that balances efficiency and strong performance in reasoning tasks.</li>\n    <li>It features a new attention mechanism called DeepSeek Sparse Attention (DSA), which reduces computing needs while keeping high performance for long contexts.</li>\n    <li>The model includes a scalable reinforcement learning framework that allows it to match or exceed the performance of other advanced models like GPT-5 and Gemini-3.0-Pro.</li>\n    <li>DeepSeek-V3.2-Speciale, a more powerful version, outperforms GPT-5 and has shown excellent reasoning skills in major competitions like the International Mathematical Olympiad.</li>\n    <li>It also has a new system for creating large amounts of training data, which helps the model learn better in complex environments.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u9700\u8981\u66f4\u52a0\u53ef\u9760\u548c\u53ef\u6269\u5c55\u3002</li>\n    <li>\u5f53\u524d\u7684\u6570\u636e\u51c6\u5907\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u89c4\u8303\u5316\uff0c\u5f71\u54cd\u4e86\u53ef\u91cd\u590d\u6027\u548c\u6a21\u578b\u6570\u636e\u751f\u6210\u652f\u6301\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u9886\u57df\u901a\u7528\u7684\u7ba1\u9053\uff0c\u6db5\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u3002</li>\n    <li>DataFlow\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u6587\u672c\u4efb\u52a1\u4e0a\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The demand for high-quality data in Large Language Models (LLMs) is increasing, but current data preparation methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that offers a structured way to prepare data for LLMs with reusable components and a user-friendly construction API.</li>\n    <li>It includes nearly 200 reusable tools and covers various areas like text, math, and code processing.</li>\n    <li>DataFlow-Agent helps users create data pipelines from natural language instructions, making it easier to use.</li>\n    <li>DataFlow shows better performance in LLM tasks compared to traditional datasets, improving accuracy significantly in several benchmarks.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u548c\u65e0\u9650\u957f\u5ea6\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u7684\u521b\u65b0\u5305\u62ec\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\uff0c\u5b83\u5728\u591a\u4e2aGPU\u4e0a\u8fdb\u884c\u53bb\u566a\u6b65\u9aa4\u7684\u6d41\u6c34\u7ebf\u5904\u7406\uff0c\u6253\u7834\u4e86\u81ea\u56de\u5f52\u74f6\u9888\uff0c\u786e\u4fdd\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u63d0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u6821\u51c6\u5916\u89c2\u6765\u4fdd\u6301\u5e8f\u5217\u4fdd\u771f\u5ea6\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u9996\u6b21\u5728\u8fd9\u4e2a\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating realistic avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>The system uses a method called Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by using multiple GPUs, allowing for low-latency streaming.</li>\n    <li>It includes a feature called the Rolling Sink Frame Mechanism (RSFM) to improve the consistency and quality of the avatars over time.</li>\n    <li>Live Avatar can generate avatars at 20 frames per second on 5 GPUs, making it one of the fastest systems available for this purpose.</li>\n    <li>This work sets a new standard for using advanced models in long video applications, making real-time avatar generation practical and high-quality.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u4f01\u4e1a\u4e2d\u7684\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u67b6\u6784\u4e0a\u8fdb\u884c\u591a\u5c42\u6b21\u7684SQL\u7ba1\u9053\u8bbe\u8ba1\u548c\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u89e3\u51b3\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u8981\u6c42\u8fdb\u884c\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u667a\u80fd\u7cfb\u7edf\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u5de5\u7a0b\u548c\u5206\u6790\u80fd\u529b\u7684\u4e0d\u8db3\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>DAComp is a benchmark that includes 210 tasks reflecting real-world data workflows in businesses.</li>\n  <li>It includes two main types of tasks: data engineering (DE) and data analysis (DA).</li>\n  <li>DE tasks involve creating and improving complex SQL data pipelines, while DA tasks focus on solving open-ended business problems.</li>\n  <li>Test results show that even advanced systems struggle with these tasks, especially in data engineering where success rates are below 20%.</li>\n  <li>DAComp aims to identify weaknesses in current data processing systems and help improve the development of autonomous data agents.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u5206\u5f00\u7684\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u5e76\u5c06\u5176\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\uff0c\u589e\u5f3a\u4e86\u5176\u89c6\u9891\u521b\u4f5c\u80fd\u529b\u3002</li>\n    <li>Kling-Omni \u4e0d\u4ec5\u662f\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\uff0c\u8fd8\u662f\u5411\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u3001\u751f\u6210\u548c\u4e0e\u590d\u6742\u4e16\u754c\u4e92\u52a8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various visual and text inputs.</li>\n    <li>It combines video generation, editing, and smart reasoning into one unified framework.</li>\n    <li>The system accepts different types of inputs, like text, images, and existing videos, to produce cohesive video content.</li>\n    <li>Kling-Omni is built on a strong data foundation and uses advanced training methods for better performance.</li>\n    <li>It shows excellent results in generating videos, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u5b9e\u73b0\u7cbe\u786e\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u63a7\u5236\u7c97\u7cd9\u548c\u53ef\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u5141\u8bb8\u5bf9\u573a\u666f\u8fdb\u884c\u7ec6\u81f4\u7684\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u751f\u6210\u7684480p\u30015\u79d2\u89c6\u9891\u5728\u8fd0\u52a8\u53ef\u63a7\u6027\u4e0a\u4e0e\u5546\u4e1a\u8f6f\u4ef6Kling 1.5 Pro\u7684Motion Brush\u76f8\u5ab2\u7f8e\u3002</li>\n    <li>\u4e3a\u4e86\u8bc4\u4f30\u6027\u80fd\uff0cWan-Move\u8fd8\u8bbe\u8ba1\u4e86MoveBench\u57fa\u51c6\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u5185\u5bb9\u7c7b\u522b\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u6807\u6ce8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over how objects move in videos.</li>\n    <li>By using dense point trajectories, it can create detailed motion for each scene element.</li>\n    <li>Wan-Move is easily adaptable to existing image-to-video models without needing major changes.</li>\n    <li>It also includes a benchmark called MoveBench for testing video quality and motion control, with public access to its resources.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01816", "authors": [{"_id": "692e5c0537312eaa83fd87b8", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:43.760Z", "hidden": false}, {"_id": "692e5c0537312eaa83fd87b9", "name": "Siyuan Li", "hidden": false}, {"_id": "692e5c0537312eaa83fd87ba", "name": "Conghui He", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bb", "name": "Lijun Wu", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bc", "user": {"_id": "64be296a46cc3cdfbb057f7e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg", "isPro": false, "fullname": "Cheng Tan", "user": "chengtan9907", "type": "user"}, "name": "Cheng Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:41.755Z", "hidden": false}], "publishedAt": "2025-12-01T15:52:31.000Z", "submittedOnDailyAt": "2025-12-02T01:31:46.625Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "submittedOnDailyBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "upvotes": 87, "discussionId": "692e5c0537312eaa83fd87bd", "projectPage": "https://opendatalab-raiser.github.io/Envision/", "githubRepo": "https://github.com/opendatalab-raiser/Envision", "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.", "ai_keywords": ["multimodal models", "text-to-image (T2I)", "causal event progression", "spatiotemporal causality", "Envision-a", "Envision-Score", "multi-dimensional consistency", "physicality", "aesthetics", "causal narrative coherence", "spatiotemporal consistency", "multi-frame reasoning", "dynamic world modeling"], "githubStars": 27, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u65e8\u5728\u8d85\u8d8a\u5355\u4e00\u6a21\u6001\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u6587\u5b57\u5230\u56fe\u50cf\u7684\u4efb\u52a1\u6765\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u5355\u56fe\u50cf\u751f\u6210\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u4e8e\u9759\u6001\u6a21\u5f0f\u5339\u914d\uff0c\u9650\u5236\u4e86\u5bf9\u52a8\u6001\u8fc7\u7a0b\u7684\u5efa\u6a21\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201cEnvision\u201d\u57fa\u51c6\uff0c\u9488\u5bf9\u94fe\u5f0f\u6587\u672c\u5230\u591a\u56fe\u50cf\u751f\u6210\u7684\u56e0\u679c\u4e8b\u4ef6\u8fdb\u7a0b\u8fdb\u884c\u8bc4\u4f30\u3002</li>\n    <li>\u8be5\u57fa\u51c6\u5305\u62ec1000\u4e2a\u56db\u9636\u6bb5\u63d0\u793a\uff0c\u6db5\u76d6\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\uff0c\u5e76\u5f15\u5165\u4e86Envision-Score\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u4e8b\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u4e13\u7528\u7684T2I\u6a21\u578b\uff0c\u4f46\u4ecd\u9762\u4e34\u65f6\u7a7a\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal models are designed to improve understanding and creation by combining different types of data, like text and images.</li>\n    <li>Current models struggle because they are trained mostly on single images, which makes them less capable of handling events that happen over time.</li>\n    <li>The authors propose a new benchmark called Envision, which focuses on generating sequences of images based on text prompts related to real-world scenarios.</li>\n    <li>Envision includes 1,000 prompts across various fields and introduces a new evaluation metric called Envision-Score to measure how well models understand real-world knowledge and follow temporal relationships.</li>\n    <li>Tests show that while specialized models are good at creating visually appealing images, unified models that combine different modalities perform better in maintaining narrative consistency, but still face challenges in modeling events over time.</li>\n</ul>"}, "publishedAt": "2025-12-01T10:52:31.000Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png", "numComments": 4, "submittedBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "fullname": "Juanxi Tian", "name": "Juanxi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13}, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u5728\u89c6\u89c9\u4e0a\u771f\u5b9e\u4e14\u65f6\u95f4\u4e0a\u8fde\u8d2f\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u6355\u6349\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u7684\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u6307\u6807\uff08\u5982FVD\uff09\u6ce8\u91cd\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u7565\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\u6846\u67b6\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002</li>\n    <li>\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u548c\u957f\u65f6\u95f4\u7a7a\u95f4\u89c4\u5212\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u5b58\u5728\u5bf9\u611f\u77e5\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\u7b49\u5173\u952e\u5c40\u9650\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic content, but their ability to simulate the real world depends on capturing various physical and logical rules.</li>\n    <li>Current evaluation methods focus on visual quality and miss reasoning errors, like ignoring physics or logical consistency.</li>\n    <li>MMGR is a new evaluation framework that tests models based on five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR assesses performance in three areas: Abstract Reasoning, Embodied Navigation, and Physical Commonsense, using detailed metrics for video and image generation.</li>\n    <li>Benchmarking reveals that while models perform well in some areas, they struggle significantly with abstract reasoning and long-term planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.01374", "authors": [{"_id": "692e6bf937312eaa83fd8890", "user": {"_id": "610b70452719facd4ea85e28", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg", "isPro": false, "fullname": "Chujie Zheng", "user": "chujiezheng", "type": "user"}, "name": "Chujie Zheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:27.206Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8891", "name": "Kai Dang", "hidden": false}, {"_id": "692e6bf937312eaa83fd8892", "name": "Bowen Yu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8893", "name": "Mingze Li", "hidden": false}, {"_id": "692e6bf937312eaa83fd8894", "user": {"_id": "6278bd42541f3d2dfa77ea70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg", "isPro": false, "fullname": "Huiqiang Jiang", "user": "iofu728", "type": "user"}, "name": "Huiqiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:41.367Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8895", "name": "Junrong Lin", "hidden": false}, {"_id": "692e6bf937312eaa83fd8896", "name": "Yuqiong Liu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8897", "user": {"_id": "62088594a5943c8a8fc94560", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png", "isPro": false, "fullname": "An Yang", "user": "yangapku", "type": "user"}, "name": "An Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:25.208Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8898", "name": "Jingren Zhou", "hidden": false}, {"_id": "692e6bf937312eaa83fd8899", "name": "Junyang Lin", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "publishedAt": "2025-12-01T07:45:39.000Z", "submittedOnDailyAt": "2025-12-02T02:47:49.367Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "submittedOnDailyBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "isPro": false, "fullname": "Bowen Yu", "user": "Tigerph", "type": "user"}, "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "upvotes": 78, "discussionId": "692e6bfa37312eaa83fd889a", "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.", "ai_keywords": ["reinforcement learning", "large language models", "sequence-level reward", "token-level objective", "policy gradient methods", "REINFORCE", "first-order approximation", "training-inference discrepancy", "policy staleness", "importance sampling correction", "clipping", "Routing Replay", "Mixture-of-Experts", "on-policy training", "off-policy updates"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\uff0c\u8bf4\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5982\u4f55\u4f18\u5316\u5e8f\u5217\u7ea7\u5956\u52b1\u3002</li>\n    <li>\u5f3a\u8c03\u4e86\u51cf\u5c11\u8bad\u7ec3\u4e0e\u63a8\u7406\u5dee\u5f02\u53ca\u7b56\u7565\u8fc7\u65f6\u6027\u7684\u91cd\u8981\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u4fee\u6b63\u7684\u57fa\u672c\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5177\u6709\u6700\u9ad8\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u4e00\u65e6\u8bad\u7ec3\u7a33\u5b9a\uff0c\u5ef6\u957f\u4f18\u5316\u65f6\u95f4\u53ef\u4ee5\u5f97\u5230\u76f8\u4f3c\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u65e0\u8bba\u521d\u59cb\u8bbe\u7f6e\u5982\u4f55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces a new way to improve reinforcement learning (RL) using large language models.</li>\n    <li>It explains how to optimize rewards at a sequence level using a token-level approach, especially in policy gradient methods like REINFORCE.</li>\n    <li>Training and inference must be closely aligned, and the policy should not be outdated for the surrogate method to work well.</li>\n    <li>Experiments show that using importance sampling correction improves training stability, especially in on-policy training.</li>\n    <li>When using off-policy updates, combining clipping and Routing Replay is necessary to reduce instability in training.</li>\n</ul>"}, "publishedAt": "2025-12-01T02:45:39.000Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png", "numComments": 2, "submittedBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "fullname": "Bowen Yu", "name": "Tigerph", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u4e00\u822c\u667a\u80fd\uff08SGI\uff09\u662f\u6307\u5728\u79d1\u5b66\u9886\u57df\u81ea\u4e3b\u6784\u601d\u3001\u7814\u7a76\u548c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u8df5\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\uff1a\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72/\u6e7f\u5b9e\u9a8c\u548c\u5b9e\u9a8c\u63a8\u7406\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u65e8\u5728\u8bc4\u4f30\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff0c\u5982\u6df1\u5ea6\u7814\u7a76\u7684\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5b9e\u9a8c\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\u7b49\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u68c0\u7d22\u589e\u5f3a\u65b0\u9896\u6027\u5956\u52b1\uff0c\u4ece\u800c\u63d0\u9ad8\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability to independently think and work across different scientific fields.</li>\n    <li>The authors propose a definition of SGI based on a model that includes steps like thinking, creating ideas, taking action, and observing results.</li>\n    <li>They developed SGI-Bench, a benchmark with over 1,000 scientific tasks to evaluate advanced AI models.</li>\n    <li>Results showed that AI struggles with deep research accuracy, generating feasible ideas, and executing experiments correctly.</li>\n    <li>The authors also introduced a new method to improve AI's ability to generate novel hypotheses during evaluation without needing a reference answer.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 30, 2025";
window.trendingPapers = {
    "today": [{"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u7531\u4e8e\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u63a5\u8fd1\u67af\u7aed\uff0c\u9884\u8bad\u7ec3\u4ece\u66f4\u591a\u7684\u6807\u8bb0\u8f6c\u5411\u66f4\u597d\u7684\u6807\u8bb0\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9759\u6001\u8fc7\u6ee4\u5668\uff0c\u8981\u4e48\u4f7f\u7528\u57fa\u4e8e\u539f\u59cb\u68af\u5ea6\u7684\u52a8\u6001\u6807\u51c6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86OPUS\uff0c\u4e00\u4e2a\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u73b0\u4ee3\u4f18\u5316\u5668\u5b9a\u4e49\u6548\u7528\u3002</li>\n    <li>OPUS\u5728\u591a\u4e2a\u8bed\u6599\u5e93\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u884c\u4e1a\u57fa\u51c6\u3002</li>\n    <li>\u5728\u7279\u5b9a\u9886\u57df\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0cOPUS\u663e\u793a\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>As high-quality public text becomes scarce, data selection for training models is shifting focus from quantity to quality.</li>\n    <li>OPUS is a new method that selects training data based on how well it helps the optimizer improve the model.</li>\n    <li>It uses advanced techniques for efficiency and variety in data selection, adding only a small extra computational cost.</li>\n    <li>OPUS has shown better results than existing methods and even outperforms training with much larger datasets.</li>\n    <li>In specific cases, OPUS can achieve strong performance using significantly less training data.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u901a\u8fc7\u611f\u77e5\u754c\u9762\u548c\u6267\u884c\u52a8\u4f5c\u4e0e\u73af\u5883\u4e92\u52a8\u3002</li>\n    <li>Code2World\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u80fd\u901a\u8fc7\u751f\u6210\u53ef\u6e32\u67d3\u4ee3\u7801\u6765\u6a21\u62df\u4e0b\u4e00\u4e2a\u89c6\u89c9\u72b6\u6001\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc78\u4e07\u5bf9\u9ad8\u8d28\u91cf\u5c4f\u5e55-\u52a8\u4f5c\u5bf9\u7684AndroidCode\u3002</li>\n    <li>Code2World\u5728\u4e0b\u4e00\u6b65\u7528\u6237\u754c\u9762\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86GPT-5\u548cGemini-3-Pro-Image\u3002</li>\n    <li>Code2World\u663e\u8457\u63d0\u9ad8\u4e86\u5728AndroidWorld\u5bfc\u822a\u4e2d\u7684\u6210\u529f\u7387\uff0c\u63d0\u5347\u4e869.5%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Code2World is a new tool that helps virtual agents predict the next visual state of user interfaces by generating code.</li>\n    <li>It creates high-quality training data from Android GUI interactions, resulting in over 80,000 screen-action pairs.</li>\n    <li>The system uses special learning techniques to improve the accuracy of visual predictions and maintain action consistency.</li>\n    <li>Experiments show Code2World performs better than other models like GPT-5 and Gemini-3-Pro-Image in predicting user interface changes.</li>\n    <li>It also improves navigation success rates in Android applications, enhancing performance by 9.5% compared to existing models.</li>\n</ul>"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09082", "authors": [{"_id": "698bea506052d3bed96309cb", "name": "Veuns-Team", "hidden": false}, {"_id": "698bea506052d3bed96309cd", "name": "Changlong Gao", "hidden": false}, {"_id": "698bea506052d3bed96309ce", "user": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "name": "Zhangxuan Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:14.456Z", "hidden": false}, {"_id": "698bea506052d3bed96309cf", "name": "Yulin Liu", "hidden": false}, {"_id": "698bea506052d3bed96309d0", "name": "Xinyu Qiu", "hidden": false}, {"_id": "698bea506052d3bed96309d1", "name": "Shuheng Shen", "hidden": false}, {"_id": "698bea506052d3bed96309d2", "name": "Yue Wen", "hidden": false}, {"_id": "698bea506052d3bed96309d3", "name": "Tianyu Xia", "hidden": false}, {"_id": "698bea506052d3bed96309d4", "name": "Zhenyu Xu", "hidden": false}, {"_id": "698bea506052d3bed96309d5", "user": {"_id": "64cb238576200ec80fe988f8", "avatarUrl": "/avatars/42c48710c7881c9dfbcc075fec3cb600.svg", "isPro": false, "fullname": "zeus", "user": "zengw", "type": "user"}, "name": "Zhengwen Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:24:43.235Z", "hidden": false}, {"_id": "698bea506052d3bed96309d6", "user": {"_id": "654c9dac09dd7ef524a0be1e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png", "isPro": false, "fullname": "beitongzhou", "user": "syorami", "type": "user"}, "name": "Beitong Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:11.859Z", "hidden": false}, {"_id": "698bea506052d3bed96309d7", "name": "Xingran Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309d8", "name": "Weizhi Chen", "hidden": false}, {"_id": "698bea506052d3bed96309d9", "name": "Sunhao Dai", "hidden": false}, {"_id": "698bea506052d3bed96309da", "name": "Jingya Dou", "hidden": false}, {"_id": "698bea506052d3bed96309db", "name": "Yichen Gong", "hidden": false}, {"_id": "698bea506052d3bed96309dc", "name": "Yuan Guo", "hidden": false}, {"_id": "698bea506052d3bed96309dd", "name": "Zhenlin Guo", "hidden": false}, {"_id": "698bea506052d3bed96309de", "user": {"_id": "65e0763a9299e96ee674876e", "avatarUrl": "/avatars/0ea342c9f72fa3b8a8f634559d094907.svg", "isPro": false, "fullname": "fengdian", "user": "fengrudian", "type": "user"}, "name": "Feng Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:04.463Z", "hidden": false}, {"_id": "698bea506052d3bed96309df", "name": "Qian Li", "hidden": false}, {"_id": "698bea506052d3bed96309e0", "name": "Jinzhen Lin", "hidden": false}, {"_id": "698bea506052d3bed96309e1", "name": "Yuqi Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309e2", "name": "Linchao Zhu", "hidden": false}, {"_id": "698bea506052d3bed96309e3", "name": "Liang Chen", "hidden": false}, {"_id": "698bea506052d3bed96309e4", "name": "Zhenyu Guo", "hidden": false}, {"_id": "698bea506052d3bed96309e5", "name": "Changhua Meng", "hidden": false}, {"_id": "698bea506052d3bed96309e6", "name": "Weiqiang Wang", "hidden": false}], "publishedAt": "2026-02-09T18:43:40.000Z", "submittedOnDailyAt": "2026-02-11T00:10:55.649Z", "title": "UI-Venus-1.5 Technical Report", "submittedOnDailyBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "upvotes": 143, "discussionId": "698bea516052d3bed96309e7", "projectPage": "https://ui-venus.github.io/UI-Venus-1.5/", "githubRepo": "https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5", "githubRepoAddedBy": "user", "ai_summary": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.", "ai_keywords": ["GUI agents", "Mid-Training stage", "Online Reinforcement Learning", "full-trajectory rollouts", "Model Merging", "dense variants", "mixture-of-experts variant"], "githubStars": 708, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>UI-Venus-1.5\u662f\u4e00\u4e2a\u5f3a\u5927\u7684GUI\u4ee3\u7406\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u6570\u5b57\u73af\u5883\u4e2d\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u6709\u4e24\u4e2a\u5bc6\u96c6\u578b\u53d8\u4f53\uff082B\u548c8B\uff09\u548c\u4e00\u4e2a\u4e13\u5bb6\u6df7\u5408\u53d8\u4f53\uff0830B-A3B\uff09\uff0c\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\u8fdb\u5c55\uff0c\u5305\u62ec\u5168\u9762\u7684\u4e2d\u671f\u8bad\u7ec3\u3001\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u5408\u5e76\u3002</li>\n    <li>UI-Venus-1.5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u7248\u672c\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6210\u7ee9\u3002</li>\n    <li>\u5b83\u5728\u4e2d\u56fd\u7684\u624b\u673a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u7528\u6237\u6307\u4ee4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UI-Venus-1.5 is a new type of GUI Agent designed to automate tasks in digital environments effectively.</li>\n    <li>It includes three model types to suit different use cases: two dense models and one mixture-of-experts model.</li>\n    <li>The model improves on its predecessor with three advancements: a Mid-Training stage using lots of data, Online Reinforcement Learning for better navigation, and a unified design merging different models.</li>\n    <li>UI-Venus-1.5 achieves record performance on several benchmarks, outperforming previous models significantly.</li>\n    <li>It also shows strong ability to navigate and follow instructions in various Chinese mobile apps.</li>\n</ul>"}, "publishedAt": "2026-02-09T13:43:40.000Z", "title": "UI-Venus-1.5 Technical Report", "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png", "numComments": 2, "submittedBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "fullname": "Zhangxuan Gu", "name": "zhangxgu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10063", "authors": [{"_id": "698bf4ef6052d3bed9630a96", "user": {"_id": "6895e7f146763431aea25ca4", "avatarUrl": "/avatars/52e550c3f7e8da2e31b63413e2e71e6c.svg", "isPro": false, "fullname": "Tianyi Jiang", "user": "LumosJiang", "type": "user"}, "name": "Tianyi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:33.352Z", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a97", "name": "Arctanx An", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a98", "name": "Hengyi Feng", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a99", "name": "Naixin Zhai", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9a", "name": "Haodong Li", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9b", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T12:34:26.745Z", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9c", "name": "Jiahui Liu", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9d", "name": "Hanwen Du", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9e", "name": "Shuo Zhang", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9f", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:35.722Z", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa0", "name": "Jie Huang", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa1", "name": "Yuhua Li", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa2", "name": "Yongxin Ni", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa3", "name": "Huacan Wang", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa4", "name": "Ronghao Chen", "hidden": false}], "publishedAt": "2026-02-10T18:31:47.000Z", "submittedOnDailyAt": "2026-02-11T00:51:58.024Z", "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.", "upvotes": 62, "discussionId": "698bf4f06052d3bed9630aa5", "githubRepo": "https://github.com/QuantaAlpha/chain-of-mindset", "githubRepoAddedBy": "user", "ai_summary": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.", "ai_keywords": ["Chain of Mindset", "CoM", "agentic framework", "step-level adaptive mindset orchestration", "Spatial mindset", "Convergent mindset", "Divergent mindset", "Algorithmic mindset", "Meta-Agent", "bidirectional Context Gate", "reasoning efficiency", "large language models"], "githubStars": 18, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u89e3\u51b3\u95ee\u9898\u65f6\u4f1a\u7ed3\u5408\u591a\u79cd\u601d\u7ef4\u65b9\u5f0f\uff0c\u800c\u4e0d\u662f\u53ea\u4f9d\u8d56\u5355\u4e00\u601d\u7ef4\u6a21\u5f0f\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5e38\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u601d\u7ef4\u65b9\u5f0f\uff0c\u5ffd\u89c6\u4e86\u89e3\u51b3\u95ee\u9898\u7684\u4e0d\u540c\u9636\u6bb5\u9700\u8981\u4e0d\u540c\u7684\u601d\u7ef4\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u94fe\u201d\uff08CoM\uff09\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6839\u636e\u9700\u8981\u52a8\u6001\u8c03\u6574\u601d\u7ef4\u65b9\u5f0f\u3002</li>\n    <li>CoM\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u56db\u79cd\u4e0d\u540c\u7684\u601d\u7ef4\u6a21\u5f0f\uff1a\u7a7a\u95f4\u601d\u7ef4\u3001\u6536\u655b\u601d\u7ef4\u3001\u53d1\u6563\u601d\u7ef4\u548c\u7b97\u6cd5\u601d\u7ef4\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cCoM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u6700\u5f3a\u57fa\u51c6\u6a21\u578b\uff0c\u4e14\u63a8\u7406\u6548\u7387\u5f97\u5230\u4e86\u5e73\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Human problem-solving involves using multiple ways of thinking, not just one approach.</li>\n    <li>Current reasoning methods for language models often use the same thinking style for all steps, which limits their intelligence.</li>\n    <li>The proposed solution, Chain of Mindset (CoM), allows for flexible thinking by using different modes for different stages of problem-solving.</li>\n    <li>CoM uses four different mindsets: Spatial, Convergent, Divergent, and Algorithmic, and adapts them based on the situation.</li>\n    <li>Tests show that CoM performs better than existing methods in various areas, improving accuracy significantly while being efficient.</li>\n</ul>"}, "publishedAt": "2026-02-10T13:31:47.000Z", "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes", "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10063.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08234", "authors": [{"_id": "698aba731b2dc6b37d61b0e4", "user": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "isPro": false, "fullname": "Peng Xia", "user": "richardxp888", "type": "user"}, "name": "Peng Xia", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:18:58.896Z", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e5", "name": "Jianwen Chen", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e6", "name": "Hanyang Wang", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e7", "name": "Jiaqi Liu", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e8", "name": "Kaide Zeng", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e9", "user": {"_id": "63234809155b0e2c44f354d6", "avatarUrl": "/avatars/60d38f8f0e12363f3f5e0388e635d7b6.svg", "isPro": false, "fullname": "Yu Wang", "user": "YuWangX", "type": "user"}, "name": "Yu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:18:56.703Z", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ea", "name": "Siwei Han", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0eb", "name": "Yiyang Zhou", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ec", "name": "Xujiang Zhao", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ed", "name": "Haifeng Chen", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ee", "name": "Zeyu Zheng", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ef", "name": "Cihang Xie", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0f0", "name": "Huaxiu Yao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"], "publishedAt": "2026-02-09T03:17:17.000Z", "submittedOnDailyAt": "2026-02-11T00:14:23.139Z", "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "submittedOnDailyBy": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "isPro": false, "fullname": "Peng Xia", "user": "richardxp888", "type": "user"}, "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "upvotes": 56, "discussionId": "698aba731b2dc6b37d61b0f1", "githubRepo": "https://github.com/aiming-lab/SkillRL", "githubRepoAddedBy": "user", "ai_summary": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.", "ai_keywords": ["large language model agents", "reinforcement learning", "skill discovery", "recursive evolution", "skill library", "SkillBank", "experience-based distillation", "adaptive retrieval strategy", "policy improvement", "token footprint"], "githubStars": 140, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u65e0\u6cd5\u4ece\u8fc7\u53bb\u7684\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u5b58\u50a8\u5197\u4f59\u4e14\u5608\u6742\u7684\u539f\u59cb\u6570\u636e\uff0c\u65e0\u6cd5\u63d0\u53d6\u53ef\u91cd\u7528\u7684\u884c\u4e3a\u6a21\u5f0f\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SkillRL\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u6280\u80fd\u53d1\u73b0\u548c\u9012\u5f52\u8fdb\u5316\uff0c\u6539\u5584\u7b56\u7565\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7ecf\u9a8c\u84b8\u998f\u673a\u5236\uff0c\u5efa\u7acb\u4e86\u5206\u5c42\u6280\u80fd\u5e93SkillBank\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSkillRL\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc715.3%\uff0c\u5e76\u5728\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model (LLM) agents excel in complex tasks but usually do not learn from past experiences.</li>\n    <li>Current memory methods store unnecessary and noisy data, hindering the learning of useful patterns.</li>\n    <li>The paper introduces SkillRL, a framework that helps agents improve by discovering skills and evolving their abilities.</li>\n    <li>Key features include a skill library (SkillBank), a strategy for retrieving helpful techniques, and a method for skills to evolve with the agent's learning.</li>\n    <li>SkillRL shows better performance in various tasks, improving results by over 15% compared to other methods, even as tasks become more complex.</li>\n</ul>"}, "publishedAt": "2026-02-08T22:17:17.000Z", "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08234.png", "numComments": 1, "submittedBy": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "fullname": "Peng Xia", "name": "richardxp888", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09443", "authors": [{"_id": "698c019f6052d3bed9630b1c", "name": "Yun Luo", "hidden": false}, {"_id": "698c019f6052d3bed9630b1d", "name": "Futing Wang", "hidden": false}, {"_id": "698c019f6052d3bed9630b1e", "name": "Qianjia Cheng", "hidden": false}, {"_id": "698c019f6052d3bed9630b1f", "name": "Fangchen Yu", "hidden": false}, {"_id": "698c019f6052d3bed9630b20", "name": "Haodi Lei", "hidden": false}, {"_id": "698c019f6052d3bed9630b21", "name": "Jianhao Yan", "hidden": false}, {"_id": "698c019f6052d3bed9630b22", "name": "Chenxi Li", "hidden": false}, {"_id": "698c019f6052d3bed9630b23", "name": "Jiacheng Chen", "hidden": false}, {"_id": "698c019f6052d3bed9630b24", "name": "Yufeng Zhao", "hidden": false}, {"_id": "698c019f6052d3bed9630b25", "name": "Haiyuan Wan", "hidden": false}, {"_id": "698c019f6052d3bed9630b26", "name": "Yuchen Zhang", "hidden": false}, {"_id": "698c019f6052d3bed9630b27", "name": "Shenghe Zheng", "hidden": false}, {"_id": "698c019f6052d3bed9630b28", "name": "Junchi Yao", "hidden": false}, {"_id": "698c019f6052d3bed9630b29", "name": "Qingyang Zhang", "hidden": false}, {"_id": "698c019f6052d3bed9630b2a", "name": "Haonan He", "hidden": false}, {"_id": "698c019f6052d3bed9630b2b", "name": "Wenxuan Zeng", "hidden": false}, {"_id": "698c019f6052d3bed9630b2c", "name": "Li Sheng", "hidden": false}, {"_id": "698c019f6052d3bed9630b2d", "name": "Chengxing Xie", "hidden": false}, {"_id": "698c019f6052d3bed9630b2e", "name": "Yuxin Zuo", "hidden": false}, {"_id": "698c019f6052d3bed9630b2f", "name": "Yizhuo Li", "hidden": false}, {"_id": "698c019f6052d3bed9630b30", "name": "Yulun Wu", "hidden": false}, {"_id": "698c019f6052d3bed9630b31", "name": "Rui Huang", "hidden": false}, {"_id": "698c019f6052d3bed9630b32", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "698c019f6052d3bed9630b33", "name": "Kai Chen", "hidden": false}, {"_id": "698c019f6052d3bed9630b34", "name": "Yu Qiao", "hidden": false}, {"_id": "698c019f6052d3bed9630b35", "name": "Lei Bai", "hidden": false}, {"_id": "698c019f6052d3bed9630b36", "name": "Yu Cheng", "hidden": false}, {"_id": "698c019f6052d3bed9630b37", "name": "Ning Ding", "hidden": false}, {"_id": "698c019f6052d3bed9630b38", "name": "Bowen Zhou", "hidden": false}, {"_id": "698c019f6052d3bed9630b39", "name": "Peng Ye", "hidden": false}, {"_id": "698c019f6052d3bed9630b3a", "name": "Ganqu Cui", "hidden": false}], "publishedAt": "2026-02-10T06:28:08.000Z", "submittedOnDailyAt": "2026-02-11T04:00:48.041Z", "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads", "submittedOnDailyBy": {"_id": "6086838b19137b3a6ba760e7", "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg", "isPro": false, "fullname": "Jianhao Yan", "user": "Elliott", "type": "user"}, "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.", "upvotes": 51, "discussionId": "698c019f6052d3bed9630b3b", "projectPage": "https://prime-rl.github.io/P1-VL", "githubRepo": "https://github.com/PRIME-RL/P1-VL", "githubRepoAddedBy": "user", "ai_summary": "Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.", "ai_keywords": ["vision-language models", "curriculum reinforcement learning", "agentic augmentation", "multimodal perception", "scientific reasoning", "physical consistency", "HiPhO benchmark", "P1-VL-235B-A22B", "Gemini-3-Pro"], "githubStars": 13, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u4ece\u7b26\u53f7\u64cd\u4f5c\u8f6c\u5411\u79d1\u5b66\u63a8\u7406\uff0c\u7269\u7406\u5b66\u662f\u5173\u952e\u6d4b\u8bd5\u9886\u57df\u3002</li>\n    <li>\u7269\u7406\u5b66\u8981\u6c42\u6a21\u578b\u4e0e\u5b87\u5b99\u5b9a\u5f8b\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u9700\u8981\u591a\u6a21\u6001\u611f\u77e5\u6765\u5c06\u62bd\u8c61\u903b\u8f91\u4e0e\u73b0\u5b9e\u7ed3\u5408\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86P1-VL\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\uff0c\u65e8\u5728\u63d0\u5347\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>P1-VL\u57282024-2025\u5e74HiPhO\u8003\u8bd5\u4e2d\u53d6\u5f97\u4e8612\u679a\u91d1\u724c\uff0c\u6210\u4e3a\u9996\u4e2a\u83b7\u5f97\u6b64\u6210\u5c31\u7684\u5f00\u6e90\u6a21\u578b\u3002</li>\n    <li>P1-VL\u4e0d\u4ec5\u5728\u7269\u7406\u5b66\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) need to improve from basic symbolic manipulation to advanced reasoning, especially in physics.</li>\n    <li>P1-VL is a new family of open-source models designed for better scientific reasoning by combining visual understanding with abstract logic.</li>\n    <li>This model uses advanced learning techniques to gradually increase difficulty and verify itself during problem-solving.</li>\n    <li>P1-VL-235B-A22B achieved impressive results, winning 12 gold medals in a set of physics exams and ranking second globally among open-source models.</li>\n    <li>P1-VL also excels in other STEM areas, paving the way for better machine understanding of physical laws and scientific discovery.</li>\n</ul>"}, "publishedAt": "2026-02-10T01:28:08.000Z", "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads", "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09443.png", "numComments": 1, "submittedBy": {"_id": "6086838b19137b3a6ba760e7", "avatarUrl": "/avatars/d63eea3e39b22c6e65b82c28192696f1.svg", "fullname": "Jianhao Yan", "name": "Elliott", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2602.10090", "authors": [{"_id": "698bf6fe6052d3bed9630ac0", "name": "Zhaoyang Wang", "hidden": false}, {"_id": "698bf6fe6052d3bed9630ac1", "name": "Canwen Xu", "hidden": false}, {"_id": "698bf6fe6052d3bed9630ac2", "name": "Boyi Liu", "hidden": false}, {"_id": "698bf6fe6052d3bed9630ac3", "name": "Yite Wang", "hidden": false}, {"_id": "698bf6fe6052d3bed9630ac4", "name": "Siwei Han", "hidden": false}, {"_id": "698bf6fe6052d3bed9630ac5", "name": "Zhewei Yao", "hidden": false}, {"_id": "698bf6fe6052d3bed9630ac6", "name": "Huaxiu Yao", "hidden": false}, {"_id": "698bf6fe6052d3bed9630ac7", "name": "Yuxiong He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/at1Sens0OXJ4Yt8ne9kAE.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/Hs08PrK-yZHZ5FBmPLoMV.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/IIL00IFjA5UOIILbLKOY9.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/o5mHd9GtSOBL8Ni_s_J8B.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/jsnsJiwl4N10Px2LgZ9Ve.png"], "publishedAt": "2026-02-10T18:55:41.000Z", "submittedOnDailyAt": "2026-02-11T02:28:53.819Z", "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning", "submittedOnDailyBy": {"_id": "633122d3f242a8532b7a928d", "avatarUrl": "/avatars/2158ffff0882a8fb4588e273fd60dea7.svg", "isPro": true, "fullname": "Chi", "user": "ChilleD", "type": "user"}, "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.", "upvotes": 39, "discussionId": "698bf6ff6052d3bed9630ac8", "projectPage": "https://github.com/Snowflake-Labs/agent-world-model", "githubRepo": "https://github.com/Snowflake-Labs/agent-world-model", "githubRepoAddedBy": "user", "ai_summary": "Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.", "ai_keywords": ["large language model", "autonomous agents", "multi-turn interactions", "tool-use agents", "reinforcement learning", "synthetic environment generation", "code-driven environments", "database-backed state transitions", "out-of-distribution generalization"], "githubStars": 23, "organization": {"_id": "62cece4aa3a23014aca72499", "name": "Snowflake", "fullname": "Snowflake", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgent World Model (AWM)\u7684\u5408\u6210\u73af\u5883\u751f\u6210\u6d41\u7a0b\u3002</li>\n    <li>\u751f\u6210\u4e861000\u4e2a\u73af\u5883\uff0c\u6db5\u76d6\u65e5\u5e38\u573a\u666f\uff0c\u6bcf\u4e2a\u73af\u5883\u5e73\u5747\u670935\u79cd\u5de5\u5177\u53ef\u4f9b\u4f7f\u7528\u3002</li>\n    <li>\u8fd9\u4e9b\u73af\u5883\u7531\u4ee3\u7801\u9a71\u52a8\uff0c\u4f7f\u7528\u6570\u636e\u5e93\u652f\u6301\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u72b6\u6001\u53d8\u5316\u3002</li>\n    <li>\u5728\u5408\u6210\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff0c\u8bbe\u8ba1\u4e86\u53ef\u9760\u7684\u5956\u52b1\u51fd\u6570\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5408\u6210\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u4ee3\u7406\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0a\u8868\u73b0\u826f\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Researchers created a new system called Agent World Model (AWM) to generate synthetic environments for training autonomous agents.</li>\n    <li>AWM allows for the creation of 1,000 diverse environments where agents can use a variety of tools (about 35 tools on average per environment).</li>\n    <li>The environments are more reliable than those generated by large language models, leading to better agent interactions.</li>\n    <li>Using these synthetic environments, the team was able to improve reinforcement learning for agents that need to perform tasks using multiple tools.</li>\n    <li>Tests showed that training in these synthetic environments helps agents perform better in new, unseen situations.</li>\n</ul>"}, "publishedAt": "2026-02-10T13:55:41.000Z", "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning", "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/at1Sens0OXJ4Yt8ne9kAE.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/Hs08PrK-yZHZ5FBmPLoMV.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/IIL00IFjA5UOIILbLKOY9.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/o5mHd9GtSOBL8Ni_s_J8B.png", "https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/jsnsJiwl4N10Px2LgZ9Ve.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10090.png", "numComments": 1, "submittedBy": {"_id": "633122d3f242a8532b7a928d", "avatarUrl": "/avatars/2158ffff0882a8fb4588e273fd60dea7.svg", "fullname": "Chi", "name": "ChilleD", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "62cece4aa3a23014aca72499", "name": "Snowflake", "fullname": "Snowflake", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.08426", "authors": [{"_id": "698bedd06052d3bed9630a13", "user": {"_id": "6191f22d08a57f265f7f5266", "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg", "isPro": false, "fullname": "XinghaoWang", "user": "Singhoo", "type": "user"}, "name": "Xinghao Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:09.483Z", "hidden": false}, {"_id": "698bedd06052d3bed9630a14", "name": "Pengyu Wang", "hidden": false}, {"_id": "698bedd06052d3bed9630a15", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:02.192Z", "hidden": false}, {"_id": "698bedd06052d3bed9630a16", "name": "Fangxu Liu", "hidden": false}, {"_id": "698bedd06052d3bed9630a17", "name": "Jason Chu", "hidden": false}, {"_id": "698bedd06052d3bed9630a18", "name": "Kai Song", "hidden": false}, {"_id": "698bedd06052d3bed9630a19", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-02-09T09:31:06.000Z", "submittedOnDailyAt": "2026-02-11T00:29:34.413Z", "title": "Prism: Spectral-Aware Block-Sparse Attention", "submittedOnDailyBy": {"_id": "6191f22d08a57f265f7f5266", "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg", "isPro": false, "fullname": "XinghaoWang", "user": "Singhoo", "type": "user"}, "summary": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.", "upvotes": 31, "discussionId": "698bedd06052d3bed9630a1a", "projectPage": "https://efficacious-citrus-7a0.notion.site/Prism-Spectral-Aware-Block-Sparse-Attention-304d97f5df9d80318802f9cb37d18c3e", "githubRepo": "https://github.com/xinghaow99/prism", "githubRepoAddedBy": "user", "ai_summary": "Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.", "ai_keywords": ["block-sparse attention", "long-context LLM", "pre-filling", "coarse-grained attention", "Rotary Positional Embeddings", "RoPE", "mean pooling", "low-pass filter", "destructive interference", "spectral-aware approach", "energy-based temperature calibration", "block selection", "attention mechanisms"], "githubStars": 19, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u5757\u7a00\u758f\u6ce8\u610f\u529b\u53ef\u4ee5\u52a0\u901f\u957f\u4e0a\u4e0b\u6587\u7684\u9884\u586b\u5145\uff0c\u4f46\u6709\u6548\u8bc6\u522b\u76f8\u5173\u5757\u4ecd\u7136\u662f\u4e00\u4e2a\u74f6\u9888\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u7c97\u7c92\u5ea6\u6ce8\u610f\u529b\u4f30\u8ba1\u5757\u7684\u91cd\u8981\u6027\uff0c\u4f46\u9700\u8981\u6602\u8d35\u7684\u9010\u6807\u8bb0\u641c\u7d22\uff0c\u5bfc\u81f4\u9009\u62e9\u5f00\u9500\u5927\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6807\u51c6\u7c97\u7c92\u5ea6\u6ce8\u610f\u529b\u4e0e\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5bfc\u81f4\u4e86\u4e0d\u51c6\u786e\u6027\u3002</li>\n    <li>\u63d0\u51fa\u4e86Prism\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u9891\u548c\u4f4e\u9891\u5206\u652f\u5206\u89e3\u5757\u9009\u62e9\uff0c\u6062\u590d\u88ab\u524a\u5f31\u7684\u4f4d\u7f6e\u4fe1\u53f7\u3002</li>\n    <li>Prism\u80fd\u591f\u5728\u4f7f\u7528\u5757\u7ea7\u64cd\u4f5c\u7684\u540c\u65f6\u4fdd\u6301\u4e0e\u5b8c\u6574\u6ce8\u610f\u529b\u76f8\u540c\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u6700\u9ad85.1\u500d\u7684\u52a0\u901f\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Block-sparse attention helps speed up long-context language model pre-filling, but finding relevant blocks efficiently is challenging.</li>\n    <li>Current methods use coarse attention but often require costly token-level searching, which slows down the process.</li>\n    <li>The authors identify that standard coarse attention methods like mean pooling can miss important local information due to interactions with Rotary Positional Embeddings.</li>\n    <li>They introduce a new method called Prism that separates block selection into high-frequency and low-frequency components for better accuracy.</li>\n    <li>Prism allows for faster block importance estimation with block-level operations, achieving the same accuracy as full attention but with up to 5.1 times faster performance.</li>\n</ul>"}, "publishedAt": "2026-02-09T04:31:06.000Z", "title": "Prism: Spectral-Aware Block-Sparse Attention", "summary": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08426.png", "numComments": 2, "submittedBy": {"_id": "6191f22d08a57f265f7f5266", "avatarUrl": "/avatars/215164e7b6d025a3c32555ff541cdd62.svg", "fullname": "XinghaoWang", "name": "Singhoo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.07035", "authors": [{"_id": "698b43df6052d3bed963079f", "user": {"_id": "698b419584704fee74958520", "avatarUrl": "/avatars/7e94be7a4396174e3546114c3e3af598.svg", "isPro": false, "fullname": "Zhao Jiahao", "user": "bubble65", "type": "user"}, "name": "Jiahao Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:42.263Z", "hidden": false}, {"_id": "698b43df6052d3bed96307a0", "name": "Shaoxuan Xu", "hidden": false}, {"_id": "698b43df6052d3bed96307a1", "user": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "isPro": false, "fullname": "SunZX", "user": "Jeryi", "type": "user"}, "name": "Zhongxiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:40.143Z", "hidden": false}, {"_id": "698b43df6052d3bed96307a2", "name": "Fengqi Zhu", "hidden": false}, {"_id": "698b43df6052d3bed96307a3", "name": "Jingyang Ou", "hidden": false}, {"_id": "698b43df6052d3bed96307a4", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:37.162Z", "hidden": false}, {"_id": "698b43df6052d3bed96307a5", "name": "Chongxuan Li", "hidden": false}, {"_id": "698b43df6052d3bed96307a6", "name": "Xiao Zhang", "hidden": false}, {"_id": "698b43df6052d3bed96307a7", "name": "Jun Xu", "hidden": false}], "publishedAt": "2026-02-03T09:12:08.000Z", "submittedOnDailyAt": "2026-02-11T01:28:30.266Z", "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents", "submittedOnDailyBy": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "isPro": false, "fullname": "SunZX", "user": "Jeryi", "type": "user"}, "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C", "upvotes": 25, "discussionId": "698b43df6052d3bed96307a8", "projectPage": "https://bubble65.github.io/dllm-searcher-pub/", "githubRepo": "https://github.com/bubble65/DLLM-Searcher", "githubRepoAddedBy": "user", "ai_summary": "Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.", "ai_keywords": ["Diffusion Large Language Models", "ReAct agent paradigm", "Latency Challenge", "Agentic Supervised Fine-Tuning", "Agentic Variance-Reduced Preference Optimization", "Parallel-Reasoning and Acting", "P-ReAct"], "githubStars": 10, "organization": {"_id": "622177ac43826d6f261f8208", "name": "RUC", "fullname": "Renmin University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\uff0c\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b(dLLMs)\u5c55\u73b0\u4e86\u72ec\u7279\u7684\u6548\u7387\u4f18\u52bf\uff0c\u5f97\u76ca\u4e8e\u5176\u5e76\u884c\u89e3\u7801\u673a\u5236\u548c\u7075\u6d3b\u7684\u751f\u6210\u6a21\u5f0f\u3002</li>\n    <li>\u641c\u7d22\u4ee3\u7406\u7684\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\u7684\u9650\u5236\uff1a\u5ef6\u8fdf\u6311\u6218\u548c\u4ee3\u7406\u80fd\u529b\u6311\u6218\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u4ee3\u7406\u80fd\u529b\u6311\u6218\uff0c\u63d0\u51fa\u4e86DLLM-Searcher\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u4e3a\u5e94\u5bf9\u5ef6\u8fdf\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4ee3\u7406\u6a21\u5f0fP-ReAct\uff0c\u4f7f\u6a21\u578b\u5728\u7b49\u5f85\u5de5\u5177\u54cd\u5e94\u65f6\u80fd\u591f\u7ee7\u7eed\u601d\u8003\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDLLM-Searcher\u7684\u6027\u80fd\u4e0e\u4e3b\u6d41LLM\u641c\u7d22\u4ee3\u7406\u76f8\u5f53\uff0cP-ReAct\u5b9e\u73b0\u4e86\u7ea615%\u7684\u63a8\u7406\u52a0\u901f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Large Language Models (dLLMs) are efficient and can process tasks in parallel, which helps with generating responses quickly.</li>\n    <li>Search Agents face a Latency Challenge because they take a long time to complete multi-step reasoning and tool interactions.</li>\n    <li>dLLMs struggle with reasoning and tool-calling abilities, which limits their effectiveness in practical use.</li>\n    <li>The paper introduces DLLM-Searcher, a framework that improves dLLMs for Search Agents by enhancing their reasoning skills and reducing latency.</li>\n    <li>Experimental results show that DLLM-Searcher performs well compared to other search agents and speeds up processing by about 15% using a new approach called P-ReAct.</li>\n</ul>"}, "publishedAt": "2026-02-03T04:12:08.000Z", "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents", "summary": "Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07035.png", "numComments": 1, "submittedBy": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "fullname": "SunZX", "name": "Jeryi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "622177ac43826d6f261f8208", "name": "RUC", "fullname": "Renmin University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10104", "authors": [{"_id": "698bfbeb6052d3bed9630ae1", "user": {"_id": "64e84d40d50f3979be9afcbb", "avatarUrl": "/avatars/6a706a4916132c1f1cda63d11dc46b87.svg", "isPro": false, "fullname": "Jiang Yuxin", "user": "YuxinJ", "type": "user"}, "name": "Yuxin Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:13.438Z", "hidden": false}, {"_id": "698bfbeb6052d3bed9630ae2", "name": "Yuchao Gu", "hidden": false}, {"_id": "698bfbeb6052d3bed9630ae3", "name": "Ivor W. Tsang", "hidden": false}, {"_id": "698bfbeb6052d3bed9630ae4", "name": "Mike Zheng Shou", "hidden": false}], "publishedAt": "2026-02-10T18:58:41.000Z", "submittedOnDailyAt": "2026-02-11T01:40:37.554Z", "title": "Olaf-World: Orienting Latent Actions for Video World Modeling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq\u0394-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.", "upvotes": 22, "discussionId": "698bfbeb6052d3bed9630ae5", "projectPage": "https://showlab.github.io/Olaf-World/", "githubRepo": "https://github.com/showlab/Olaf-World", "githubRepoAddedBy": "user", "ai_summary": "Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.", "ai_keywords": ["action-controllable world models", "latent action learning", "temporal feature differences", "self-supervised video encoder", "sequence-level control-effect alignment", "action-conditioned video world models", "zero-shot action transfer", "data-efficient adaptation"], "githubStars": 33, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u53ef\u63a7\u4e16\u754c\u6a21\u578b\u5728\u52a8\u4f5c\u6807\u7b7e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u6269\u5c55\u3002</li>\n    <li>\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u53ef\u4ee5\u4ece\u672a\u6807\u8bb0\u7684\u89c6\u9891\u4e2d\u63d0\u53d6\u63a7\u5236\u63a5\u53e3\uff0c\u4f46\u5176\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u5e38\u5e38\u65e0\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8f6c\u79fb\u3002</li>\n    <li>\u6807\u51c6\u76ee\u6807\u53ea\u5728\u6bcf\u4e2a\u89c6\u9891\u7247\u6bb5\u5185\u64cd\u4f5c\uff0c\u65e0\u6cd5\u5bf9\u9f50\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u52a8\u4f5c\u8bed\u4e49\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faSeq\u0394-REPA\uff0c\u5229\u7528\u53ef\u89c2\u5bdf\u7684\u8bed\u4e49\u6548\u679c\u4f5c\u4e3a\u5171\u4eab\u53c2\u8003\uff0c\u6765\u5bf9\u9f50\u52a8\u4f5c\u3002</li>\n    <li>\u901a\u8fc7Olaf-World\u7ba1\u9053\uff0c\u6211\u4eec\u80fd\u591f\u4ece\u5927\u89c4\u6a21\u88ab\u52a8\u89c6\u9891\u4e2d\u9884\u8bad\u7ec3\u52a8\u4f5c\u6761\u4ef6\u7684\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u96f6-shot\u52a8\u4f5c\u8f6c\u79fb\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Action-controllable world models struggle because there aren't enough action labels available.</li>\n    <li>Current methods for learning actions from unlabeled videos often fail to work in different situations.</li>\n    <li>Standard learning methods don't help align actions across different contexts since they focus only on individual video clips.</li>\n    <li>Seq\u0394-REPA is a new method that uses observable effects of actions as a guide to align actions better.</li>\n    <li>Olaf-World is a system that trains video models using a lot of passive video data, resulting in better action transfer and adaptation to new controls.</li>\n</ul>"}, "publishedAt": "2026-02-10T13:58:41.000Z", "title": "Olaf-World: Orienting Latent Actions for Video World Modeling", "summary": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq\u0394-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10104.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u7531\u4e8e\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u63a5\u8fd1\u67af\u7aed\uff0c\u9884\u8bad\u7ec3\u4ece\u66f4\u591a\u7684\u6807\u8bb0\u8f6c\u5411\u66f4\u597d\u7684\u6807\u8bb0\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9759\u6001\u8fc7\u6ee4\u5668\uff0c\u8981\u4e48\u4f7f\u7528\u57fa\u4e8e\u539f\u59cb\u68af\u5ea6\u7684\u52a8\u6001\u6807\u51c6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86OPUS\uff0c\u4e00\u4e2a\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u73b0\u4ee3\u4f18\u5316\u5668\u5b9a\u4e49\u6548\u7528\u3002</li>\n    <li>OPUS\u5728\u591a\u4e2a\u8bed\u6599\u5e93\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u884c\u4e1a\u57fa\u51c6\u3002</li>\n    <li>\u5728\u7279\u5b9a\u9886\u57df\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0cOPUS\u663e\u793a\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>As high-quality public text becomes scarce, data selection for training models is shifting focus from quantity to quality.</li>\n    <li>OPUS is a new method that selects training data based on how well it helps the optimizer improve the model.</li>\n    <li>It uses advanced techniques for efficiency and variety in data selection, adding only a small extra computational cost.</li>\n    <li>OPUS has shown better results than existing methods and even outperforms training with much larger datasets.</li>\n    <li>In specific cases, OPUS can achieve strong performance using significantly less training data.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u901a\u8fc7\u611f\u77e5\u754c\u9762\u548c\u6267\u884c\u52a8\u4f5c\u4e0e\u73af\u5883\u4e92\u52a8\u3002</li>\n    <li>Code2World\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u80fd\u901a\u8fc7\u751f\u6210\u53ef\u6e32\u67d3\u4ee3\u7801\u6765\u6a21\u62df\u4e0b\u4e00\u4e2a\u89c6\u89c9\u72b6\u6001\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc78\u4e07\u5bf9\u9ad8\u8d28\u91cf\u5c4f\u5e55-\u52a8\u4f5c\u5bf9\u7684AndroidCode\u3002</li>\n    <li>Code2World\u5728\u4e0b\u4e00\u6b65\u7528\u6237\u754c\u9762\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86GPT-5\u548cGemini-3-Pro-Image\u3002</li>\n    <li>Code2World\u663e\u8457\u63d0\u9ad8\u4e86\u5728AndroidWorld\u5bfc\u822a\u4e2d\u7684\u6210\u529f\u7387\uff0c\u63d0\u5347\u4e869.5%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Code2World is a new tool that helps virtual agents predict the next visual state of user interfaces by generating code.</li>\n    <li>It creates high-quality training data from Android GUI interactions, resulting in over 80,000 screen-action pairs.</li>\n    <li>The system uses special learning techniques to improve the accuracy of visual predictions and maintain action consistency.</li>\n    <li>Experiments show Code2World performs better than other models like GPT-5 and Gemini-3-Pro-Image in predicting user interface changes.</li>\n    <li>It also improves navigation success rates in Android applications, enhancing performance by 9.5% compared to existing models.</li>\n</ul>"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09082", "authors": [{"_id": "698bea506052d3bed96309cb", "name": "Veuns-Team", "hidden": false}, {"_id": "698bea506052d3bed96309cd", "name": "Changlong Gao", "hidden": false}, {"_id": "698bea506052d3bed96309ce", "user": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "name": "Zhangxuan Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:14.456Z", "hidden": false}, {"_id": "698bea506052d3bed96309cf", "name": "Yulin Liu", "hidden": false}, {"_id": "698bea506052d3bed96309d0", "name": "Xinyu Qiu", "hidden": false}, {"_id": "698bea506052d3bed96309d1", "name": "Shuheng Shen", "hidden": false}, {"_id": "698bea506052d3bed96309d2", "name": "Yue Wen", "hidden": false}, {"_id": "698bea506052d3bed96309d3", "name": "Tianyu Xia", "hidden": false}, {"_id": "698bea506052d3bed96309d4", "name": "Zhenyu Xu", "hidden": false}, {"_id": "698bea506052d3bed96309d5", "user": {"_id": "64cb238576200ec80fe988f8", "avatarUrl": "/avatars/42c48710c7881c9dfbcc075fec3cb600.svg", "isPro": false, "fullname": "zeus", "user": "zengw", "type": "user"}, "name": "Zhengwen Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:24:43.235Z", "hidden": false}, {"_id": "698bea506052d3bed96309d6", "user": {"_id": "654c9dac09dd7ef524a0be1e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png", "isPro": false, "fullname": "beitongzhou", "user": "syorami", "type": "user"}, "name": "Beitong Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:11.859Z", "hidden": false}, {"_id": "698bea506052d3bed96309d7", "name": "Xingran Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309d8", "name": "Weizhi Chen", "hidden": false}, {"_id": "698bea506052d3bed96309d9", "name": "Sunhao Dai", "hidden": false}, {"_id": "698bea506052d3bed96309da", "name": "Jingya Dou", "hidden": false}, {"_id": "698bea506052d3bed96309db", "name": "Yichen Gong", "hidden": false}, {"_id": "698bea506052d3bed96309dc", "name": "Yuan Guo", "hidden": false}, {"_id": "698bea506052d3bed96309dd", "name": "Zhenlin Guo", "hidden": false}, {"_id": "698bea506052d3bed96309de", "user": {"_id": "65e0763a9299e96ee674876e", "avatarUrl": "/avatars/0ea342c9f72fa3b8a8f634559d094907.svg", "isPro": false, "fullname": "fengdian", "user": "fengrudian", "type": "user"}, "name": "Feng Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:04.463Z", "hidden": false}, {"_id": "698bea506052d3bed96309df", "name": "Qian Li", "hidden": false}, {"_id": "698bea506052d3bed96309e0", "name": "Jinzhen Lin", "hidden": false}, {"_id": "698bea506052d3bed96309e1", "name": "Yuqi Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309e2", "name": "Linchao Zhu", "hidden": false}, {"_id": "698bea506052d3bed96309e3", "name": "Liang Chen", "hidden": false}, {"_id": "698bea506052d3bed96309e4", "name": "Zhenyu Guo", "hidden": false}, {"_id": "698bea506052d3bed96309e5", "name": "Changhua Meng", "hidden": false}, {"_id": "698bea506052d3bed96309e6", "name": "Weiqiang Wang", "hidden": false}], "publishedAt": "2026-02-09T18:43:40.000Z", "submittedOnDailyAt": "2026-02-11T00:10:55.649Z", "title": "UI-Venus-1.5 Technical Report", "submittedOnDailyBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "upvotes": 143, "discussionId": "698bea516052d3bed96309e7", "projectPage": "https://ui-venus.github.io/UI-Venus-1.5/", "githubRepo": "https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5", "githubRepoAddedBy": "user", "ai_summary": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.", "ai_keywords": ["GUI agents", "Mid-Training stage", "Online Reinforcement Learning", "full-trajectory rollouts", "Model Merging", "dense variants", "mixture-of-experts variant"], "githubStars": 708, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>UI-Venus-1.5\u662f\u4e00\u4e2a\u5f3a\u5927\u7684GUI\u4ee3\u7406\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u6570\u5b57\u73af\u5883\u4e2d\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u6709\u4e24\u4e2a\u5bc6\u96c6\u578b\u53d8\u4f53\uff082B\u548c8B\uff09\u548c\u4e00\u4e2a\u4e13\u5bb6\u6df7\u5408\u53d8\u4f53\uff0830B-A3B\uff09\uff0c\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\u8fdb\u5c55\uff0c\u5305\u62ec\u5168\u9762\u7684\u4e2d\u671f\u8bad\u7ec3\u3001\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u5408\u5e76\u3002</li>\n    <li>UI-Venus-1.5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u7248\u672c\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6210\u7ee9\u3002</li>\n    <li>\u5b83\u5728\u4e2d\u56fd\u7684\u624b\u673a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u7528\u6237\u6307\u4ee4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UI-Venus-1.5 is a new type of GUI Agent designed to automate tasks in digital environments effectively.</li>\n    <li>It includes three model types to suit different use cases: two dense models and one mixture-of-experts model.</li>\n    <li>The model improves on its predecessor with three advancements: a Mid-Training stage using lots of data, Online Reinforcement Learning for better navigation, and a unified design merging different models.</li>\n    <li>UI-Venus-1.5 achieves record performance on several benchmarks, outperforming previous models significantly.</li>\n    <li>It also shows strong ability to navigate and follow instructions in various Chinese mobile apps.</li>\n</ul>"}, "publishedAt": "2026-02-09T13:43:40.000Z", "title": "UI-Venus-1.5 Technical Report", "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png", "numComments": 2, "submittedBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "fullname": "Zhangxuan Gu", "name": "zhangxgu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u578b\u7684alpha\u6316\u6398\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u91d1\u878d\u5e02\u573a\u4e2d\u7684\u566a\u58f0\u548c\u53d8\u5316\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u8f68\u8ff9\u7ea7\u522b\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u6765\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\uff0c\u4f18\u5316\u6bcf\u6b21\u6316\u6398\u7684\u6b65\u9aa4\u3002</li>\n    <li>\u5728\u751f\u6210\u56e0\u5b50\u65f6\uff0cQuantaAlpha\u786e\u4fdd\u5047\u8bbe\u3001\u56e0\u5b50\u8868\u8fbe\u548c\u53ef\u6267\u884c\u4ee3\u7801\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cQuantaAlpha\u5728\u4e2d\u56fd\u8bc1\u5238\u6307\u6570300\uff08CSI 300\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u53d6\u5f97\u4e8627.75%\u7684\u5e74\u5316\u6536\u76ca\u7387\u3002</li>\n    <li>\u6316\u6398\u7684\u56e0\u5b50\u5728\u5176\u4ed6\u5e02\u573a\uff08\u5982CSI 500\u548c\u6807\u51c6\u666e\u5c14500\uff09\u4e5f\u6709\u6548\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets are unpredictable, making it hard to find reliable investment strategies (alpha mining).</li>\n    <li>The proposed solution, QuantaAlpha, improves alpha mining by using a method that combines and adjusts different strategies over time.</li>\n    <li>QuantaAlpha focuses on refining its approaches by revising weak steps and merging successful segments to create better strategies.</li>\n    <li>Tests on the China Securities Index 300 showed that QuantaAlpha outperformed other models, achieving a 27.75% annual return.</li>\n    <li>Strategies developed with QuantaAlpha also worked well on other indices, showing its effectiveness across different market conditions.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08794", "authors": [{"_id": "698ac65d1b2dc6b37d61b1c2", "name": "SII-OpenMOSS Team", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c4", "name": "Donghua Yu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c5", "name": "Mingshu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c6", "name": "Qi Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c7", "name": "Qi Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c8", "name": "Qianyi Wu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c9", "user": {"_id": "63ec4715c81b6a52391c46b8", "avatarUrl": "/avatars/496819b5075a1a834a2b9edeb068c80e.svg", "isPro": false, "fullname": "QinyuanCheng", "user": "Cqy2019", "type": "user"}, "name": "Qinyuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:07.400Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ca", "name": "Ruixiao Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cb", "user": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "name": "Tianyi Liang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:10.522Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cc", "name": "Wenbo Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cd", "name": "Wenming Tu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ce", "name": "Xiangyu Peng", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cf", "name": "Yang Gao", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d0", "name": "Yanru Huo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d1", "user": {"_id": "69158ffc0153b85a677dcc46", "avatarUrl": "/avatars/c9c5f60522f2a8f370d790ea9938b090.svg", "isPro": false, "fullname": "Ying Zhu", "user": "Auraithm", "type": "user"}, "name": "Ying Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:27:41.440Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d2", "name": "Yinze Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d3", "name": "Yiyang Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d4", "name": "Yuerong Song", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d5", "name": "Zhe Xu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d6", "name": "Zhiyu Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d7", "name": "Chenchen Yang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d8", "name": "Cheng Chang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d9", "name": "Chushu Zhou", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1da", "name": "Hanfu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1db", "name": "Hongnan Ma", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dc", "name": "Jiaxi Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dd", "name": "Jingqi Tong", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1de", "name": "Junxi Liu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1df", "name": "Ke Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e0", "name": "Shimin Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e1", "name": "Songlin Wang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e2", "name": "Wei Jiang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e3", "name": "Zhaoye Fei", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e4", "name": "Zhiyuan Ning", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e5", "name": "Chunguo Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e6", "name": "Chenhui Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e7", "name": "Ziwei He", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e8", "name": "Zengfeng Huang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e9", "name": "Xie Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ea", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-02-09T15:31:54.000Z", "submittedOnDailyAt": "2026-02-10T03:18:59.260Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "submittedOnDailyBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "upvotes": 131, "discussionId": "698ac65e1b2dc6b37d61b1eb", "projectPage": "https://mosi.cn/models/mova", "githubRepo": "https://github.com/OpenMOSS/MOVA", "githubRepoAddedBy": "user", "ai_summary": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.", "ai_keywords": ["Mixture-of-Experts", "MoE", "audio-visual content", "lip-synced speech", "sound effects", "content-aligned music", "IT2VA", "efficient inference", "LoRA fine-tuning", "prompt enhancement"], "githubStars": 579, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u97f3\u9891\u5728\u73b0\u5b9e\u89c6\u9891\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u4e3b\u8981\u5ffd\u7565\u4e86\u97f3\u9891\u90e8\u5206\u3002</li>\n    <li>\u76ee\u524d\u7684\u97f3\u89c6\u9891\u5185\u5bb9\u751f\u6210\u65b9\u6cd5\u5e38\u5e38\u4f9d\u8d56\u7ea7\u8054\u7ba1\u9053\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>MOVA\uff08MOSS \u89c6\u9891\u548c\u97f3\u9891\uff09\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u97f3\u89c6\u9891\u5185\u5bb9\uff0c\u5305\u62ec\u903c\u771f\u7684\u53e3\u578b\u540c\u6b65\u8bed\u97f3\u548c\u73af\u5883\u97f3\u6548\u3002</li>\n    <li>MOVA\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u603b\u5171\u6709320\u4ebf\u4e2a\u53c2\u6570\uff0c\u5176\u4e2d180\u4ebf\u4e2a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6d3b\u8dc3\u3002</li>\n    <li>\u901a\u8fc7\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\uff0c\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u5e76\u4fc3\u8fdb\u521b\u4f5c\u8005\u793e\u533a\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Audio is crucial for video, but many models do not focus on audio generation.</li>\n    <li>Current methods often use separate processes for audio and video, which can lead to higher costs and lower quality.</li>\n    <li>MOVA (MOSS Video and Audio) is a new open-source model that creates high-quality audio-visual content with synchronized audio, including lip-synced speech and sound effects.</li>\n    <li>The model has a large architecture with 32 billion parameters, making it capable of advanced tasks like generating video from images and text.</li>\n    <li>By providing the model's code and weights, the creators aim to support research and encourage a community of developers and creators.</li>\n</ul>"}, "publishedAt": "2026-02-09T10:31:54.000Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08794.png", "numComments": 1, "submittedBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "fullname": "SII-liangtianyi", "name": "tianyilt", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08222", "authors": [{"_id": "698ad20e1b2dc6b37d61b227", "user": {"_id": "64afe1653361f887816da303", "avatarUrl": "/avatars/320d71adacfad9dd5db064b4ed3dec2b.svg", "isPro": false, "fullname": "chenzehao", "user": "chhao", "type": "user"}, "name": "Zehao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:42.021Z", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b228", "name": "Gongxun Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b229", "name": "Tianxiang Ai", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22a", "name": "Yifei Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22b", "name": "Zixuan Huang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22c", "name": "Wang Zhou", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22d", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22e", "name": "Xianglong Liu", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22f", "name": "Jianxin Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b230", "name": "Deqing Wang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b231", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun B", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:44.617Z", "hidden": false}], "publishedAt": "2026-02-09T02:50:40.000Z", "submittedOnDailyAt": "2026-02-10T04:36:28.975Z", "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun B", "user": "Yikunb", "type": "user"}, "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "upvotes": 118, "discussionId": "698ad20e1b2dc6b37d61b232", "githubRepo": "https://github.com/chenzehao82/Weak-Driven-Learning", "githubRepoAddedBy": "user", "ai_summary": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.", "ai_keywords": ["post-training optimization", "large language models", "saturation bottleneck", "weak checkpoints", "entropy dynamics", "compensatory learning", "learning gaps"], "githubStars": 39, "summary_zh": "<ul>\n    <li>\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u540e\u7eed\u8bad\u7ec3\u4e2d\uff0c\u5b58\u5728\u4e00\u4e2a\u74f6\u9888\uff0c\u5373\u6a21\u578b\u4e00\u65e6\u53d8\u5f97\u9ad8\u5ea6\u81ea\u4fe1\uff0c\u8fdb\u4e00\u6b65\u8bad\u7ec3\u7684\u6548\u679c\u4f1a\u5927\u5e45\u51cf\u5c11\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u5f3a\u5316\u76ee\u6807\u9884\u6d4b\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u6a21\u578b\u5386\u53f2\u4e0a\u7684\u5f31\u72b6\u6001\u4e2d\u4ecd\u7136\u5b58\u5728\u6709\u4ef7\u503c\u7684\u76d1\u7763\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86WMSS\uff08\u5f31\u4ee3\u7406\u53ef\u4ee5\u4f7f\u5f3a\u4ee3\u7406\u66f4\u5f3a\uff09\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u5f31\u7684\u68c0\u67e5\u70b9\u6765\u6307\u5bfc\u540e\u7eed\u4f18\u5316\u3002</li>\n    <li>WMSS\u901a\u8fc7\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\u5e76\u901a\u8fc7\u8865\u507f\u5b66\u4e60\u6765\u5f3a\u5316\u5b83\u4eec\uff0c\u4f7f\u5f97\u5f3a\u4ee3\u7406\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u7684\u8bad\u7ec3\u9971\u548c\u72b6\u6001\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u6211\u4eec\u65b9\u6cd5\u7684\u4ee3\u7406\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e14\u6ca1\u6709\u589e\u52a0\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models face a problem where more training leads to smaller improvements once they become very confident.</li>\n    <li>Current methods focus on reinforcing what models already predict well, but there are still useful insights in their earlier, less confident states.</li>\n    <li>We introduce WMSS (Weak Agents Can Make Strong Agents Stronger), a new approach that uses these earlier weak states to help models keep improving.</li>\n    <li>WMSS identifies areas where the model can still learn and uses targeted learning to address these gaps.</li>\n    <li>Tests show that models trained with WMSS perform better on tasks like math reasoning and code generation without increasing the cost of using them.</li>\n</ul>"}, "publishedAt": "2026-02-08T21:50:40.000Z", "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08222.png", "numComments": 4, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun B", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10063", "authors": [{"_id": "698bf4ef6052d3bed9630a96", "user": {"_id": "6895e7f146763431aea25ca4", "avatarUrl": "/avatars/52e550c3f7e8da2e31b63413e2e71e6c.svg", "isPro": false, "fullname": "Tianyi Jiang", "user": "LumosJiang", "type": "user"}, "name": "Tianyi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:33.352Z", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a97", "name": "Arctanx An", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a98", "name": "Hengyi Feng", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a99", "name": "Naixin Zhai", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9a", "name": "Haodong Li", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9b", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T12:34:26.745Z", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9c", "name": "Jiahui Liu", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9d", "name": "Hanwen Du", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9e", "name": "Shuo Zhang", "hidden": false}, {"_id": "698bf4ef6052d3bed9630a9f", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:35.722Z", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa0", "name": "Jie Huang", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa1", "name": "Yuhua Li", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa2", "name": "Yongxin Ni", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa3", "name": "Huacan Wang", "hidden": false}, {"_id": "698bf4ef6052d3bed9630aa4", "name": "Ronghao Chen", "hidden": false}], "publishedAt": "2026-02-10T18:31:47.000Z", "submittedOnDailyAt": "2026-02-11T00:51:58.024Z", "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.", "upvotes": 62, "discussionId": "698bf4f06052d3bed9630aa5", "githubRepo": "https://github.com/QuantaAlpha/chain-of-mindset", "githubRepoAddedBy": "user", "ai_summary": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.", "ai_keywords": ["Chain of Mindset", "CoM", "agentic framework", "step-level adaptive mindset orchestration", "Spatial mindset", "Convergent mindset", "Divergent mindset", "Algorithmic mindset", "Meta-Agent", "bidirectional Context Gate", "reasoning efficiency", "large language models"], "githubStars": 18, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u89e3\u51b3\u95ee\u9898\u65f6\u4f1a\u7ed3\u5408\u591a\u79cd\u601d\u7ef4\u65b9\u5f0f\uff0c\u800c\u4e0d\u662f\u53ea\u4f9d\u8d56\u5355\u4e00\u601d\u7ef4\u6a21\u5f0f\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5e38\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u601d\u7ef4\u65b9\u5f0f\uff0c\u5ffd\u89c6\u4e86\u89e3\u51b3\u95ee\u9898\u7684\u4e0d\u540c\u9636\u6bb5\u9700\u8981\u4e0d\u540c\u7684\u601d\u7ef4\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u94fe\u201d\uff08CoM\uff09\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6839\u636e\u9700\u8981\u52a8\u6001\u8c03\u6574\u601d\u7ef4\u65b9\u5f0f\u3002</li>\n    <li>CoM\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u56db\u79cd\u4e0d\u540c\u7684\u601d\u7ef4\u6a21\u5f0f\uff1a\u7a7a\u95f4\u601d\u7ef4\u3001\u6536\u655b\u601d\u7ef4\u3001\u53d1\u6563\u601d\u7ef4\u548c\u7b97\u6cd5\u601d\u7ef4\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cCoM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u6700\u5f3a\u57fa\u51c6\u6a21\u578b\uff0c\u4e14\u63a8\u7406\u6548\u7387\u5f97\u5230\u4e86\u5e73\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Human problem-solving involves using multiple ways of thinking, not just one approach.</li>\n    <li>Current reasoning methods for language models often use the same thinking style for all steps, which limits their intelligence.</li>\n    <li>The proposed solution, Chain of Mindset (CoM), allows for flexible thinking by using different modes for different stages of problem-solving.</li>\n    <li>CoM uses four different mindsets: Spatial, Convergent, Divergent, and Algorithmic, and adapts them based on the situation.</li>\n    <li>Tests show that CoM performs better than existing methods in various areas, improving accuracy significantly while being efficient.</li>\n</ul>"}, "publishedAt": "2026-02-10T13:31:47.000Z", "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes", "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10063.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.05386", "authors": [{"_id": "698599b14ad556f294b7ecdc", "name": "Zhenxiong Yu", "hidden": false}, {"_id": "698599b14ad556f294b7ecdd", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:50:37.407Z", "hidden": false}, {"_id": "698599b14ad556f294b7ecde", "name": "Zhiheng Jin", "hidden": false}, {"_id": "698599b14ad556f294b7ecdf", "name": "Shuhe Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ece0", "name": "Heng Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ece1", "name": "Yanlin Fei", "hidden": false}, {"_id": "698599b14ad556f294b7ece2", "name": "Lingfeng Zeng", "hidden": false}, {"_id": "698599b14ad556f294b7ece3", "name": "Fangqi Lou", "hidden": false}, {"_id": "698599b14ad556f294b7ece4", "name": "Shuo Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ece5", "name": "Tu Hu", "hidden": false}, {"_id": "698599b14ad556f294b7ece6", "name": "Jingping Liu", "hidden": false}, {"_id": "698599b14ad556f294b7ece7", "name": "Rongze Chen", "hidden": false}, {"_id": "698599b14ad556f294b7ece8", "name": "Xingyu Zhu", "hidden": false}, {"_id": "698599b14ad556f294b7ece9", "name": "Kunyi Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ecea", "name": "Chaofa Yuan", "hidden": false}, {"_id": "698599b14ad556f294b7eceb", "name": "Xin Guo", "hidden": false}, {"_id": "698599b14ad556f294b7ecec", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698599b14ad556f294b7eced", "name": "Feipeng Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ecee", "name": "Jie Huang", "hidden": false}, {"_id": "698599b14ad556f294b7ecef", "name": "Huacan Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ecf0", "name": "Ronghao Chen", "hidden": false}, {"_id": "698599b14ad556f294b7ecf1", "name": "Liwen Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:11:05.000Z", "submittedOnDailyAt": "2026-02-06T05:15:48.526Z", "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.", "upvotes": 56, "discussionId": "698599b14ad556f294b7ecf2", "githubRepo": "https://github.com/aifinlab/Spider-Sense", "githubRepoAddedBy": "user", "ai_summary": "Spider-Sense framework provides intrinsic and selective agent security through event-driven defense with intrinsic risk sensing, achieving low attack success and false positive rates with minimal latency overhead.", "ai_keywords": ["large language models", "autonomous agents", "security challenges", "mandatory checking paradigm", "event-driven defense", "Intrinsic Risk Sensing", "hierarchical defense mechanism", "lightweight similarity matching", "deep internal reasoning", "lifecycle-aware benchmark", "Attack Success Rate", "False Positive Rate"], "githubStars": 9, "organization": {"_id": "696875114bc2a5524dd8fcb7", "name": "AIFin-Lab", "fullname": "AIFin Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9010\u6e10\u6210\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u4f46\u9762\u4e34\u65b0\u7684\u5b89\u5168\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u4ee3\u7406\u9632\u5fa1\u673a\u5236\u901a\u5e38\u5728\u9884\u5b9a\u9636\u6bb5\u5f3a\u5236\u8fdb\u884c\u5b89\u5168\u9a8c\u8bc1\u3002</li>\n    <li>\u672c\u6587\u63d0\u51faSpider-Sense\u6846\u67b6\uff0c\u57fa\u4e8e\u5185\u5728\u98ce\u9669\u611f\u77e5\uff08IRS\uff09\uff0c\u4f7f\u4ee3\u7406\u5728\u611f\u77e5\u98ce\u9669\u65f6\u4e3b\u52a8\u89e6\u53d1\u9632\u5fa1\u3002</li>\n    <li>Spider-Sense\u91c7\u7528\u5c42\u7ea7\u9632\u5fa1\u673a\u5236\uff0c\u5e73\u8861\u6548\u7387\u548c\u7cbe\u786e\u5ea6\uff0c\u5904\u7406\u5df2\u77e5\u6a21\u5f0f\u4e0e\u6a21\u7cca\u60c5\u51b5\u3002</li>\n    <li>\u901a\u8fc7S^2Bench\u57fa\u51c6\u6d4b\u8bd5\uff0cSpider-Sense\u5728\u9632\u5fa1\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u653b\u51fb\u6210\u529f\u7387\u548c\u8bef\u62a5\u7387\u6700\u4f4e\uff0c\u5ef6\u8fdf\u4ec5\u589e\u52a08.3%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are becoming more autonomous, leading to new security issues.</li>\n    <li>Current security methods often check for threats at set stages, but this paper suggests a more integrated approach.</li>\n    <li>The authors propose a new framework called Spider-Sense that senses risks and only activates defenses when needed.</li>\n    <li>Spider-Sense uses a two-tier defense system that balances speed and accuracy by matching known threats quickly and analyzing unclear cases more deeply.</li>\n    <li>Testing shows that Spider-Sense performs well, with low rates of attack success and false positives, and only a small delay in processing.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:11:05.000Z", "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening", "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05386.png", "numComments": 3, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "696875114bc2a5524dd8fcb7", "name": "AIFin-Lab", "fullname": "AIFin Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08234", "authors": [{"_id": "698aba731b2dc6b37d61b0e4", "user": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "isPro": false, "fullname": "Peng Xia", "user": "richardxp888", "type": "user"}, "name": "Peng Xia", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:18:58.896Z", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e5", "name": "Jianwen Chen", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e6", "name": "Hanyang Wang", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e7", "name": "Jiaqi Liu", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e8", "name": "Kaide Zeng", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0e9", "user": {"_id": "63234809155b0e2c44f354d6", "avatarUrl": "/avatars/60d38f8f0e12363f3f5e0388e635d7b6.svg", "isPro": false, "fullname": "Yu Wang", "user": "YuWangX", "type": "user"}, "name": "Yu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:18:56.703Z", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ea", "name": "Siwei Han", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0eb", "name": "Yiyang Zhou", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ec", "name": "Xujiang Zhao", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ed", "name": "Haifeng Chen", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ee", "name": "Zeyu Zheng", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0ef", "name": "Cihang Xie", "hidden": false}, {"_id": "698aba731b2dc6b37d61b0f0", "name": "Huaxiu Yao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"], "publishedAt": "2026-02-09T03:17:17.000Z", "submittedOnDailyAt": "2026-02-11T00:14:23.139Z", "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "submittedOnDailyBy": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "isPro": false, "fullname": "Peng Xia", "user": "richardxp888", "type": "user"}, "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "upvotes": 56, "discussionId": "698aba731b2dc6b37d61b0f1", "githubRepo": "https://github.com/aiming-lab/SkillRL", "githubRepoAddedBy": "user", "ai_summary": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.", "ai_keywords": ["large language model agents", "reinforcement learning", "skill discovery", "recursive evolution", "skill library", "SkillBank", "experience-based distillation", "adaptive retrieval strategy", "policy improvement", "token footprint"], "githubStars": 140, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u65e0\u6cd5\u4ece\u8fc7\u53bb\u7684\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u5b58\u50a8\u5197\u4f59\u4e14\u5608\u6742\u7684\u539f\u59cb\u6570\u636e\uff0c\u65e0\u6cd5\u63d0\u53d6\u53ef\u91cd\u7528\u7684\u884c\u4e3a\u6a21\u5f0f\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SkillRL\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u6280\u80fd\u53d1\u73b0\u548c\u9012\u5f52\u8fdb\u5316\uff0c\u6539\u5584\u7b56\u7565\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7ecf\u9a8c\u84b8\u998f\u673a\u5236\uff0c\u5efa\u7acb\u4e86\u5206\u5c42\u6280\u80fd\u5e93SkillBank\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSkillRL\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc715.3%\uff0c\u5e76\u5728\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model (LLM) agents excel in complex tasks but usually do not learn from past experiences.</li>\n    <li>Current memory methods store unnecessary and noisy data, hindering the learning of useful patterns.</li>\n    <li>The paper introduces SkillRL, a framework that helps agents improve by discovering skills and evolving their abilities.</li>\n    <li>Key features include a skill library (SkillBank), a strategy for retrieving helpful techniques, and a method for skills to evolve with the agent's learning.</li>\n    <li>SkillRL shows better performance in various tasks, improving results by over 15% compared to other methods, even as tasks become more complex.</li>\n</ul>"}, "publishedAt": "2026-02-08T22:17:17.000Z", "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08234.png", "numComments": 1, "submittedBy": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "fullname": "Peng Xia", "name": "richardxp888", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06717", "authors": [{"_id": "69898989beecc443208d2741", "name": "Daniil Plyusov", "hidden": false}, {"_id": "69898989beecc443208d2742", "user": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "name": "Alexey Gorbatovski", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:53.815Z", "hidden": false}, {"_id": "69898989beecc443208d2743", "name": "Boris Shaposhnikov", "hidden": false}, {"_id": "69898989beecc443208d2744", "name": "Viacheslav Sinii", "hidden": false}, {"_id": "69898989beecc443208d2745", "user": {"_id": "636e71b2b0ebc04888157b71", "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg", "isPro": false, "fullname": "Alexey Malakhov", "user": "ZeL1k7", "type": "user"}, "name": "Alexey Malakhov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:06:45.653Z", "hidden": false}, {"_id": "69898989beecc443208d2746", "user": {"_id": "62a9c8edc19f92ae443ab37f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9c8edc19f92ae443ab37f/yczqpBOntLco_2Jn4hnT7.jpeg", "isPro": false, "fullname": "Daniil Gavrilov", "user": "kefirski", "type": "user"}, "name": "Daniil Gavrilov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:57.059Z", "hidden": false}], "publishedAt": "2026-02-06T14:07:30.000Z", "submittedOnDailyAt": "2026-02-09T04:48:51.744Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "submittedOnDailyBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "upvotes": 54, "discussionId": "69898989beecc443208d2747", "ai_summary": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.", "ai_keywords": ["reinforcement learning", "verifiable rewards", "group sampling", "advantage estimation", "policy updates", "Focal loss", "GRPO", "DAPO", "CISPO", "pass@k metrics"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n  <li>\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5e38\u7528\u7fa4\u4f53\u62bd\u6837\u6765\u4f30\u8ba1\u4f18\u52bf\u548c\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u3002</li>\n  <li>\u5927\u7fa4\u4f53\u7531\u4e8e\u8ba1\u7b97\u9650\u5236\u4e0d\u53ef\u884c\uff0c\u5bfc\u81f4\u5b66\u4e60\u504f\u5411\u4e8e\u5df2\u7ecf\u53ef\u80fd\u7684\u8f68\u8ff9\u3002</li>\n  <li>\u5c0f\u7fa4\u4f53\u5bb9\u6613\u9519\u8fc7\u7a00\u6709\u6b63\u786e\u8f68\u8ff9\uff0c\u540c\u65f6\u4ecd\u5305\u542b\u6df7\u5408\u5956\u52b1\u3002</li>\n  <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56f0\u96be\u611f\u77e5\u7684\u4f18\u52bf\u7f29\u653e\u7cfb\u6570\uff0c\u53ef\u4ee5\u51cf\u5f31\u9ad8\u6210\u529f\u63d0\u793a\u7684\u66f4\u65b0\u3002</li>\n  <li>\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u800c\u6ca1\u6709\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) often uses group sampling to help learn better, but large groups can be too slow and small groups may miss important rare cases.</li>\n    <li>The study shows how the size of the group affects the chances of missing rare correct outcomes and how updates can unintentionally focus on common solutions.</li>\n    <li>To address these issues, a new method is introduced that adjusts how rewards are scaled based on the difficulty of tasks, inspired by a technique called Focal loss.</li>\n    <li>This new approach can be easily added to existing RLVR algorithms like GRPO, DAPO, and CISPO.</li>\n    <li>Testing on the Qwen2.5-7B model shows significant improvements in performance without needing larger groups or higher computational costs.</li>\n</ul>"}, "publishedAt": "2026-02-06T09:07:30.000Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06717.png", "numComments": 1, "submittedBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "fullname": "Alexey Gorbatovski", "name": "Myashka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u7531\u4e8e\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u63a5\u8fd1\u67af\u7aed\uff0c\u9884\u8bad\u7ec3\u4ece\u66f4\u591a\u7684\u6807\u8bb0\u8f6c\u5411\u66f4\u597d\u7684\u6807\u8bb0\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9759\u6001\u8fc7\u6ee4\u5668\uff0c\u8981\u4e48\u4f7f\u7528\u57fa\u4e8e\u539f\u59cb\u68af\u5ea6\u7684\u52a8\u6001\u6807\u51c6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86OPUS\uff0c\u4e00\u4e2a\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u73b0\u4ee3\u4f18\u5316\u5668\u5b9a\u4e49\u6548\u7528\u3002</li>\n    <li>OPUS\u5728\u591a\u4e2a\u8bed\u6599\u5e93\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u884c\u4e1a\u57fa\u51c6\u3002</li>\n    <li>\u5728\u7279\u5b9a\u9886\u57df\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0cOPUS\u663e\u793a\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>As high-quality public text becomes scarce, data selection for training models is shifting focus from quantity to quality.</li>\n    <li>OPUS is a new method that selects training data based on how well it helps the optimizer improve the model.</li>\n    <li>It uses advanced techniques for efficiency and variety in data selection, adding only a small extra computational cost.</li>\n    <li>OPUS has shown better results than existing methods and even outperforms training with much larger datasets.</li>\n    <li>In specific cases, OPUS can achieve strong performance using significantly less training data.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>ERNIE 5.0\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u7edf\u4e00\u7684\u9884\u6d4b\u76ee\u6807\u4e0b\uff0c\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u6240\u6709\u6a21\u6001\uff0c\u4f7f\u7528\u8d85\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u3002</li>\n    <li>ERNIE 5.0\u91c7\u7528\u4e86\u5f39\u6027\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5355\u6b21\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u4e0d\u540c\u6df1\u5ea6\u548c\u5bb9\u91cf\u7684\u5b50\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u79cd\u6a21\u6001\u4e0a\u8868\u73b0\u5f3a\u52b2\u4e14\u5747\u8861\uff0c\u662f\u516c\u5f00\u6a21\u578b\u4e2d\u9996\u4e2a\u652f\u6301\u4e07\u4ebf\u53c2\u6570\u7684\u7edf\u4e00\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u62a5\u544a\u4e2d\u63d0\u4f9b\u4e86\u4e13\u5bb6\u8def\u7531\u7684\u53ef\u89c6\u5316\u548c\u5f39\u6027\u8bad\u7ec3\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new model that can understand and generate text, images, videos, and audio all at once.</li>\n    <li>The model uses a special approach to training that allows it to work efficiently in different resource settings.</li>\n    <li>It learns different versions of itself during training, which helps balance performance and efficiency.</li>\n    <li>ERNIE 5.0 is the first large-scale model with a trillion parameters that can handle multiple types of data together.</li>\n    <li>The report includes helpful visuals and analyses to support further research in this area.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Green-VLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7eff\u8272\u4eba\u5f62\u673a\u5668\u4eba\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u591a\u9636\u6bb5\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6846\u67b6\u3002</li>\n    <li>Green-VLA\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\uff1a\u57fa\u7840VLM\u3001\u8de8\u6a21\u6001\u57fa\u7840\u3001\u591a\u4f53\u6001\u9884\u8bad\u7ec3\u3001\u7279\u5b9a\u8eab\u4f53\u9002\u5e94\u548c\u5f3a\u5316\u5b66\u4e60\u653f\u7b56\u5bf9\u9f50\u3002</li>\n    <li>\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\uff083000\u5c0f\u65f6\u7684\u6f14\u793a\uff09\uff0c\u7ed3\u5408\u65f6\u95f4\u5bf9\u9f50\u548c\u8d28\u91cf\u8fc7\u6ee4\u3002</li>\n    <li>\u63d0\u4f9b\u7edf\u4e00\u7684\u3001\u9002\u5e94\u8eab\u4f53\u7684\u52a8\u4f5c\u63a5\u53e3\uff0c\u4f7f\u5355\u4e00\u7b56\u7565\u53ef\u4ee5\u63a7\u5236\u591a\u79cd\u673a\u5668\u4eba\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\uff0c\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u65f6\u95f4\u6548\u7387\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a new framework designed for using the Green humanoid robot in real-world situations.</li>\n    <li>It includes five stages: foundational models, multimodal understanding, training for different robot types, adapting to specific robots, and aligning with reinforcement learning.</li>\n    <li>The framework uses a large dataset of 3,000 hours of demonstrations to improve performance.</li>\n    <li>It features a unified control system that works for various types of robots, including humanoids and mobile arms.</li>\n    <li>Tests show that Green-VLA improves performance and safety through better prediction and guidance methods.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u901a\u8fc7\u611f\u77e5\u754c\u9762\u548c\u6267\u884c\u52a8\u4f5c\u4e0e\u73af\u5883\u4e92\u52a8\u3002</li>\n    <li>Code2World\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u80fd\u901a\u8fc7\u751f\u6210\u53ef\u6e32\u67d3\u4ee3\u7801\u6765\u6a21\u62df\u4e0b\u4e00\u4e2a\u89c6\u89c9\u72b6\u6001\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc78\u4e07\u5bf9\u9ad8\u8d28\u91cf\u5c4f\u5e55-\u52a8\u4f5c\u5bf9\u7684AndroidCode\u3002</li>\n    <li>Code2World\u5728\u4e0b\u4e00\u6b65\u7528\u6237\u754c\u9762\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86GPT-5\u548cGemini-3-Pro-Image\u3002</li>\n    <li>Code2World\u663e\u8457\u63d0\u9ad8\u4e86\u5728AndroidWorld\u5bfc\u822a\u4e2d\u7684\u6210\u529f\u7387\uff0c\u63d0\u5347\u4e869.5%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Code2World is a new tool that helps virtual agents predict the next visual state of user interfaces by generating code.</li>\n    <li>It creates high-quality training data from Android GUI interactions, resulting in over 80,000 screen-action pairs.</li>\n    <li>The system uses special learning techniques to improve the accuracy of visual predictions and maintain action consistency.</li>\n    <li>Experiments show Code2World performs better than other models like GPT-5 and Gemini-3-Pro-Image in predicting user interface changes.</li>\n    <li>It also improves navigation success rates in Android applications, enhancing performance by 9.5% compared to existing models.</li>\n</ul>"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86Kimi K2.5\uff0c\u4e00\u4e2a\u5f00\u653e\u6e90\u4ee3\u7801\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u901a\u7528\u667a\u80fd\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u4e24\u79cd\u6a21\u6001\u76f8\u4e92\u589e\u5f3a\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5305\u62ec\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9SFT\u548c\u8054\u5408\u6587\u672c-\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\uff0c\u4e00\u4e2a\u81ea\u6211\u5bfc\u5411\u7684\u5e76\u884c\u4ee3\u7406\u534f\u8c03\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u5b50\u95ee\u9898\u5e76\u540c\u65f6\u6267\u884c\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cKimi K2.5\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u4e14Agent Swarm\u7684\u5ef6\u8fdf\u6bd4\u5355\u4ee3\u7406\u57fa\u7ebf\u51cf\u5c11\u4e86\u6700\u591a4.5\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Kimi K2.5 is an open-source model that combines text and vision to improve intelligent behavior.</li>\n  <li>It uses techniques like joint training of text and vision, and advanced learning methods to make both modalities work better together.</li>\n  <li>The model features a system called Agent Swarm, which helps break down complex tasks into smaller parts and handle them at the same time.</li>\n  <li>Kimi K2.5 has been tested and shows top performance in areas like coding, vision, reasoning, and intelligent tasks.</li>\n  <li>The K2.5 model is available for others to use in research and practical applications.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.09082", "authors": [{"_id": "698bea506052d3bed96309cb", "name": "Veuns-Team", "hidden": false}, {"_id": "698bea506052d3bed96309cd", "name": "Changlong Gao", "hidden": false}, {"_id": "698bea506052d3bed96309ce", "user": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "name": "Zhangxuan Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:14.456Z", "hidden": false}, {"_id": "698bea506052d3bed96309cf", "name": "Yulin Liu", "hidden": false}, {"_id": "698bea506052d3bed96309d0", "name": "Xinyu Qiu", "hidden": false}, {"_id": "698bea506052d3bed96309d1", "name": "Shuheng Shen", "hidden": false}, {"_id": "698bea506052d3bed96309d2", "name": "Yue Wen", "hidden": false}, {"_id": "698bea506052d3bed96309d3", "name": "Tianyu Xia", "hidden": false}, {"_id": "698bea506052d3bed96309d4", "name": "Zhenyu Xu", "hidden": false}, {"_id": "698bea506052d3bed96309d5", "user": {"_id": "64cb238576200ec80fe988f8", "avatarUrl": "/avatars/42c48710c7881c9dfbcc075fec3cb600.svg", "isPro": false, "fullname": "zeus", "user": "zengw", "type": "user"}, "name": "Zhengwen Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:24:43.235Z", "hidden": false}, {"_id": "698bea506052d3bed96309d6", "user": {"_id": "654c9dac09dd7ef524a0be1e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png", "isPro": false, "fullname": "beitongzhou", "user": "syorami", "type": "user"}, "name": "Beitong Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:11.859Z", "hidden": false}, {"_id": "698bea506052d3bed96309d7", "name": "Xingran Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309d8", "name": "Weizhi Chen", "hidden": false}, {"_id": "698bea506052d3bed96309d9", "name": "Sunhao Dai", "hidden": false}, {"_id": "698bea506052d3bed96309da", "name": "Jingya Dou", "hidden": false}, {"_id": "698bea506052d3bed96309db", "name": "Yichen Gong", "hidden": false}, {"_id": "698bea506052d3bed96309dc", "name": "Yuan Guo", "hidden": false}, {"_id": "698bea506052d3bed96309dd", "name": "Zhenlin Guo", "hidden": false}, {"_id": "698bea506052d3bed96309de", "user": {"_id": "65e0763a9299e96ee674876e", "avatarUrl": "/avatars/0ea342c9f72fa3b8a8f634559d094907.svg", "isPro": false, "fullname": "fengdian", "user": "fengrudian", "type": "user"}, "name": "Feng Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:04.463Z", "hidden": false}, {"_id": "698bea506052d3bed96309df", "name": "Qian Li", "hidden": false}, {"_id": "698bea506052d3bed96309e0", "name": "Jinzhen Lin", "hidden": false}, {"_id": "698bea506052d3bed96309e1", "name": "Yuqi Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309e2", "name": "Linchao Zhu", "hidden": false}, {"_id": "698bea506052d3bed96309e3", "name": "Liang Chen", "hidden": false}, {"_id": "698bea506052d3bed96309e4", "name": "Zhenyu Guo", "hidden": false}, {"_id": "698bea506052d3bed96309e5", "name": "Changhua Meng", "hidden": false}, {"_id": "698bea506052d3bed96309e6", "name": "Weiqiang Wang", "hidden": false}], "publishedAt": "2026-02-09T18:43:40.000Z", "submittedOnDailyAt": "2026-02-11T00:10:55.649Z", "title": "UI-Venus-1.5 Technical Report", "submittedOnDailyBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "upvotes": 143, "discussionId": "698bea516052d3bed96309e7", "projectPage": "https://ui-venus.github.io/UI-Venus-1.5/", "githubRepo": "https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5", "githubRepoAddedBy": "user", "ai_summary": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.", "ai_keywords": ["GUI agents", "Mid-Training stage", "Online Reinforcement Learning", "full-trajectory rollouts", "Model Merging", "dense variants", "mixture-of-experts variant"], "githubStars": 708, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>UI-Venus-1.5\u662f\u4e00\u4e2a\u5f3a\u5927\u7684GUI\u4ee3\u7406\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u6570\u5b57\u73af\u5883\u4e2d\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u6709\u4e24\u4e2a\u5bc6\u96c6\u578b\u53d8\u4f53\uff082B\u548c8B\uff09\u548c\u4e00\u4e2a\u4e13\u5bb6\u6df7\u5408\u53d8\u4f53\uff0830B-A3B\uff09\uff0c\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\u8fdb\u5c55\uff0c\u5305\u62ec\u5168\u9762\u7684\u4e2d\u671f\u8bad\u7ec3\u3001\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u5408\u5e76\u3002</li>\n    <li>UI-Venus-1.5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u7248\u672c\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f73\u6210\u7ee9\u3002</li>\n    <li>\u5b83\u5728\u4e2d\u56fd\u7684\u624b\u673a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u7528\u6237\u6307\u4ee4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UI-Venus-1.5 is a new type of GUI Agent designed to automate tasks in digital environments effectively.</li>\n    <li>It includes three model types to suit different use cases: two dense models and one mixture-of-experts model.</li>\n    <li>The model improves on its predecessor with three advancements: a Mid-Training stage using lots of data, Online Reinforcement Learning for better navigation, and a unified design merging different models.</li>\n    <li>UI-Venus-1.5 achieves record performance on several benchmarks, outperforming previous models significantly.</li>\n    <li>It also shows strong ability to navigate and follow instructions in various Chinese mobile apps.</li>\n</ul>"}, "publishedAt": "2026-02-09T13:43:40.000Z", "title": "UI-Venus-1.5 Technical Report", "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png", "numComments": 2, "submittedBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "fullname": "Zhangxuan Gu", "name": "zhangxgu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u578b\u7684alpha\u6316\u6398\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u91d1\u878d\u5e02\u573a\u4e2d\u7684\u566a\u58f0\u548c\u53d8\u5316\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u8f68\u8ff9\u7ea7\u522b\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u6765\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\uff0c\u4f18\u5316\u6bcf\u6b21\u6316\u6398\u7684\u6b65\u9aa4\u3002</li>\n    <li>\u5728\u751f\u6210\u56e0\u5b50\u65f6\uff0cQuantaAlpha\u786e\u4fdd\u5047\u8bbe\u3001\u56e0\u5b50\u8868\u8fbe\u548c\u53ef\u6267\u884c\u4ee3\u7801\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cQuantaAlpha\u5728\u4e2d\u56fd\u8bc1\u5238\u6307\u6570300\uff08CSI 300\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u53d6\u5f97\u4e8627.75%\u7684\u5e74\u5316\u6536\u76ca\u7387\u3002</li>\n    <li>\u6316\u6398\u7684\u56e0\u5b50\u5728\u5176\u4ed6\u5e02\u573a\uff08\u5982CSI 500\u548c\u6807\u51c6\u666e\u5c14500\uff09\u4e5f\u6709\u6548\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets are unpredictable, making it hard to find reliable investment strategies (alpha mining).</li>\n    <li>The proposed solution, QuantaAlpha, improves alpha mining by using a method that combines and adjusts different strategies over time.</li>\n    <li>QuantaAlpha focuses on refining its approaches by revising weak steps and merging successful segments to create better strategies.</li>\n    <li>Tests on the China Securities Index 300 showed that QuantaAlpha outperformed other models, achieving a 27.75% annual return.</li>\n    <li>Strategies developed with QuantaAlpha also worked well on other indices, showing its effectiveness across different market conditions.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u591a\u79cd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u6a21\u578b\u80fd\u8f83\u597d\u5206\u5272\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7c7b\u522b\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces contain many important social entities that can be identified from satellite images.</li>\n    <li>Current models are good at recognizing physical objects but struggle with socially defined categories like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg, which includes satellite images, maps, and labels for social entities.</li>\n    <li>Our method, called SocioReasoner, uses vision-language reasoning to identify these social entities more effectively.</li>\n    <li>Our experiments show that our approach outperforms existing models and works well even with unseen data.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>LongCat-Flash-Thinking-2601 \u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u51fa\u8272\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u4e00\u7cfb\u5217\u81ea\u4e3b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u81ea\u4e3b\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u3002</li>\n    <li>\u6a21\u578b\u80fd\u591f\u5728\u590d\u6742\u5de5\u5177\u4e92\u52a8\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u566a\u58f0\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\u7684\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u201c\u91cd\u601d\u8003\u201d\u6a21\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5e76\u652f\u6301\u591a\u73af\u5883\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source reasoning model with 560 billion parameters that excels in agentic reasoning tasks.</li>\n    <li>The model performs exceptionally well on various benchmarks, showcasing its ability to handle complex tool interactions and work effectively in noisy environments.</li>\n    <li>It uses a unique training approach that combines multiple expert training and careful design of data and algorithms from beginning to end.</li>\n    <li>To improve performance in real-world tasks, the model incorporates noise patterns and has been trained in over 10,000 different environments.</li>\n    <li>It features a Heavy Thinking mode that enhances reasoning capabilities by allowing deeper and broader thinking during testing.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08794", "authors": [{"_id": "698ac65d1b2dc6b37d61b1c2", "name": "SII-OpenMOSS Team", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c4", "name": "Donghua Yu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c5", "name": "Mingshu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c6", "name": "Qi Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c7", "name": "Qi Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c8", "name": "Qianyi Wu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c9", "user": {"_id": "63ec4715c81b6a52391c46b8", "avatarUrl": "/avatars/496819b5075a1a834a2b9edeb068c80e.svg", "isPro": false, "fullname": "QinyuanCheng", "user": "Cqy2019", "type": "user"}, "name": "Qinyuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:07.400Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ca", "name": "Ruixiao Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cb", "user": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "name": "Tianyi Liang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:10.522Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cc", "name": "Wenbo Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cd", "name": "Wenming Tu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ce", "name": "Xiangyu Peng", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cf", "name": "Yang Gao", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d0", "name": "Yanru Huo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d1", "user": {"_id": "69158ffc0153b85a677dcc46", "avatarUrl": "/avatars/c9c5f60522f2a8f370d790ea9938b090.svg", "isPro": false, "fullname": "Ying Zhu", "user": "Auraithm", "type": "user"}, "name": "Ying Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:27:41.440Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d2", "name": "Yinze Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d3", "name": "Yiyang Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d4", "name": "Yuerong Song", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d5", "name": "Zhe Xu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d6", "name": "Zhiyu Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d7", "name": "Chenchen Yang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d8", "name": "Cheng Chang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d9", "name": "Chushu Zhou", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1da", "name": "Hanfu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1db", "name": "Hongnan Ma", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dc", "name": "Jiaxi Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dd", "name": "Jingqi Tong", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1de", "name": "Junxi Liu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1df", "name": "Ke Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e0", "name": "Shimin Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e1", "name": "Songlin Wang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e2", "name": "Wei Jiang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e3", "name": "Zhaoye Fei", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e4", "name": "Zhiyuan Ning", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e5", "name": "Chunguo Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e6", "name": "Chenhui Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e7", "name": "Ziwei He", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e8", "name": "Zengfeng Huang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e9", "name": "Xie Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ea", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-02-09T15:31:54.000Z", "submittedOnDailyAt": "2026-02-10T03:18:59.260Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "submittedOnDailyBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "upvotes": 131, "discussionId": "698ac65e1b2dc6b37d61b1eb", "projectPage": "https://mosi.cn/models/mova", "githubRepo": "https://github.com/OpenMOSS/MOVA", "githubRepoAddedBy": "user", "ai_summary": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.", "ai_keywords": ["Mixture-of-Experts", "MoE", "audio-visual content", "lip-synced speech", "sound effects", "content-aligned music", "IT2VA", "efficient inference", "LoRA fine-tuning", "prompt enhancement"], "githubStars": 579, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u97f3\u9891\u5728\u73b0\u5b9e\u89c6\u9891\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u4e3b\u8981\u5ffd\u7565\u4e86\u97f3\u9891\u90e8\u5206\u3002</li>\n    <li>\u76ee\u524d\u7684\u97f3\u89c6\u9891\u5185\u5bb9\u751f\u6210\u65b9\u6cd5\u5e38\u5e38\u4f9d\u8d56\u7ea7\u8054\u7ba1\u9053\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>MOVA\uff08MOSS \u89c6\u9891\u548c\u97f3\u9891\uff09\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u97f3\u89c6\u9891\u5185\u5bb9\uff0c\u5305\u62ec\u903c\u771f\u7684\u53e3\u578b\u540c\u6b65\u8bed\u97f3\u548c\u73af\u5883\u97f3\u6548\u3002</li>\n    <li>MOVA\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u603b\u5171\u6709320\u4ebf\u4e2a\u53c2\u6570\uff0c\u5176\u4e2d180\u4ebf\u4e2a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6d3b\u8dc3\u3002</li>\n    <li>\u901a\u8fc7\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\uff0c\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u5e76\u4fc3\u8fdb\u521b\u4f5c\u8005\u793e\u533a\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Audio is crucial for video, but many models do not focus on audio generation.</li>\n    <li>Current methods often use separate processes for audio and video, which can lead to higher costs and lower quality.</li>\n    <li>MOVA (MOSS Video and Audio) is a new open-source model that creates high-quality audio-visual content with synchronized audio, including lip-synced speech and sound effects.</li>\n    <li>The model has a large architecture with 32 billion parameters, making it capable of advanced tasks like generating video from images and text.</li>\n    <li>By providing the model's code and weights, the creators aim to support research and encourage a community of developers and creators.</li>\n</ul>"}, "publishedAt": "2026-02-09T10:31:54.000Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08794.png", "numComments": 1, "submittedBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "fullname": "SII-liangtianyi", "name": "tianyilt", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 12, 2026";
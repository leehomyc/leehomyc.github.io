window.trendingPapers = {
    "today": [{"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u5f88\u591a\u5e94\u7528\u5f88\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u9ad8\u7ea7\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas contain many different types of features, which are important to identify from satellite images.</li>\n    <li>Current models can identify physical features like buildings and water but struggle with socially defined features like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg, which includes satellite images, digital maps, and detailed labels for social features.</li>\n    <li>We developed a new method called SocioReasoner that mimics how humans identify and label these social features using advanced reasoning techniques.</li>\n    <li>Our approach shows better performance than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7ba1\u9053\u5b9e\u73b0\uff0c\u5177\u67091.2T\u591a\u6a21\u6001\u6570\u636e\u7684\u8bad\u7ec3\u57fa\u7840\u3002</li>\n    <li>\u91c7\u7528\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6280\u672f\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u89c6\u89c9\u63a8\u7406\u3002</li>\n    <li>\u5c3d\u7ba1\u4f53\u79ef\u5c0f\uff08\u4ec510\u4ebf\u53c2\u6570\uff09\uff0cSTEP3-VL-10B \u7684\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u592710\u523020\u500d\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>It uses a combined pre-training approach on a large dataset to improve how it understands both language and vision.</li>\n    <li>The model features a unique reasoning method called Parallel Coordinated Reasoning (PaCoRe) to enhance its performance during testing.</li>\n    <li>Despite being smaller in size, STEP3-VL-10B performs as well as or better than much larger models and top proprietary models.</li>\n    <li>The developers have released the model for public use, aiming to provide a strong and efficient resource for the community.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u4f46\u5b83\u5e38\u5e38\u9762\u4e34\u201c\u63a2\u7d22\u5d29\u6e83\u201d\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u5728\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u90a3\u4e9b\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u7ed3\u679c\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u6839\u636e\u805a\u7c7b\u5927\u5c0f\u8c03\u6574\u7b56\u7565\u4f18\u52bf\u7684\u6743\u91cd\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u901a\u8fc7\u7387\uff08pass@k\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u8fc7\u7387\uff08pass@1\uff09\uff0c\u5e76\u53d1\u73b0\u4e86\u66f4\u591a\u591a\u6837\u5316\u7684\u89e3\u51b3\u7b56\u7565\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning is key for improving large language models, especially for reasoning tasks.</li>\n    <li>Current methods often focus too much on common reasoning patterns, which limits diversity in solutions.</li>\n    <li>The proposed method, Uniqueness-Aware Reinforcement Learning, rewards unique and correct solutions with rare strategies.</li>\n    <li>This approach uses a judge to group similar solutions and adjusts rewards based on the uniqueness of these groups.</li>\n    <li>The method shows improved performance across various benchmarks without losing effectiveness on common tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09667", "authors": [{"_id": "6969b0f732f0333869ff9476", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:48.445Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9477", "user": {"_id": "662b4e3bc709a61df840fda1", "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg", "isPro": false, "fullname": "Hu Yunhai", "user": "AlexCCtop", "type": "user"}, "name": "Yunhai Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:37:06.706Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9478", "user": {"_id": "650026d30339dae3dba2cec5", "avatarUrl": "/avatars/fcc9ea4336f8d4bb177e5c9eacdd05c9.svg", "isPro": false, "fullname": "Juncheng Liu", "user": "juncliu", "type": "user"}, "name": "Juncheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:33.401Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9479", "name": "Shuyue Stella Li", "hidden": false}, {"_id": "6969b0f732f0333869ff947a", "name": "Yucheng Wang", "hidden": false}, {"_id": "6969b0f732f0333869ff947b", "user": {"_id": "638e40d450a4e4beef98196b", "avatarUrl": "/avatars/fe27e019baf48caeb44e19b7289db9fb.svg", "isPro": false, "fullname": "Zhen Xu", "user": "zhenxu", "type": "user"}, "name": "Zhen Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:04.868Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947c", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0f732f0333869ff947d", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:15.855Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947e", "name": "Xinxing Xu", "hidden": false}, {"_id": "6969b0f732f0333869ff947f", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:25.577Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9480", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:31.289Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9481", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:36.481Z", "hidden": false}], "publishedAt": "2026-01-14T17:57:43.000Z", "submittedOnDailyAt": "2026-01-16T01:01:32.343Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "upvotes": 63, "discussionId": "6969b0f832f0333869ff9482", "ai_summary": "Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.", "ai_keywords": ["multi-agent systems", "reinforcement learning", "test-time reinforcement learning", "multi-agent reinforcement learning", "credit assignment", "multi-expert teams", "dialogue systems", "distribution-shift-robust reasoning"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5df2\u6210\u4e3a\u8bb8\u591a\u5e94\u7528\u7684\u5f3a\u5927\u5408\u4f5c\u4f19\u4f34\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8017\u8d44\u6e90\u4e14\u4e0d\u7a33\u5b9a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08MATTRL\uff09\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u7ed3\u6784\u5316\u7684\u6587\u672c\u7ecf\u9a8c\u878d\u5165\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u3002</li>\n    <li>MATTRL \u7ec4\u5efa\u4e86\u4e00\u4e2a\u591a\u4e13\u5bb6\u56e2\u961f\uff0c\u7528\u4e8e\u591a\u8f6e\u8ba8\u8bba\uff0c\u6574\u5408\u6d4b\u8bd5\u65f6\u7684\u7ecf\u9a8c\u5e76\u8fbe\u6210\u4e00\u81f4\u51b3\u7b56\u3002</li>\n    <li>\u5728\u533b\u5b66\u3001\u6570\u5b66\u548c\u6559\u80b2\u7b49\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATTRL \u7684\u51c6\u786e\u6027\u5e73\u5747\u63d0\u9ad8\u4e86 3.67%\u3002</li>\n    <li>\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u624b\u52a8\u8c03\u6574\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems can work well together, but training them is often hard and requires a lot of resources.</li>\n    <li>Introducing Multi-Agent Test-Time Reinforcement Learning (MATTRL) helps improve how these agents collaborate during decision-making.</li>\n    <li>MATTRL uses specialized teams for discussions, retrieves relevant experiences, and helps agents agree on decisions.</li>\n    <li>The approach shows a significant accuracy improvement in various fields like medicine and education compared to other methods.</li>\n    <li>Different strategies for assigning credit in the system were tested to see how they impact training results.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:57:43.000Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09667.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02242", "authors": [{"_id": "695cafde6aa73bc11f091566", "user": {"_id": "65e7151ef7c2e46887e225b1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e7151ef7c2e46887e225b1/NXj_CrUkzwdUT8T3a-H8N.jpeg", "isPro": false, "fullname": "Grigorii Alekseenko", "user": "Riko0", "type": "user"}, "name": "Grigorii Alekseenko", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:36:29.469Z", "hidden": false}, {"_id": "695cafde6aa73bc11f091567", "user": {"_id": "65ae526111a0a3ff61d7d726", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ae526111a0a3ff61d7d726/x6OVUbowh-eXH9bxWCERB.jpeg", "isPro": false, "fullname": "Aleksandr", "user": "grac20101", "type": "user"}, "name": "Aleksandr Gordeev", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T14:16:49.474Z", "hidden": false}, {"_id": "695cafde6aa73bc11f091568", "user": {"_id": "6498095fce9190ebb8699113", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png", "isPro": true, "fullname": "Irina Tolstykh", "user": "iitolstykh", "type": "user"}, "name": "Irina Tolstykh", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T12:42:37.939Z", "hidden": false}, {"_id": "695cafde6aa73bc11f091569", "name": "Bulat Suleimanov", "hidden": false}, {"_id": "695cafde6aa73bc11f09156a", "name": "Vladimir Dokholyan", "hidden": false}, {"_id": "695cafde6aa73bc11f09156b", "name": "Georgii Fedorov", "hidden": false}, {"_id": "695cafde6aa73bc11f09156c", "name": "Sergey Yakubson", "hidden": false}, {"_id": "695cafde6aa73bc11f09156d", "name": "Aleksandra Tsybina", "hidden": false}, {"_id": "695cafde6aa73bc11f09156e", "name": "Mikhail Chernyshov", "hidden": false}, {"_id": "695cafde6aa73bc11f09156f", "user": {"_id": "6416d8ef8f689506e70dd2e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1679218871593-noauth.jpeg", "isPro": false, "fullname": "Maksim Kuprashevich", "user": "WildChlamydia", "type": "user"}, "name": "Maksim Kuprashevich", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:34:25.729Z", "hidden": false}], "publishedAt": "2026-01-05T16:17:20.000Z", "submittedOnDailyAt": "2026-01-16T08:00:29.947Z", "title": "VIBE: Visual Instruction Based Editor", "submittedOnDailyBy": {"_id": "6498095fce9190ebb8699113", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png", "isPro": true, "fullname": "Irina Tolstykh", "user": "iitolstykh", "type": "user"}, "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "upvotes": 46, "discussionId": "695cafde6aa73bc11f091570", "projectPage": "https://riko0.github.io/VIBE/", "githubRepo": "https://github.com/ai-forever/vibe", "githubRepoAddedBy": "user", "ai_summary": "A compact image editing system uses a 2B-parameter model for guidance and a 1.6B-parameter diffusion model to achieve high-quality edits with low computational requirements and strict source consistency.", "ai_keywords": ["diffusion models", "Qwen3-VL", "Sana1.5", "instruction-based image editing", "image generation", "source consistency", "inference efficiency", "parameter-efficient models", "ImgEdit benchmark", "GEdit benchmark"], "githubStars": 22, "summary_zh": "<ul>\n    <li>\u6307\u4ee4\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u662f\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u4e2d\u53d1\u5c55\u6700\u5feb\u7684\u9886\u57df\u4e4b\u4e00\u3002</li>\n    <li>\u76ee\u524d\u53ea\u6709\u5c11\u6570\u5f00\u6e90\u65b9\u6cd5\u80fd\u8fbe\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u9ad8\u8d28\u91cf\u6807\u51c6\uff0c\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u901a\u5e38\u5f88\u5927\u4e14\u8ba1\u7b97\u5f00\u9500\u9ad8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u9ad8\u6548\u7684\u56fe\u50cf\u7f16\u8f91\u7ba1\u9053\uff0c\u4f7f\u75282B\u53c2\u6570\u7684Qwen3-VL\u6a21\u578b\u548c1.6B\u53c2\u6570\u7684Sana1.5\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u63a8\u7406\u548c\u4e25\u683c\u7684\u6e90\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5728ImgEdit\u548cGEdit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4fdd\u7559\u8f93\u5165\u56fe\u50cf\u7684\u7f16\u8f91\u4efb\u52a1\u4e0a\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Instruction-based image editing is rapidly advancing in generative AI, with many new open-source and commercial models released.</li>\n    <li>Most open-source models do not yet meet real-world quality standards, and popular models are often very large and costly to run.</li>\n    <li>This paper introduces a compact image editing pipeline using a smaller 2B-parameter model for guidance and a 1.6B-parameter model for image creation.</li>\n    <li>The new method aims for low-cost performance while maintaining high-quality results, especially for edits that need to keep the original image intact.</li>\n    <li>It performs well on benchmarks, uses less GPU memory, and can generate images quickly, achieving good results even with fewer resources.</li>\n</ul>"}, "publishedAt": "2026-01-05T11:17:20.000Z", "title": "VIBE: Visual Instruction Based Editor", "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02242.png", "numComments": 2, "submittedBy": {"_id": "6498095fce9190ebb8699113", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6498095fce9190ebb8699113/ZQi6EFxaiz6IreEda3uf2.png", "fullname": "Irina Tolstykh", "name": "iitolstykh", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 27, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07641", "authors": [{"_id": "696745d2c5e371f6b235d1f1", "user": {"_id": "67e655f7d6b8333a8f78eadf", "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg", "isPro": false, "fullname": "Jiaxuan Lu", "user": "Blue-Giant", "type": "user"}, "name": "Jiaxuan Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:53.789Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f2", "user": {"_id": "65617bf9f5532ac1bde64d07", "avatarUrl": "/avatars/6659fe26eecfce8ac699caa73b823fe1.svg", "isPro": false, "fullname": "ZIYU KONG", "user": "ziyukong", "type": "user"}, "name": "Ziyu Kong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:59.511Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f3", "name": "Yemin Wang", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f4", "name": "Rong Fu", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f5", "user": {"_id": "691b0f528411a45dc9ee9de8", "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg", "isPro": false, "fullname": "Haiyuan Wan", "user": "HY-Wan", "type": "user"}, "name": "Haiyuan Wan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:47.426Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f6", "user": {"_id": "67c443afb753bd020f9c97d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png", "isPro": false, "fullname": "Cheng", "user": "YangC777", "type": "user"}, "name": "Cheng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:18.927Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f7", "name": "Wenjie Lou", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f8", "name": "Haoran Sun", "hidden": false}, {"_id": "696745d2c5e371f6b235d1f9", "user": {"_id": "67fc7887864dfcbd93ce6322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1ecRk5ZWDXALss7e4fjtQ.png", "isPro": false, "fullname": "Lilong Wang", "user": "Eason2025", "type": "user"}, "name": "Lilong Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:20.390Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fa", "user": {"_id": "671280b9a895018bbc281dea", "avatarUrl": "/avatars/55f3c2d3698011bc718ec18295519caa.svg", "isPro": false, "fullname": "Yankai Jiang", "user": "yankaijiang", "type": "user"}, "name": "Yankai Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:26.741Z", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fb", "name": "Xiaosong Wang", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fc", "name": "Xiao Sun", "hidden": false}, {"_id": "696745d2c5e371f6b235d1fd", "user": {"_id": "6538b861613fe158bd581e35", "avatarUrl": "/avatars/6817dbfe903675721fd227058b0a91ac.svg", "isPro": false, "fullname": "Dongzhan Zhou", "user": "schrodingers-tiger", "type": "user"}, "name": "Dongzhan Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:36.629Z", "hidden": false}], "publishedAt": "2026-01-12T15:22:51.000Z", "submittedOnDailyAt": "2026-01-16T05:19:21.135Z", "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning", "submittedOnDailyBy": {"_id": "67e655f7d6b8333a8f78eadf", "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg", "isPro": false, "fullname": "Jiaxuan Lu", "user": "Blue-Giant", "type": "user"}, "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.", "upvotes": 35, "discussionId": "696745d2c5e371f6b235d1fe", "githubRepo": "https://github.com/lujiaxuan0520/Test-Time-Tool-Evol", "githubRepoAddedBy": "user", "ai_summary": "Test-Time Tool Evolution enables AI agents to dynamically create and refine computational tools during inference, overcoming limitations of static tool libraries in scientific applications.", "ai_keywords": ["LLM-based agents", "tool libraries", "scientific reasoning", "computational methods", "test-time tool evolution", "SciEvo benchmark", "tool synthesis", "tool verification", "tool evolution", "cross-domain adaptation"], "githubStars": 34, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>AI\u5728\u79d1\u5b66\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\u662f\u521b\u9020\u5f00\u653e\u5f0f\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u4f9d\u8d56\u4e8e\u9759\u6001\u5de5\u5177\u5e93\uff0c\u8fd9\u5728\u79d1\u5b66\u9886\u57df\u662f\u6709\u9650\u7684\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u6d4b\u8bd5\u65f6\u5de5\u5177\u8fdb\u5316\uff08TTE\uff09\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5408\u6210\u3001\u9a8c\u8bc1\u548c\u8fdb\u5316\u53ef\u6267\u884c\u5de5\u5177\u3002</li>\n    <li>TTE\u5c06\u5de5\u5177\u8f6c\u53d8\u4e3a\u4ee5\u95ee\u9898\u4e3a\u9a71\u52a8\u7684\u6210\u679c\uff0c\u4ece\u800c\u514b\u670d\u4e86\u9759\u6001\u5de5\u5177\u5e93\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86SciEvo\u57fa\u51c6\uff0c\u5305\u542b1590\u4e2a\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u548c925\u4e2a\u81ea\u52a8\u8fdb\u5316\u7684\u5de5\u5177\uff0c\u5b9e\u9a8c\u8bc1\u660eTTE\u5728\u51c6\u786e\u6027\u548c\u5de5\u5177\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The main issue in using AI for science is not just reasoning, but also creating flexible tools for various scientific problems.</li>\n    <li>Current AI models depend on fixed tool libraries, which are not suitable for the diverse and incomplete nature of scientific tools.</li>\n    <li>This paper introduces a new method called Test-Time Tool Evolution (TTE) that allows AI to create and improve tools while solving problems.</li>\n    <li>TTE helps overcome the limitations of static tool libraries by making tools that adapt to specific scientific challenges.</li>\n    <li>A new benchmark called SciEvo has been created, featuring many scientific tasks and evolved tools, showing that TTE performs very well in accuracy and efficiency.</li>\n</ul>"}, "publishedAt": "2026-01-12T10:22:51.000Z", "title": "Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning", "summary": "The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07641.png", "numComments": 1, "submittedBy": {"_id": "67e655f7d6b8333a8f78eadf", "avatarUrl": "/avatars/11cc80d3c03747fd869e4dc1dbdd031a.svg", "fullname": "Jiaxuan Lu", "name": "Blue-Giant", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10305", "authors": [{"_id": "6969a0b932f0333869ff9381", "user": {"_id": "67e289aea1e569cd0a41db1d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/w7b_6u7nZDH9Lp6NJKdVJ.png", "isPro": false, "fullname": "shen hengyu", "user": "dewecho", "type": "user"}, "name": "Hengyu Shen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:54.646Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9382", "user": {"_id": "641030c77a15af878ae5bd8f", "avatarUrl": "/avatars/8a5037edf55c78ebc317c8b191343671.svg", "isPro": false, "fullname": "TianchengGu", "user": "TianchengGu", "type": "user"}, "name": "Tiancheng Gu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:00.059Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9383", "name": "Bin Qin", "hidden": false}, {"_id": "6969a0b932f0333869ff9384", "name": "Lan Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff9385", "name": "Yuling Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff9386", "name": "Shuo Tan", "hidden": false}, {"_id": "6969a0b932f0333869ff9387", "user": {"_id": "63dfc05342591dda0b945e58", "avatarUrl": "/avatars/3fd796035c2243d6b03cc361bc06e64e.svg", "isPro": false, "fullname": "Zelong Sun", "user": "dfgdgh", "type": "user"}, "name": "Zelong Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:29.711Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9388", "name": "Jun Wang", "hidden": false}, {"_id": "6969a0b932f0333869ff9389", "name": "Nan Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff938a", "name": "Xiang An", "hidden": false}, {"_id": "6969a0b932f0333869ff938b", "user": {"_id": "6760a8f5e4b55ba1b2b0a7b4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NddUMmwmZFbS25v1q8KyS.png", "isPro": false, "fullname": "Weidong Cai", "user": "SeriousBro", "type": "user"}, "name": "Weidong Cai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:16.748Z", "hidden": false}, {"_id": "6969a0b932f0333869ff938c", "user": {"_id": "694d00c3ece16a65e2b84774", "avatarUrl": "/avatars/72bfeec4602ba4069faf0dba02c2be96.svg", "isPro": false, "fullname": "Ziyong Feng", "user": "fengziyong", "type": "user"}, "name": "Ziyong Feng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:10.787Z", "hidden": false}, {"_id": "6969a0b932f0333869ff938d", "user": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "isPro": false, "fullname": "Kaicheng Yang", "user": "Kaichengalex", "type": "user"}, "name": "Kaicheng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:30:51.089Z", "hidden": false}], "publishedAt": "2026-01-15T11:28:58.000Z", "submittedOnDailyAt": "2026-01-16T00:37:46.383Z", "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset", "submittedOnDailyBy": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "isPro": false, "fullname": "Kaicheng Yang", "user": "Kaichengalex", "type": "user"}, "summary": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.", "upvotes": 29, "discussionId": "6969a0b932f0333869ff938e", "projectPage": "https://deepglint.github.io/DanQing/", "githubRepo": "https://github.com/deepglint/DanQing", "githubRepoAddedBy": "user", "ai_summary": "A large-scale Chinese image-text dataset called DanQing is introduced to advance vision-language pretraining, demonstrating superior performance in various downstream tasks through continual pretraining of the SigLIP2 model.", "ai_keywords": ["Vision-Language Pre-training", "contrastive pretraining", "cross-modal retrieval", "image captioning", "SigLIP2", "continual pre-training"], "githubStars": 12, "summary_zh": "<ul>\n    <li>Vision-Language\u9884\u8bad\u7ec3\u6a21\u578b\u901a\u8fc7\u5bf9\u5927\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u8fdb\u884c\u5bf9\u6bd4\u9884\u8bad\u7ec3\uff0c\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>\u867d\u7136\u5df2\u6709\u5f88\u591a\u82f1\u6587\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\uff0c\u4f46\u4e2d\u6587\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u53d1\u5c55\u4ecd\u7136\u6ede\u540e\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u6570\u636e\u3002</li>\n    <li>\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6784\u5efa\u9ad8\u8d28\u91cf\u4e2d\u6587\u8de8\u6a21\u6001\u6570\u636e\u96c6\u7684\u5168\u9762\u6d41\u7a0b\uff0c\u63a8\u51fa\u4e86DanQing\u6570\u636e\u96c6\uff0c\u5305\u542b1\u4ebf\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u3002</li>\n    <li>DanQing\u7684\u6570\u636e\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\uff0c\u4e3b\u8981\u6765\u6e90\u4e8e2024-2025\u5e74\u7684\u7f51\u7edc\u6570\u636e\uff0c\u66f4\u597d\u5730\u6355\u6349\u8bed\u4e49\u8d8b\u52bf\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDanQing\u5728\u4e2d\u6587\u7684\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6211\u4eec\u5c06\u4ee5CC-BY 4.0\u8bb8\u53ef\u8bc1\u5f00\u6e90\u8be5\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language Pre-training (VLP) models are effective for tasks like image captioning and retrieval using large image-text datasets.</li>\n    <li>Chinese VLP models have not progressed as much due to a lack of quality Chinese image-text data.</li>\n    <li>A new dataset called DanQing has been created, containing 100 million high-quality Chinese image-text pairs from the web.</li>\n    <li>DanQing is curated with a rigorous selection process and includes recent web data, which helps models understand current trends.</li>\n    <li>Experiments show that DanQing improves performance in various Chinese tasks and will be available for research under a Creative Commons license.</li>\n</ul>"}, "publishedAt": "2026-01-15T06:28:58.000Z", "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset", "summary": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10305.png", "numComments": 2, "submittedBy": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "fullname": "Kaicheng Yang", "name": "Kaichengalex", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10402", "authors": [{"_id": "6969f54c844a787c4fdea404", "user": {"_id": "65352517fca2c10e43035e32", "avatarUrl": "/avatars/f404cef8a79f27435865ac4d85ef0927.svg", "isPro": false, "fullname": "xinyuzhu", "user": "xinyuzhu", "type": "user"}, "name": "Xinyu Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:28:31.884Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea405", "user": {"_id": "6614ed4809c63bfbddc53ddf", "avatarUrl": "/avatars/018bfc168bb4010cf6018e42148e0f51.svg", "isPro": false, "fullname": "Yuzhu Cai", "user": "Ethical-Lens", "type": "user"}, "name": "Yuzhu Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:36:59.231Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea406", "user": {"_id": "651e1f5522890326303fb1f4", "avatarUrl": "/avatars/4e47b8d7669c3aafcab4e1438ed386e7.svg", "isPro": false, "fullname": "Zexi Liu", "user": "ZeroXLeo", "type": "user"}, "name": "Zexi Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:28:33.861Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea407", "user": {"_id": "690320c2eba76a9b99854b4d", "avatarUrl": "/avatars/a612962c3627bdb7986a5ab5f14af2eb.svg", "isPro": false, "fullname": "Bingyang Zheng", "user": "bulibuliyang", "type": "user"}, "name": "Bingyang Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:38:51.672Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea408", "name": "Cheng Wang", "hidden": false}, {"_id": "6969f54c844a787c4fdea409", "name": "Rui Ye", "hidden": false}, {"_id": "6969f54c844a787c4fdea40a", "user": {"_id": "64f793d4dcd7b028c15bbe50", "avatarUrl": "/avatars/fb5d7392736309dd5c80ac32750d164b.svg", "isPro": false, "fullname": "Jiaao Chen", "user": "Jiaaoc", "type": "user"}, "name": "Jiaao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:08.324Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea40b", "user": {"_id": "677e237c70bd7b5431f88450", "avatarUrl": "/avatars/cc64b0af06588e43c94c185900362751.svg", "isPro": false, "fullname": "Hanrui Wang", "user": "azrealwang", "type": "user"}, "name": "Hanrui Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:14.281Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea40c", "user": {"_id": "68c08c9da619a2f12eae4913", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FYHEbHaFSOcWScBmeHQcF.png", "isPro": false, "fullname": "Wei Chen Wang", "user": "bumbigby", "type": "user"}, "name": "Wei-Chen Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:21.035Z", "hidden": false}, {"_id": "6969f54c844a787c4fdea40d", "name": "Yuzhi Zhang", "hidden": false}, {"_id": "6969f54c844a787c4fdea40e", "name": "Linfeng Zhang", "hidden": false}, {"_id": "6969f54c844a787c4fdea40f", "name": "Weinan E", "hidden": false}, {"_id": "6969f54c844a787c4fdea410", "name": "Di Jin", "hidden": false}, {"_id": "6969f54c844a787c4fdea411", "user": {"_id": "65257545b017be1fc1915364", "avatarUrl": "/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg", "isPro": false, "fullname": "Siheng Chen", "user": "sihengchen", "type": "user"}, "name": "Siheng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:39:39.159Z", "hidden": false}], "publishedAt": "2026-01-15T13:52:04.000Z", "submittedOnDailyAt": "2026-01-16T06:18:42.647Z", "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "submittedOnDailyBy": {"_id": "6614ed4809c63bfbddc53ddf", "avatarUrl": "/avatars/018bfc168bb4010cf6018e42148e0f51.svg", "isPro": false, "fullname": "Yuzhu Cai", "user": "Ethical-Lens", "type": "user"}, "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "upvotes": 26, "discussionId": "6969f54c844a787c4fdea412", "projectPage": "https://sjtu-sai-agents.github.io/ML-Master/", "githubRepo": "https://github.com/sjtu-sai-agents/ML-Master", "githubRepoAddedBy": "user", "ai_summary": "ML-Master 2.0 enables long-term autonomous machine learning engineering through hierarchical cognitive caching that manages extended context and learns from execution traces.", "ai_keywords": ["Large Language Models", "ultra-long-horizon autonomy", "machine learning engineering", "Hierarchical Cognitive Caching", "cognitive accumulation", "context management", "experimental strategy", "execution traces", "cross-task wisdom"], "githubStars": 332, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u4eba\u5de5\u667a\u80fd\u5728\u81ea\u4e3b\u79d1\u5b66\u7814\u7a76\u65b9\u9762\u7684\u8fdb\u5c55\u53d7\u9650\u4e8e\u8d85\u957f\u65f6\u95f4\u72ec\u7acb\u6027\u7684\u6311\u6218\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u671f\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u6218\u7565\u4e00\u81f4\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ML-Master 2.0\uff0c\u4e00\u4e2a\u80fd\u591f\u638c\u63e1\u8d85\u957f\u65f6\u95f4\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u7684\u81ea\u4e3b\u4ee3\u7406\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u8ba4\u77e5\u7f13\u5b58\uff08HCC\uff09\uff0c\u8be5\u65b9\u6cd5\u6539\u5584\u4e86\u77e5\u8bc6\u7ba1\u7406\u548c\u7ecf\u9a8c\u79ef\u7d2f\u3002</li>\n    <li>\u5728OpenAI\u7684MLE-Bench\u6d4b\u8bd5\u4e2d\uff0cML-Master 2.0\u7684\u8868\u73b0\u8fbe\u5230\u4e8656.44%\u7684\u6700\u4f73\u6210\u7ee9\uff0c\u663e\u793a\u51fa\u8d85\u957f\u65f6\u95f4\u81ea\u4e3b\u6027\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AI development is currently facing challenges in maintaining long-term strategic thinking over extended periods, such as days or weeks.</li>\n    <li>Large Language Models (LLMs) perform well in short tasks but struggle with complex, real-world situations that require ongoing adjustments and learning.</li>\n    <li>ML-Master 2.0 is introduced as an AI agent designed to handle long-term machine learning projects effectively.</li>\n    <li>This AI uses a method called Hierarchical Cognitive Caching (HCC) to manage and learn from experiences over time, improving its decision-making process.</li>\n    <li>In tests, ML-Master 2.0 achieved a high success rate, showing promise for future AI that can independently tackle complex tasks without human intervention.</li>\n</ul>"}, "publishedAt": "2026-01-15T08:52:04.000Z", "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10402.png", "numComments": 1, "submittedBy": {"_id": "6614ed4809c63bfbddc53ddf", "avatarUrl": "/avatars/018bfc168bb4010cf6018e42148e0f51.svg", "fullname": "Yuzhu Cai", "name": "Ethical-Lens", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10061", "authors": [{"_id": "6969aeaf32f0333869ff9459", "name": "Chengzhuo Tong", "hidden": false}, {"_id": "6969aeaf32f0333869ff945a", "user": {"_id": "662db438137b72821671db2f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hLMBYuvyF6sfkS0sSrDt-.jpeg", "isPro": false, "fullname": "Mingkun Chang", "user": "D4isyC", "type": "user"}, "name": "Mingkun Chang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:17.228Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945b", "user": {"_id": "67c14a89f85f9a6c361226ba", "avatarUrl": "/avatars/538eede44205f49fe5a562dcce992d7c.svg", "isPro": false, "fullname": "shenglong", "user": "zhangshenglong", "type": "user"}, "name": "Shenglong Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:31.154Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945c", "user": {"_id": "65e71ef39cf349af2940b317", "avatarUrl": "/avatars/fc1cd8d3510946fc947d67b16b51834b.svg", "isPro": false, "fullname": "Yuran Wang", "user": "Ryann829", "type": "user"}, "name": "Yuran Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:37.205Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945d", "name": "Cheng Liang", "hidden": false}, {"_id": "6969aeaf32f0333869ff945e", "user": {"_id": "6713a71e7dfe714b425cccfb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/95YYcbv_f6J8yWTunwn4z.png", "isPro": false, "fullname": "zhizhengzhao", "user": "zhizhengzhao", "type": "user"}, "name": "Zhizheng Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:42.205Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff945f", "name": "Ruichuan An", "hidden": false}, {"_id": "6969aeaf32f0333869ff9460", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:37:03.210Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff9461", "user": {"_id": "673c7319d11b1c2e246ead9c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg", "isPro": false, "fullname": "Yang Shi", "user": "DogNeverSleep", "type": "user"}, "name": "Yang Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:39.706Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff9462", "name": "Yifan Dai", "hidden": false}, {"_id": "6969aeaf32f0333869ff9463", "user": {"_id": "68418019d777f13c594ffe5f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a2yvPF9IoCcXKFz7TkADe.png", "isPro": false, "fullname": "Ziming Zhao", "user": "ZimingZhao", "type": "user"}, "name": "Ziming Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:38:15.698Z", "hidden": false}, {"_id": "6969aeaf32f0333869ff9464", "name": "Guanbin Li", "hidden": false}, {"_id": "6969aeaf32f0333869ff9465", "name": "Pengfei Wan", "hidden": false}, {"_id": "6969aeaf32f0333869ff9466", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6969aeaf32f0333869ff9467", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2026-01-15T04:33:06.000Z", "submittedOnDailyAt": "2026-01-16T00:52:07.049Z", "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.", "upvotes": 25, "discussionId": "6969aeaf32f0333869ff9468", "projectPage": "https://cof-t2i.github.io/", "githubRepo": "https://github.com/VisionChengzhuo/CoF-T2I", "githubRepoAddedBy": "user", "ai_summary": "Chain-of-Frame reasoning is integrated into text-to-image generation through progressive visual refinement with explicit intermediate steps, achieving superior performance on benchmark datasets.", "ai_keywords": ["Chain-of-Frame reasoning", "text-to-image generation", "progressive visual refinement", "CoF trajectories", "GenEval", "Imagine-Bench"], "githubStars": 18, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5c55\u793a\u4e86\u201c\u9010\u5e27\u63a8\u7406\u201d\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u9010\u5e27\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\uff0c\u4f46\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CoF-T2I\u6a21\u578b\uff0c\u5c06\u9010\u5e27\u63a8\u7406\u6574\u5408\u5230\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6CoF-Evol-Instruct\uff0c\u4ee5\u652f\u6301\u4ece\u8bed\u4e49\u5230\u7f8e\u5b66\u7684\u751f\u6210\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoF-T2I\u6a21\u578b\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u793a\u51fa\u89c6\u9891\u6a21\u578b\u5728\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New video models can reason frame-by-frame, helping with tasks like solving mazes and visual puzzles.</li>\n    <li>There's potential to use this reasoning in text-to-image (T2I) generation, but it's not fully explored yet.</li>\n    <li>The proposed model, CoF-T2I, uses frame-by-frame reasoning to improve T2I generation through a process of visual refinement.</li>\n    <li>A new dataset, CoF-Evol-Instruct, helps model the generation process from meaning to visual appeal.</li>\n    <li>Experiments show CoF-T2I performs much better than previous models and achieves strong results on key benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-14T23:33:06.000Z", "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation", "summary": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10061.png", "numComments": 1, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10332", "authors": [{"_id": "6969a7d132f0333869ff93bc", "name": "Siqi Kou", "hidden": false}, {"_id": "6969a7d132f0333869ff93bd", "name": "Jiachun Jin", "hidden": false}, {"_id": "6969a7d132f0333869ff93be", "name": "Zetong Zhou", "hidden": false}, {"_id": "6969a7d132f0333869ff93bf", "name": "Ye Ma", "hidden": false}, {"_id": "6969a7d132f0333869ff93c0", "name": "Yugang Wang", "hidden": false}, {"_id": "6969a7d132f0333869ff93c1", "name": "Quan Chen", "hidden": false}, {"_id": "6969a7d132f0333869ff93c2", "name": "Peng Jiang", "hidden": false}, {"_id": "6969a7d132f0333869ff93c3", "name": "Xiao Yang", "hidden": false}, {"_id": "6969a7d132f0333869ff93c4", "name": "Jun Zhu", "hidden": false}, {"_id": "6969a7d132f0333869ff93c5", "name": "Kai Yu", "hidden": false}, {"_id": "6969a7d132f0333869ff93c6", "name": "Zhijie Deng", "hidden": false}], "publishedAt": "2026-01-15T12:19:05.000Z", "submittedOnDailyAt": "2026-01-16T00:29:52.118Z", "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders", "submittedOnDailyBy": {"_id": "654e330f350abceb30a1390b", "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg", "isPro": false, "fullname": "KouSiqi", "user": "karrykkk", "type": "user"}, "summary": "Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.", "upvotes": 20, "discussionId": "6969a7d132f0333869ff93c7", "githubRepo": "https://github.com/zhijie-group/Think-Then-Generate", "githubRepoAddedBy": "user", "ai_summary": "Text-to-image diffusion models enhanced with language model reasoning capabilities achieve improved factual consistency and semantic alignment through a think-then-generate paradigm with dual-gradient reinforcement optimization.", "ai_keywords": ["text-to-image diffusion models", "language model-based text encoders", "think-then-generate paradigm", "supervised fine-tuning", "dual-gradient reinforcement optimization", "image-grounded rewards", "semantic alignment", "factual consistency", "visual realism"], "githubStars": 37, "organization": {"_id": "673d5fe8d031224e947dc235", "name": "SJTU-DENG-Lab", "fullname": "DENG Lab @ SJTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08T2I DMs\uff09\u53ef\u4ee5\u6839\u636e\u5404\u79cd\u6587\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002</li>\n    <li>\u5927\u591a\u6570\u73b0\u6709\u7684T2I DMs\u4ec5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u201c\u5148\u601d\u8003\u7136\u540e\u751f\u6210\u201d\uff08T2G\uff09\u8303\u5f0f\uff0c\u9f13\u52b1LLM\u5bf9\u7528\u6237\u63d0\u793a\u8fdb\u884c\u63a8\u7406\u548c\u91cd\u5199\u3002</li>\n    <li>\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u6211\u4eec\u6fc0\u6d3b\u4e86LLM\u7f16\u7801\u5668\u7684\u601d\u8003\u548c\u91cd\u5199\u6a21\u5f0f\uff0c\u786e\u4fdd\u751f\u6210\u7684\u5185\u5bb9\u66f4\u52a0\u4e00\u81f4\u548c\u51c6\u786e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New text-to-image models can create high-quality images from text descriptions.</li>\n    <li>Current models mainly just convert text to images without understanding the meaning behind them.</li>\n    <li>The proposed think-then-generate (T2G) approach helps the model think and rewrite the text before generating images.</li>\n    <li>This process includes fine-tuning the model to improve its reasoning abilities and ensure the images match the text better.</li>\n    <li>Tests show that this method greatly improves the accuracy and quality of the generated images.</li>\n</ul>"}, "publishedAt": "2026-01-15T07:19:05.000Z", "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders", "summary": "Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10332.png", "numComments": 2, "submittedBy": {"_id": "654e330f350abceb30a1390b", "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg", "fullname": "KouSiqi", "name": "karrykkk", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "673d5fe8d031224e947dc235", "name": "SJTU-DENG-Lab", "fullname": "DENG Lab @ SJTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64bba541da140e461924dfed/_WPqM9jCqIIkS73aTeZP-.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u7ebf\u7d22\u3001\u8fdb\u884c\u7f51\u9875\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u9a8c\u8bc1\u3002</li>\n    <li>\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u7c7b\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u83b7\u53d6\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u7814\u7a76\u6837\u672c\uff0c\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u3002</li>\n    <li>\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0Agentic\u65b9\u6cd5\u7684\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u4f9d\u8d56\u4e8e\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u76ee\u6807\u6f02\u79fb\u548c\u957f\u8fdc\u4e00\u81f4\u6027\u662f\u6838\u5fc3\u74f6\u9888\uff0cVideoDR\u4e3a\u672a\u6765\u7684\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\u548c\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often needs to combine information from videos and the web, making it complex.</li>\n    <li>The new benchmark, VideoDR, focuses on answering questions about videos using information from both the video and the internet.</li>\n    <li>VideoDR includes high-quality video samples from six different topics, created with careful human review.</li>\n    <li>Different models were tested, and findings showed that their success varies based on how well they can track video details during searches.</li>\n    <li>VideoDR helps identify important challenges for developing future tools that answer questions about videos using web content.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u5f88\u591a\u5e94\u7528\u5f88\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u9ad8\u7ea7\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas contain many different types of features, which are important to identify from satellite images.</li>\n    <li>Current models can identify physical features like buildings and water but struggle with socially defined features like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg, which includes satellite images, digital maps, and detailed labels for social features.</li>\n    <li>We developed a new method called SocioReasoner that mimics how humans identify and label these social features using advanced reasoning techniques.</li>\n    <li>Our approach shows better performance than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7ba1\u9053\u5b9e\u73b0\uff0c\u5177\u67091.2T\u591a\u6a21\u6001\u6570\u636e\u7684\u8bad\u7ec3\u57fa\u7840\u3002</li>\n    <li>\u91c7\u7528\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6280\u672f\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u89c6\u89c9\u63a8\u7406\u3002</li>\n    <li>\u5c3d\u7ba1\u4f53\u79ef\u5c0f\uff08\u4ec510\u4ebf\u53c2\u6570\uff09\uff0cSTEP3-VL-10B \u7684\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u592710\u523020\u500d\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>It uses a combined pre-training approach on a large dataset to improve how it understands both language and vision.</li>\n    <li>The model features a unique reasoning method called Parallel Coordinated Reasoning (PaCoRe) to enhance its performance during testing.</li>\n    <li>Despite being smaller in size, STEP3-VL-10B performs as well as or better than much larger models and top proprietary models.</li>\n    <li>The developers have released the model for public use, aiming to provide a strong and efficient resource for the community.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u4f46\u5b83\u5e38\u5e38\u9762\u4e34\u201c\u63a2\u7d22\u5d29\u6e83\u201d\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u5728\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u90a3\u4e9b\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u7ed3\u679c\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u6839\u636e\u805a\u7c7b\u5927\u5c0f\u8c03\u6574\u7b56\u7565\u4f18\u52bf\u7684\u6743\u91cd\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u901a\u8fc7\u7387\uff08pass@k\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u8fc7\u7387\uff08pass@1\uff09\uff0c\u5e76\u53d1\u73b0\u4e86\u66f4\u591a\u591a\u6837\u5316\u7684\u89e3\u51b3\u7b56\u7565\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning is key for improving large language models, especially for reasoning tasks.</li>\n    <li>Current methods often focus too much on common reasoning patterns, which limits diversity in solutions.</li>\n    <li>The proposed method, Uniqueness-Aware Reinforcement Learning, rewards unique and correct solutions with rare strategies.</li>\n    <li>This approach uses a judge to group similar solutions and adjusts rewards based on the uniqueness of these groups.</li>\n    <li>The method shows improved performance across various benchmarks without losing effectiveness on common tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u7684\u201c\u751f\u6210-\u9a8c\u8bc1-\u4f18\u5316\u201d\u5faa\u73af\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\u3002</li>\n    <li>\u4f4e\u6548\u7387\u7684\u539f\u56e0\u5305\u62ec\u521d\u59cb\u5316\u504f\u89c1\u3001\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u968f\u673a\u64cd\u4f5c\u548c\u4efb\u52a1\u95f4\u7ecf\u9a8c\u5229\u7528\u4e0d\u8db3\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53d7\u63a7\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002</li>\n    <li>CSE\u901a\u8fc7\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u53cd\u9988\u6307\u5bfc\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5206\u5c42\u8fdb\u5316\u8bb0\u5fc6\u6765\u4f18\u5316\u4ee3\u7801\u751f\u6210\u3002</li>\n    <li>\u5728EffiBench-X\u5b9e\u9a8c\u4e2d\uff0cCSE\u5728\u4e0d\u540c\u7684LLM\u57fa\u7840\u4e0a\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\uff0c\u5e76\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u6301\u7eed\u63d0\u9ad8\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods improve code generation but struggle with exploring better solutions efficiently.</li>\n    <li>Issues arise from biases in initial solutions, random operations without guidance, and poor use of experiences across tasks.</li>\n    <li>The proposed Controlled Self-Evolution (CSE) includes three main features: diverse planning for better strategy coverage, feedback-guided evolution for better mutations, and memory for learning from both successes and failures.</li>\n    <li>Tests on EffiBench-X show that CSE performs better than existing methods with various LLMs and improves quickly from early on.</li>\n    <li>The code for CSE is available online for public use at https://github.com/QuantaAlpha/EvoControl.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09688", "authors": [{"_id": "696864c90ac10a06522f6a4a", "name": "Yibo Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4b", "name": "Lei Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4c", "name": "Yue Deng", "hidden": false}, {"_id": "696864c90ac10a06522f6a4d", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:02:22.232Z", "hidden": false}, {"_id": "696864c90ac10a06522f6a4e", "name": "Yao Xiao", "hidden": false}, {"_id": "696864c90ac10a06522f6a4f", "name": "Huanjin Yao", "hidden": false}, {"_id": "696864c90ac10a06522f6a50", "name": "Liwei Kang", "hidden": false}, {"_id": "696864c90ac10a06522f6a51", "name": "Hai Ye", "hidden": false}, {"_id": "696864c90ac10a06522f6a52", "name": "Yongcheng Jing", "hidden": false}, {"_id": "696864c90ac10a06522f6a53", "name": "Lidong Bing", "hidden": false}], "publishedAt": "2026-01-14T18:38:31.000Z", "submittedOnDailyAt": "2026-01-15T01:33:59.520Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "submittedOnDailyBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "upvotes": 90, "discussionId": "696864c90ac10a06522f6a54", "githubRepo": "https://github.com/Infinity-AILab/DeepResearchEval", "githubRepoAddedBy": "user", "ai_summary": "DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.", "ai_keywords": ["automated framework", "deep research task construction", "agentic evaluation", "persona-driven pipeline", "task qualification", "search necessity", "adaptive point-wise quality evaluation", "active fact-checking", "web search", "multi-source evidence integration"], "githubStars": 67, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7528\u4e8e\u591a\u6b65\u9aa4\u7684\u7f51\u7edc\u7814\u7a76\u548c\u5206\u6790\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u4ecd\u7136\u5f88\u56f0\u96be\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\uff0c\u4f9d\u8d56\u9759\u6001\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6216\u5728\u7f3a\u5c11\u5f15\u7528\u65f6\u65e0\u6cd5\u53ef\u9760\u5730\u9a8c\u8bc1\u4e8b\u5b9e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DeepResearchEval\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u548c\u8bc4\u4f30\u3002</li>\n    <li>\u4efb\u52a1\u6784\u5efa\u91c7\u7528\u4ee5\u7528\u6237\u89d2\u8272\u4e3a\u9a71\u52a8\u7684\u6d41\u7a0b\uff0c\u751f\u6210\u590d\u6742\u7684\u7814\u7a76\u4efb\u52a1\uff0c\u5e76\u7b5b\u9009\u51fa\u9700\u8981\u591a\u6765\u6e90\u8bc1\u636e\u7684\u4efb\u52a1\u3002</li>\n    <li>\u8bc4\u4f30\u65b9\u9762\u5305\u62ec\u52a8\u6001\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\u7ef4\u5ea6\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u4ee5\u53ca\u81ea\u52a8\u63d0\u53d6\u548c\u9a8c\u8bc1\u62a5\u544a\u5185\u5bb9\u7684\u4e8b\u5b9e\u68c0\u67e5\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepResearchEval is a new system designed to improve how we evaluate multi-step web research tasks.</li>\n    <li>It creates realistic research tasks based on different user profiles, ensuring they require using multiple sources.</li>\n    <li>The framework includes a dynamic evaluation system that adapts to the specific task at hand.</li>\n    <li>It features an automatic fact-checking process that verifies information found online, even without citations.</li>\n    <li>This approach aims to address common challenges in evaluating deep research systems.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:38:31.000Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png", "numComments": 1, "submittedBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "fullname": "Keming Wu", "name": "wukeming11", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09259", "authors": [{"_id": "696856230ac10a06522f69dd", "name": "Jian Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69de", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:12.229Z", "hidden": false}, {"_id": "696856230ac10a06522f69df", "name": "Zhangqi Wang", "hidden": false}, {"_id": "696856230ac10a06522f69e0", "name": "Yu He", "hidden": false}, {"_id": "696856230ac10a06522f69e1", "name": "Haoran Luo", "hidden": false}, {"_id": "696856230ac10a06522f69e2", "name": "li yuan", "hidden": false}, {"_id": "696856230ac10a06522f69e3", "name": "Lingling Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69e4", "name": "Rui Mao", "hidden": false}, {"_id": "696856230ac10a06522f69e5", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:14.086Z", "hidden": false}, {"_id": "696856230ac10a06522f69e6", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T07:48:00.000Z", "submittedOnDailyAt": "2026-01-15T00:22:01.292Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "upvotes": 81, "discussionId": "696856230ac10a06522f69e7", "githubRepo": "https://github.com/exoskeletonzj/MAXS", "githubRepoAddedBy": "user", "ai_summary": "MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.", "ai_keywords": ["LLM agents", "tool execution", "reasoning planning", "lookahead strategy", "advantage value", "step consistency variance", "inter-step trend slopes", "trajectory convergence", "multi-tool reasoning", "inference efficiency"], "githubStars": 5, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u591a\u4e2a\u5de5\u5177\u534f\u4f5c\u4e2d\u5c55\u73b0\u51fa\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002</li>\n    <li>\u5f53\u524d\u65b9\u6cd5\u9762\u4e34\u5c40\u90e8\u751f\u6210\u7f3a\u4e4f\u524d\u77bb\u6027\u548c\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u504f\u79bb\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAXS\u7684\u5143\u81ea\u9002\u5e94\u63a2\u7d22\u6846\u67b6\uff0c\u80fd\u591f\u7075\u6d3b\u6574\u5408\u5de5\u5177\u6267\u884c\u4e0e\u63a8\u7406\u89c4\u5212\u3002</li>\n    <li>MAXS\u91c7\u7528\u524d\u77bb\u7b56\u7565\uff0c\u63d0\u524d\u51e0\u6b65\u6269\u5c55\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u9009\u62e9\u7a33\u5b9a\u548c\u9ad8\u4ef7\u503c\u7684\u63a8\u7406\u6b65\u9aa4\u3002</li>\n    <li>\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0cMAXS\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model (LLM) Agents can reason well using multiple tools but face issues like short-sighted decisions and unstable paths.</li>\n    <li>To improve this, we introduce a new approach called meta-adaptive exploration with LLM agents (MAXS) that combines reasoning planning and tool use.</li>\n    <li>MAXS uses a lookahead strategy to plan ahead and choose the best reasoning steps, ensuring consistent and effective outcomes.</li>\n    <li>It also includes a mechanism to stop the process early when a stable path is found, saving computational resources.</li>\n    <li>Tests show that MAXS performs better than existing methods in effectiveness and efficiency across various models and datasets.</li>\n</ul>"}, "publishedAt": "2026-01-14T02:48:00.000Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09259.png", "numComments": 3, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09274", "authors": [{"_id": "6968568f0ac10a06522f69e9", "name": "Jian Zhang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ea", "name": "Yu He", "hidden": false}, {"_id": "6968568f0ac10a06522f69eb", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:02.764Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69ec", "name": "Zhangqi Wang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ed", "name": "Kai He", "hidden": false}, {"_id": "6968568f0ac10a06522f69ee", "name": "Fangzhi Xu", "hidden": false}, {"_id": "6968568f0ac10a06522f69ef", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:05.035Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69f0", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T08:17:41.000Z", "submittedOnDailyAt": "2026-01-15T00:23:45.077Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "upvotes": 74, "discussionId": "6968568f0ac10a06522f69f1", "projectPage": "https://a3-bench.github.io/", "githubRepo": "https://github.com/exoskeletonzj/A3-Bench", "githubRepoAddedBy": "user", "githubStars": 0, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u63a8\u7406\u4e0d\u4ec5\u4f9d\u8d56\u903b\u8f91\u63a8\u7406\uff0c\u8fd8\u9700\u8981\u6fc0\u6d3b\u5148\u524d\u7684\u77e5\u8bc6\u548c\u7ecf\u9a8c\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u9a71\u52a8\u673a\u5236\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86A^3-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u7684\u65b0\u57fa\u51c6\uff0c\u57fa\u4e8e\u951a\u70b9\u548c\u5438\u5f15\u529b\u6fc0\u6d3b\u3002</li>\n    <li>\u6211\u4eec\u6807\u6ce8\u4e862198\u4e2a\u79d1\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u5c3a\u5ea6\u8bb0\u5fc6\u8bc4\u4f30\u6846\u67b6\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1A^3-Bench\uff0c\u5206\u6790\u8bb0\u5fc6\u6fc0\u6d3b\u5bf9\u63a8\u7406\u8868\u73b0\u7684\u5f71\u54cd\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Scientific reasoning involves using logic and activating previous knowledge.</li>\n  <li>Current evaluations focus on final answers, not on how memory helps reasoning.</li>\n  <li>A^3-Bench is a new benchmark that evaluates scientific reasoning through memory activation.</li>\n  <li>It includes 2,198 annotated science reasoning problems using a specific process.</li>\n  <li>The benchmark measures how well memory activation helps improve reasoning performance.</li>\n</ul>"}, "publishedAt": "2026-01-14T03:17:41.000Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09274.png", "numComments": 2, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09667", "authors": [{"_id": "6969b0f732f0333869ff9476", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:48.445Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9477", "user": {"_id": "662b4e3bc709a61df840fda1", "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg", "isPro": false, "fullname": "Hu Yunhai", "user": "AlexCCtop", "type": "user"}, "name": "Yunhai Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:37:06.706Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9478", "user": {"_id": "650026d30339dae3dba2cec5", "avatarUrl": "/avatars/fcc9ea4336f8d4bb177e5c9eacdd05c9.svg", "isPro": false, "fullname": "Juncheng Liu", "user": "juncliu", "type": "user"}, "name": "Juncheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:33.401Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9479", "name": "Shuyue Stella Li", "hidden": false}, {"_id": "6969b0f732f0333869ff947a", "name": "Yucheng Wang", "hidden": false}, {"_id": "6969b0f732f0333869ff947b", "user": {"_id": "638e40d450a4e4beef98196b", "avatarUrl": "/avatars/fe27e019baf48caeb44e19b7289db9fb.svg", "isPro": false, "fullname": "Zhen Xu", "user": "zhenxu", "type": "user"}, "name": "Zhen Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:04.868Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947c", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0f732f0333869ff947d", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:15.855Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947e", "name": "Xinxing Xu", "hidden": false}, {"_id": "6969b0f732f0333869ff947f", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:25.577Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9480", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:31.289Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9481", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:36.481Z", "hidden": false}], "publishedAt": "2026-01-14T17:57:43.000Z", "submittedOnDailyAt": "2026-01-16T01:01:32.343Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "upvotes": 63, "discussionId": "6969b0f832f0333869ff9482", "ai_summary": "Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.", "ai_keywords": ["multi-agent systems", "reinforcement learning", "test-time reinforcement learning", "multi-agent reinforcement learning", "credit assignment", "multi-expert teams", "dialogue systems", "distribution-shift-robust reasoning"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5df2\u6210\u4e3a\u8bb8\u591a\u5e94\u7528\u7684\u5f3a\u5927\u5408\u4f5c\u4f19\u4f34\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8017\u8d44\u6e90\u4e14\u4e0d\u7a33\u5b9a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08MATTRL\uff09\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u7ed3\u6784\u5316\u7684\u6587\u672c\u7ecf\u9a8c\u878d\u5165\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u3002</li>\n    <li>MATTRL \u7ec4\u5efa\u4e86\u4e00\u4e2a\u591a\u4e13\u5bb6\u56e2\u961f\uff0c\u7528\u4e8e\u591a\u8f6e\u8ba8\u8bba\uff0c\u6574\u5408\u6d4b\u8bd5\u65f6\u7684\u7ecf\u9a8c\u5e76\u8fbe\u6210\u4e00\u81f4\u51b3\u7b56\u3002</li>\n    <li>\u5728\u533b\u5b66\u3001\u6570\u5b66\u548c\u6559\u80b2\u7b49\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATTRL \u7684\u51c6\u786e\u6027\u5e73\u5747\u63d0\u9ad8\u4e86 3.67%\u3002</li>\n    <li>\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u624b\u52a8\u8c03\u6574\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems can work well together, but training them is often hard and requires a lot of resources.</li>\n    <li>Introducing Multi-Agent Test-Time Reinforcement Learning (MATTRL) helps improve how these agents collaborate during decision-making.</li>\n    <li>MATTRL uses specialized teams for discussions, retrieves relevant experiences, and helps agents agree on decisions.</li>\n    <li>The approach shows a significant accuracy improvement in various fields like medicine and education compared to other methods.</li>\n    <li>Different strategies for assigning credit in the system were tested to see how they impact training results.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:57:43.000Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09667.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06789", "authors": [{"_id": "69671036c5e371f6b235d143", "user": {"_id": "692881094c3f4293dfe29e3d", "avatarUrl": "/avatars/bddfaae8041a45498d46ef65ba17c920.svg", "isPro": false, "fullname": "qihao wang", "user": "jimson991", "type": "user"}, "name": "Qihao Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:07.079Z", "hidden": false}, {"_id": "69671036c5e371f6b235d144", "user": {"_id": "64b74fca17570fdff9b2aded", "avatarUrl": "/avatars/8b3519a7011af52dadc87ffef700c77c.svg", "isPro": false, "fullname": "Ziming Cheng", "user": "cadche", "type": "user"}, "name": "Ziming Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:13.370Z", "hidden": false}, {"_id": "69671036c5e371f6b235d145", "user": {"_id": "6513ee3c9af40a65586b43f5", "avatarUrl": "/avatars/815ed3876cefa12b25bf955edcbf71a3.svg", "isPro": false, "fullname": "shuo zhang", "user": "shuozhang", "type": "user"}, "name": "Shuo Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:18.640Z", "hidden": false}, {"_id": "69671036c5e371f6b235d146", "name": "Fan Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d147", "name": "Rui Xu", "hidden": false}, {"_id": "69671036c5e371f6b235d148", "name": "Heng Lian", "hidden": false}, {"_id": "69671036c5e371f6b235d149", "user": {"_id": "65bb3c545a5dbabc818e9044", "avatarUrl": "/avatars/4c239557bd5e33179cbf4f3a440bbf33.svg", "isPro": false, "fullname": "Kunyi Wang", "user": "KunyiWang", "type": "user"}, "name": "Kunyi Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:25.411Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14a", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaoming Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:47.145Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14b", "name": "Jianghao Yin", "hidden": false}, {"_id": "69671036c5e371f6b235d14c", "name": "Sen Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14d", "name": "Yue Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14e", "user": {"_id": "64803e5dc57f629056c601f1", "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg", "isPro": false, "fullname": "Shaolei Zhang", "user": "zhangshaolei", "type": "user"}, "name": "Shaolei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:13.491Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14f", "name": "Yanbing Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d150", "user": {"_id": "6874f7f0f8e67e9b5714adf2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png", "isPro": false, "fullname": "RongHao Chen", "user": "SuPA4ki", "type": "user"}, "name": "Ronghao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:02.800Z", "hidden": false}, {"_id": "69671036c5e371f6b235d151", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:57.426Z", "hidden": false}], "publishedAt": "2026-01-11T06:41:26.000Z", "submittedOnDailyAt": "2026-01-14T01:40:39.607Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "upvotes": 59, "discussionId": "69671036c5e371f6b235d152", "githubRepo": "https://github.com/QuantaAlpha/MemGovern", "githubRepoAddedBy": "user", "ai_summary": "MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.", "ai_keywords": ["autonomous software engineering", "SWE agents", "closed-world limitation", "open-world experience", "GitHub", "experience governance", "experience cards", "agentic experience search", "SWE-bench Verified"], "githubStars": 19, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4ee3\u7406\u5728\u7f16\u7a0b\u4e2d\u9762\u4e34\u201c\u5c01\u95ed\u4e16\u754c\u201d\u9650\u5236\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5386\u53f2\u7ecf\u9a8c\u3002</li>\n    <li>\u73b0\u6709\u95ee\u9898\u8ddf\u8e2a\u6570\u636e\u7ed3\u6784\u6df7\u4e71\uff0c\u96be\u4ee5\u8bbf\u95eeGitHub\u4e0a\u7684\u4eba\u7c7b\u7ecf\u9a8c\u3002</li>\n    <li>\u63d0\u51fa\u4e86MemGovern\u6846\u67b6\uff0c\u5c06\u539f\u59cbGitHub\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u7ecf\u9a8c\u8bb0\u5fc6\u3002</li>\n    <li>MemGovern\u901a\u8fc7\u7ecf\u9a8c\u6cbb\u7406\u521b\u5efa\u4e86135K\u7ecf\u9a8c\u5361\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7684\u89e3\u51b3\u7387\u3002</li>\n    <li>\u4f5c\u4e3a\u63d2\u4ef6\u65b9\u6cd5\uff0cMemGovern\u4e3a\u4ee3\u7406\u63d0\u4f9b\u4e86\u53cb\u597d\u7684\u8bb0\u5fc6\u57fa\u7840\u8bbe\u65bd\u89e3\u51b3\u65b9\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autonomous software engineering agents are changing how programming is done but have limitations in fixing bugs using only local information.</li>\n    <li>These agents do not utilize the vast human experiences available on platforms like GitHub due to the messy nature of real-world data.</li>\n    <li>The paper presents MemGovern, a framework that organizes GitHub data into useful experience cards for agents.</li>\n    <li>MemGovern includes a method for agents to search for and retrieve relevant human expertise effectively.</li>\n    <li>By creating 135,000 experience cards, MemGovern significantly improves bug resolution rates by 4.65% and offers a way to enhance agents' memory capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-11T01:41:26.000Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06789.png", "numComments": 1, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u7ebf\u7d22\u3001\u8fdb\u884c\u7f51\u9875\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u9a8c\u8bc1\u3002</li>\n    <li>\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u7c7b\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u83b7\u53d6\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u7814\u7a76\u6837\u672c\uff0c\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u3002</li>\n    <li>\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0Agentic\u65b9\u6cd5\u7684\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u4f9d\u8d56\u4e8e\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u76ee\u6807\u6f02\u79fb\u548c\u957f\u8fdc\u4e00\u81f4\u6027\u662f\u6838\u5fc3\u74f6\u9888\uff0cVideoDR\u4e3a\u672a\u6765\u7684\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\u548c\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often needs to combine information from videos and the web, making it complex.</li>\n    <li>The new benchmark, VideoDR, focuses on answering questions about videos using information from both the video and the internet.</li>\n    <li>VideoDR includes high-quality video samples from six different topics, created with careful human review.</li>\n    <li>Different models were tested, and findings showed that their success varies based on how well they can track video details during searches.</li>\n    <li>VideoDR helps identify important challenges for developing future tools that answer questions about videos using web content.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u8868\u73b0\u8584\u5f31\uff0c\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cBabyVision\u201d\u57fa\u51c6\uff0c\u4e13\u95e8\u8bc4\u4f30MLLMs\u7684\u57fa\u672c\u89c6\u89c9\u80fd\u529b\uff0c\u4e0d\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\uff0c\u6db5\u76d6\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u5c24\u5176\u662f\u513f\u7ae5\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u6709\u52a9\u4e8e\u63d0\u5347\u673a\u5668\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop basic visual skills early, but modern Multimodal LLMs (MLLMs) depend too much on language to understand visuals.</li>\n    <li>MLLMs struggle with simple visual tasks that even young children can do easily.</li>\n    <li>BabyVision is a new benchmark created to test visual skills in MLLMs without using language.</li>\n    <li>This benchmark includes 388 tasks in 22 subclasses across four main categories.</li>\n    <li>Current top MLLMs perform poorly compared to humans, with scores much lower than what 6-year-olds and adults achieve.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u5f88\u591a\u5e94\u7528\u5f88\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u9ad8\u7ea7\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas contain many different types of features, which are important to identify from satellite images.</li>\n    <li>Current models can identify physical features like buildings and water but struggle with socially defined features like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg, which includes satellite images, digital maps, and detailed labels for social features.</li>\n    <li>We developed a new method called SocioReasoner that mimics how humans identify and label these social features using advanced reasoning techniques.</li>\n    <li>Our approach shows better performance than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7ba1\u9053\u5b9e\u73b0\uff0c\u5177\u67091.2T\u591a\u6a21\u6001\u6570\u636e\u7684\u8bad\u7ec3\u57fa\u7840\u3002</li>\n    <li>\u91c7\u7528\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6280\u672f\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u89c6\u89c9\u63a8\u7406\u3002</li>\n    <li>\u5c3d\u7ba1\u4f53\u79ef\u5c0f\uff08\u4ec510\u4ebf\u53c2\u6570\uff09\uff0cSTEP3-VL-10B \u7684\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u592710\u523020\u500d\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>It uses a combined pre-training approach on a large dataset to improve how it understands both language and vision.</li>\n    <li>The model features a unique reasoning method called Parallel Coordinated Reasoning (PaCoRe) to enhance its performance during testing.</li>\n    <li>Despite being smaller in size, STEP3-VL-10B performs as well as or better than much larger models and top proprietary models.</li>\n    <li>The developers have released the model for public use, aiming to provide a strong and efficient resource for the community.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u6839\u636e\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u672a\u80fd\u5229\u7528\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5c06\u5176\u5f62\u6210\u4e00\u4e2a\u4ee3\u7406\u4e0e\u5730\u56fe\u7684\u5faa\u73af\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728500\u7c73\u51c6\u786e\u7387\u4e0a\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The image geolocalization task predicts where a photo was taken anywhere on Earth using visual clues.</li>\n    <li>This study introduces a new approach that uses maps, which is a common human strategy, and incorporates it into a model called Thinking with Map.</li>\n    <li>The method includes a two-step process: first, using reinforcement learning to improve the model's performance, and second, using a technique to explore different options before making a final prediction.</li>\n    <li>A new benchmark called MAPBench was created with real-world images to evaluate the model's performance.</li>\n    <li>Results show that this new method significantly outperforms existing models, improving accuracy from 8.0% to 22.1% in certain cases.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u4f46\u5b83\u5e38\u5e38\u9762\u4e34\u201c\u63a2\u7d22\u5d29\u6e83\u201d\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u5728\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u90a3\u4e9b\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u7ed3\u679c\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u6839\u636e\u805a\u7c7b\u5927\u5c0f\u8c03\u6574\u7b56\u7565\u4f18\u52bf\u7684\u6743\u91cd\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u901a\u8fc7\u7387\uff08pass@k\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u8fc7\u7387\uff08pass@1\uff09\uff0c\u5e76\u53d1\u73b0\u4e86\u66f4\u591a\u591a\u6837\u5316\u7684\u89e3\u51b3\u7b56\u7565\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning is key for improving large language models, especially for reasoning tasks.</li>\n    <li>Current methods often focus too much on common reasoning patterns, which limits diversity in solutions.</li>\n    <li>The proposed method, Uniqueness-Aware Reinforcement Learning, rewards unique and correct solutions with rare strategies.</li>\n    <li>This approach uses a judge to group similar solutions and adjusts rewards based on the uniqueness of these groups.</li>\n    <li>The method shows improved performance across various benchmarks without losing effectiveness on common tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 103, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 90, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u662f\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>HGMem\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5185\u5b58\u673a\u5236\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u5b58\u50a8\uff0c\u652f\u6301\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u4f7f\u7528\u8d85\u56fe\u8868\u793a\u5185\u5b58\uff0c\u901a\u8fc7\u9ad8\u9636\u4ea4\u4e92\u5f62\u6210\u66f4\u5f3a\u7684\u77e5\u8bc6\u7ed3\u6784\uff0c\u4fc3\u8fdb\u6df1\u5c42\u63a8\u7406\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30HGMem\uff0c\u7ed3\u679c\u663e\u793a\u5176\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) improve reasoning and understanding.</li>\n    <li>Current memory designs in RAG systems mainly store facts but don't connect them effectively, limiting reasoning ability.</li>\n    <li>HGMem is a new memory system that uses a hypergraph structure to create connections between facts for better reasoning.</li>\n    <li>This approach allows for the development of deeper insights and stronger guidance in problem-solving.</li>\n    <li>Tests show that HGMem significantly enhances multi-step RAG performance compared to existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u6a21\u578b\u9700\u8981\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\u548c\u7b26\u5408\u4eba\u7c7b\u591a\u6837\u5316\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u671d\u7740\u671f\u671b\u884c\u4e3a\u53d1\u5c55\u3002</li>\n    <li>\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u7684\u65b9\u6cd5\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u53d8\u5f97\u76f8\u540c\uff0c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff08GDPO\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4e2a\u522b\u5956\u52b1\u7684\u89c4\u8303\u5316\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>GDPO\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u63a8\u5e7f\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to provide accurate responses and behave according to different human preferences.</li>\n    <li>To help models learn these preferences, multiple rewards are used in Reinforcement Learning (RL) pipelines.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can cause problems, leading to less effective training.</li>\n    <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that improves training by keeping the rewards distinct.</li>\n    <li>GDPO outperforms GRPO in three tasks, showing better accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u7684\u201c\u751f\u6210-\u9a8c\u8bc1-\u4f18\u5316\u201d\u5faa\u73af\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\u3002</li>\n    <li>\u4f4e\u6548\u7387\u7684\u539f\u56e0\u5305\u62ec\u521d\u59cb\u5316\u504f\u89c1\u3001\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u968f\u673a\u64cd\u4f5c\u548c\u4efb\u52a1\u95f4\u7ecf\u9a8c\u5229\u7528\u4e0d\u8db3\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u53d7\u63a7\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002</li>\n    <li>CSE\u901a\u8fc7\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u53cd\u9988\u6307\u5bfc\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5206\u5c42\u8fdb\u5316\u8bb0\u5fc6\u6765\u4f18\u5316\u4ee3\u7801\u751f\u6210\u3002</li>\n    <li>\u5728EffiBench-X\u5b9e\u9a8c\u4e2d\uff0cCSE\u5728\u4e0d\u540c\u7684LLM\u57fa\u7840\u4e0a\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u57fa\u51c6\uff0c\u5e76\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u6301\u7eed\u63d0\u9ad8\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods improve code generation but struggle with exploring better solutions efficiently.</li>\n    <li>Issues arise from biases in initial solutions, random operations without guidance, and poor use of experiences across tasks.</li>\n    <li>The proposed Controlled Self-Evolution (CSE) includes three main features: diverse planning for better strategy coverage, feedback-guided evolution for better mutations, and memory for learning from both successes and failures.</li>\n    <li>Tests on EffiBench-X show that CSE performs better than existing methods with various LLMs and improves quickly from early on.</li>\n    <li>The code for CSE is available online for public use at https://github.com/QuantaAlpha/EvoControl.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>\u81ea\u52a8\u5f62\u5f0f\u5316\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u7ffb\u8bd1\u4e3a\u673a\u5668\u53ef\u4ee5\u7406\u89e3\u7684\u5f62\u5f0f\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u7269\u7406\u5b66\u4e2d\u9700\u8981\u4ece\u89c6\u89c9\u5143\u7d20\u63a8\u65ad\u9690\u542b\u7ea6\u675f\uff08\u5982\u8d28\u91cf\u6216\u80fd\u91cf\uff09\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMFormalizer\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u6570\u5b66\u548c\u7269\u7406\u9886\u57df\u7684\u5b9e\u4f53\uff0c\u6269\u5c55\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u5e94\u7528\uff0c\u8d85\u8d8a\u4e86\u6587\u672c\u3002 </li>\n    <li>MMFormalizer\u901a\u8fc7\u9012\u5f52\u6784\u5efa\u548c\u516c\u7406\u7ec4\u5408\uff0c\u4ece\u611f\u77e5\u57fa\u7840\u751f\u6210\u6b63\u5f0f\u547d\u9898\uff0c\u786e\u4fdd\u6bcf\u4e2a\u62bd\u8c61\u90fd\u6709\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5PhyX-AF\u4e0a\u8bc4\u4f30MMFormalizer\uff0c\u7ed3\u679c\u663e\u793aGPT-5\u548cGemini-3-Pro\u5728\u7f16\u8bd1\u548c\u8bed\u4e49\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cGPT-5\u5728\u7269\u7406\u63a8\u7406\u4e0a\u7a81\u51fa\u3002</li>\n    <li>MMFormalizer\u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization translates natural language math into formal statements for machine reasoning, but it struggles with complex real-world scenarios.</li>\n    <li>MMFormalizer is a new tool that enhances this process by combining visual elements with mathematical and physical concepts.</li>\n    <li>It creates formal propositions using visual evidence and established rules, ensuring all ideas are well-supported.</li>\n    <li>MMFormalizer was tested on a new benchmark called PhyX-AF, which includes various tasks related to math and physics.</li>\n    <li>Results show that advanced models perform well, especially in physical reasoning, while geometry is still challenging.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 18, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5b9e\u73b0\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\uff0c\u6269\u5c55\u6027\u6709\u9650\uff0c\u800cWan-Move\u80fd\u63d0\u4f9b\u7cbe\u786e\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u5b9e\u73b0\u5bf9\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u751f\u6210\u5bf9\u9f50\u7684\u65f6\u7a7a\u7279\u5f81\u56fe\u3002</li>\n    <li>Wan-Move\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u56fe\u50cf\u8f6c\u89c6\u9891\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\uff0c\u65e0\u9700\u66f4\u6539\u67b6\u6784\uff0c\u5e76\u4e14\u6613\u4e8e\u6269\u5c55\u3002</li>\n    <li>\u7ecf\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cWan-Move\u5728\u8fd0\u52a8\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5546\u4e1a\u5de5\u5177\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It provides precise and high-quality control over how objects move in videos, addressing issues found in previous methods.</li>\n    <li>The framework uses dense point trajectories to represent object movements, allowing for detailed scene control.</li>\n    <li>Wan-Move integrates smoothly into existing video models without needing major changes, making it easy to use and scale.</li>\n    <li>It produces high-quality videos and has been evaluated with a comprehensive benchmark called MoveBench, showing better motion quality compared to other tools.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.08478", "authors": [{"_id": "6938e00fdfc35938ba129f4f", "name": "Yuning Gong", "hidden": false}, {"_id": "6938e00fdfc35938ba129f50", "name": "Yifei Liu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f51", "name": "Yifan Zhan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f52", "name": "Muyao Niu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f53", "name": "Xueying Li", "hidden": false}, {"_id": "6938e00fdfc35938ba129f54", "name": "Yuanjun Liao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f55", "name": "Jiaming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f56", "name": "Yuanyuan Gao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f57", "name": "Jiaqi Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f58", "name": "Minming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f59", "name": "Li Zhou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5a", "name": "Yuning Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5b", "name": "Wei Wang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5c", "name": "Xiaoqing Hou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5d", "name": "Huaxi Huang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5e", "name": "Shixiang Tang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5f", "name": "Le Ma", "hidden": false}, {"_id": "6938e00fdfc35938ba129f60", "name": "Dingwen Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f61", "name": "Xue Yang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f62", "name": "Junchi Yan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f63", "name": "Yanchi Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f64", "name": "Yinqiang Zheng", "hidden": false}, {"_id": "6938e00fdfc35938ba129f65", "name": "Xiao Sun", "hidden": false}, {"_id": "6938e00fdfc35938ba129f66", "user": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "name": "Zhihang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:56:28.162Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "publishedAt": "2025-12-09T10:54:58.000Z", "submittedOnDailyAt": "2025-12-10T07:43:37.566Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "submittedOnDailyBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "upvotes": 64, "discussionId": "6938e00fdfc35938ba129f67", "projectPage": "https://visionary-laboratory.github.io/visionary/", "githubRepo": "https://github.com/Visionary-Laboratory/visionary", "githubRepoAddedBy": "user", "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.", "ai_keywords": ["Neural rendering", "3D Gaussian Splatting", "3DGS", "WebGPU", "ONNX inference", "Gaussian Generator contract", "three.js", "TypeScript API", "MLP-based 3DGS", "4DGS", "neural avatars", "style transformation", "GPU-based primitive sorting", "World Model Carrier"], "githubStars": 162, "summary_zh": "<ul>\n    <li>Visionary\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7f51\u9875\u5e73\u53f0\uff0c\u4e13\u6ce8\u4e8e\u5b9e\u65f6\u9ad8\u65af\u70b9\u4e91\u548c\u7f51\u683c\u6e32\u67d3\u3002</li>\n    <li>\u8be5\u5e73\u53f0\u4f7f\u7528\u9ad8\u6548\u7684WebGPU\u6e32\u67d3\u5668\uff0c\u652f\u6301\u52a8\u6001\u795e\u7ecf\u5904\u7406\uff0c\u63d0\u4f9b\u8f7b\u4fbf\u7684\u6d4f\u89c8\u5668\u4f53\u9a8c\u3002</li>\n    <li>Visionary\u5f15\u5165\u4e86\u6807\u51c6\u5316\u7684\u9ad8\u65af\u751f\u6210\u5668\u5408\u540c\uff0c\u652f\u6301\u591a\u79cd\u7b97\u6cd5\u751f\u6210\u6216\u66f4\u65b0\u9ad8\u65af\u6570\u636e\u3002</li>\n    <li>\u5e73\u53f0\u4e0ethree.js\u5e93\u517c\u5bb9\uff0c\u63d0\u4f9b\u7b80\u6d01\u7684TypeScript API\uff0c\u4fbf\u4e8e\u4e0e\u73b0\u6709\u7f51\u9875\u5e94\u7528\u96c6\u6210\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cVisionary\u5728\u6e32\u67d3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709Web\u67e5\u770b\u5668\uff0c\u652f\u6301\u591a\u79cd3DGS\u53d8\u4f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Visionary is a new, open platform for rendering 3D models using a technique called Gaussian Splatting.</li>\n  <li>It runs directly in web browsers and is designed for real-time performance, making it easy to use with just a click.</li>\n  <li>The platform allows for dynamic updates of 3D models and supports various algorithms for generating content.</li>\n  <li>Visionary is more efficient than existing viewers, as it uses advanced GPU techniques for better rendering speed.</li>\n  <li>It includes tools for easy integration into web applications and supports various types of neural networks and enhancements.</li>\n</ul>"}, "publishedAt": "2025-12-09T05:54:58.000Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08478.png", "numComments": 3, "submittedBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "fullname": "Zhihang Zhong", "name": "Zuica96", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07951", "authors": [{"_id": "6938e892dfc35938ba129ff5", "name": "Zekai Luo", "hidden": false}, {"_id": "6938e892dfc35938ba129ff6", "name": "Zongze Du", "hidden": false}, {"_id": "6938e892dfc35938ba129ff7", "name": "Zhouhang Zhu", "hidden": false}, {"_id": "6938e892dfc35938ba129ff8", "name": "Hao Zhong", "hidden": false}, {"_id": "6938e892dfc35938ba129ff9", "user": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "name": "Muzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T10:51:43.541Z", "hidden": false}, {"_id": "6938e892dfc35938ba129ffa", "name": "Wen Wang", "hidden": false}, {"_id": "6938e892dfc35938ba129ffb", "name": "Yuling Xi", "hidden": false}, {"_id": "6938e892dfc35938ba129ffc", "name": "Chenchen Jing", "hidden": false}, {"_id": "6938e892dfc35938ba129ffd", "name": "Hao Chen", "hidden": false}, {"_id": "6938e892dfc35938ba129ffe", "name": "Chunhua Shen", "hidden": false}], "publishedAt": "2025-12-08T19:00:04.000Z", "submittedOnDailyAt": "2025-12-10T00:59:39.366Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "submittedOnDailyBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "upvotes": 40, "discussionId": "6938e892dfc35938ba129fff", "projectPage": "https://aim-uofa.github.io/LivingSwap", "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.", "ai_keywords": ["keyframe conditioning", "video reference guidance", "temporal stitching", "identity preservation", "high-fidelity reconstruction", "Face2Face dataset"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u5728\u7535\u5f71\u548c\u5a31\u4e50\u5236\u4f5c\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5728\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4ecd\u7136\u5f88\u5177\u6311\u6218\u6027\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86LivingSwap\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u53c2\u8003\u7684\u89c6\u9891\u6362\u8138\u6a21\u578b\uff0c\u5229\u7528\u6e90\u89c6\u9891\u7684\u4e30\u5bcc\u89c6\u89c9\u5c5e\u6027\u6765\u63d0\u5347\u6362\u8138\u6548\u679c\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u5173\u952e\u5e27\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u7075\u6d3b\u63a7\u5236\u76ee\u6807\u8eab\u4efd\u7684\u6ce8\u5165\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u8eab\u4efd\u4fdd\u7559\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u53c2\u8003\u6307\u5bfc\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u914d\u5bf9\u6362\u8138\u6570\u636e\u96c6Face2Face\uff0c\u5e76\u53cd\u8f6c\u6570\u636e\u5bf9\u4ee5\u786e\u4fdd\u53ef\u9760\u7684\u76d1\u7763\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c06\u76ee\u6807\u8eab\u4efd\u4e0e\u6e90\u89c6\u9891\u7684\u8868\u60c5\u3001\u5149\u7167\u548c\u8fd0\u52a8\u65e0\u7f1d\u878d\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5236\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4eba\u5de5\u5de5\u4f5c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video face swapping is important for movies, but it's hard to keep it looking good over long videos.</li>\n    <li>The research introduces LivingSwap, a new method that uses keyframes from videos to improve face swapping quality and consistency.</li>\n    <li>LivingSwap allows for flexible editing by using keyframes to add the target person's identity while maintaining stable visuals.</li>\n    <li>A new dataset called Face2Face was created to help train the model and ensure good results.</li>\n    <li>Tests show that LivingSwap achieves top results, blending the target's identity with the source video's look and feel, making production easier.</li>\n</ul>"}, "publishedAt": "2025-12-08T14:00:04.000Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07951.png", "numComments": 1, "submittedBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "fullname": "zhumuzhi", "name": "Z-MU-Z", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07802", "authors": [{"_id": "69392ceedfc35938ba12a187", "user": {"_id": "65e5eae6958b39864e8b683e", "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg", "isPro": true, "fullname": "Zhaochong An", "user": "ZhaochongAn", "type": "user"}, "name": "Zhaochong An", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T13:07:50.839Z", "hidden": false}, {"_id": "69392ceedfc35938ba12a188", "name": "Menglin Jia", "hidden": false}, {"_id": "69392ceedfc35938ba12a189", "name": "Haonan Qiu", "hidden": false}, {"_id": "69392ceedfc35938ba12a18a", "name": "Zijian Zhou", "hidden": false}, {"_id": "69392ceedfc35938ba12a18b", "name": "Xiaoke Huang", "hidden": false}, {"_id": "69392ceedfc35938ba12a18c", "name": "Zhiheng Liu", "hidden": false}, {"_id": "69392ceedfc35938ba12a18d", "name": "Weiming Ren", "hidden": false}, {"_id": "69392ceedfc35938ba12a18e", "user": {"_id": "65a12ec4c9873890d15c4ab9", "avatarUrl": "/avatars/ce4d46f575ba757f78eabdb25b394171.svg", "isPro": false, "fullname": "Kumara Kahatapitiya", "user": "kumarak", "type": "user"}, "name": "Kumara Kahatapitiya", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T10:51:00.011Z", "hidden": false}, {"_id": "69392ceedfc35938ba12a18f", "name": "Ding Liu", "hidden": false}, {"_id": "69392ceedfc35938ba12a190", "name": "Sen He", "hidden": false}, {"_id": "69392ceedfc35938ba12a191", "name": "Chenyang Zhang", "hidden": false}, {"_id": "69392ceedfc35938ba12a192", "name": "Tao Xiang", "hidden": false}, {"_id": "69392ceedfc35938ba12a193", "name": "Fanny Yang", "hidden": false}, {"_id": "69392ceedfc35938ba12a194", "name": "Serge Belongie", "hidden": false}, {"_id": "69392ceedfc35938ba12a195", "name": "Tian Xie", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e5eae6958b39864e8b683e/iJ5L2BbZWW8XiE7ORZuFK.mp4"], "publishedAt": "2025-12-08T18:32:24.000Z", "submittedOnDailyAt": "2025-12-10T06:06:45.364Z", "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "submittedOnDailyBy": {"_id": "65e5eae6958b39864e8b683e", "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg", "isPro": true, "fullname": "Zhaochong An", "user": "ZhaochongAn", "type": "user"}, "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "upvotes": 31, "discussionId": "69392ceedfc35938ba12a196", "projectPage": "https://zhaochongan.github.io/projects/OneStory/", "ai_summary": "OneStory generates coherent multi-shot videos by modeling global cross-shot context through a Frame Selection module and an Adaptive Conditioner, leveraging pretrained image-to-video models and a curated dataset.", "ai_keywords": ["multi-shot video generation", "next-shot generation", "autoregressive shot synthesis", "pretrained image-to-video models", "Frame Selection module", "Adaptive Conditioner", "semantically-relevant global memory", "importance-guided patchification", "referential captions", "text-conditioned", "image-conditioned"], "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u53d9\u4e8b\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ea\u4f9d\u8d56\u6709\u9650\u7684\u65f6\u95f4\u7a97\u53e3\u6216\u5355\u4e2a\u5173\u952e\u5e27\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86OneStory\uff0c\u80fd\u591f\u6709\u6548\u5efa\u6a21\u8de8\u955c\u5934\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u751f\u6210\u4e00\u81f4\u4e14\u53ef\u6269\u5c55\u7684\u53d9\u4e8b\u3002</li>\n    <li>OneStory\u5c06\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0b\u4e00\u4e2a\u955c\u5934\u751f\u6210\u4efb\u52a1\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u6761\u4ef6\u5316\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u5e27\u9009\u62e9\u6a21\u5757\u548c\u81ea\u9002\u5e94\u6761\u4ef6\u5668\uff0c\u4ee5\u63d0\u9ad8\u8bed\u4e49\u76f8\u5173\u6027\u548c\u751f\u6210\u7684\u7d27\u51d1\u6027\u3002</li>\n    <li>\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0cOneStory\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u6700\u65b0\u6c34\u5e73\uff0c\u652f\u6301\u53ef\u63a7\u548c\u6c89\u6d78\u5f0f\u7684\u957f\u89c6\u9891\u53d9\u4e8b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Storytelling in videos often uses multiple clips that are connected to tell a complete story.</li>\n    <li>Current methods for generating multi-shot videos struggle with understanding the context of shots over longer distances.</li>\n    <li>OneStory is a new approach that improves video generation by focusing on the next shot, using advanced techniques for better context understanding.</li>\n    <li>It includes two important features: a Frame Selection module to choose relevant frames from previous shots and an Adaptive Conditioner to create compact context for better video generation.</li>\n    <li>OneStory outperforms existing methods in creating coherent narratives in videos, using a large dataset designed to reflect real-world storytelling.</li>\n</ul>"}, "publishedAt": "2025-12-08T13:32:24.000Z", "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e5eae6958b39864e8b683e/iJ5L2BbZWW8XiE7ORZuFK.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07802.png", "numComments": 1, "submittedBy": {"_id": "65e5eae6958b39864e8b683e", "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg", "fullname": "Zhaochong An", "name": "ZhaochongAn", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07843", "authors": [{"_id": "6938e51ddfc35938ba129fb2", "user": {"_id": "63797c273f575acc2f6893c0", "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg", "isPro": true, "fullname": "Long(Tony) Lian", "user": "longlian", "type": "user"}, "name": "Long Lian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:56:15.315Z", "hidden": false}, {"_id": "6938e51ddfc35938ba129fb3", "name": "Sida Wang", "hidden": false}, {"_id": "6938e51ddfc35938ba129fb4", "user": {"_id": "6417cf37dce1e4c0229f17b1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417cf37dce1e4c0229f17b1/7h-ZCB5f4wif7TsnF-B1M.jpeg", "isPro": false, "fullname": "Felix Xu", "user": "katanaxu", "type": "user"}, "name": "Felix Juefei-Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T13:07:56.324Z", "hidden": false}, {"_id": "6938e51ddfc35938ba129fb5", "name": "Tsu-Jui Fu", "hidden": false}, {"_id": "6938e51ddfc35938ba129fb6", "name": "Xiuyu Li", "hidden": false}, {"_id": "6938e51ddfc35938ba129fb7", "name": "Adam Yala", "hidden": false}, {"_id": "6938e51ddfc35938ba129fb8", "name": "Trevor Darrell", "hidden": false}, {"_id": "6938e51ddfc35938ba129fb9", "name": "Alane Suhr", "hidden": false}, {"_id": "6938e51ddfc35938ba129fba", "name": "Yuandong Tian", "hidden": false}, {"_id": "6938e51ddfc35938ba129fbb", "name": "Xi Victoria Lin", "hidden": false}], "publishedAt": "2025-11-24T18:55:59.000Z", "submittedOnDailyAt": "2025-12-10T02:14:20.520Z", "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.", "upvotes": 18, "discussionId": "6938e51edfc35938ba129fbc", "ai_summary": "ThreadWeaver, a framework for adaptive parallel reasoning, achieves accuracy comparable to sequential models while reducing inference latency through parallel trajectory generation, trie-based training-inference co-design, and parallelization-aware reinforcement learning.", "ai_keywords": ["Large Language Models", "sequential decoding", "adaptive parallel reasoning", "long chain-of-thought", "CoT", "ThreadWeaver", "parallel trajectory generator", "trie-based training-inference co-design", "parallelization-aware reinforcement learning", "Qwen3-8B", "AIME24"], "summary_zh": "<ul>\n    <li>ThreadWeaver\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5e76\u884c\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002</li>\n    <li>\u5b83\u80fd\u591f\u5728\u4fdd\u6301\u4e0e\u4f20\u7edf\u5e8f\u5217\u63a8\u7406\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002</li>\n    <li>ThreadWeaver\u7684\u521b\u65b0\u5305\u62ec\uff1a\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5e76\u884c\u6807\u6ce8\u6570\u636e\u3001\u652f\u6301\u4efb\u610f\u81ea\u56de\u5f52\u63a8\u7406\u5f15\u64ce\u7684\u5e76\u884c\u63a8\u7406\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u6709\u6548\u5e76\u884c\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002</li>\n    <li>\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cThreadWeaver\u7684\u8868\u73b0\u4e0e\u6700\u5148\u8fdb\u7684\u5e8f\u5217\u63a8\u7406\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u63d0\u5347\u4e861.53\u500d\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u51c6\u786e\u7387\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u65b0\u5e73\u8861\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Large Language Models (LLMs) can reason well, but their sequential process can slow down performance on complex tasks.</li>\n  <li>Adaptive parallel reasoning helps by breaking down tasks into simultaneous threads, but current methods often struggle with accuracy and require special setups.</li>\n  <li>ThreadWeaver is a new framework that improves reasoning speed without sacrificing accuracy compared to traditional models.</li>\n  <li>It features a two-stage generator for creating high-quality training data, and works with standard inference engines without needing special modifications.</li>\n  <li>Tested on tough math problems, ThreadWeaver shows similar accuracy to leading models while being up to 1.53 times faster.</li>\n</ul>"}, "publishedAt": "2025-11-24T13:55:59.000Z", "title": "ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models", "summary": "Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07843.png", "numComments": 2, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8881}, "isAuthorParticipating": true}, {"paper": {"id": "2512.06864", "authors": [{"_id": "69394fefdfc35938ba12a212", "name": "Kaixuan Lu", "hidden": false}, {"_id": "69394fefdfc35938ba12a213", "user": {"_id": "63be9021da08ed0544f36c38", "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg", "isPro": false, "fullname": "onurcan", "user": "monurcan", "type": "user"}, "name": "Mehmet Onurcan Kaya", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T15:44:47.317Z", "hidden": false}, {"_id": "69394fefdfc35938ba12a214", "name": "Dim P. Papadopoulos", "hidden": false}], "publishedAt": "2025-12-07T14:37:12.000Z", "submittedOnDailyAt": "2025-12-10T08:59:10.180Z", "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training", "submittedOnDailyBy": {"_id": "63be9021da08ed0544f36c38", "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg", "isPro": false, "fullname": "onurcan", "user": "monurcan", "type": "user"}, "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.", "upvotes": 12, "discussionId": "69394fefdfc35938ba12a215", "githubRepo": "https://github.com/wcbup/AutoQ-VIS/", "githubRepoAddedBy": "user", "ai_summary": "AutoQ-VIS achieves state-of-the-art results in unsupervised Video Instance Segmentation using quality-guided self-training to bridge the synthetic-to-real domain gap.", "ai_keywords": ["unsupervised methods", "VideoCutLER", "quality-guided self-training", "closed-loop system", "pseudo-label generation", "automatic quality assessment", "YouTubeVIS-2019", "AP50"], "githubStars": 3, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08VIS\uff09\u9762\u4e34\u7740\u6ce8\u91ca\u6311\u6218\uff0c\u9700\u8981\u50cf\u7d20\u7ea7\u63a9\u7801\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u6807\u7b7e\u3002</li>\n    <li>\u73b0\u6709\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u5982VideoCutLER\u901a\u8fc7\u5408\u6210\u6570\u636e\u6d88\u9664\u4e86\u5149\u6d41\u4f9d\u8d56\uff0c\u4f46\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u6846\u67b6AutoQ-VIS\uff0c\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u6211\u8bad\u7ec3\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u4e4b\u95f4\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u5408\u6210\u89c6\u9891\u5230\u771f\u5b9e\u89c6\u9891\u7684\u6e10\u8fdb\u9002\u5e94\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAutoQ-VIS\u5728YouTubeVIS-2019\u9a8c\u8bc1\u96c6\u4e0a\u53d6\u5f97\u4e8652.6\u7684AP_{50}\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u65b9\u6cd5VideoCutLER\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u6ce8\u91ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Video Instance Segmentation (VIS) needs detailed labels for both image pixels and time consistency, making it hard to annotate.</li>\n  <li>Recent methods like VideoCutLER use synthetic data to avoid issues with optical flow, but they struggle to work well with real videos.</li>\n  <li>Our new method, AutoQ-VIS, improves the connection between synthetic and real video data through self-training that focuses on quality.</li>\n  <li>In tests, AutoQ-VIS achieved a score of 52.6 on the YouTubeVIS-2019 validation set, outperforming the previous best method by 4.4% without needing human labels.</li>\n  <li>We will share the code for AutoQ-VIS on GitHub.</li>\n</ul>"}, "publishedAt": "2025-12-07T09:37:12.000Z", "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 AP_{50} on YouTubeVIS-2019 val set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06864.png", "numComments": 1, "submittedBy": {"_id": "63be9021da08ed0544f36c38", "avatarUrl": "/avatars/42511d8b1d3a3ef99bc154c98b72dfba.svg", "fullname": "onurcan", "name": "monurcan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}, {"paper": {"id": "2512.05033", "authors": [{"_id": "693919e9dfc35938ba12a151", "name": "Monishwaran Maheswaran", "hidden": false}, {"_id": "693919e9dfc35938ba12a152", "name": "Rishabh Tiwari", "hidden": false}, {"_id": "693919e9dfc35938ba12a153", "user": {"_id": "67dc66fe55c24fc4f981a4ab", "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg", "isPro": false, "fullname": "Yuezhou Hu", "user": "yuezhouhu", "type": "user"}, "name": "Yuezhou Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:39:25.344Z", "hidden": false}, {"_id": "693919e9dfc35938ba12a154", "name": "Kerem Dilmen", "hidden": false}, {"_id": "693919e9dfc35938ba12a155", "name": "Coleman Hooper", "hidden": false}, {"_id": "693919e9dfc35938ba12a156", "name": "Haocheng Xi", "hidden": false}, {"_id": "693919e9dfc35938ba12a157", "name": "Nicholas Lee", "hidden": false}, {"_id": "693919e9dfc35938ba12a158", "name": "Mehrdad Farajtabar", "hidden": false}, {"_id": "693919e9dfc35938ba12a159", "name": "Michael W. Mahoney", "hidden": false}, {"_id": "693919e9dfc35938ba12a15a", "name": "Kurt Keutzer", "hidden": false}, {"_id": "693919e9dfc35938ba12a15b", "name": "Amir Gholami", "hidden": false}], "publishedAt": "2025-12-04T17:50:53.000Z", "submittedOnDailyAt": "2025-12-10T18:56:28.645Z", "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation", "submittedOnDailyBy": {"_id": "67dc66fe55c24fc4f981a4ab", "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg", "isPro": false, "fullname": "Yuezhou Hu", "user": "yuezhouhu", "type": "user"}, "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to sim2times at matched accuracy.", "upvotes": 12, "discussionId": "693919e9dfc35938ba12a15c", "projectPage": "https://www.monishwaran.com/arbitrage.html", "githubRepo": "https://github.com/SqueezeAILab/Arbitrage", "githubRepoAddedBy": "user", "ai_summary": "Arbitrage is a dynamic routing framework for speculative decoding that improves efficiency in large language model inference by predicting when the target model will provide a better step, outperforming traditional step-level methods.", "ai_keywords": ["Speculative Decoding", "draft model", "target model", "token-level Speculative Decoding", "step-level semantic verification", "Arbitrage", "router", "Arbitrage Oracle", "inference latency"], "githubStars": 2, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u7684\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\u3002</li>\n    <li>\u6295\u673a\u89e3\u7801\u6280\u672f\u901a\u8fc7\u4f7f\u7528\u5feb\u901f\u4f46\u4e0d\u51c6\u786e\u7684\u8349\u7a3f\u6a21\u578b\u6765\u52a0\u901f\u63a8\u7406\u3002</li>\n    <li>\u4f20\u7edf\u7684\u6295\u673a\u89e3\u7801\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u8bed\u4e49\u5339\u914d\u9519\u8bef\u800c\u9020\u6210\u4e0d\u5fc5\u8981\u7684\u62d2\u7edd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u6846\u67b6\u201c\u5957\u5229\u201d\uff0c\u6839\u636e\u8349\u7a3f\u6a21\u578b\u548c\u76ee\u6807\u6a21\u578b\u7684\u76f8\u5bf9\u4f18\u52bf\u52a8\u6001\u9009\u62e9\u751f\u6210\u6b65\u9aa4\u3002</li>\n    <li>\u5957\u5229\u6cd5\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c11\u4e86\u6700\u591a2\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Modern Large Language Models can reason well but are expensive to run during inference.</li>\n    <li>Speculative Decoding speeds up inference by using a fast draft model to suggest tokens, which are checked by a more accurate target model.</li>\n    <li>Traditional methods face challenges in reasoning tasks due to unnecessary rejections of tokens that are actually equivalent.</li>\n    <li>The new method, Arbitrage, improves efficiency by dynamically deciding when to accept or reject steps based on predictions of quality from models.</li>\n    <li>Arbitrage shows better performance than previous methods, cutting down inference time by up to two times while maintaining accuracy.</li>\n</ul>"}, "publishedAt": "2025-12-04T12:50:53.000Z", "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation", "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to sim2times at matched accuracy.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05033.png", "numComments": 1, "submittedBy": {"_id": "67dc66fe55c24fc4f981a4ab", "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg", "fullname": "Yuezhou Hu", "name": "yuezhouhu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.06628", "authors": [{"_id": "6937b61c19d912300c34a377", "name": "Ruicheng Zhang", "hidden": false}, {"_id": "6937b61c19d912300c34a378", "name": "Mingyang Zhang", "hidden": false}, {"_id": "6937b61c19d912300c34a379", "name": "Jun Zhou", "hidden": false}, {"_id": "6937b61c19d912300c34a37a", "name": "Zhangrui Guo", "hidden": false}, {"_id": "6937b61c19d912300c34a37b", "name": "Xiaofan Liu", "hidden": false}, {"_id": "6937b61c19d912300c34a37c", "name": "Zunnan Xu", "hidden": false}, {"_id": "6937b61c19d912300c34a37d", "name": "Zhizhou Zhong", "hidden": false}, {"_id": "6937b61c19d912300c34a37e", "name": "Puxin Yan", "hidden": false}, {"_id": "6937b61c19d912300c34a37f", "name": "Haocheng Luo", "hidden": false}, {"_id": "6937b61c19d912300c34a380", "name": "Xiu Li", "hidden": false}], "publishedAt": "2025-12-07T02:28:06.000Z", "submittedOnDailyAt": "2025-12-10T05:46:39.074Z", "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment", "submittedOnDailyBy": {"_id": "6481523b3fb124fc9850afed", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481523b3fb124fc9850afed/XqXS2k42HF8HjeUXc21RX.jpeg", "isPro": false, "fullname": "Zunnan Xu", "user": "kkakkkka", "type": "user"}, "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.", "upvotes": 9, "discussionId": "6937b61c19d912300c34a381", "projectPage": "https://github.com/Richard-Zhang-AI/MIND-V", "githubRepo": "https://github.com/Richard-Zhang-AI/MIND-V", "githubRepoAddedBy": "user", "ai_summary": "MIND-V generates long-horizon robotic manipulation videos by integrating semantic reasoning, domain-invariant representations, and physical plausibility through a hierarchical framework.", "ai_keywords": ["Semantic Reasoning Hub", "Behavioral Semantic Bridge", "Motor Video Generator", "Staged Visual Future Rollouts", "GRPO reinforcement learning", "Physical Foresight Coherence", "V-JEPA world model"], "githubStars": 12, "organization": {"_id": "628735cbc83a2d6ab8d14a66", "name": "Tsinghua", "fullname": "Tsinghua University"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIND-V\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u957f\u65f6\u95f4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u3002</li>\n    <li>MIND-V\u7ed3\u5408\u4e86\u9ad8\u5c42\u6b21\u63a8\u7406\u4e0e\u50cf\u7d20\u7ea7\u5408\u6210\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u90e8\u5206\uff1a\u8bed\u4e49\u63a8\u7406\u4e2d\u5fc3\u3001\u884c\u4e3a\u8bed\u4e49\u6865\u548c\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u5668\u3002</li>\n    <li>\u6846\u67b6\u4f7f\u7528\u4e86\u9636\u6bb5\u6027\u89c6\u89c9\u672a\u6765\u56de\u6eda\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u957f\u65f6\u95f4\u64cd\u4f5c\u7684\u7a33\u5065\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7269\u7406\u524d\u77bb\u4e00\u81f4\u6027\u5956\u52b1\uff0c\u786e\u4fdd\u751f\u6210\u7684\u89c6\u9891\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u3002</li>\n    <li>MIND-V\u5728\u957f\u65f6\u95f4\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u53ef\u63a7\u5236\u7684\u6570\u636e\u5408\u6210\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MIND-V is a new framework that creates realistic videos of robots performing complex tasks over longer periods.</li>\n    <li>It combines high-level thinking with detailed video generation using three main parts: a planning hub, a translation bridge, and a video generator.</li>\n    <li>The framework uses a special strategy to improve the robot's ability to perform tasks over time.</li>\n    <li>A reinforcement learning phase ensures the generated videos follow physical laws, making them more realistic.</li>\n    <li>MIND-V shows superior results in generating long robotic manipulation videos compared to existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-06T21:28:06.000Z", "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment", "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06628.png", "numComments": 1, "submittedBy": {"_id": "6481523b3fb124fc9850afed", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6481523b3fb124fc9850afed/XqXS2k42HF8HjeUXc21RX.jpeg", "fullname": "Zunnan Xu", "name": "kkakkkka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "628735cbc83a2d6ab8d14a66", "name": "Tsinghua", "fullname": "Tsinghua University"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02231", "authors": [{"_id": "692fa66626742347f61dab17", "name": "Le Thien Phuc Nguyen", "hidden": false}, {"_id": "692fa66626742347f61dab18", "name": "Zhuoran Yu", "hidden": false}, {"_id": "692fa66626742347f61dab19", "name": "Samuel Low Yu Hang", "hidden": false}, {"_id": "692fa66626742347f61dab1a", "name": "Subin An", "hidden": false}, {"_id": "692fa66626742347f61dab1b", "name": "Jeongik Lee", "hidden": false}, {"_id": "692fa66626742347f61dab1c", "name": "Yohan Ban", "hidden": false}, {"_id": "692fa66626742347f61dab1d", "name": "SeungEun Chung", "hidden": false}, {"_id": "692fa66626742347f61dab1e", "name": "Thanh-Huy Nguyen", "hidden": false}, {"_id": "692fa66626742347f61dab1f", "name": "JuWan Maeng", "hidden": false}, {"_id": "692fa66626742347f61dab20", "name": "Soochahn Lee", "hidden": false}, {"_id": "692fa66626742347f61dab21", "name": "Yong Jae Lee", "hidden": false}], "publishedAt": "2025-12-01T21:57:26.000Z", "submittedOnDailyAt": "2025-12-10T17:22:04.669Z", "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models", "submittedOnDailyBy": {"_id": "660047c56ab19a1d21e9d764", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFPd87Uh5Z3Y5WoCnJqIx.png", "isPro": false, "fullname": "Le Thien Phuc Nguyen", "user": "plnguyen2908", "type": "user"}, "summary": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.", "upvotes": 7, "discussionId": "692fa66726742347f61dab22", "projectPage": "https://plnguyen2908.github.io/AV-SpeakerBench-project-page/", "githubRepo": "https://github.com/plnguyen2908/AV-SpeakerBench", "githubRepoAddedBy": "user", "ai_summary": "AV-SpeakerBench is a benchmark for evaluating speaker-centric audiovisual reasoning in videos, highlighting the importance of audiovisual fusion in multimodal large language models.", "ai_keywords": ["multimodal large language models", "MLMLs", "audiovisual reasoning", "AV-SpeakerBench", "speaker-centric", "fusion-grounded", "expert-curated", "Gemini family", "audiovisual fusion", "Qwen3-Omni-30B"], "githubStars": 1, "organization": {"_id": "6318959fda3063b19c1c1d9b", "name": "Wisconsin", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644645655004f2cb3aefc452/UqU99v2mCOrNNsD8hYv5Q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9700\u8981\u5171\u540c\u7406\u89e3\u89c6\u89c9\u3001\u97f3\u9891\u548c\u8bed\u8a00\uff0c\u4f46\u73b0\u6709\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4eba\u7c7b\u8bed\u8a00\u7684\u7ec6\u81f4\u63a8\u7406\u8bc4\u4f30\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86AV-SpeakerBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b3212\u4e2a\u9009\u62e9\u9898\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u73b0\u5b9e\u89c6\u9891\u4e2d\u7684\u4ee5\u8bf4\u8bdd\u8005\u4e3a\u4e2d\u5fc3\u7684\u89c6\u542c\u63a8\u7406\u3002</li>\n    <li>\u8be5\u57fa\u51c6\u7684\u7279\u70b9\u5305\u62ec\uff1a\u4ee5\u8bf4\u8bdd\u8005\u4e3a\u4e2d\u5fc3\u7684\u9898\u76ee\u8bbe\u8ba1\uff0c\u5d4c\u5165\u89c6\u542c\u4f9d\u8d56\u7684\u95ee\u7b54\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u786e\u4fdd\u65f6\u95f4\u7cbe\u5ea6\u548c\u8de8\u6a21\u6001\u6709\u6548\u6027\u7684\u4e13\u5bb6\u6ce8\u91ca\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0cGemini\u7cfb\u5217\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5f00\u6e90\u7cfb\u7edf\uff0c\u5176\u4e2dGemini 2.5 Pro\u8868\u73b0\u6700\u4f73\u3002</li>\n    <li>\u6211\u4eec\u8ba4\u4e3aAV-SpeakerBench\u4e3a\u672a\u6765\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u7ec6\u81f4\u7684\u89c6\u542c\u63a8\u7406\u65b9\u9762\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AV-SpeakerBench is a new benchmark with 3,212 questions focused on understanding speakers in videos.</li>\n    <li>It emphasizes the importance of who is speaking, what they say, and when they say it, rather than just the visual scene.</li>\n    <li>The questions are designed to reflect how audio and visual elements work together in real-world videos.</li>\n    <li>Testing shows that the Gemini 2.5 Pro model performs best, while open-source models like Qwen3-Omni-30B lag behind in audiovisual reasoning.</li>\n    <li>This benchmark aims to improve how future models understand and reason about audio-visual information.</li>\n</ul>"}, "publishedAt": "2025-12-01T16:57:26.000Z", "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models", "summary": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02231.png", "numComments": 2, "submittedBy": {"_id": "660047c56ab19a1d21e9d764", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFPd87Uh5Z3Y5WoCnJqIx.png", "fullname": "Le Thien Phuc Nguyen", "name": "plnguyen2908", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "6318959fda3063b19c1c1d9b", "name": "Wisconsin", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/644645655004f2cb3aefc452/UqU99v2mCOrNNsD8hYv5Q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07921", "authors": [{"_id": "6938e671dfc35938ba129fd5", "name": "Zongwei Li", "hidden": false}, {"_id": "6938e671dfc35938ba129fd6", "name": "Zhonghang Li", "hidden": false}, {"_id": "6938e671dfc35938ba129fd7", "name": "Zirui Guo", "hidden": false}, {"_id": "6938e671dfc35938ba129fd8", "name": "Xubin Ren", "hidden": false}, {"_id": "6938e671dfc35938ba129fd9", "name": "Chao Huang", "hidden": false}], "publishedAt": "2025-12-08T16:07:13.000Z", "submittedOnDailyAt": "2025-12-10T00:48:19.359Z", "title": "DeepCode: Open Agentic Coding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.", "upvotes": 6, "discussionId": "6938e672dfc35938ba129fda", "githubRepo": "https://github.com/HKUDS/DeepCode", "githubRepoAddedBy": "user", "ai_summary": "DeepCode, a fully autonomous framework, addresses the challenges of document-to-codebase synthesis by optimizing information flow through source compression, structured indexing, knowledge injection, and error correction, achieving state-of-the-art performance and surpassing human experts.", "ai_keywords": ["large language models", "coding agents", "document-to-codebase synthesis", "information overload", "context bottlenecks", "DeepCode", "channel optimization", "blueprint distillation", "stateful code memory", "retrieval-augmented generation", "closed-loop error correction", "PaperBench", "autonomous scientific reproduction"], "githubStars": 11750, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u4f7f\u5f97\u4ee3\u7801\u52a9\u624b\u80fd\u591f\u53d1\u5c55\u4e3a\u4ee3\u7801\u5de5\u7a0b\u5e08\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u9ad8\u4fdd\u771f\u5ea6\u6587\u6863\u5230\u4ee3\u7801\u5e93\u5408\u6210\u7684\u6311\u6218\u3002</li>\n    <li>DeepCode\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u6d41\u7ba1\u7406\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5c06\u4ee3\u7801\u5e93\u5408\u6210\u89c6\u4e3a\u4fe1\u9053\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u56db\u79cd\u4fe1\u606f\u64cd\u4f5c\u6765\u6700\u5927\u5316\u76f8\u5173\u4fe1\u53f7\u3002</li>\n    <li>DeepCode\u5728PaperBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4e3b\u8981\u5546\u4e1a\u4ee3\u7406\u548c\u9876\u5c16\u4eba\u7c7b\u4e13\u5bb6\u3002</li>\n    <li>\u8be5\u7814\u7a76\u4e3a\u81ea\u4e3b\u79d1\u5b66\u91cd\u73b0\u5960\u5b9a\u4e86\u65b0\u57fa\u7840\uff0c\u80fd\u591f\u52a0\u901f\u7814\u7a76\u8bc4\u4f30\u548c\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in large language models have created advanced coding assistants that can now function like code engineers.</li>\n    <li>Current methods struggle to convert documents, like scientific papers, into code effectively due to information overload and limitations in context understanding.</li>\n    <li>DeepCode is a new framework that solves these issues by managing information flow through various techniques.</li>\n    <li>It uses methods like compressing source information, indexing code, and correcting errors to improve code generation based on documents.</li>\n    <li>Tests show DeepCode performs better than leading commercial coding agents and even outperforms top human experts in producing quality code from paper specifications.</li>\n</ul>"}, "publishedAt": "2025-12-08T11:07:13.000Z", "title": "DeepCode: Open Agentic Coding", "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07921.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\u6846\u67b6\uff0c\u4f7f\u7528140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u548c\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\uff0c\u901a\u8fc7\u591aGPU\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\uff0c\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u5f00\u521b\u4e86\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u65b0\u8303\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Existing video generation methods struggle with slow processing and inconsistent results, making them unsuitable for real-time applications like avatar creation.</li>\n    <li>Live Avatar is a new system that allows for fast and high-quality avatar generation using a powerful 14-billion-parameter model.</li>\n    <li>The system uses a method called Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by using multiple GPUs simultaneously.</li>\n    <li>To improve the quality and consistency of the generated avatars, it introduces the Rolling Sink Frame Mechanism (RSFM), which helps maintain appearance over time.</li>\n    <li>Live Avatar achieves impressive performance, generating avatars at 20 frames per second on 5 GPUs, setting a new standard for real-time video synthesis.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5b9e\u73b0\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\uff0c\u6269\u5c55\u6027\u6709\u9650\uff0c\u800cWan-Move\u80fd\u63d0\u4f9b\u7cbe\u786e\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u5b9e\u73b0\u5bf9\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u751f\u6210\u5bf9\u9f50\u7684\u65f6\u7a7a\u7279\u5f81\u56fe\u3002</li>\n    <li>Wan-Move\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u56fe\u50cf\u8f6c\u89c6\u9891\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\uff0c\u65e0\u9700\u66f4\u6539\u67b6\u6784\uff0c\u5e76\u4e14\u6613\u4e8e\u6269\u5c55\u3002</li>\n    <li>\u7ecf\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cWan-Move\u5728\u8fd0\u52a8\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5546\u4e1a\u5de5\u5177\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It provides precise and high-quality control over how objects move in videos, addressing issues found in previous methods.</li>\n    <li>The framework uses dense point trajectories to represent object movements, allowing for detailed scene control.</li>\n    <li>Wan-Move integrates smoothly into existing video models without needing major changes, making it easy to use and scale.</li>\n    <li>It produces high-quality videos and has been evaluated with a comprehensive benchmark called MoveBench, showing better motion quality compared to other tools.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04987", "authors": [{"_id": "6932458a6d1060ca587a2618", "name": "Nex-AGI Team", "hidden": false}, {"_id": "6932458a6d1060ca587a261a", "name": "Yuxuan Cai", "hidden": false}, {"_id": "6932458a6d1060ca587a261b", "name": "Lu Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a261c", "name": "Qiaoling Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a261d", "name": "Yuyang Ding", "hidden": false}, {"_id": "6932458a6d1060ca587a261e", "name": "Liwen Fan", "hidden": false}, {"_id": "6932458a6d1060ca587a261f", "name": "Wenjie Fu", "hidden": false}, {"_id": "6932458a6d1060ca587a2620", "user": {"_id": "658bd417925aadd43303566a", "avatarUrl": "/avatars/e97f6696817caaa4564a33f12c7b9090.svg", "isPro": false, "fullname": "Gao", "user": "Yufei0707", "type": "user"}, "name": "Yufei Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:36.106Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2621", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:43.376Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2622", "user": {"_id": "6461e09759daabed7575b7a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6461e09759daabed7575b7a2/sxfko49Q7ta0dCfgrpqoB.jpeg", "isPro": false, "fullname": "PinxueGuo", "user": "PinxueGuo", "type": "user"}, "name": "Pinxue Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:14:54.854Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2623", "name": "Zhenhua Han", "hidden": false}, {"_id": "6932458a6d1060ca587a2624", "name": "Zhengfu He", "hidden": false}, {"_id": "6932458a6d1060ca587a2625", "name": "Hanglei Hu", "hidden": false}, {"_id": "6932458a6d1060ca587a2626", "name": "Kai Hu", "hidden": false}, {"_id": "6932458a6d1060ca587a2627", "name": "Shengjia Hua", "hidden": false}, {"_id": "6932458a6d1060ca587a2628", "name": "Tianyu Huai", "hidden": false}, {"_id": "6932458a6d1060ca587a2629", "name": "Baodai Huang", "hidden": false}, {"_id": "6932458a6d1060ca587a262a", "name": "Li Ji", "hidden": false}, {"_id": "6932458a6d1060ca587a262b", "name": "Zhen Jiang", "hidden": false}, {"_id": "6932458a6d1060ca587a262c", "name": "Zhikai Lei", "hidden": false}, {"_id": "6932458a6d1060ca587a262d", "name": "Bufan Li", "hidden": false}, {"_id": "6932458a6d1060ca587a262e", "name": "Jiahang Lin", "hidden": false}, {"_id": "6932458a6d1060ca587a262f", "name": "Lizhi Lin", "hidden": false}, {"_id": "6932458a6d1060ca587a2630", "name": "Jinxiu Liu", "hidden": false}, {"_id": "6932458a6d1060ca587a2631", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:39.885Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2632", "name": "Ziming Liu", "hidden": false}, {"_id": "6932458a6d1060ca587a2633", "name": "Yuchen Ni", "hidden": false}, {"_id": "6932458a6d1060ca587a2634", "name": "Pengfang Qian", "hidden": false}, {"_id": "6932458a6d1060ca587a2635", "name": "Yujiong Shen", "hidden": false}, {"_id": "6932458a6d1060ca587a2636", "name": "Qingyun Shi", "hidden": false}, {"_id": "6932458a6d1060ca587a2637", "name": "Wentao Shu", "hidden": false}, {"_id": "6932458a6d1060ca587a2638", "name": "Peng Sun", "hidden": false}, {"_id": "6932458a6d1060ca587a2639", "name": "Yiran Suo", "hidden": false}, {"_id": "6932458a6d1060ca587a263a", "name": "Tian Tang", "hidden": false}, {"_id": "6932458a6d1060ca587a263b", "name": "Boyu Tian", "hidden": false}, {"_id": "6932458a6d1060ca587a263c", "user": {"_id": "6542391f3fcd1aee202383d2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5O7dZAVGmwhsx1THSJ-gn.jpeg", "isPro": false, "fullname": "wang", "user": "guoteng", "type": "user"}, "name": "Guoteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:14:58.106Z", "hidden": false}, {"_id": "6932458a6d1060ca587a263d", "name": "Junzhe Wang", "hidden": false}, {"_id": "6932458a6d1060ca587a263e", "name": "Peixin Wang", "hidden": false}, {"_id": "6932458a6d1060ca587a263f", "user": {"_id": "653a6e5cae155b92bae77b74", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg", "isPro": false, "fullname": "Zhiheng Xi", "user": "WooooDyy", "type": "user"}, "name": "Zhiheng Xi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:45.863Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2640", "name": "Hang Yan", "hidden": false}, {"_id": "6932458a6d1060ca587a2641", "user": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "name": "Jie Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:12.116Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2642", "name": "Zhixiong Yang", "hidden": false}, {"_id": "6932458a6d1060ca587a2643", "name": "Tianchu Yao", "hidden": false}, {"_id": "6932458a6d1060ca587a2644", "name": "Guangze Ye", "hidden": false}, {"_id": "6932458a6d1060ca587a2645", "name": "Qianxi Yu", "hidden": false}, {"_id": "6932458a6d1060ca587a2646", "user": {"_id": "6334f2f1259c518276efa730", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334f2f1259c518276efa730/z_SH_OBkDyj4RCN9mqsKS.jpeg", "isPro": false, "fullname": "Shuo Zhang", "user": "Meteonis", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:41.640Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2647", "name": "Xinyue Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2648", "name": "Yiqi Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2649", "name": "Jiarong Zhao", "hidden": false}, {"_id": "6932458a6d1060ca587a264a", "name": "Miao Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a264b", "name": "Rui Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a264c", "name": "Enyu Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264d", "name": "Jiazheng Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264e", "name": "Maosen Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264f", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a2650", "name": "Tao Gui", "hidden": false}, {"_id": "6932458a6d1060ca587a2651", "name": "Yining Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a2652", "name": "Xinchi Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a2653", "name": "Jie Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a2654", "user": {"_id": "62061f8f03825909dcbeba27", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62061f8f03825909dcbeba27/byOi6gxlycUtsUPIUPscQ.jpeg", "isPro": false, "fullname": "Siyuan Feng", "user": "Siyuan", "type": "user"}, "name": "Siyuan Feng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:38.168Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2655", "name": "Qin Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a2656", "name": "Liang He", "hidden": false}, {"_id": "6932458a6d1060ca587a2657", "name": "Qi Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2658", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6932458a6d1060ca587a2659", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2025-12-04T16:57:02.000Z", "submittedOnDailyAt": "2025-12-05T00:08:10.618Z", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "upvotes": 66, "discussionId": "6932458a6d1060ca587a265a", "githubRepo": "https://github.com/nex-agi/Nex-N1", "ai_summary": "The introduction of NexAU, NexA4A, and NexGAP enables the scaling of complexity, diversity, and fidelity in interactive environments for training large language models as autonomous agents, resulting in superior performance.", "ai_keywords": ["Large Language Models", "autonomous agents", "incentive-driven decision making", "policy learning", "interaction signals", "NexAU", "agent hierarchies", "NexA4A", "natural language", "NexGAP", "simulation-reality gap", "grounded trajectories synthesis", "SWE-bench", "tau2"], "githubStars": 71, "organization": {"_id": "6907441c72f7d95376e910a5", "name": "nex-agi", "fullname": "Nex AGI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u4ece\u88ab\u52a8\u54cd\u5e94\u8005\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u8fd9\u9700\u8981\u6539\u53d8\u5b66\u4e60\u65b9\u5f0f\u3002</li>\n    <li>\u8fd9\u79cd\u8f6c\u53d8\u53d7\u5230\u7f3a\u4e4f\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u4e92\u52a8\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u65b9\u9762\u6765\u6269\u5c55\u4e92\u52a8\u73af\u5883\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff1a\u590d\u6742\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578bNex-N1\u5728\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u4e92\u52a8\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u524d\u6cbf\u7684\u4e13\u6709\u6a21\u578b\u76f8\u7ade\u4e89\u3002</li>\n    <li>\u6211\u4eec\u5c06Nex\u751f\u6001\u7cfb\u7edf\u548c\u6a21\u578b\u6743\u91cd\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving from simple responders to more independent agents, requiring new ways of learning.</li>\n    <li>This evolution is hindered by the lack of systems that can create useful feedback for teaching these agents.</li>\n    <li>We present a new method to enhance the variety and complexity of interactive environments for training agents.</li>\n    <li>The method focuses on three key areas: building complex agent structures, generating diverse agent hierarchies, and improving real-world simulation accuracy.</li>\n    <li>Our trained model, Nex-N1, outperforms existing open-source models and competes well with leading proprietary ones, and we are sharing our tools and models for further research.</li>\n</ul>"}, "publishedAt": "2025-12-04T11:57:02.000Z", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04987.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "6907441c72f7d95376e910a5", "name": "nex-agi", "fullname": "Nex AGI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08478", "authors": [{"_id": "6938e00fdfc35938ba129f4f", "name": "Yuning Gong", "hidden": false}, {"_id": "6938e00fdfc35938ba129f50", "name": "Yifei Liu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f51", "name": "Yifan Zhan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f52", "name": "Muyao Niu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f53", "name": "Xueying Li", "hidden": false}, {"_id": "6938e00fdfc35938ba129f54", "name": "Yuanjun Liao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f55", "name": "Jiaming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f56", "name": "Yuanyuan Gao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f57", "name": "Jiaqi Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f58", "name": "Minming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f59", "name": "Li Zhou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5a", "name": "Yuning Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5b", "name": "Wei Wang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5c", "name": "Xiaoqing Hou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5d", "name": "Huaxi Huang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5e", "name": "Shixiang Tang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5f", "name": "Le Ma", "hidden": false}, {"_id": "6938e00fdfc35938ba129f60", "name": "Dingwen Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f61", "name": "Xue Yang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f62", "name": "Junchi Yan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f63", "name": "Yanchi Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f64", "name": "Yinqiang Zheng", "hidden": false}, {"_id": "6938e00fdfc35938ba129f65", "name": "Xiao Sun", "hidden": false}, {"_id": "6938e00fdfc35938ba129f66", "user": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "name": "Zhihang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:56:28.162Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "publishedAt": "2025-12-09T10:54:58.000Z", "submittedOnDailyAt": "2025-12-10T07:43:37.566Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "submittedOnDailyBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "upvotes": 64, "discussionId": "6938e00fdfc35938ba129f67", "projectPage": "https://visionary-laboratory.github.io/visionary/", "githubRepo": "https://github.com/Visionary-Laboratory/visionary", "githubRepoAddedBy": "user", "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.", "ai_keywords": ["Neural rendering", "3D Gaussian Splatting", "3DGS", "WebGPU", "ONNX inference", "Gaussian Generator contract", "three.js", "TypeScript API", "MLP-based 3DGS", "4DGS", "neural avatars", "style transformation", "GPU-based primitive sorting", "World Model Carrier"], "githubStars": 162, "summary_zh": "<ul>\n    <li>Visionary\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7f51\u9875\u5e73\u53f0\uff0c\u4e13\u6ce8\u4e8e\u5b9e\u65f6\u9ad8\u65af\u70b9\u4e91\u548c\u7f51\u683c\u6e32\u67d3\u3002</li>\n    <li>\u8be5\u5e73\u53f0\u4f7f\u7528\u9ad8\u6548\u7684WebGPU\u6e32\u67d3\u5668\uff0c\u652f\u6301\u52a8\u6001\u795e\u7ecf\u5904\u7406\uff0c\u63d0\u4f9b\u8f7b\u4fbf\u7684\u6d4f\u89c8\u5668\u4f53\u9a8c\u3002</li>\n    <li>Visionary\u5f15\u5165\u4e86\u6807\u51c6\u5316\u7684\u9ad8\u65af\u751f\u6210\u5668\u5408\u540c\uff0c\u652f\u6301\u591a\u79cd\u7b97\u6cd5\u751f\u6210\u6216\u66f4\u65b0\u9ad8\u65af\u6570\u636e\u3002</li>\n    <li>\u5e73\u53f0\u4e0ethree.js\u5e93\u517c\u5bb9\uff0c\u63d0\u4f9b\u7b80\u6d01\u7684TypeScript API\uff0c\u4fbf\u4e8e\u4e0e\u73b0\u6709\u7f51\u9875\u5e94\u7528\u96c6\u6210\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cVisionary\u5728\u6e32\u67d3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709Web\u67e5\u770b\u5668\uff0c\u652f\u6301\u591a\u79cd3DGS\u53d8\u4f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Visionary is a new, open platform for rendering 3D models using a technique called Gaussian Splatting.</li>\n  <li>It runs directly in web browsers and is designed for real-time performance, making it easy to use with just a click.</li>\n  <li>The platform allows for dynamic updates of 3D models and supports various algorithms for generating content.</li>\n  <li>Visionary is more efficient than existing viewers, as it uses advanced GPU techniques for better rendering speed.</li>\n  <li>It includes tools for easy integration into web applications and supports various types of neural networks and enhancements.</li>\n</ul>"}, "publishedAt": "2025-12-09T05:54:58.000Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08478.png", "numComments": 3, "submittedBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "fullname": "Zhihang Zhong", "name": "Zuica96", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07461", "authors": [{"_id": "6937b96219d912300c34a398", "user": {"_id": "626b889ff451470f861d8c78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651214465695-noauth.jpeg", "isPro": false, "fullname": "victor wu", "user": "victor-wu", "type": "user"}, "name": "Tong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:22.731Z", "hidden": false}, {"_id": "6937b96219d912300c34a399", "user": {"_id": "6191cc9e6d34e827404cebab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg", "isPro": false, "fullname": "Yang", "user": "jacklanda", "type": "user"}, "name": "Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:20.278Z", "hidden": false}, {"_id": "6937b96219d912300c34a39a", "user": {"_id": "624505fcd083d28d314de3dd", "avatarUrl": "/avatars/92cf6b6a1d81d7958dbbd21f0bf63f8f.svg", "isPro": false, "fullname": "bai jun", "user": "ba1jun", "type": "user"}, "name": "Jun Bai", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:17.404Z", "hidden": false}, {"_id": "6937b96219d912300c34a39b", "name": "Zixia Jia", "hidden": false}, {"_id": "6937b96219d912300c34a39c", "name": "Shuyi Zhang", "hidden": false}, {"_id": "6937b96219d912300c34a39d", "name": "Ziyong Lin", "hidden": false}, {"_id": "6937b96219d912300c34a39e", "user": {"_id": "64b119c4372d43407723136b", "avatarUrl": "/avatars/d523e181993eea06b7f6a71a592c995e.svg", "isPro": false, "fullname": "YANTING WANG", "user": "Noane", "type": "user"}, "name": "Yanting Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:14.418Z", "hidden": false}, {"_id": "6937b96219d912300c34a39f", "name": "Song-Chun Zhu", "hidden": false}, {"_id": "6937b96219d912300c34a3a0", "name": "Zilong Zheng", "hidden": false}], "publishedAt": "2025-12-08T11:39:43.000Z", "submittedOnDailyAt": "2025-12-09T04:12:55.960Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "submittedOnDailyBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "isPro": false, "fullname": "Zilong Zheng", "user": "zlzheng", "type": "user"}, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "upvotes": 49, "discussionId": "6937b96219d912300c34a3a1", "projectPage": "https://bigai-nlco.github.io/Native-Parallel-Reasoner/", "githubRepo": "https://github.com/bigai-nlco/Native-Parallel-Reasoner", "ai_summary": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.", "ai_keywords": ["Native Parallel Reasoner", "Large Language Models", "self-evolve", "parallel reasoning", "self-distilled progressive training", "cold-start format discovery", "topological constraints", "Parallel-Aware Policy Optimization", "branching policies", "execution graph", "adaptive decomposition", "trial and error", "NPR Engine", "memory management", "flow control", "parallel RL training", "reasoning benchmarks", "Qwen3-4B", "genuine parallel execution", "autoregressive decoding", "agentic reasoning"], "githubStars": 18, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u672c\u5730\u5e76\u884c\u63a8\u7406\u5668\u201d\uff08NPR\uff09\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u6211\u53d1\u5c55\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>NPR\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u5c06\u6a21\u578b\u4ece\u987a\u5e8f\u6a21\u62df\u8f6c\u53d8\u4e3a\u672c\u5730\u5e76\u884c\u8ba4\u77e5\u3002</li>\n    <li>\u5b83\u91c7\u7528\u81ea\u6211\u84b8\u998f\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u4ece\u201c\u51b7\u542f\u52a8\u201d\u683c\u5f0f\u53d1\u73b0\u8fc7\u6e21\u5230\u4e25\u683c\u7684\u62d3\u6251\u7ea6\u675f\u3002</li>\n    <li>NPR\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08PAPO\uff09\uff0c\u5141\u8bb8\u6a21\u578b\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u9002\u5e94\u6027\u5206\u89e3\u3002</li>\n    <li>\u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNPR\u5728Qwen3-4B\u4e0a\u7684\u8bad\u7ec3\u8868\u73b0\u63d0\u9ad8\u4e86\u6700\u591a24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e86\u6700\u591a4.6\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The Native Parallel Reasoner (NPR) is a new system that helps Large Language Models (LLMs) learn to think in parallel without needing a teacher.</li>\n    <li>NPR uses three main innovations to improve how models process information: a new training method, a special optimization algorithm, and an advanced memory management system.</li>\n    <li>With NPR, models can adapt and learn by trying different approaches, improving their reasoning skills significantly.</li>\n    <li>In tests, NPR achieved up to 24.5% better performance and made reasoning tasks up to 4.6 times faster compared to older methods.</li>\n    <li>NPR allows for full parallel processing, setting a new benchmark for efficient and scalable reasoning in AI models.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:39:43.000Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07461.png", "numComments": 1, "submittedBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "fullname": "Zilong Zheng", "name": "zlzheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05111", "authors": [{"_id": "693267756d1060ca587a2780", "name": "Shengyuan Ding", "hidden": false}, {"_id": "693267756d1060ca587a2781", "user": {"_id": "64f5f8dd9b17cd59c453c57f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg", "isPro": false, "fullname": "Xinyu Fang", "user": "nebulae09", "type": "user"}, "name": "Xinyu Fang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:08.410Z", "hidden": false}, {"_id": "693267756d1060ca587a2782", "name": "Ziyu Liu", "hidden": false}, {"_id": "693267756d1060ca587a2783", "user": {"_id": "63859cf3b2906edaf83af9f0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png", "isPro": true, "fullname": "Yuhang Zang", "user": "yuhangzang", "type": "user"}, "name": "Yuhang Zang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:11.324Z", "hidden": false}, {"_id": "693267756d1060ca587a2784", "name": "Yuhang Cao", "hidden": false}, {"_id": "693267756d1060ca587a2785", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "693267756d1060ca587a2786", "name": "Haodong Duan", "hidden": false}, {"_id": "693267756d1060ca587a2787", "name": "Xiaoyi Dong", "hidden": false}, {"_id": "693267756d1060ca587a2788", "name": "Jianze Liang", "hidden": false}, {"_id": "693267756d1060ca587a2789", "name": "Bin Wang", "hidden": false}, {"_id": "693267756d1060ca587a278a", "name": "Conghui He", "hidden": false}, {"_id": "693267756d1060ca587a278b", "name": "Dahua Lin", "hidden": false}, {"_id": "693267756d1060ca587a278c", "name": "Jiaqi Wang", "hidden": false}], "publishedAt": "2025-12-04T18:59:52.000Z", "submittedOnDailyAt": "2025-12-05T02:38:10.825Z", "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning", "submittedOnDailyBy": {"_id": "646cd947da8e99940b6e55cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg", "isPro": false, "fullname": "Shengyuan Ding", "user": "ChrisDing1105", "type": "user"}, "summary": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.", "upvotes": 43, "discussionId": "693267766d1060ca587a278d", "projectPage": "https://github.com/InternLM/ARM-Thinker", "githubRepo": "https://github.com/InternLM/ARM-Thinker", "ai_summary": "ARM-Thinker, an agentic reward model, uses external tools for verification, improving accuracy and interpretability in complex multimodal reasoning tasks.", "ai_keywords": ["reward models", "vision-language systems", "hallucination", "visual grounding", "external tools", "image cropping", "doc page retrieval", "multi-stage reinforcement learning", "tool-calling decisions", "judgment accuracy", "agentic reward modeling", "ARMBench-VL", "fine-grained visual grounding", "multi-page document understanding", "instruction following", "multimodal math", "logical reasoning"], "githubStars": 53, "organization": {"_id": "64a2d5fa81252883206f24c9", "name": "internlm", "fullname": "Intern Large Models", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"}, "summary_zh": "<ul>\n    <li>\u5956\u52b1\u6a21\u578b\u5bf9\u89c6\u89c9-\u8bed\u8a00\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u3001\u89c6\u89c9\u57fa\u7840\u8584\u5f31\u548c\u65e0\u6cd5\u4f7f\u7528\u5de5\u5177\u9a8c\u8bc1\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ARM-Thinker\uff0c\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u8c03\u7528\u5916\u90e8\u5de5\u5177\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u7528\u53ef\u9a8c\u8bc1\u7684\u8bc1\u636e\u6765\u652f\u6491\u5224\u65ad\u3002</li>\n    <li>ARM-Thinker\u80fd\u591f\u9a8c\u8bc1\u7ec6\u81f4\u7684\u89c6\u89c9\u7ec6\u8282\u3001\u8de8\u9875\u8bc1\u636e\u5e76\u9a8c\u8bc1\u63a8\u7406\u4e3b\u5f20\uff0c\u8fd9\u662f\u73b0\u6709\u5956\u52b1\u6a21\u578b\u6240\u7f3a\u4e4f\u7684\u80fd\u529b\u3002</li>\n    <li>\u901a\u8fc7\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3ARM-Thinker\uff0c\u4f18\u5316\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u548c\u5224\u65ad\u51c6\u786e\u6027\u3002</li>\n    <li>ARM-Thinker\u5728\u5956\u52b1\u5efa\u6a21\u57fa\u51c6\u4e0a\u63d0\u9ad8\u4e86+16.2%\u7684\u5e73\u5747\u5206\u6570\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u4e0a\u8d85\u8fc7\u4e86\u57fa\u7ebf\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current reward models for vision-language systems have issues like hallucination and weak visual grounding.</li>\n    <li>ARM-Thinker is a new reward model that can use external tools to verify its judgments, improving reliability.</li>\n    <li>It can check detailed visual information, reference multiple documents, and validate reasoning claims, which other models cannot do.</li>\n    <li>ARM-Thinker is trained using multi-stage reinforcement learning to improve decision-making and judgment accuracy.</li>\n    <li>It shows significant improvements in various benchmarks, enhancing both the accuracy and interpretability of reward models.</li>\n</ul>"}, "publishedAt": "2025-12-04T13:59:52.000Z", "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning", "summary": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05111.png", "numComments": 2, "submittedBy": {"_id": "646cd947da8e99940b6e55cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg", "fullname": "Shengyuan Ding", "name": "ChrisDing1105", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "64a2d5fa81252883206f24c9", "name": "internlm", "fullname": "Intern Large Models", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07525", "authors": [{"_id": "693794d319d912300c34a291", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:38:01.257Z", "hidden": false}, {"_id": "693794d319d912300c34a292", "name": "Yuerong Song", "hidden": false}, {"_id": "693794d319d912300c34a293", "name": "Zhigeng Liu", "hidden": false}, {"_id": "693794d319d912300c34a294", "user": {"_id": "64805e6dde559d48dbb00627", "avatarUrl": "/avatars/29ca34546411dcc28bbc934e3c26a2ba.svg", "isPro": false, "fullname": "Zengfeng", "user": "ZengfengHuang", "type": "user"}, "name": "Zengfeng Huang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:11.754Z", "hidden": false}, {"_id": "693794d319d912300c34a295", "user": {"_id": "6491cd52b1e5d3444528edb1", "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg", "isPro": false, "fullname": "Qipeng Guo", "user": "QipengGuo", "type": "user"}, "name": "Qipeng Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:18.117Z", "hidden": false}, {"_id": "693794d319d912300c34a296", "name": "Zhaoxiang Liu", "hidden": false}, {"_id": "693794d319d912300c34a297", "name": "Shiguo Lian", "hidden": false}, {"_id": "693794d319d912300c34a298", "user": {"_id": "64de18f41d826d7355c285e7", "avatarUrl": "/avatars/23e2c44e3a593415becc02463980f6e8.svg", "isPro": false, "fullname": "Ziwei He", "user": "ziweihe", "type": "user"}, "name": "Ziwei He", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T20:17:41.271Z", "hidden": false}, {"_id": "693794d319d912300c34a299", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:28.411Z", "hidden": false}], "publishedAt": "2025-12-08T12:59:54.000Z", "submittedOnDailyAt": "2025-12-09T00:49:29.234Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "upvotes": 41, "discussionId": "693794d419d912300c34a29a", "githubRepo": "https://github.com/OpenMOSS/rope_pp", "ai_summary": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.", "ai_keywords": ["Rotary Position Embeddings", "Large Language Models", "complex-valued dot product", "attention score", "long-context dependencies", "positional information"], "githubStars": 12, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7f16\u7801\u5e8f\u5217\u987a\u5e8f\u7684\u6807\u51c6\u65b9\u6cd5\u3002</li>\n    <li>\u76ee\u524d\u7684\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u8fdb\u884c\u6ce8\u610f\u529b\u5206\u6570\u8ba1\u7b97\uff0c\u5ffd\u7565\u4e86\u5305\u542b\u91cd\u8981\u76f8\u4f4d\u4fe1\u606f\u7684\u865a\u90e8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u5f15\u5165\u88ab\u4e22\u5f03\u7684\u865a\u90e8\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u6765\u521b\u5efa\u53cc\u6210\u5206\u6ce8\u610f\u529b\u5206\u6570\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u5efa\u6a21\u957f\u671f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u65b9\u9762\u6548\u679c\u66f4\u4f73\uff0c\u80fd\u4fdd\u7559\u66f4\u591a\u4f4d\u7f6e\u4fe1\u606f\u3002</li>\n    <li>\u5728\u591a\u4e2a\u957f\u671f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6807\u51c6\u7684RoPE\uff0c\u5c24\u5176\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u660e\u663e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Rotary Position Embeddings (RoPE) help Large Language Models understand the order of sequences by using rotations in the complex plane.</li>\n    <li>Current methods only use the real part of the complex dot product for attention scores, ignoring the imaginary part which has important information.</li>\n    <li>This paper introduces a new approach that includes the imaginary component, creating a dual-component attention score.</li>\n    <li>This method improves the model's ability to handle long-context dependencies by keeping more positional information.</li>\n    <li>Tests show that this new approach consistently outperforms standard RoPE, especially with longer contexts, and the code is available online.</li>\n</ul>"}, "publishedAt": "2025-12-08T07:59:54.000Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07525.png", "numComments": 1, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07951", "authors": [{"_id": "6938e892dfc35938ba129ff5", "name": "Zekai Luo", "hidden": false}, {"_id": "6938e892dfc35938ba129ff6", "name": "Zongze Du", "hidden": false}, {"_id": "6938e892dfc35938ba129ff7", "name": "Zhouhang Zhu", "hidden": false}, {"_id": "6938e892dfc35938ba129ff8", "name": "Hao Zhong", "hidden": false}, {"_id": "6938e892dfc35938ba129ff9", "user": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "name": "Muzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T10:51:43.541Z", "hidden": false}, {"_id": "6938e892dfc35938ba129ffa", "name": "Wen Wang", "hidden": false}, {"_id": "6938e892dfc35938ba129ffb", "name": "Yuling Xi", "hidden": false}, {"_id": "6938e892dfc35938ba129ffc", "name": "Chenchen Jing", "hidden": false}, {"_id": "6938e892dfc35938ba129ffd", "name": "Hao Chen", "hidden": false}, {"_id": "6938e892dfc35938ba129ffe", "name": "Chunhua Shen", "hidden": false}], "publishedAt": "2025-12-08T19:00:04.000Z", "submittedOnDailyAt": "2025-12-10T00:59:39.366Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "submittedOnDailyBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "upvotes": 40, "discussionId": "6938e892dfc35938ba129fff", "projectPage": "https://aim-uofa.github.io/LivingSwap", "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.", "ai_keywords": ["keyframe conditioning", "video reference guidance", "temporal stitching", "identity preservation", "high-fidelity reconstruction", "Face2Face dataset"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u5728\u7535\u5f71\u548c\u5a31\u4e50\u5236\u4f5c\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5728\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4ecd\u7136\u5f88\u5177\u6311\u6218\u6027\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86LivingSwap\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u53c2\u8003\u7684\u89c6\u9891\u6362\u8138\u6a21\u578b\uff0c\u5229\u7528\u6e90\u89c6\u9891\u7684\u4e30\u5bcc\u89c6\u89c9\u5c5e\u6027\u6765\u63d0\u5347\u6362\u8138\u6548\u679c\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u5173\u952e\u5e27\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u7075\u6d3b\u63a7\u5236\u76ee\u6807\u8eab\u4efd\u7684\u6ce8\u5165\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u8eab\u4efd\u4fdd\u7559\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u53c2\u8003\u6307\u5bfc\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u914d\u5bf9\u6362\u8138\u6570\u636e\u96c6Face2Face\uff0c\u5e76\u53cd\u8f6c\u6570\u636e\u5bf9\u4ee5\u786e\u4fdd\u53ef\u9760\u7684\u76d1\u7763\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c06\u76ee\u6807\u8eab\u4efd\u4e0e\u6e90\u89c6\u9891\u7684\u8868\u60c5\u3001\u5149\u7167\u548c\u8fd0\u52a8\u65e0\u7f1d\u878d\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5236\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4eba\u5de5\u5de5\u4f5c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video face swapping is important for movies, but it's hard to keep it looking good over long videos.</li>\n    <li>The research introduces LivingSwap, a new method that uses keyframes from videos to improve face swapping quality and consistency.</li>\n    <li>LivingSwap allows for flexible editing by using keyframes to add the target person's identity while maintaining stable visuals.</li>\n    <li>A new dataset called Face2Face was created to help train the model and ensure good results.</li>\n    <li>Tests show that LivingSwap achieves top results, blending the target's identity with the source video's look and feel, making production easier.</li>\n</ul>"}, "publishedAt": "2025-12-08T14:00:04.000Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07951.png", "numComments": 1, "submittedBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "fullname": "zhumuzhi", "name": "Z-MU-Z", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04678", "authors": [{"_id": "693248cc6d1060ca587a26fa", "name": "Yunhong Lu", "hidden": false}, {"_id": "693248cc6d1060ca587a26fb", "name": "Yanhong Zeng", "hidden": false}, {"_id": "693248cc6d1060ca587a26fc", "name": "Haobo Li", "hidden": false}, {"_id": "693248cc6d1060ca587a26fd", "name": "Hao Ouyang", "hidden": false}, {"_id": "693248cc6d1060ca587a26fe", "user": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "name": "Qiuyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:25.231Z", "hidden": false}, {"_id": "693248cc6d1060ca587a26ff", "name": "Ka Leong Cheng", "hidden": false}, {"_id": "693248cc6d1060ca587a2700", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "693248cc6d1060ca587a2701", "name": "Hengyuan Cao", "hidden": false}, {"_id": "693248cc6d1060ca587a2702", "name": "Zhipeng Zhang", "hidden": false}, {"_id": "693248cc6d1060ca587a2703", "name": "Xing Zhu", "hidden": false}, {"_id": "693248cc6d1060ca587a2704", "name": "Yujun Shen", "hidden": false}, {"_id": "693248cc6d1060ca587a2705", "name": "Min Zhang", "hidden": false}], "publishedAt": "2025-12-04T11:12:13.000Z", "submittedOnDailyAt": "2025-12-05T00:25:31.113Z", "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "submittedOnDailyBy": {"_id": "63d4b843df01ef426a0f79fb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg", "isPro": false, "fullname": "Yanhong Zeng", "user": "zengyh1900", "type": "user"}, "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "upvotes": 36, "discussionId": "693248cc6d1060ca587a2706", "projectPage": "https://reward-forcing.github.io/", "githubRepo": "https://github.com/JaydenLyh/Reward-Forcing", "ai_summary": "The paper introduces Reward Forcing, which enhances video generation by updating sink tokens with EMA-Sink and using Rewarded Distribution Matching Distillation to prioritize dynamic content.", "ai_keywords": ["video diffusion models", "sliding window attention", "sink tokens", "Reward Forcing", "EMA-Sink", "exponential moving average", "Rewarded Distribution Matching Distillation", "Re-DMD", "vision-language model", "motion dynamics", "data fidelity", "streaming video generation"], "githubStars": 111, "summary_zh": "<ul>\n    <li>\u9ad8\u6548\u7684\u6d41\u5a92\u4f53\u89c6\u9891\u751f\u6210\u5bf9\u4e8e\u6a21\u62df\u4e92\u52a8\u548c\u52a8\u6001\u4e16\u754c\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u6807\u8bb0\uff0c\u5bfc\u81f4\u89c6\u9891\u5e27\u8fc7\u4e8e\u4f9d\u8d56\u521d\u59cb\u5e27\uff0c\u4ece\u800c\u5f71\u54cd\u52a8\u6001\u6548\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u201c\u5956\u52b1\u5f3a\u5236\u201d\u6846\u67b6\uff0c\u5305\u62ecEMA-Sink\u548c\u5956\u52b1\u5206\u5e03\u5339\u914d\u84b8\u998f\uff08Re-DMD\uff09\u3002</li>\n    <li>EMA-Sink\u901a\u8fc7\u66f4\u65b0\u56fa\u5b9a\u5927\u5c0f\u7684\u6807\u8bb0\uff0c\u6355\u6349\u957f\u671f\u548c\u8fd1\u671f\u52a8\u6001\uff0c\u907f\u514d\u4e86\u521d\u59cb\u5e27\u7684\u590d\u5236\u3002</li>\n    <li>Re-DMD\u4f18\u5148\u8003\u8651\u52a8\u6001\u5185\u5bb9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u7684\u771f\u5b9e\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Efficient video generation is important for creating interactive simulations, but current methods rely too heavily on initial frames, which reduces motion dynamics.</li>\n    <li>The new approach, called Reward Forcing, includes two main features: EMA-Sink and Rewarded Distribution Matching Distillation (Re-DMD).</li>\n    <li>EMA-Sink uses fixed-size tokens that adapt over time to better capture both long-term context and recent changes, preventing issues from copying initial frames.</li>\n    <li>Re-DMD improves the model\u2019s ability to focus on dynamic content by prioritizing more dynamic samples, leading to better motion quality.</li>\n    <li>This method achieves top performance in benchmarks and generates high-quality streaming video at 23.1 frames per second on a single GPU.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:12:13.000Z", "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04678.png", "numComments": 3, "submittedBy": {"_id": "63d4b843df01ef426a0f79fb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg", "fullname": "Yanhong Zeng", "name": "zengyh1900", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04926", "authors": [{"_id": "693241dd6d1060ca587a25fd", "name": "Yueming Pan", "hidden": false}, {"_id": "693241dd6d1060ca587a25fe", "name": "Ruoyu Feng", "hidden": false}, {"_id": "693241dd6d1060ca587a25ff", "name": "Qi Dai", "hidden": false}, {"_id": "693241dd6d1060ca587a2600", "name": "Yuqi Wang", "hidden": false}, {"_id": "693241dd6d1060ca587a2601", "name": "Wenfeng Lin", "hidden": false}, {"_id": "693241dd6d1060ca587a2602", "name": "Mingyu Guo", "hidden": false}, {"_id": "693241dd6d1060ca587a2603", "name": "Chong Luo", "hidden": false}, {"_id": "693241dd6d1060ca587a2604", "name": "Nanning Zheng", "hidden": false}], "publishedAt": "2025-12-04T15:57:27.000Z", "submittedOnDailyAt": "2025-12-05T00:25:00.105Z", "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion", "submittedOnDailyBy": {"_id": "644101cd1a80f6d83cae6ef1", "avatarUrl": "/avatars/f1860d1dad3938e02e767b6ec108cf0d.svg", "isPro": false, "fullname": "FishNotFish", "user": "RuoyuFeng", "type": "user"}, "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.", "upvotes": 33, "discussionId": "693241dd6d1060ca587a2605", "projectPage": "https://yuemingpan.github.io/SFD.github.io/", "githubRepo": "https://github.com/yuemingPAN/SFD", "ai_summary": "Semantic-First Diffusion (SFD) enhances image generation by asynchronously denoising semantic and texture latents, improving convergence and quality.", "ai_keywords": ["Latent Diffusion Models", "SFD", "Semantic VAE", "noise schedules", "FID", "ImageNet", "ReDi", "VA-VAE"], "githubStars": 180, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u91c7\u7528\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5148\u751f\u6210\u9ad8\u5c42\u8bed\u4e49\u7ed3\u6784\uff0c\u518d\u751f\u6210\u7ec6\u8282\u7eb9\u7406\u3002</li>\n    <li>\u6700\u8fd1\u7684\u8fdb\u5c55\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8bed\u4e49\u5148\u9a8c\u6574\u5408\u5230LDMs\u4e2d\uff0c\u4f46\u4ecd\u7136\u540c\u6b65\u53bb\u566a\u8bed\u4e49\u548c\u7eb9\u7406\u7f16\u7801\uff0c\u672a\u8003\u8651\u751f\u6210\u987a\u5e8f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u4e49\u4f18\u5148\u6269\u6563\uff08SFD\uff09\uff0c\u4f18\u5148\u6784\u5efa\u8bed\u4e49\uff0c\u5148\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u6f5c\u53d8\u91cf\uff0c\u518d\u4e0e\u7eb9\u7406\u6f5c\u53d8\u91cf\u7ed3\u5408\u3002</li>\n    <li>SFD\u4f7f\u7528\u4e0d\u540c\u7684\u53bb\u566a\u65f6\u95f4\u8868\u5f02\u6b65\u5904\u7406\u8bed\u4e49\u548c\u7eb9\u7406\uff0c\u5148\u5904\u7406\u8bed\u4e49\uff0c\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u9ad8\u5c42\u6307\u5bfc\uff0c\u4ece\u800c\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>\u5728ImageNet 256x256\u4e0a\uff0cSFD\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u901f\u5ea6\u6bd4\u539f\u59cb\u65b9\u6cd5\u5feb100\u500d\uff0c\u5e76\u6539\u5584\u4e86\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Latent Diffusion Models (LDMs) create images by first focusing on the overall structure before adding details.</li>\n    <li>Recent improvements have added semantic information from visual models, but they still treat structure and texture generation together.</li>\n    <li>The new method, Semantic-First Diffusion (SFD), prioritizes creating the semantic structure before the texture.</li>\n    <li>SFD combines semantic and texture information and processes them separately, allowing for better and faster image generation.</li>\n    <li>In tests, SFD showed significant improvements in image quality and speed compared to previous methods.</li>\n</ul>"}, "publishedAt": "2025-12-04T10:57:27.000Z", "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion", "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04926.png", "numComments": 2, "submittedBy": {"_id": "644101cd1a80f6d83cae6ef1", "avatarUrl": "/avatars/f1860d1dad3938e02e767b6ec108cf0d.svg", "fullname": "FishNotFish", "name": "RuoyuFeng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u8f6f\u4ef6\u5f00\u53d1\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u8f6c\u5316\u4e3a\u53ef\u7528\u4ee3\u7801\u3002</li>\n    <li>\u7814\u7a76\u5206\u6790\u4e86\u4ece\u6570\u636e\u51c6\u5907\u5230\u540e\u671f\u8bad\u7ec3\u7684\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u591a\u79cd\u8bad\u7ec3\u65b9\u6cd5\u548c\u63d0\u793a\u6280\u672f\u3002</li>\n    <li>\u6bd4\u8f83\u4e86\u901a\u7528LLMs\uff08\u5982GPT-4\uff09\u4e0e\u4e13\u95e8\u9488\u5bf9\u4ee3\u7801\u7684LLMs\uff08\u5982StarCoder\uff09\u7684\u6027\u80fd\u548c\u8bbe\u8ba1\u51b3\u7b56\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5173\u6ce8\u4ee3\u7801\u7684\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u4e0e\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u6574\u5408\u3002</li>\n    <li>\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u4ee3\u7801\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c\uff0c\u6db5\u76d6\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u96c6\u6bd4\u8f83\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) help turn natural language descriptions into working code, making software development easier.</li>\n    <li>Tools like GitHub Copilot and others have improved software development and achieved high success rates on coding tasks.</li>\n    <li>This work reviews the entire process of developing code LLMs, from data gathering to advanced coding techniques.</li>\n    <li>It examines both general LLMs and those specialized for coding, discussing their strengths and weaknesses.</li>\n    <li>The study highlights gaps between academic research and real-world coding needs, and suggests future research directions.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2511.08892", "authors": [{"_id": "69154dffa1b06ca3cc81351e", "name": "Weihao Tan", "hidden": false}, {"_id": "69154dffa1b06ca3cc81351f", "name": "Xiangyang Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813520", "name": "Yunhao Fang", "hidden": false}, {"_id": "69154dffa1b06ca3cc813521", "name": "Heyuan Yao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813522", "name": "Shi Yan", "hidden": false}, {"_id": "69154dffa1b06ca3cc813523", "name": "Hao Luo", "hidden": false}, {"_id": "69154dffa1b06ca3cc813524", "name": "Tenglong Ao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813525", "name": "Huihui Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813526", "name": "Hongbin Ren", "hidden": false}, {"_id": "69154dffa1b06ca3cc813527", "user": {"_id": "6369d92f64aad59d4d44d362", "avatarUrl": "/avatars/73956400cfbfd53116aefc17b3c9f0fd.svg", "isPro": false, "fullname": "Yi", "user": "Bairen", "type": "user"}, "name": "Bairen Yi", "status": "claimed_verified", "statusLastChangedAt": "2025-11-17T10:31:49.043Z", "hidden": false}, {"_id": "69154dffa1b06ca3cc813528", "name": "Yujia Qin", "hidden": false}, {"_id": "69154dffa1b06ca3cc813529", "name": "Bo An", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352a", "name": "Libin Liu", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352b", "name": "Guang Shi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "publishedAt": "2025-11-12T02:01:26.000Z", "submittedOnDailyAt": "2025-11-13T00:49:21.639Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "upvotes": 185, "discussionId": "69154dffa1b06ca3cc81352c", "projectPage": "https://www.lumine-ai.org/", "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.", "ai_keywords": ["vision-language model", "end-to-end", "3D open-world environments", "human-like interaction", "real-time", "zero-shot cross-game generalization"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Lumine\u662f\u7b2c\u4e00\u4e2a\u5f00\u653e\u7684\u901a\u7528\u667a\u80fd\u4f53\u5f00\u53d1\u914d\u65b9\uff0c\u80fd\u591f\u5728\u590d\u6742\u76843D\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u65f6\u5b8c\u6210\u957f\u8fbe\u6570\u5c0f\u65f6\u7684\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u91c7\u7528\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u7edf\u4e00\u5728\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u3002</li>\n    <li>Lumine\u5728\u300a\u539f\u795e\u300b\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u6548\u7387\u76f8\u5f53\u5730\u5b8c\u6210\u4e94\u5c0f\u65f6\u7684\u4e3b\u7ebf\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u5404\u79cd\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u4e0d\u4ec5\u5728\u7279\u5b9a\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u591f\u5728\u6ca1\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5176\u4ed6\u6e38\u620f\u4e2d\u5b8c\u6210\u957f\u8fbe100\u5206\u949f\u7684\u4efb\u52a1\u3002</li>\n    <li>Lumine\u7684\u6210\u529f\u5c55\u793a\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5f00\u53d1\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Lumine is a new open recipe for creating generalist agents that can handle long, complex tasks in 3D open-world games.</li>\n    <li>It uses a human-like approach to combine sensing, thinking, and acting, using a vision-language model.</li>\n    <li>Lumine processes visual information quickly and efficiently to perform actions based on natural language commands.</li>\n    <li>It can complete the five-hour main storyline of Genshin Impact at a level similar to human players and follow various instructions across different tasks.</li>\n    <li>Lumine shows strong performance in other games without needing extra training, successfully completing missions in Wuthering Waves and Honkai: Star Rail.</li>\n</ul>"}, "publishedAt": "2025-11-11T21:01:26.000Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08892.png", "numComments": 12, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86DeepSeek-V3.2\u6a21\u578b\uff0c\u517c\u987e\u9ad8\u6548\u8ba1\u7b97\u4e0e\u4f18\u8d8a\u7684\u63a8\u7406\u548c\u4ee3\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7a00\u758f\u6ce8\u610f\u673a\u5236\uff08DSA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u540c\u65f6\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u540e\u671f\u8ba1\u7b97\u6269\u5c55\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u8d85\u8d8a\u4e86GPT-5\uff0c\u63a8\u7406\u80fd\u529b\u63a5\u8fd1Gemini-3.0-Pro\u3002</li>\n    <li>\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u83b7\u5f97\u91d1\u724c\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e86\u590d\u6742\u4e92\u52a8\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9075\u5faa\u6307\u4ee4\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines efficient computing with excellent reasoning and performance.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which makes processing faster and better for long tasks without losing quality.</li>\n    <li>The model uses a strong reinforcement learning system, allowing it to perform as well as GPT-5, and its special version outperforms GPT-5 and matches the reasoning skills of Gemini-3.0-Pro.</li>\n    <li>DeepSeek-V3.2 achieved top results in prestigious competitions like the International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It has a new method for creating large amounts of training data, which helps improve the model's ability to follow instructions and adapt to complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u76ee\u524d\uff0c\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u4e00\u4e9b\u5f00\u6e90\u66ff\u4ee3\u54c1\uff08\u5982Qwen-Image\u548cFLUX.2\uff09\u53c2\u6570\u91cf\u5de8\u5927\uff0c\u96be\u4ee5\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u8fdb\u884c\u63a8\u7406\u548c\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u7684\u8bad\u7ec3\u6d41\u7a0b\u7ecf\u8fc7\u4f18\u5316\uff0c\u4ec5\u9700314K H800 GPU\u5c0f\u65f6\uff0c\u6210\u672c\u7ea6\u4e3a630K\u7f8e\u5143\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6548\u679c\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u4e14\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u5728\u7ebf\u6f14\u793a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The current high-performance image generation models are mainly proprietary, with open-source alternatives being large and difficult to use on regular hardware.</li>\n    <li>Z-Image is a new 6B-parameter model that uses an innovative architecture, aiming to provide high efficiency without needing massive resources.</li>\n    <li>It completes the training process quickly and cost-effectively, taking about 314,000 GPU hours and costing around $630,000.</li>\n    <li>Z-Image offers fast inference times, working well on both enterprise and consumer-grade hardware, making it more accessible.</li>\n    <li>The model performs very well in generating realistic images and handling bilingual text, competing with top commercial models while being more efficient.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.11793", "authors": [{"_id": "691be81b6bfd5965c0fd37e2", "name": "MiroMind Team", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e3", "name": "Song Bai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e4", "name": "Lidong Bing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e5", "name": "Carson Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e6", "name": "Guanzheng Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e7", "user": {"_id": "632dab84fdb35759ea6646a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632dab84fdb35759ea6646a0/IxO5mbtzHJsr0YHW-YtVk.jpeg", "isPro": false, "fullname": "Yuntao Chen", "user": "YuntaoChen", "type": "user"}, "name": "Yuntao Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:45.403Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e8", "name": "Zhe Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e9", "name": "Ziyi Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ea", "name": "Jifeng Dai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37eb", "name": "Xuan Dong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ec", "name": "Yue Deng", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ed", "name": "Yunjie Fu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ee", "name": "Junqi Ge", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ef", "name": "Chenxia Han", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f0", "name": "Tammy Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f1", "name": "Zhenhang Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f2", "name": "Jerry Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f3", "name": "Shilei Jiang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f4", "name": "Tianyu Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f5", "user": {"_id": "64be2455b567ae97c34bb948", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be2455b567ae97c34bb948/QuCdStDDGaXjDmp4V-dBj.jpeg", "isPro": false, "fullname": "Xiaoqi Jian", "user": "mx1024", "type": "user"}, "name": "Xiaoqi Jian", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:52.417Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f6", "name": "Lei Lei", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f7", "user": {"_id": "6466e7be1343dce20e59191b", "avatarUrl": "/avatars/6779560b203c3773dc76372c0b8cbe4e.svg", "isPro": false, "fullname": "Li Ruilin", "user": "Eric-LRL-130", "type": "user"}, "name": "Ruilin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:43.774Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f8", "name": "Ryan Luo", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f9", "name": "Tiantong Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fa", "name": "Xiang Lin", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fb", "name": "Ziyuan Liu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fc", "name": "Zhiqi Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fd", "name": "Jie Ni", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fe", "name": "Qiang Ren", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ff", "name": "Pax Sun", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3800", "name": "Shiqian Su", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3801", "name": "Chenxin Tao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3802", "name": "Bin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3803", "name": "Hellen Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3804", "name": "Haonan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3805", "name": "James Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3806", "name": "Jin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3807", "name": "Jojo Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3808", "name": "Letian Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3809", "name": "Shizun Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380a", "user": {"_id": "63d34004b734eaa4d4faeccf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg", "isPro": false, "fullname": "Weizhi Wang", "user": "weizhiwang", "type": "user"}, "name": "Weizhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:47.000Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380b", "name": "Zixuan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380c", "name": "Jinfan Xu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380d", "name": "Sen Xing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380e", "user": {"_id": "637f347a52229c639211bee8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg", "isPro": false, "fullname": "Chenyu Yang", "user": "cyyang822", "type": "user"}, "name": "Chenyu Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:48.746Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380f", "user": {"_id": "6239888e7fef05b7bdd5fcff", "avatarUrl": "/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg", "isPro": false, "fullname": "Hai Ye", "user": "oceanpty", "type": "user"}, "name": "Hai Ye", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:50.623Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3810", "name": "Jiaheng Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3811", "name": "Yue Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3812", "name": "Muyan Zhong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3813", "name": "Tianchen Zhao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3814", "name": "Xizhou Zhu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3815", "name": "Yanpeng Zhou", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3816", "name": "Yifan Zhang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3817", "name": "Zhi Zhu", "hidden": false}], "publishedAt": "2025-11-14T18:52:07.000Z", "submittedOnDailyAt": "2025-11-18T02:00:07.077Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "upvotes": 153, "discussionId": "691be81b6bfd5965c0fd3818", "projectPage": "https://dr.miromind.ai/", "githubRepo": "https://github.com/MiroMindAI/MiroThinker", "githubStars": 1133, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MiroThinker v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u548c\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002</li>\n    <li>MiroThinker\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u652f\u6301\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u9002\u5e94\u590d\u6742\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>MiroThinker\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u4ea4\u4e92\u6df1\u5ea6\u7684\u6269\u5c55\u4e0e\u6a21\u578b\u7684\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u7279\u6027\uff0c\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u91cd\u8981\u7ef4\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MiroThinker v1.0 is an open-source research agent aimed at improving reasoning and information-seeking skills.</li>\n    <li>It focuses on enhancing how the model interacts with its environment, rather than just increasing model size or context length.</li>\n    <li>The model uses reinforcement learning to effectively manage many tool calls (up to 600) during tasks, supporting complex research activities.</li>\n    <li>MiroThinker outperforms previous open-source agents in various benchmarks, showing accuracy rates that approach those of advanced commercial models.</li>\n    <li>The research highlights that better performance comes from deeper and more frequent interactions with the environment, suggesting that this interaction is crucial for future research agent development.</li>\n</ul>"}, "publishedAt": "2025-11-14T13:52:07.000Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11793.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u601d\u8003\u957f\u89c6\u9891\u201d\u6765\u6539\u5584\u89c6\u9891\u63a8\u7406\u80fd\u529b\uff0c\u6a21\u4eff\u4eba\u7c7b\u7684\u7406\u89e3\u65b9\u5f0f\u3002</li>\n    <li>LongVT\u5229\u7528\u591a\u6a21\u6001\u5de5\u5177\u94fe\u8fdb\u884c\u5168\u7403\u5230\u5c40\u90e8\u7684\u63a8\u7406\uff0c\u5e2e\u52a9\u83b7\u53d6\u66f4\u5177\u4f53\u7684\u89c6\u9891\u7247\u6bb5\u548c\u7ec6\u8282\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5305\u542b\u5927\u91cf\u6837\u672c\u3002</li>\n    <li>LongVT\u7ecf\u8fc7\u4e25\u683c\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Long multimodal models (LMMs) can analyze videos with text but often make mistakes, especially with long videos where information is spread out.</li>\n    <li>LongVT is a new framework that helps LMMs think about long videos by first looking at the whole video and then focusing on specific parts for details.</li>\n    <li>The framework uses the natural ability of LMMs to select specific video clips and refine the details needed for answering questions.</li>\n    <li>To help train and evaluate this system, a new dataset called VideoSIAH will be released, containing hundreds of thousands of samples and curated question-answer pairs.</li>\n    <li>LongVT has been tested and shows better performance than existing methods on difficult long-video understanding tasks.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\u6846\u67b6\uff0c\u4f7f\u7528140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u548c\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\uff0c\u901a\u8fc7\u591aGPU\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\uff0c\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u5f00\u521b\u4e86\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u65b0\u8303\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Existing video generation methods struggle with slow processing and inconsistent results, making them unsuitable for real-time applications like avatar creation.</li>\n    <li>Live Avatar is a new system that allows for fast and high-quality avatar generation using a powerful 14-billion-parameter model.</li>\n    <li>The system uses a method called Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by using multiple GPUs simultaneously.</li>\n    <li>To improve the quality and consistency of the generated avatars, it introduces the Rolling Sink Frame Mechanism (RSFM), which helps maintain appearance over time.</li>\n    <li>Live Avatar achieves impressive performance, generating avatars at 20 frames per second on 5 GPUs, setting a new standard for real-time video synthesis.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 11, 2025";
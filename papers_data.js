window.trendingPapers = [{"paper": {"id": "2511.21087", "authors": [{"_id": "6928a2681f3fae537858b4a4", "name": "Ziyun Zeng", "hidden": false}, {"_id": "6928a2681f3fae537858b4a5", "name": "Hang Hua", "hidden": false}, {"_id": "6928a2681f3fae537858b4a6", "name": "Jiebo Luo", "hidden": false}], "publishedAt": "2025-11-26T06:13:32.000Z", "submittedOnDailyAt": "2025-11-28T00:06:07.849Z", "title": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing", "submittedOnDailyBy": {"_id": "639f8277beb95d698de007dd", "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg", "isPro": false, "fullname": "HangHua", "user": "hhua2", "type": "user"}, "summary": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.", "upvotes": 4, "discussionId": "6928a2691f3fae537858b4a7", "ai_summary": "MIRA, a multimodal reasoning agent, enhances diffusion-based image editing by iteratively interpreting complex instructions, improving both semantic consistency and perceptual quality.", "ai_keywords": ["diffusion-based editing models", "multimodal reasoning agent", "iterative perception-reasoning-action loop", "atomic edit instructions", "visual feedback", "multimodal tool-use dataset", "MIRA-Editing", "two-stage SFT + GRPO training pipeline", "semantic consistency", "perceptual quality", "Flux.1-Kontext", "Step1X-Edit", "Qwen-Image-Edit", "GPT-Image", "Nano-Banana"], "summary_zh": "<ul>\n    <li>\u6307\u4ee4\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u8ba9\u7528\u6237\u53ef\u4ee5\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u7f16\u8f91\uff0c\u4f46\u6269\u6563\u6a21\u578b\u5e38\u5e38\u96be\u4ee5\u7406\u89e3\u590d\u6742\u6307\u4ee4\u3002</li>\n    <li>\u63d0\u51fa\u4e86MIRA\uff08\u591a\u6a21\u6001\u8fed\u4ee3\u63a8\u7406\u4ee3\u7406\uff09\uff0c\u5b83\u901a\u8fc7\u8fed\u4ee3\u7684\u611f\u77e5-\u63a8\u7406-\u884c\u52a8\u5faa\u73af\u8fdb\u884c\u7f16\u8f91\uff0c\u6a21\u62df\u4eba\u673a\u4e92\u52a8\u8fc7\u7a0b\u3002</li>\n    <li>MIRA\u9010\u6b65\u9884\u6d4b\u7f16\u8f91\u6307\u4ee4\uff0c\u4f7f\u7528\u89c6\u89c9\u53cd\u9988\u5e2e\u52a9\u51b3\u7b56\uff0c\u800c\u4e0d\u662f\u4e00\u6b21\u6027\u7ed9\u51fa\u9759\u6001\u8ba1\u5212\u3002</li>\n    <li>\u7ed3\u5408150K\u7684\u591a\u6a21\u6001\u5de5\u5177\u4f7f\u7528\u6570\u636e\u96c6MIRA-Editing\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7fMIRA\u80fd\u5904\u7406\u590d\u6742\u7684\u7f16\u8f91\u6307\u4ee4\u3002</li>\n    <li>MIRA\u4e0e\u5f00\u6e90\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7ed3\u5408\u540e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\uff0c\u6027\u80fd\u4e0e\u4e13\u6709\u7cfb\u7edf\u76f8\u5f53\u6216\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MIRA is a new tool that helps users edit images using natural language instructions.</li>\n    <li>It addresses issues where some editing models have difficulty understanding complex instructions.</li>\n    <li>MIRA works by breaking down the editing process into smaller steps, using visual feedback to guide decisions.</li>\n    <li>It is trained on a large dataset and uses a two-step training process to improve its editing abilities.</li>\n    <li>MIRA outperforms some advanced proprietary editing systems in terms of accuracy and quality of edits.</li>\n</ul>"}, "publishedAt": "2025-11-26T01:13:32.000Z", "title": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing", "summary": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21087.png", "numComments": 1, "submittedBy": {"_id": "639f8277beb95d698de007dd", "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg", "fullname": "HangHua", "name": "hhua2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21662", "authors": [{"_id": "6927d4bd243b2216fb75cdb6", "name": "Tianyi Xiong", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdb7", "name": "Yi Ge", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdb8", "name": "Ming Li", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdb9", "name": "Zuolong Zhang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdba", "name": "Pranav Kulkarni", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbb", "name": "Kaishen Wang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbc", "name": "Qi He", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbd", "name": "Zeying Zhu", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbe", "name": "Chenxi Liu", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbf", "name": "Ruibo Chen", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc0", "name": "Tong Zheng", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc1", "name": "Yanshuo Chen", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc2", "name": "Xiyao Wang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc3", "name": "Renrui Zhang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc4", "name": "Wenhu Chen", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc5", "name": "Heng Huang", "hidden": false}], "publishedAt": "2025-11-26T18:35:17.000Z", "submittedOnDailyAt": "2025-11-28T00:23:35.734Z", "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following", "submittedOnDailyBy": {"_id": "6570977f87a92b76922c9950", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg", "isPro": false, "fullname": "Tianyi Xiong", "user": "txiong23", "type": "user"}, "summary": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.", "upvotes": 2, "discussionId": "6927d4be243b2216fb75cdc6", "projectPage": "https://multi-crit.github.io/", "ai_summary": "Multi-Crit evaluates multimodal models on following diverse criteria with metrics for pluralistic adherence, criterion-switching flexibility, and recognizing preference conflicts, revealing gaps in model capabilities.", "ai_keywords": ["LMMs", "multimodal evaluation systems", "instruction following", "human preferences", "multi-criterion human annotations", "pluralistic criteria", "criterion-level judgments", "open-ended generation", "verifiable reasoning tasks", "holistic judgment signals", "visual grounding", "reasoning fine-tuning", "test-time scaling", "boundary consistency"], "organization": {"_id": "68b3c3bbc375e05b059370b2", "name": "UMCP", "fullname": "University of Maryland College Park", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\uff0c\u56e0\u5176\u80fd\u591f\u826f\u597d\u5730\u9075\u5faa\u6307\u4ee4\u5e76\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002</li>\n    <li>\u7814\u7a76\u5f00\u53d1\u4e86Multi-Crit\u57fa\u51c6\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u9075\u5faa\u591a\u5143\u5316\u8bc4\u4f30\u6807\u51c6\u65b9\u9762\u7684\u80fd\u529b\u3002</li>\n    <li>Multi-Crit\u901a\u8fc7\u4e25\u8c28\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u6536\u96c6\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u54cd\u5e94\u5bf9\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u6807\u51c6\u7684\u4eba\u7c7b\u6807\u6ce8\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u4e13\u6709\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u8bc4\u4f30\u4e2d\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\uff0c\u5f00\u6e90\u6a21\u578b\u5728\u7075\u6d3b\u5e94\u5bf9\u591a\u6837\u6807\u51c6\u65b9\u9762\u8868\u73b0\u66f4\u5dee\u3002</li>\n    <li>\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u53ef\u9760\u4e14\u53ef\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Large multimodal models (LMMs) are being used more as judges in evaluation systems because they follow instructions well and align with human preferences.</li>\n  <li>Multi-Crit is a new benchmark designed to test how well these models can follow different evaluation criteria and make reliable judgments.</li>\n  <li>The benchmark includes tasks that require open-ended responses and reasoning, using carefully curated data with human feedback.</li>\n  <li>Analysis of 25 LMMs shows that proprietary models struggle with diverse criteria, while open-source models perform even worse.</li>\n  <li>Fine-tuning with holistic judgment helps with visual tasks but does not improve the ability to handle varied evaluation criteria.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:35:17.000Z", "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following", "summary": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21662.png", "numComments": 1, "submittedBy": {"_id": "6570977f87a92b76922c9950", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg", "fullname": "Tianyi Xiong", "name": "txiong23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "68b3c3bbc375e05b059370b2", "name": "UMCP", "fullname": "University of Maryland College Park", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21678", "authors": [{"_id": "692918571f3fae537858b537", "name": "Weihao Bo", "hidden": false}, {"_id": "692918571f3fae537858b538", "name": "Shan Zhang", "hidden": false}, {"_id": "692918571f3fae537858b539", "name": "Yanpeng Sun", "hidden": false}, {"_id": "692918571f3fae537858b53a", "name": "Jingjing Wu", "hidden": false}, {"_id": "692918571f3fae537858b53b", "name": "Qunyi Xie", "hidden": false}, {"_id": "692918571f3fae537858b53c", "name": "Xiao Tan", "hidden": false}, {"_id": "692918571f3fae537858b53d", "name": "Kunbin Chen", "hidden": false}, {"_id": "692918571f3fae537858b53e", "name": "Wei He", "hidden": false}, {"_id": "692918571f3fae537858b53f", "name": "Xiaofan Li", "hidden": false}, {"_id": "692918571f3fae537858b540", "name": "Na Zhao", "hidden": false}, {"_id": "692918571f3fae537858b541", "name": "Jingdong Wang", "hidden": false}, {"_id": "692918571f3fae537858b542", "name": "Zechao Li", "hidden": false}], "publishedAt": "2025-11-26T18:55:08.000Z", "submittedOnDailyAt": "2025-11-28T01:10:12.370Z", "title": "Agentic Learner with Grow-and-Refine Multimodal Semantic Memory", "submittedOnDailyBy": {"_id": "64297212e5f33939cf3a3d9b", "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg", "isPro": false, "fullname": "yanpeng_sun", "user": "syp115", "type": "user"}, "summary": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.", "upvotes": 0, "discussionId": "692918571f3fae537858b543", "ai_summary": "ViLoMem, a dual-stream memory framework, enhances MLLMs by preserving multimodal semantic knowledge, reducing errors, and improving accuracy across benchmarks.", "ai_keywords": ["memory-augmented agents", "trajectory-based memory", "brevity bias", "semantic memory", "multimodal problem-solving", "visual attention", "logical reasoning", "ViLoMem", "dual-stream memory", "schema-based memory", "visual distraction patterns", "logical reasoning errors", "pass@1 accuracy", "catastrophic forgetting", "lifelong learning", "cross-domain learning"], "summary_zh": "<ul>\n    <li>MLLMs\u5728\u72ec\u7acb\u67e5\u8be2\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u6bcf\u4e2a\u95ee\u9898\u90fd\u662f\u5355\u72ec\u89e3\u51b3\u7684\uff0c\u5bb9\u6613\u91cd\u590d\u9519\u8bef\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u589e\u5f3a\u4ee3\u7406\u4ec5\u5b58\u50a8\u8fc7\u53bb\u7684\u8f68\u8ff9\uff0c\u5bb9\u6613\u9057\u5fd8\u91cd\u8981\u7684\u9886\u57df\u77e5\u8bc6\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u95ee\u9898\u89e3\u51b3\u4e2d\u53ea\u8bb0\u5f55\u5355\u4e00\u6a21\u6001\u7684\u884c\u4e3a\uff0c\u672a\u80fd\u6709\u6548\u6574\u5408\u89c6\u89c9\u6ce8\u610f\u548c\u903b\u8f91\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ViLoMem\uff0c\u4e00\u4e2a\u53cc\u6d41\u8bb0\u5fc6\u6846\u67b6\uff0c\u53ef\u4ee5\u5206\u522b\u7f16\u7801\u89c6\u89c9\u5e72\u6270\u6a21\u5f0f\u548c\u903b\u8f91\u63a8\u7406\u9519\u8bef\u3002</li>\n    <li>ViLoMem\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\uff0c\u5e76\u51cf\u5c11\u4e86\u91cd\u590d\u7684\u89c6\u89c9\u548c\u903b\u8f91\u9519\u8bef\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current machine learning models solve problems independently and often repeat the same mistakes.</li>\n    <li>Existing memory systems mainly store past actions but lose important knowledge over time.</li>\n    <li>These systems only keep single-type information, missing how visual and logical reasoning work together.</li>\n    <li>ViLoMem is a new memory framework that separately tracks visual distractions and logical errors to improve learning.</li>\n    <li>Tests show that ViLoMem increases accuracy and reduces mistakes in problem-solving by better integrating different types of knowledge.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:55:08.000Z", "title": "Agentic Learner with Grow-and-Refine Multimodal Semantic Memory", "summary": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21678.png", "numComments": 1, "submittedBy": {"_id": "64297212e5f33939cf3a3d9b", "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg", "fullname": "yanpeng_sun", "name": "syp115", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}];
window.recentPapers = [{"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u89c4\u6a21\u6269\u5927\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ROOT\uff0c\u4e00\u79cd\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u9c81\u68d2\u6b63\u4ea4\u4f18\u5316\u5668\uff0c\u89e3\u51b3\u4e86\u4f18\u5316\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002</li>\n    <li>ROOT\u4f7f\u7528\u9002\u5e94\u6027\u725b\u987f\u8fed\u4ee3\u548c\u7cbe\u7ec6\u7cfb\u6570\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u67b6\u6784\u4e0b\u7684\u6b63\u4ea4\u5316\u7cbe\u5ea6\u4e00\u81f4\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\uff0c\u6765\u589e\u5f3a\u4f18\u5316\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u76f8\u6bd4\u4e8e\u5176\u4ed6\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u4f18\u7684\u6700\u7ec8\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Optimizing large language models (LLMs) is challenging due to issues with training stability and algorithm precision.</li>\n    <li>Recent optimizers have made progress but still struggle with problems like sensitivity to dimensional changes and noise from outliers.</li>\n    <li>ROOT is a new optimizer that improves stability by using two main techniques: adaptive Newton iterations for better orthogonalization and a framework to reduce noise from outliers.</li>\n    <li>Tests show that ROOT is more robust, converges faster, and performs better than existing optimizers like Muon and Adam, especially in difficult scenarios.</li>\n    <li>The code for ROOT will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.17592", "authors": [{"_id": "6926bc70243b2216fb75cb97", "name": "Valentin Khrulkov", "hidden": false}, {"_id": "6926bc70243b2216fb75cb98", "user": {"_id": "661e44cf1d8ffc49b57ba07e", "avatarUrl": "/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg", "isPro": false, "fullname": "Andrey Galichin", "user": "andreuka18", "type": "user"}, "name": "Andrey Galichin", "status": "admin_assigned", "statusLastChangedAt": "2025-11-26T11:53:55.369Z", "hidden": false}, {"_id": "6926bc70243b2216fb75cb99", "name": "Denis Bashkirov", "hidden": false}, {"_id": "6926bc70243b2216fb75cb9a", "name": "Dmitry Vinichenko", "hidden": false}, {"_id": "6926bc70243b2216fb75cb9b", "user": {"_id": "6926d952a511b9ef1838188f", "avatarUrl": "/avatars/f3692c867f3969d94dcc050a3f8807c3.svg", "isPro": false, "fullname": "Oleg Travkin", "user": "Oitravkin", "type": "user"}, "name": "Oleg Travkin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:38.388Z", "hidden": false}, {"_id": "6926bc70243b2216fb75cb9c", "name": "Roman Alferov", "hidden": false}, {"_id": "6926bc70243b2216fb75cb9d", "name": "Andrey Kuznetsov", "hidden": false}, {"_id": "6926bc70243b2216fb75cb9e", "name": "Ivan Oseledets", "hidden": false}], "publishedAt": "2025-11-17T14:44:47.000Z", "submittedOnDailyAt": "2025-11-26T06:09:36.043Z", "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms", "submittedOnDailyBy": {"_id": "643984dceb7c5616ef3f5d54", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg", "isPro": false, "fullname": "Andrey Kuznetsov", "user": "kuznetsoffandrey", "type": "user"}, "summary": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.", "upvotes": 106, "discussionId": "6926bc70243b2216fb75cb9f", "projectPage": "https://airi-institute.github.io/gigaevo-cover/", "githubRepo": "https://github.com/FusionBrainLab/gigaevo-core", "ai_summary": "GigaEvo is an open-source framework for LLM-guided evolutionary computation, offering modular and concurrent tools for research and experimentation in solving complex optimization problems.", "ai_keywords": ["LLM-guided evolutionary computation", "AlphaEvolve", "MAP-Elites", "quality-diversity algorithms", "asynchronous DAG-based evaluation", "LLM-driven mutation", "bidirectional lineage tracking", "multi-island evolutionary strategies", "Heilbronn triangle placement", "circle packing", "high-dimensional kissing numbers"], "githubStars": 41, "organization": {"_id": "62a1fefca12f9cb8a15a5219", "name": "AIRI-Institute", "fullname": " AIRI - Artificial Intelligence Research Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"}, "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aGigaEvo\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u4f7f\u7528LLM\u548c\u8fdb\u5316\u8ba1\u7b97\u7684\u6df7\u5408\u65b9\u6cd5\u3002</li>\n    <li>GigaEvo\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u652f\u6301\u591a\u6837\u6027\u7b97\u6cd5\u548c\u7075\u6d3b\u7684\u8fdb\u5316\u7b56\u7565\u3002</li>\n    <li>\u6846\u67b6\u7ecf\u8fc7\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4f8b\u5982Heilbronn\u4e09\u89d2\u5f62\u653e\u7f6e\u548c\u9ad8\u7ef4\u4eb2\u543b\u6570\u3002</li>\n    <li>\u5f3a\u8c03\u6a21\u5757\u5316\u3001\u5e76\u53d1\u6027\u548c\u5b9e\u9a8c\u7684\u7b80\u4fbf\u6027\uff0c\u652f\u6301\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3002</li>\n    <li>\u6240\u6709\u4ee3\u7801\u548c\u6846\u67b6\u4fe1\u606f\u53ef\u5728GitHub\u4e0a\u83b7\u5f97\uff0c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>GigaEvo is a new open-source framework for studying LLM-guided evolutionary computation, inspired by AlphaEvolve.</li>\n    <li>The framework includes modular tools for quality-diversity algorithms, evaluation pipelines, and mutation operators.</li>\n    <li>GigaEvo allows researchers to experiment with evolutionary strategies and aims to improve reproducibility in research.</li>\n    <li>The system has been tested on complex problems to validate its effectiveness, including triangle placement and circle packing.</li>\n    <li>All details about the framework and its code are available online for researchers to use and explore.</li>\n</ul>"}, "publishedAt": "2025-11-17T09:44:47.000Z", "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms", "summary": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17592.png", "numComments": 3, "submittedBy": {"_id": "643984dceb7c5616ef3f5d54", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg", "fullname": "Andrey Kuznetsov", "name": "kuznetsoffandrey", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 25}, "organization": {"_id": "62a1fefca12f9cb8a15a5219", "name": "AIRI-Institute", "fullname": " AIRI - Artificial Intelligence Research Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.15552", "authors": [{"_id": "6920b3b2b5612535ed9554f9", "name": "Artem Chervyakov", "hidden": false}, {"_id": "6920b3b2b5612535ed9554fa", "user": {"_id": "6207bbdb2f97dabc41b3b32b", "avatarUrl": "/avatars/6e568045b0ebaa05ac1d469d941d4c96.svg", "isPro": false, "fullname": "Ulyana", "user": "ulyanaisaeva", "type": "user"}, "name": "Ulyana Isaeva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:04.347Z", "hidden": false}, {"_id": "6920b3b2b5612535ed9554fb", "name": "Anton Emelyanov", "hidden": false}, {"_id": "6920b3b2b5612535ed9554fc", "name": "Artem Safin", "hidden": false}, {"_id": "6920b3b2b5612535ed9554fd", "name": "Maria Tikhonova", "hidden": false}, {"_id": "6920b3b2b5612535ed9554fe", "name": "Alexander Kharitonov", "hidden": false}, {"_id": "6920b3b2b5612535ed9554ff", "name": "Yulia Lyakh", "hidden": false}, {"_id": "6920b3b2b5612535ed955500", "name": "Petr Surovtsev", "hidden": false}, {"_id": "6920b3b2b5612535ed955501", "name": "Denis Shevelev", "hidden": false}, {"_id": "6920b3b2b5612535ed955502", "name": "Vildan Saburov", "hidden": false}, {"_id": "6920b3b2b5612535ed955503", "user": {"_id": "622b1f6b9f6139daa8e998ce", "avatarUrl": "/avatars/842719c100a5969be75d04da97333675.svg", "isPro": false, "fullname": "Vasily Konovalov", "user": "Vasily", "type": "user"}, "name": "Vasily Konovalov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:06.412Z", "hidden": false}, {"_id": "6920b3b2b5612535ed955504", "user": {"_id": "636b867ecde3707d10999b96", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1667991162387-noauth.png", "isPro": false, "fullname": "Rykov Elisei", "user": "lmeribal", "type": "user"}, "name": "Elisei Rykov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:09.480Z", "hidden": false}, {"_id": "6920b3b2b5612535ed955505", "user": {"_id": "61dedb1b2066746d68b63adb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg", "isPro": false, "fullname": "Ivan Sviridov", "user": "univanxx", "type": "user"}, "name": "Ivan Sviridov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T10:00:36.928Z", "hidden": false}, {"_id": "6920b3b2b5612535ed955506", "name": "Amina Miftakhova", "hidden": false}, {"_id": "6920b3b2b5612535ed955507", "name": "Ilseyar Alimova", "hidden": false}, {"_id": "6920b3b2b5612535ed955508", "name": "Alexander Panchenko", "hidden": false}, {"_id": "6920b3b2b5612535ed955509", "name": "Alexander Kapitanov", "hidden": false}, {"_id": "6920b3b2b5612535ed95550a", "name": "Alena Fenogenova", "hidden": false}], "publishedAt": "2025-11-19T15:43:53.000Z", "submittedOnDailyAt": "2025-11-27T10:05:09.849Z", "title": "Multimodal Evaluation of Russian-language Architectures", "submittedOnDailyBy": {"_id": "61dedb1b2066746d68b63adb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg", "isPro": false, "fullname": "Ivan Sviridov", "user": "univanxx", "type": "user"}, "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.", "upvotes": 73, "discussionId": "6920b3b2b5612535ed95550b", "projectPage": "https://mera.a-ai.ru/en/multi", "githubRepo": "https://github.com/MERA-Evaluation/MERA_MULTIMODAL/tree/main", "ai_summary": "Mera Multi is an open multimodal evaluation framework for Russian-spoken architectures, addressing the lack of such benchmarks with 18 newly constructed tasks and a methodology to prevent benchmark leakage.", "ai_keywords": ["multimodal large language models", "Mera Multi", "multimodal evaluation framework", "Russian language", "multimodal abilities", "benchmark leakage", "watermarking"], "githubStars": 11, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u667a\u80fd\u3001\u5c40\u9650\u6027\u548c\u98ce\u9669\u4ecd\u4e0d\u591f\u6e05\u695a\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86Mera Multi\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u4fc4\u8bed\u7684\u5f00\u653e\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b18\u4e2a\u65b0\u6784\u5efa\u7684\u8bc4\u4f30\u4efb\u52a1\u3002</li>\n    <li>\u8bc4\u4f30\u6846\u67b6\u5305\u62ec\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u9002\u7528\u4e8e\u901a\u7528\u6a21\u578b\u548c\u7279\u5b9a\u6a21\u6001\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u591a\u6a21\u6001\u80fd\u529b\u5206\u7c7b\u6cd5\uff0c\u5e76\u5173\u6ce8\u4fc4\u8bed\u6587\u5316\u548c\u8bed\u8a00\u7279\u70b9\uff0c\u5236\u5b9a\u4e86\u7edf\u4e00\u7684\u63d0\u793a\u548c\u6307\u6807\u3002</li>\n    <li>\u8be5\u57fa\u51c6\u6d4b\u8bd5\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u5176\u4ed6\u8bed\u8a00\u4e2d\u590d\u5236\uff0c\u7279\u522b\u662f\u5728\u65af\u62c9\u592b\u8bed\u8a00\u5bb6\u65cf\u4e2d\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal large language models (MLLMs) are improving quickly, but their intelligence and risks are not fully understood.</li>\n    <li>Mera Multi is a new evaluation framework for multimodal models specifically for the Russian language.</li>\n    <li>The framework includes 18 new tasks that test models using text, images, audio, and video.</li>\n    <li>Key contributions include a taxonomy of multimodal abilities, unique datasets that reflect Russian culture, and baseline results for various models.</li>\n    <li>The methodology can also be used to create multimodal benchmarks for other languages, especially within the Slavic family.</li>\n</ul>"}, "publishedAt": "2025-11-19T10:43:53.000Z", "title": "Multimodal Evaluation of Russian-language Architectures", "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15552.png", "numComments": 3, "submittedBy": {"_id": "61dedb1b2066746d68b63adb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg", "fullname": "Ivan Sviridov", "name": "univanxx", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20639", "authors": [{"_id": "6927c504243b2216fb75cd62", "user": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "name": "Jiaru Zou", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:58:44.029Z", "hidden": false}, {"_id": "6927c504243b2216fb75cd63", "name": "Xiyuan Yang", "hidden": false}, {"_id": "6927c504243b2216fb75cd64", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6927c504243b2216fb75cd65", "name": "Gaotang Li", "hidden": false}, {"_id": "6927c504243b2216fb75cd66", "name": "Katherine Tieu", "hidden": false}, {"_id": "6927c504243b2216fb75cd67", "user": {"_id": "60f5f68fa7fd83d025749234", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg", "isPro": false, "fullname": "Pan Lu", "user": "lupantech", "type": "user"}, "name": "Pan Lu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:58:40.924Z", "hidden": false}, {"_id": "6927c504243b2216fb75cd68", "name": "Ke Shen", "hidden": false}, {"_id": "6927c504243b2216fb75cd69", "name": "Hanghang Tong", "hidden": false}, {"_id": "6927c504243b2216fb75cd6a", "name": "Yejin Choi", "hidden": false}, {"_id": "6927c504243b2216fb75cd6b", "name": "Jingrui He", "hidden": false}, {"_id": "6927c504243b2216fb75cd6c", "name": "James Zou", "hidden": false}, {"_id": "6927c504243b2216fb75cd6d", "name": "Mengdi Wang", "hidden": false}, {"_id": "6927c504243b2216fb75cd6e", "name": "Ling Yang", "hidden": false}], "publishedAt": "2025-11-25T18:56:57.000Z", "submittedOnDailyAt": "2025-11-27T01:00:26.981Z", "title": "Latent Collaboration in Multi-Agent Systems", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "upvotes": 64, "discussionId": "6927c504243b2216fb75cd6f", "githubRepo": "https://github.com/Gen-Verse/LatentMAS", "ai_summary": "LatentMAS enables efficient and effective collaboration among LLM agents using latent space representations, enhancing reasoning quality and reducing computational costs.", "ai_keywords": ["multi-agent systems", "large language models", "latent space", "LatentMAS", "auto-regressive latent thoughts generation", "last-layer hidden embeddings", "shared latent working memory", "expressiveness", "information preservation", "complexity", "math and science reasoning", "commonsense understanding", "code generation", "accuracy", "output token usage", "end-to-end inference"], "githubStars": 79, "organization": {"_id": "67a21d7efeeacb7707bf40de", "name": "Gen-Verse", "fullname": "Princeton-AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"}, "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u8fdb\u884c\u534f\u540c\u667a\u80fd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u72ec\u7acb\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LatentMAS\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5141\u8bb8LLM\u667a\u80fd\u4f53\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u534f\u4f5c\u3002</li>\n    <li>\u6bcf\u4e2a\u667a\u80fd\u4f53\u901a\u8fc7\u6700\u540e\u4e00\u5c42\u9690\u85cf\u5d4c\u5165\u751f\u6210\u81ea\u56de\u5f52\u6f5c\u5728\u601d\u7ef4\uff0c\u5e76\u4f7f\u7528\u5171\u4eab\u7684\u6f5c\u5728\u5de5\u4f5c\u8bb0\u5fc6\u8fdb\u884c\u4fe1\u606f\u4ea4\u6362\u3002</li>\n    <li>\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cLatentMAS\u5728\u8868\u73b0\u529b\u548c\u4fe1\u606f\u4fdd\u5b58\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u6bd4\u4f20\u7edf\u6587\u672c\u57fa\u7840\u7684MAS\u590d\u6742\u5ea6\u4f4e\u3002</li>\n    <li>\u57289\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLatentMAS\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LatentMAS allows large language model agents to work together directly in a shared space, rather than just using text to communicate.</li>\n    <li>Each agent generates ideas using their hidden data, which is then shared through a common memory to keep information intact.</li>\n    <li>This method is more efficient and expressive than traditional text-based collaboration, requiring less complexity.</li>\n    <li>Tests show that LatentMAS outperforms other models in various tasks, achieving up to 14.6% higher accuracy and reducing the amount of data used significantly.</li>\n    <li>The system is faster, providing results 4 to 4.3 times quicker without needing extra training, and the resources are available for public use.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:56:57.000Z", "title": "Latent Collaboration in Multi-Agent Systems", "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20639.png", "numComments": 5, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67a21d7efeeacb7707bf40de", "name": "Gen-Verse", "fullname": "Princeton-AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.19046", "authors": [{"_id": "69267465243b2216fb75ca9d", "user": {"_id": "66214b651fc3a06144ef8f4b", "avatarUrl": "/avatars/dab669ad56b67805d55fef8c4fcf1326.svg", "isPro": false, "fullname": "Anglin Liu", "user": "lal-Joey", "type": "user"}, "name": "Anglin Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-26T09:27:33.112Z", "hidden": false}, {"_id": "69267465243b2216fb75ca9e", "name": "Rundong Xue", "hidden": false}, {"_id": "69267465243b2216fb75ca9f", "user": {"_id": "5fc9f05d52770aca770bd3d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc9f05d52770aca770bd3d9/axiKCXPYttXEWC-RqCSfh.jpeg", "isPro": true, "fullname": "Xu Cao", "user": "IrohXu", "type": "user"}, "name": "Xu R. Cao", "status": "claimed_verified", "statusLastChangedAt": "2025-11-26T09:27:30.698Z", "hidden": false}, {"_id": "69267465243b2216fb75caa0", "user": {"_id": "65e387095132c2edd193ae49", "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg", "isPro": false, "fullname": "Yifan Shen", "user": "SivanSX", "type": "user"}, "name": "Yifan Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:55.425Z", "hidden": false}, {"_id": "69267465243b2216fb75caa1", "name": "Yi Lu", "hidden": false}, {"_id": "69267465243b2216fb75caa2", "name": "Xiang Li", "hidden": false}, {"_id": "69267465243b2216fb75caa3", "name": "Qianqian Chen", "hidden": false}, {"_id": "69267465243b2216fb75caa4", "name": "Jintai Chen", "hidden": false}], "publishedAt": "2025-11-24T12:34:38.000Z", "submittedOnDailyAt": "2025-11-26T01:01:06.095Z", "title": "MedSAM3: Delving into Segment Anything with Medical Concepts", "submittedOnDailyBy": {"_id": "65e387095132c2edd193ae49", "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg", "isPro": false, "fullname": "Yifan Shen", "user": "SivanSX", "type": "user"}, "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.", "upvotes": 44, "discussionId": "69267466243b2216fb75caa5", "githubRepo": "https://github.com/Joey-S-Liu/MedSAM3", "ai_summary": "MedSAM-3, a text-promptable medical segmentation model fine-tuned on SAM 3 architecture, achieves superior performance across various medical imaging modalities using semantic conceptual labels and multimodal large language models.", "ai_keywords": ["Segment Anything Model (SAM)", "text promptable", "medical segmentation", "Promptable Concept Segmentation (PCS)", "Multimodal Large Language Models (MLLMs)", "X-ray", "MRI", "Ultrasound", "CT", "video"], "githubStars": 59, "summary_zh": "<ul>\n    <li>\u533b\u7597\u56fe\u50cf\u5206\u5272\u5bf9\u751f\u7269\u533b\u5b66\u7814\u7a76\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9700\u8981\u8017\u8d39\u5927\u91cf\u65f6\u95f4\u8fdb\u884c\u624b\u52a8\u6807\u6ce8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MedSAM-3\uff0c\u4e00\u4e2a\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u7684\u6a21\u578b\u3002</li>\n    <li>MedSAM-3\u901a\u8fc7\u4e0e\u8bed\u4e49\u6807\u7b7e\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u5bf9Segment Anything Model (SAM) 3\u67b6\u6784\u8fdb\u884c\u5fae\u8c03\uff0c\u652f\u6301\u7cbe\u51c6\u7684\u89e3\u5256\u7ed3\u6784\u5206\u5272\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86MedSAM-3 Agent\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u590d\u6742\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\u3002</li>\n    <li>\u5728X\u5c04\u7ebf\u3001MRI\u3001\u8d85\u58f0\u3001CT\u548c\u89c6\u9891\u7b49\u591a\u79cd\u533b\u5b66\u6210\u50cf\u6a21\u5f0f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u4e13\u4e1a\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MedSAM-3 is a new model for segmenting medical images and videos, making it easier to identify specific body parts.</li>\n    <li>It uses text prompts to help with segmentation, allowing users to describe what they want to target instead of just using geometric shapes.</li>\n    <li>The model is built on the Segment Anything Model (SAM) 3 and is fine-tuned with medical images and labels.</li>\n    <li>It includes a framework that uses advanced language models to improve the segmentation process.</li>\n    <li>Tests show that MedSAM-3 works much better than current models across various medical imaging types like X-ray, MRI, and CT.</li>\n</ul>"}, "publishedAt": "2025-11-24T07:34:38.000Z", "title": "MedSAM3: Delving into Segment Anything with Medical Concepts", "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19046.png", "numComments": 3, "submittedBy": {"_id": "65e387095132c2edd193ae49", "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg", "fullname": "Yifan Shen", "name": "SivanSX", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}, {"paper": {"id": "2511.19900", "authors": [{"_id": "6926877a243b2216fb75caca", "user": {"_id": "684ff37fa383bc5d6b0ff77f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png", "isPro": false, "fullname": "JiaqiLiu", "user": "JiaaqiLiu", "type": "user"}, "name": "Jiaqi Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-26T09:27:22.964Z", "hidden": false}, {"_id": "6926877a243b2216fb75cacb", "name": "Kaiwen Xiong", "hidden": false}, {"_id": "6926877a243b2216fb75cacc", "user": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "isPro": false, "fullname": "Peng Xia", "user": "richardxp888", "type": "user"}, "name": "Peng Xia", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:53.380Z", "hidden": false}, {"_id": "6926877a243b2216fb75cacd", "name": "Yiyang Zhou", "hidden": false}, {"_id": "6926877a243b2216fb75cace", "name": "Haonian Ji", "hidden": false}, {"_id": "6926877a243b2216fb75cacf", "name": "Lu Feng", "hidden": false}, {"_id": "6926877a243b2216fb75cad0", "name": "Siwei Han", "hidden": false}, {"_id": "6926877a243b2216fb75cad1", "name": "Mingyu Ding", "hidden": false}, {"_id": "6926877a243b2216fb75cad2", "name": "Huaxiu Yao", "hidden": false}], "publishedAt": "2025-11-25T04:15:14.000Z", "submittedOnDailyAt": "2025-11-26T02:25:56.245Z", "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "684ff37fa383bc5d6b0ff77f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png", "isPro": false, "fullname": "JiaqiLiu", "user": "JiaaqiLiu", "type": "user"}, "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.", "upvotes": 42, "discussionId": "6926877a243b2216fb75cad3", "projectPage": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL", "githubRepo": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL", "ai_summary": "Agent0-VL, a self-evolving vision-language agent, incorporates tool usage into both reasoning and self-evaluation, enabling continual improvement through evidence-grounded analysis and reinforcement learning.", "ai_keywords": ["self-rewarding approaches", "tool-integrated reasoning", "self-evaluation", "self-repair", "introspect", "evidence-grounded analysis", "Solver", "Verifier", "Self-Evolving Reasoning Cycle", "reinforcement learning", "geometric problem solving", "visual scientific analysis"], "githubStars": 544, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "summary_zh": "<ul>\n    <li>Agent0-VL \u662f\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u7684\u89c6\u89c9-\u8bed\u8a00\u4ee3\u7406\uff0c\u7ed3\u5408\u4e86\u5de5\u5177\u96c6\u6210\u63a8\u7406\uff0c\u80fd\u591f\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4e0d\u4ec5\u5728\u63a8\u7406\u4e2d\u4f7f\u7528\u5de5\u5177\uff0c\u8fd8\u5728\u81ea\u6211\u8bc4\u4f30\u548c\u81ea\u6211\u4fee\u6b63\u4e2d\u8fd0\u7528\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>Agent0-VL \u7edf\u4e00\u4e86\u6c42\u89e3\u8005\u548c\u9a8c\u8bc1\u8005\u4e24\u4e2a\u89d2\u8272\uff0c\u901a\u8fc7\u5de5\u5177\u57fa\u7840\u7684\u6279\u8bc4\u751f\u6210\u53cd\u9988\u548c\u5956\u52b1\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u63a8\u7406\u5faa\u73af\uff0c\u5229\u7528\u5de5\u5177\u9a8c\u8bc1\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u81ea\u6211\u63d0\u5347\u3002</li>\n    <li>\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u548c\u89c6\u89c9\u79d1\u5b66\u5206\u6790\u7684\u5b9e\u9a8c\u4e2d\uff0cAgent0-VL \u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u9ad8\u4e86 12.5%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language agents can do well in understanding and reasoning about images and text, but they rely heavily on human-provided guidance.</li>\n    <li>New self-rewarding methods allow these agents to evaluate themselves, but purely text-based evaluations can miss complex visual reasoning.</li>\n    <li>Agent0-VL is a new type of vision-language agent that improves itself by using tools for reasoning and self-evaluation.</li>\n    <li>It has two main roles: a Solver that reasons with tools and a Verifier that gives feedback and rewards based on tool use.</li>\n    <li>Agent0-VL shows a 12.5% improvement in performance on tasks compared to earlier models, and it does this without needing human annotations or external rewards.</li>\n</ul>"}, "publishedAt": "2025-11-24T23:15:14.000Z", "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning", "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19900.png", "numComments": 2, "submittedBy": {"_id": "684ff37fa383bc5d6b0ff77f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png", "fullname": "JiaqiLiu", "name": "JiaaqiLiu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20714", "authors": [{"_id": "6927bc37243b2216fb75cd3c", "name": "Inferix Team", "hidden": false}, {"_id": "6927bc37243b2216fb75cd3d", "name": "Tianyu Feng", "hidden": false}, {"_id": "6927bc37243b2216fb75cd3e", "name": "Yizeng Han", "hidden": false}, {"_id": "6927bc37243b2216fb75cd3f", "name": "Jiahao He", "hidden": false}, {"_id": "6927bc37243b2216fb75cd40", "name": "Yuanyu He", "hidden": false}, {"_id": "6927bc37243b2216fb75cd41", "name": "Xi Lin", "hidden": false}, {"_id": "6927bc37243b2216fb75cd42", "name": "Teng Liu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd43", "name": "Hanfeng Lu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd44", "name": "Jiasheng Tang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd45", "name": "Wei Wang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd46", "name": "Zhiyuan Wang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd47", "name": "Jichao Wu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd48", "name": "Mingyang Yang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd49", "name": "Yinghao Yu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd4a", "user": {"_id": "64ec877bb93654d4ca5c92e9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg", "isPro": false, "fullname": "Zeyu Zhang", "user": "SteveZeyuZhang", "type": "user"}, "name": "Zeyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:06.036Z", "hidden": false}, {"_id": "6927bc37243b2216fb75cd4b", "name": "Bohan Zhuang", "hidden": false}], "publishedAt": "2025-11-25T01:45:04.000Z", "submittedOnDailyAt": "2025-11-27T00:19:36.886Z", "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.", "upvotes": 38, "discussionId": "6927bc38243b2216fb75cd4c", "githubRepo": "https://github.com/alibaba-damo-academy/Inferix", "ai_summary": "Inferix is a next-generation inference engine designed for immersive world synthesis using semi-autoregressive decoding, combining diffusion and autoregressive methods for high-quality, real-time video generation and interaction.", "ai_keywords": ["world models", "agentic AI", "embodied AI", "gaming", "visual perception", "understanding", "reasoning", "semi-autoregressive", "block-diffusion", "diffusion", "autoregressive methods", "video tokens", "KV Cache management", "Inferix", "vLLM", "SGLang", "xDiTs", "interactive video streaming", "profiling", "LV-Bench", "minute-long video generation"], "githubStars": 24, "summary_zh": "<ul>\n    <li>\u4e16\u754c\u6a21\u578b\u662f\u7528\u4e8e\u667a\u80fdAI\u3001\u4f53\u73b0AI\u548c\u6e38\u620f\u7684\u6838\u5fc3\u6a21\u62df\u5668\uff0c\u53ef\u4ee5\u751f\u6210\u957f\u65f6\u95f4\u3001\u771f\u5b9e\u4e14\u4e92\u52a8\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u901a\u8fc7\u6269\u5927\u8fd9\u4e9b\u6a21\u578b\uff0c\u53ef\u4ee5\u89e3\u9501\u89c6\u89c9\u611f\u77e5\u3001\u7406\u89e3\u548c\u63a8\u7406\u7684\u65b0\u80fd\u529b\uff0c\u63a8\u52a8\u8d85\u8d8a\u76ee\u524d\u4ee5LLM\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u65b0\u8303\u5f0f\u3002</li>\n    <li>\u5173\u952e\u7a81\u7834\u5728\u4e8e\u534a\u81ea\u56de\u5f52\u89e3\u7801\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u6269\u6563\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u751f\u6210\u66f4\u8fde\u8d2f\u548c\u7a33\u5b9a\u7684\u89c6\u9891\u5e8f\u5217\u3002</li>\n    <li>Inferix\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u4e0b\u4e00\u4ee3\u63a8\u7406\u5f15\u64ce\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u534a\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u5b9e\u73b0\u6c89\u6d78\u5f0f\u4e16\u754c\u5408\u6210\u3002</li>\n    <li>Inferix\u652f\u6301\u5b9e\u65f6\u4e92\u52a8\u548c\u903c\u771f\u7684\u6a21\u62df\uff0c\u8fd8\u4e0eLV-Bench\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u9ad8\u6548\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>World models are important for AI, gaming, and creating realistic videos.</li>\n  <li>Scaling these models can improve visual perception and reasoning abilities.</li>\n  <li>A new method called semi-autoregressive decoding helps create smoother video sequences.</li>\n  <li>Inferix is a new engine designed for better world simulation using this decoding method.</li>\n  <li>It includes features for interactive video streaming and real-time interaction, along with a new evaluation benchmark for video generation.</li>\n</ul>"}, "publishedAt": "2025-11-24T20:45:04.000Z", "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation", "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20714.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.19320", "authors": [{"_id": "692525d916eb3a9f13103974", "user": {"_id": "65645169ec7e239899136895", "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg", "isPro": false, "fullname": "Jiaming Zhang", "user": "jiamingZ", "type": "user"}, "name": "Jiaming Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T12:10:05.653Z", "hidden": false}, {"_id": "692525d916eb3a9f13103975", "name": "Shengming Cao", "hidden": false}, {"_id": "692525d916eb3a9f13103976", "name": "Rui Li", "hidden": false}, {"_id": "692525d916eb3a9f13103977", "name": "Xiaotong Zhao", "hidden": false}, {"_id": "692525d916eb3a9f13103978", "name": "Yutao Cui", "hidden": false}, {"_id": "692525d916eb3a9f13103979", "name": "Xinglin Hou", "hidden": false}, {"_id": "692525d916eb3a9f1310397a", "name": "Gangshan Wu", "hidden": false}, {"_id": "692525d916eb3a9f1310397b", "name": "Haolan Chen", "hidden": false}, {"_id": "692525d916eb3a9f1310397c", "name": "Yu Xu", "hidden": false}, {"_id": "692525d916eb3a9f1310397d", "name": "Limin Wang", "hidden": false}, {"_id": "692525d916eb3a9f1310397e", "name": "Kai Ma", "hidden": false}], "publishedAt": "2025-11-24T17:15:55.000Z", "submittedOnDailyAt": "2025-11-26T00:24:22.899Z", "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation", "submittedOnDailyBy": {"_id": "65645169ec7e239899136895", "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg", "isPro": false, "fullname": "Jiaming Zhang", "user": "jiamingZ", "type": "user"}, "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.", "upvotes": 37, "discussionId": "692525d916eb3a9f1310397f", "projectPage": "https://mcg-nju.github.io/steadydancer-web", "githubRepo": "https://github.com/MCG-NJU/SteadyDancer", "ai_summary": "SteadyDancer, an Image-to-Video framework, ensures first-frame identity preservation and precise motion control through harmonized conditions, adaptive pose representation, and hierarchical training objectives.", "ai_keywords": ["Image-to-Motion Binding", "Reference-to-Video", "Image-to-Video", "Condition-Reconciliation Mechanism", "Synergistic Pose Modulation Modules", "Staged Decoupled-Objective Training Pipeline", "motion fidelity", "visual quality", "temporal coherence"], "githubStars": 116, "organization": {"_id": "62c77fde1e080b83746468bd", "name": "MCG-NJU", "fullname": "Multimedia Computing Group-Nanjing University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"}, "summary_zh": "<ul>\n    <li>SteadyDancer\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4eba\u7c7b\u56fe\u50cf\u52a8\u753b\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u548c\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6846\u67b6\u9996\u6b21\u6709\u6548\u5730\u786e\u4fdd\u4e86\u52a8\u753b\u4e2d\u7b2c\u4e00\u5e27\u7684\u8eab\u4efd\u4e0d\u53d8\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6761\u4ef6\u8c03\u548c\u673a\u5236\uff0c\u4ee5\u7cbe\u786e\u63a7\u5236\u52a8\u753b\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u534f\u540c\u59ff\u6001\u8c03\u5236\u6a21\u5757\uff0c\u4ee5\u751f\u6210\u4e0e\u53c2\u8003\u56fe\u50cf\u9ad8\u5ea6\u517c\u5bb9\u7684\u59ff\u6001\u8868\u793a\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSteadyDancer\u5728\u5916\u89c2\u771f\u5b9e\u5ea6\u548c\u8fd0\u52a8\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SteadyDancer is a new system for animating images of people while keeping their identity consistent and movements accurate.</li>\n    <li>It addresses problems with existing methods that can cause the person's identity to change or visual errors during animation.</li>\n    <li>The system uses a special method to balance different animation needs, allowing for precise control without losing image quality.</li>\n    <li>It also includes features that create smooth and adaptable poses that match the original image well.</li>\n    <li>SteadyDancer achieves high-quality results in both how the person looks and how they move, using less training resources than other methods.</li>\n</ul>"}, "publishedAt": "2025-11-24T12:15:55.000Z", "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation", "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19320.png", "numComments": 2, "submittedBy": {"_id": "65645169ec7e239899136895", "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg", "fullname": "Jiaming Zhang", "name": "jiamingZ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "62c77fde1e080b83746468bd", "name": "MCG-NJU", "fullname": "Multimedia Computing Group-Nanjing University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20635", "authors": [{"_id": "69266dec243b2216fb75ca56", "name": "Zhoujie Fu", "hidden": false}, {"_id": "69266dec243b2216fb75ca57", "name": "Xianfang Zeng", "hidden": false}, {"_id": "69266dec243b2216fb75ca58", "name": "Jinghong Lan", "hidden": false}, {"_id": "69266dec243b2216fb75ca59", "name": "Xinyao Liao", "hidden": false}, {"_id": "69266dec243b2216fb75ca5a", "name": "Cheng Chen", "hidden": false}, {"_id": "69266dec243b2216fb75ca5b", "name": "Junyi Chen", "hidden": false}, {"_id": "69266dec243b2216fb75ca5c", "name": "Jiacheng Wei", "hidden": false}, {"_id": "69266dec243b2216fb75ca5d", "user": {"_id": "64b914c8ace99c0723ad83a9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg", "isPro": false, "fullname": "Wei Cheng", "user": "wchengad", "type": "user"}, "name": "Wei Cheng", "status": "claimed_verified", "statusLastChangedAt": "2025-11-26T09:27:36.745Z", "hidden": false}, {"_id": "69266dec243b2216fb75ca5e", "name": "Shiyu Liu", "hidden": false}, {"_id": "69266dec243b2216fb75ca5f", "name": "Yunuo Chen", "hidden": false}, {"_id": "69266dec243b2216fb75ca60", "name": "Gang Yu", "hidden": false}, {"_id": "69266dec243b2216fb75ca61", "name": "Guosheng Lin", "hidden": false}], "publishedAt": "2025-11-25T18:54:16.000Z", "submittedOnDailyAt": "2025-11-26T00:33:21.436Z", "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.", "upvotes": 29, "discussionId": "69266dec243b2216fb75ca62", "projectPage": "https://kr1sjfu.github.io/iMontage-web/", "githubRepo": "https://github.com/Kr1sJFU/iMontage", "ai_summary": "iMontage repurposes pre-trained video models to generate high-quality, diverse image sets with natural transitions and enhanced dynamics through a unified framework and tailored adaptation strategy.", "ai_keywords": ["pre-trained video models", "temporal coherence", "continuous nature", "image data", "content diversity", "iMontage", "unified framework", "image generator", "variable-length image sets", "image generation", "image editing", "adaptation strategy", "data curation", "training paradigm", "image manipulation", "motion priors", "cross-image contextual consistency"], "githubStars": 53, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n  <li>iMontage\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u5f3a\u5927\u7684\u89c6\u9891\u6a21\u578b\u8f6c\u53d8\u4e3a\u4e00\u4e2a\u5168\u80fd\u7684\u56fe\u50cf\u751f\u6210\u5668\u3002</li>\n  <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u7684\u56fe\u50cf\u96c6\uff0c\u6574\u5408\u591a\u79cd\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u3002</li>\n  <li>\u901a\u8fc7\u4f18\u96c5\u7684\u9002\u5e94\u7b56\u7565\u548c\u6570\u636e\u7b56\u5212\u8fc7\u7a0b\uff0ciMontage\u80fd\u591f\u589e\u5f3a\u56fe\u50cf\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u8fd0\u52a8\u7279\u6027\u3002</li>\n  <li>iMontage\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u4fdd\u6301\u56fe\u50cf\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u8fd8\u80fd\u751f\u6210\u8d85\u51fa\u4f20\u7edf\u8303\u56f4\u7684\u52a8\u6001\u573a\u666f\u3002</li>\n  <li>\u66f4\u591a\u4fe1\u606f\u8bf7\u8bbf\u95ee\uff1a<a href=\"https://kr1sjfu.github.io/iMontage-web/\">iMontage\u4e3b\u9875</a>\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>iMontage is a new framework that transforms a powerful video model into an image generator.</li>\n    <li>It aims to combine the strong coherence of video with the diverse content found in images.</li>\n    <li>The framework can create variable-length sets of images for various generation and editing tasks.</li>\n    <li>It uses a careful adaptation strategy and data curation to enhance image manipulation without losing motion quality.</li>\n    <li>iMontage performs well in generating dynamic and contextually consistent images beyond traditional limits.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:54:16.000Z", "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation", "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20635.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20561", "authors": [{"_id": "692671a3243b2216fb75ca91", "name": "Yuwei Niu", "hidden": false}, {"_id": "692671a3243b2216fb75ca92", "user": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "name": "Weiyang Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:57.589Z", "hidden": false}, {"_id": "692671a3243b2216fb75ca93", "name": "Jiaqi Liao", "hidden": false}, {"_id": "692671a3243b2216fb75ca94", "name": "Chaoran Feng", "hidden": false}, {"_id": "692671a3243b2216fb75ca95", "name": "Peng Jin", "hidden": false}, {"_id": "692671a3243b2216fb75ca96", "name": "Bin Lin", "hidden": false}, {"_id": "692671a3243b2216fb75ca97", "name": "Zongjian Li", "hidden": false}, {"_id": "692671a3243b2216fb75ca98", "name": "Bin Zhu", "hidden": false}, {"_id": "692671a3243b2216fb75ca99", "name": "Weihao Yu", "hidden": false}, {"_id": "692671a3243b2216fb75ca9a", "name": "Li Yuan", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"], "publishedAt": "2025-11-25T17:58:48.000Z", "submittedOnDailyAt": "2025-11-26T01:19:47.715Z", "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward", "submittedOnDailyBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "summary": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox", "upvotes": 28, "discussionId": "692671a3243b2216fb75ca9b", "githubRepo": "https://github.com/PKU-YuanGroup/UniSandBox", "ai_summary": "UniSandbox evaluates Unified Multimodal Models, revealing a gap between understanding and generation, and identifies Chain-of-Thought and self-training as means to bridge this gap.", "ai_keywords": ["Unified Multimodal Models", "UniSandbox", "decoupled evaluation framework", "synthetic datasets", "understanding-generation gap", "reasoning generation", "knowledge transfer", "Chain-of-Thought", "self-training", "query-based architectures"], "githubStars": 25, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86UniSandbox\uff0c\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u7406\u89e3\u662f\u5426\u771f\u6b63\u5f71\u54cd\u751f\u6210\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u63a8\u7406\u751f\u6210\u548c\u77e5\u8bc6\u8f6c\u79fb\u4e24\u4e2a\u65b9\u9762\u3002</li>\n    <li>\u5728\u63a8\u7406\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u660e\u786e\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u80fd\u591f\u6709\u6548\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002</li>\n    <li>\u81ea\u6211\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u5185\u5316\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u751f\u6210\u4e2d\u7684\u9690\u6027\u63a8\u7406\u3002</li>\n    <li>\u5728\u77e5\u8bc6\u8f6c\u79fb\u4efb\u52a1\u4e2d\uff0cCoT\u5e2e\u52a9\u68c0\u7d22\u65b0\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u67e5\u8be2\u57fa\u7840\u67b6\u6784\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3cCoT\u7684\u7279\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Researchers introduced UniSandbox, a new framework to study how understanding affects generation in models that use multiple types of data.</li>\n    <li>They found a significant gap between understanding and generating responses, particularly in reasoning tasks and transferring knowledge.</li>\n    <li>Using a method called Chain-of-Thought (CoT) in the understanding phase helps improve reasoning during generation.</li>\n    <li>CoT also aids in retrieving new knowledge for generation tasks, and some model architectures naturally exhibit CoT-like traits.</li>\n    <li>UniSandbox offers insights for building better models and training methods that connect understanding and generation more effectively.</li>\n</ul>"}, "publishedAt": "2025-11-25T12:58:48.000Z", "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward", "summary": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20561.png", "numComments": 2, "submittedBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "fullname": "Weiyang Jin", "name": "Wayne-King", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": true}];
window.papersLastUpdated = "Nov 28, 2025";
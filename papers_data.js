window.trendingPapers = {
    "today": [{"paper": {"id": "2512.05965", "authors": [{"_id": "693638ba3962c926cf680724", "name": "Hongyu Li", "hidden": false}, {"_id": "693638ba3962c926cf680725", "name": "Manyuan Zhang", "hidden": false}, {"_id": "693638ba3962c926cf680726", "name": "Dian Zheng", "hidden": false}, {"_id": "693638ba3962c926cf680727", "name": "Ziyu Guo", "hidden": false}, {"_id": "693638ba3962c926cf680728", "name": "Yimeng Jia", "hidden": false}, {"_id": "693638ba3962c926cf680729", "name": "Kaituo Feng", "hidden": false}, {"_id": "693638ba3962c926cf68072a", "name": "Hao Yu", "hidden": false}, {"_id": "693638ba3962c926cf68072b", "name": "Yexin Liu", "hidden": false}, {"_id": "693638ba3962c926cf68072c", "name": "Yan Feng", "hidden": false}, {"_id": "693638ba3962c926cf68072d", "name": "Peng Pei", "hidden": false}, {"_id": "693638ba3962c926cf68072e", "name": "Xunliang Cai", "hidden": false}, {"_id": "693638ba3962c926cf68072f", "name": "Linjiang Huang", "hidden": false}, {"_id": "693638ba3962c926cf680730", "name": "Hongsheng Li", "hidden": false}, {"_id": "693638ba3962c926cf680731", "name": "Si Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/8f-me49cubcrpFx-hfaoM.gif"], "publishedAt": "2025-12-05T18:58:09.000Z", "submittedOnDailyAt": "2025-12-08T00:03:16.853Z", "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.", "upvotes": 20, "discussionId": "693638ba3962c926cf680732", "projectPage": "https://appletea233.github.io/think-while-edit/", "ai_summary": "A deliberative editing framework with a reasoning engine improves instruction-following in image editing through iterative critique and refinement, significantly enhancing performance.", "ai_keywords": ["MLLM", "EditThinker", "reinforcement learning", "Think-while-Edit cycle", "Critiquing", "Refining instructions", "generation", "image editing", "instruction-following capability", "data construction framework"], "summary_zh": "<ul>\n    <li>\u6307\u4ee4\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u9886\u57df\uff0c\u4f9d\u8d56\u4e8e\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5df2\u5b9e\u73b0\u9ad8\u6c34\u5e73\u7684\u7f8e\u5b66\u8d28\u91cf\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u6216\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u6307\u4ee4\u7684\u9075\u5faa\u6027\uff0c\u4f46\u5355\u6b21\u6210\u529f\u7387\u4ecd\u53d7\u9650\u4e8e\u968f\u673a\u6027\u548c\u7f3a\u4e4f\u6df1\u601d\u719f\u8651\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u601d\u719f\u8651\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5faa\u73af\u8fdb\u884c\u6279\u8bc4\u3001\u6539\u8fdb\u6307\u4ee4\u5e76\u91cd\u590d\u751f\u6210\uff0c\u76f4\u5230\u6ee1\u610f\u4e3a\u6b62\u3002</li>\n    <li>\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u540d\u4e3aEditThinker\u7684\u5355\u4e00\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u4f5c\u4e3a\u8be5\u6846\u67b6\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u751f\u6210\u6279\u8bc4\u5206\u6570\u3001\u63a8\u7406\u8fc7\u7a0b\u548c\u6539\u8fdb\u6307\u4ee4\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f7fEditThinker\u7684\u601d\u8003\u4e0e\u7f16\u8f91\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u5e76\u8ba1\u5212\u5411\u793e\u533a\u53d1\u5e03\u6570\u636e\u6784\u5efa\u6846\u67b6\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Instruction-based image editing is a growing field that aims to improve how well models follow user instructions.</li>\n    <li>Current methods have limited success due to randomness and a lack of careful thinking during the editing process.</li>\n    <li>This work introduces a new framework that mimics human thinking by allowing models to critique and refine their instructions while editing.</li>\n    <li>The framework uses a model called EditThinker, which combines reasoning and editing to improve instruction-following.</li>\n    <li>Experiments show that this approach greatly enhances how well image editing models follow instructions, and resources will be shared with the community.</li>\n</ul>"}, "publishedAt": "2025-12-05T13:58:09.000Z", "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor", "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/8f-me49cubcrpFx-hfaoM.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05965.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05150", "authors": [{"_id": "69365dca3962c926cf6807f7", "name": "Zhenglin Cheng", "hidden": false}, {"_id": "69365dca3962c926cf6807f8", "name": "Peng Sun", "hidden": false}, {"_id": "69365dca3962c926cf6807f9", "name": "Jianguo Li", "hidden": false}, {"_id": "69365dca3962c926cf6807fa", "name": "Tao Lin", "hidden": false}], "publishedAt": "2025-12-03T07:45:46.000Z", "submittedOnDailyAt": "2025-12-08T02:49:09.906Z", "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows", "submittedOnDailyBy": {"_id": "65028e8389707f182386588c", "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg", "isPro": true, "fullname": "Zhenglin Cheng (SII)", "user": "kenshinn", "type": "user"}, "summary": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.", "upvotes": 12, "discussionId": "69365dcb3962c926cf6807fb", "ai_summary": "TwinFlow is a 1-step generative model framework that enhances inference efficiency without requiring fixed pretrained teacher models or standard adversarial networks, achieving high performance on text-to-image tasks and scaling efficiently.", "ai_keywords": ["diffusion", "flow matching", "inference efficiency", "Number of Function Evaluations", "few-step methods", "progressive distillation", "consistency distillation", "adversarial training", "DMD/DMD2", "SANA-Sprint", "TwinFlow", "GenEval", "RCGM", "Qwen-Image-20B", "DPG-Bench"], "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u5927\u578b\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u63a8\u7406\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u898140\u5230100\u6b21\u51fd\u6570\u8bc4\u4f30\u3002</li>\n    <li>\u73b0\u6709\u7684\u51e0\u6b65\u63a8\u7406\u65b9\u6cd5\u6709\u660e\u663e\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u975e\u5e38\u5c11\u7684\u6b65\u9aa4\u4e0b\uff08\u5c11\u4e8e4\u6b21\u51fd\u6570\u8bc4\u4f30\uff09\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86TwinFlow\uff0c\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u4e00\u6b65\u751f\u6210\u6a21\u578b\uff0c\u907f\u514d\u4e86\u56fa\u5b9a\u7684\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u548c\u5bf9\u6297\u7f51\u7edc\u7684\u4f7f\u7528\u3002</li>\n    <li>TwinFlow\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e860.83\u7684GenEval\u5f97\u5206\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5982SANA-Sprint\u548cRCGM\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u4ec5\u97001\u6b21\u51fd\u6570\u8bc4\u4f30\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4e0e\u539f\u59cb100\u6b21\u51fd\u6570\u8bc4\u4f30\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u5339\u914d\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86100\u500d\uff0c\u8d28\u91cf\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent models for generating images and videos are complex and slow, requiring many evaluations (40-100 NFEs) for good results.</li>\n    <li>Existing faster methods have limitations, like needing complex training procedures or resulting in lower quality at very few steps.</li>\n    <li>The proposed TwinFlow framework allows for efficient 1-step model training without relying on complicated pre-trained models or adversarial networks.</li>\n    <li>TwinFlow achieves a high GenEval score of 0.83 in just 1-NFE, outperforming other competitive models.</li>\n    <li>This framework significantly reduces computational costs by 100 times while maintaining nearly the same quality as slower models.</li>\n</ul>"}, "publishedAt": "2025-12-03T02:45:46.000Z", "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows", "summary": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05150.png", "numComments": 1, "submittedBy": {"_id": "65028e8389707f182386588c", "avatarUrl": "/avatars/86a748a3264e6e0f4ee5eaf8f7032ecb.svg", "fullname": "Zhenglin Cheng (SII)", "name": "kenshinn", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05591", "authors": [{"_id": "6936451a3962c926cf6807d6", "name": "Zhenpeng Su", "hidden": false}, {"_id": "6936451a3962c926cf6807d7", "name": "Leiyu Pan", "hidden": false}, {"_id": "6936451a3962c926cf6807d8", "name": "Minxuan Lv", "hidden": false}, {"_id": "6936451a3962c926cf6807d9", "name": "Tiehua Mei", "hidden": false}, {"_id": "6936451a3962c926cf6807da", "name": "Zijia Lin", "hidden": false}, {"_id": "6936451a3962c926cf6807db", "name": "Yuntao Li", "hidden": false}, {"_id": "6936451a3962c926cf6807dc", "name": "Wenping Hu", "hidden": false}, {"_id": "6936451a3962c926cf6807dd", "name": "Ruiming Tang", "hidden": false}, {"_id": "6936451a3962c926cf6807de", "name": "Kun Gai", "hidden": false}, {"_id": "6936451a3962c926cf6807df", "name": "Guorui Zhou", "hidden": false}], "publishedAt": "2025-12-05T10:26:32.000Z", "submittedOnDailyAt": "2025-12-08T00:57:20.286Z", "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning", "submittedOnDailyBy": {"_id": "61c2cf8d1172fa7969904d99", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c2cf8d1172fa7969904d99/R10G5h3d9Q_YQ__Hc-H4k.jpeg", "isPro": false, "fullname": "suu", "user": "Suu", "type": "user"}, "summary": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.", "upvotes": 11, "discussionId": "6936451b3962c926cf6807e0", "ai_summary": "Entropy Ratio Clipping (ERC) mechanism stabilizes policy updates in reinforcement learning by addressing global distributional shifts, improving performance over existing methods.", "ai_keywords": ["reinforcement learning", "distribution shift", "trust region", "policy entropy", "unstable gradients", "PPO-Clip", "entropy ratio", "Entropy Ratio Clipping", "ERC", "DAPO", "GPPO"], "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u671f\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u6a21\u578b\u80fd\u529b\u548c\u5bf9\u9f50\u8d28\u91cf\u3002</li>\n    <li>\u79bb\u7ebf\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u8868\u73b0\u4e3a\u7b56\u7565\u71b5\u6ce2\u52a8\u548c\u68af\u5ea6\u4e0d\u7a33\u5b9a\u3002</li>\n    <li>PPO-Clip\u901a\u8fc7\u91cd\u8981\u6027\u88c1\u526a\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u4ecd\u7136\u5ffd\u89c6\u4e86\u52a8\u4f5c\u7684\u5168\u5c40\u5206\u5e03\u53d8\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u5f53\u524d\u548c\u4e4b\u524d\u7b56\u7565\u7684\u71b5\u6bd4\u4f5c\u4e3a\u65b0\u7684\u5168\u5c40\u6307\u6807\uff0c\u6709\u6548\u91cf\u5316\u7b56\u7565\u63a2\u7d22\u7684\u76f8\u5bf9\u53d8\u5316\u3002</li>\n    <li>\u5f15\u5165\u71b5\u6bd4\u88c1\u526a\uff08ERC\uff09\u673a\u5236\uff0c\u7a33\u5b9a\u5168\u5c40\u5206\u5e03\u6c34\u5e73\u7684\u7b56\u7565\u66f4\u65b0\uff0c\u5b9e\u9a8c\u8868\u660eERC\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models use reinforcement learning after training to improve their performance.</li>\n    <li>Off-policy training can cause instability in the model due to changes in the distribution of actions.</li>\n    <li>PPO-Clip helps with some instability but doesn't fully address global shifts in action distribution.</li>\n    <li>The proposed Entropy Ratio Clipping (ERC) method measures and controls changes in policy exploration, stabilizing updates.</li>\n    <li>ERC has been added to existing reinforcement learning algorithms and shows improved performance in tests.</li>\n</ul>"}, "publishedAt": "2025-12-05T05:26:32.000Z", "title": "Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning", "summary": "Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an Entropy Ratio Clipping (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05591.png", "numComments": 1, "submittedBy": {"_id": "61c2cf8d1172fa7969904d99", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61c2cf8d1172fa7969904d99/R10G5h3d9Q_YQ__Hc-H4k.jpeg", "fullname": "suu", "name": "Suu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04784", "authors": [{"_id": "693385573962c926cf68049c", "name": "Bowen Ping", "hidden": false}, {"_id": "693385573962c926cf68049d", "name": "Chengyou Jia", "hidden": false}, {"_id": "693385573962c926cf68049e", "name": "Minnan Luo", "hidden": false}, {"_id": "693385573962c926cf68049f", "name": "Changliang Xia", "hidden": false}, {"_id": "693385573962c926cf6804a0", "name": "Xin Shen", "hidden": false}, {"_id": "693385573962c926cf6804a1", "name": "Zhuohang Dang", "hidden": false}, {"_id": "693385573962c926cf6804a2", "name": "Hangwei Qian", "hidden": false}], "publishedAt": "2025-12-02T13:39:03.000Z", "submittedOnDailyAt": "2025-12-08T01:22:57.723Z", "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling", "submittedOnDailyBy": {"_id": "6602548a68d519ed324b47c5", "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg", "isPro": false, "fullname": "ChengyouJia", "user": "ChengyouJia", "type": "user"}, "summary": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.", "upvotes": 11, "discussionId": "693385583962c926cf6804a3", "projectPage": "https://x-gengroup.github.io/HomePage_PaCo-RL/", "githubRepo": "https://github.com/X-GenGroup/PaCo-RL", "ai_summary": "PaCo-RL combines reinforcement learning with a specialized consistency reward model and an efficient optimization strategy to improve consistent image generation.", "ai_keywords": ["reinforcement learning", "PaCo-RL", "PaCo-Reward", "pairwise consistency evaluator", "generative scoring mechanism", "autoregressive scoring mechanism", "task-aware instructions", "CoT reasons", "PaCo-GRPO", "resolution-decoupled optimization", "log-tamed multi-reward aggregation", "consistency performance", "training efficiency", "stability"], "githubStars": 0, "organization": {"_id": "692ea0d54f38e991f11118e3", "name": "X-GenGroup", "fullname": "X-Gen Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692e9b90b58acaf7e3a46774/rr1cR56-fHn0WHPm6dYwd.jpeg"}, "summary_zh": "<ul>\n    <li>\u4e00\u81f4\u7684\u56fe\u50cf\u751f\u6210\u9700\u8981\u5728\u591a\u4e2a\u56fe\u50cf\u4e2d\u4fdd\u6301\u8eab\u4efd\u3001\u98ce\u683c\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u8fd9\u5bf9\u8bb2\u6545\u4e8b\u548c\u89d2\u8272\u8bbe\u8ba1\u7b49\u5e94\u7528\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u4f20\u7edf\u7684\u76d1\u7763\u8bad\u7ec3\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5927\u578b\u6570\u636e\u96c6\u548c\u5efa\u6a21\u4eba\u7c7b\u611f\u77e5\u504f\u597d\u7684\u590d\u6742\u6027\u800c\u96be\u4ee5\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014PaCo-RL\uff0c\u5b83\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u5b66\u4e60\u590d\u6742\u7684\u89c6\u89c9\u6807\u51c6\uff0c\u907f\u514d\u4e86\u5bf9\u5927\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u3002</li>\n    <li>PaCo-RL\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1aPaCo-Reward\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf\u4e00\u81f4\u6027\uff0cPaCo-GRPO\u5219\u901a\u8fc7\u4f18\u5316\u7b56\u7565\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPaCo-RL\u5728\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u4e00\u81f4\u56fe\u50cf\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Consistent image generation is important for storytelling and character design, as it needs to maintain identities, styles, and logic across images.</li>\n    <li>Traditional supervised training has difficulties due to the lack of large datasets and the complexity of human visual preferences.</li>\n    <li>This paper introduces PaCo-RL, a new framework that uses reinforcement learning to learn visual consistency without needing large datasets.</li>\n    <li>PaCo-RL includes two parts: PaCo-Reward, which evaluates image consistency using a specialized scoring system, and PaCo-GRPO, which optimizes the learning process to improve efficiency.</li>\n    <li>Experiments show that PaCo-RL significantly aligns with human views on visual consistency and improves the efficiency and stability of training for image generation.</li>\n</ul>"}, "publishedAt": "2025-12-02T08:39:03.000Z", "title": "PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling", "summary": "Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04784.png", "numComments": 1, "submittedBy": {"_id": "6602548a68d519ed324b47c5", "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg", "fullname": "ChengyouJia", "name": "ChengyouJia", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "692ea0d54f38e991f11118e3", "name": "X-GenGroup", "fullname": "X-Gen Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/692e9b90b58acaf7e3a46774/rr1cR56-fHn0WHPm6dYwd.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04563", "authors": [{"_id": "693530fb3962c926cf6805ef", "name": "Zefeng Zhang", "hidden": false}, {"_id": "693530fb3962c926cf6805f0", "name": "Xiangzhao Hao", "hidden": false}, {"_id": "693530fb3962c926cf6805f1", "name": "Hengzhu Tang", "hidden": false}, {"_id": "693530fb3962c926cf6805f2", "name": "Zhenyu Zhang", "hidden": false}, {"_id": "693530fb3962c926cf6805f3", "name": "Jiawei Sheng", "hidden": false}, {"_id": "693530fb3962c926cf6805f4", "name": "Xiaodong Li", "hidden": false}, {"_id": "693530fb3962c926cf6805f5", "name": "Zhenyang Li", "hidden": false}, {"_id": "693530fb3962c926cf6805f6", "name": "Li Gao", "hidden": false}, {"_id": "693530fb3962c926cf6805f7", "name": "Daiting Shi", "hidden": false}, {"_id": "693530fb3962c926cf6805f8", "name": "Dawei Yin", "hidden": false}, {"_id": "693530fb3962c926cf6805f9", "name": "Tingwen Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/J9N--2Y_pvsvuy2qjS63y.png", "https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/Te_kKXnGIMDd7njmIkXQ3.png"], "publishedAt": "2025-12-04T08:26:04.000Z", "submittedOnDailyAt": "2025-12-08T01:11:05.761Z", "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence", "submittedOnDailyBy": {"_id": "642c2dcec3694d2b74565c48", "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg", "isPro": false, "fullname": "zhangzef", "user": "Starrrrrry", "type": "user"}, "summary": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.", "upvotes": 7, "discussionId": "693530fc3962c926cf6805fa", "githubRepo": "https://github.com/zhangzef/COOPER", "ai_summary": "A unified multimodal large language model (MLLM) that integrates depth and segmentation modalities enhances spatial reasoning and perception through adaptive interleaved reasoning, improving spatial intelligence and general performance.", "ai_keywords": ["Multimodal Large Language Models (MLLMs)", "visual spatial reasoning", "3D-aware reasoning", "spatial VQA datasets", "reinforcement learning", "auxiliary modalities", "depth", "segmentation", "adaptive interleaved reasoning", "spatial perception", "spatial intelligence", "distance estimation", "size estimation"], "githubStars": 3, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u5bf9\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7406\u89e3\u7269\u4f53\u7279\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u5f53\u524d\u6a21\u578b\u57283D\u63a8\u7406\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5206\u522b\u589e\u5f3a\u611f\u77e5\u6216\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86COOPER\uff0c\u8fd9\u662f\u4e00\u79cd\u7edf\u4e00\u7684MLLM\uff0c\u5229\u7528\u6df1\u5ea6\u548c\u5206\u5272\u4f5c\u4e3a\u8f85\u52a9\u6a21\u6001\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u6765\u63d0\u9ad8\u7a7a\u95f4\u667a\u80fd\u3002</li>\n    <li>COOPER\u5728\u7a7a\u95f4\u63a8\u7406\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e866.91%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5373\u4f7f\u662f\u53ea\u8bad\u7ec3\u8f85\u52a9\u6a21\u6001\u751f\u6210\u7684\u53d8\u4f53\uff0c\u4e5f\u5728\u8ddd\u79bb\u548c\u5927\u5c0f\u4f30\u8ba1\u4e0a\u83b7\u5f97\u4e867.92%\u7684\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visual Spatial Reasoning helps Multimodal Large Language Models (MLLMs) understand objects and their relationships in space.</li>\n    <li>Current models often focus separately on improving perception (using depth and segmentation) and reasoning (using specific datasets and reinforcement learning).</li>\n    <li>COOPER is a new MLLM that combines perception and reasoning to improve spatial intelligence.</li>\n    <li>COOPER was trained in two stages to generate auxiliary information and use adaptive reasoning, resulting in a 6.91% improvement in spatial reasoning.</li>\n    <li>A simpler version of COOPER focused on generating auxiliary information alone improved distance and size estimation by 7.92%, showing that this helps with understanding space better.</li>\n</ul>"}, "publishedAt": "2025-12-04T03:26:04.000Z", "title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence", "summary": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/J9N--2Y_pvsvuy2qjS63y.png", "https://cdn-uploads.huggingface.co/production/uploads/642c2dcec3694d2b74565c48/Te_kKXnGIMDd7njmIkXQ3.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04563.png", "numComments": 2, "submittedBy": {"_id": "642c2dcec3694d2b74565c48", "avatarUrl": "/avatars/31243bb505f8c511ebd7492eaf3ea1a9.svg", "fullname": "zhangzef", "name": "Starrrrrry", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02580", "authors": [{"_id": "6934d2ba3962c926cf680575", "name": "Changpeng Yang", "hidden": false}, {"_id": "6934d2ba3962c926cf680576", "name": "Jinyang Wu", "hidden": false}, {"_id": "6934d2ba3962c926cf680577", "name": "Yuchen Liu", "hidden": false}, {"_id": "6934d2ba3962c926cf680578", "name": "Shuai Zhang", "hidden": false}, {"_id": "6934d2ba3962c926cf680579", "name": "Yang Li", "hidden": false}, {"_id": "6934d2ba3962c926cf68057a", "name": "Qiliang Liang", "hidden": false}, {"_id": "6934d2ba3962c926cf68057b", "name": "Hongzhen Wang", "hidden": false}, {"_id": "6934d2ba3962c926cf68057c", "name": "Shuai Nie", "hidden": false}, {"_id": "6934d2ba3962c926cf68057d", "name": "Jiaming Xu", "hidden": false}, {"_id": "6934d2ba3962c926cf68057e", "name": "Runyu Shi", "hidden": false}, {"_id": "6934d2ba3962c926cf68057f", "name": "Ying Huang", "hidden": false}, {"_id": "6934d2ba3962c926cf680580", "name": "Guoquan Zhang", "hidden": false}], "publishedAt": "2025-12-02T09:48:57.000Z", "submittedOnDailyAt": "2025-12-08T01:16:07.593Z", "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.", "upvotes": 7, "discussionId": "6934d2ba3962c926cf680581", "ai_summary": "CAPO, a curriculum advantage policy optimization, enhances reinforcement learning for large language models by strategically introducing positive and negative advantage signals, improving reasoning capabilities and generalization.", "ai_keywords": ["reinforcement learning", "post-training", "large language models", "reasoning capabilities", "advantage value", "positive signals", "negative signals", "curriculum mechanism", "imitation learning", "discriminative capabilities", "generalization", "GRPO", "PPO", "RLOO", "Reinforce++", "mathematical reasoning tasks", "multimodal Graphical User Interface (GUI) reasoning scenarios", "optimization framework"], "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u6b63\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e2d\u79ef\u6781\u548c\u6d88\u6781\u4fe1\u53f7\u6df7\u5408\u4f7f\u7528\u53ef\u80fd\u5bfc\u81f4\u6307\u5bfc\u4e0d\u660e\u786e\uff0c\u6548\u679c\u6709\u9650\u3002</li>\n    <li>\u63d0\u51fa\u4e86**CAPO**\u673a\u5236\uff0c\u901a\u8fc7\u6b63\u5411\u4fe1\u53f7\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\uff0c\u5efa\u7acb\u7a33\u56fa\u57fa\u7840\u3002</li>\n    <li>\u968f\u540e\u5f15\u5165\u8d1f\u5411\u4fe1\u53f7\uff0c\u57f9\u517b\u6a21\u578b\u7684\u533a\u5206\u80fd\u529b\uff0c\u4ece\u800c\u6539\u5584\u590d\u6742\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4e0e\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u517c\u5bb9\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning helps improve the reasoning skills of large language models after they are trained.</li>\n    <li>Current methods mix positive and negative training signals, which can create confusion and limit improvements.</li>\n    <li>The proposed method, CAPO, uses only positive signals initially to build a strong foundation before adding negative signals for better learning.</li>\n    <li>CAPO works well with various optimization techniques and shows consistent improvements in mathematical reasoning tasks.</li>\n    <li>It also effectively adapts to multimodal GUI reasoning tasks, making it a flexible optimization tool.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:48:57.000Z", "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks", "summary": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02580.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05044", "authors": [{"_id": "69363ff73962c926cf680790", "name": "Yanran Zhang", "hidden": false}, {"_id": "69363ff73962c926cf680791", "name": "Ziyi Wang", "hidden": false}, {"_id": "69363ff73962c926cf680792", "name": "Wenzhao Zheng", "hidden": false}, {"_id": "69363ff73962c926cf680793", "name": "Zheng Zhu", "hidden": false}, {"_id": "69363ff73962c926cf680794", "name": "Jie Zhou", "hidden": false}, {"_id": "69363ff73962c926cf680795", "name": "Jiwen Lu", "hidden": false}], "publishedAt": "2025-12-04T17:59:10.000Z", "submittedOnDailyAt": "2025-12-08T00:40:23.601Z", "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image", "submittedOnDailyBy": {"_id": "661cfae9a853782abad2a495", "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg", "isPro": false, "fullname": "Yanran Zhang", "user": "Yanran21", "type": "user"}, "summary": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.", "upvotes": 6, "discussionId": "69363ff83962c926cf680796", "projectPage": "https://ivg-yanranzhang.github.io/MoRe4D/", "githubRepo": "https://github.com/Zhangyr2022/MoRe4D", "ai_summary": "MoRe4D generates high-quality 4D scenes with multi-view consistency and dynamic details from a single image using a diffusion-based trajectory generator and depth-guided motion normalization.", "ai_keywords": ["TrajScene-60K", "diffusion-based 4D Scene Trajectory Generator", "4D-STraG", "depth-guided motion normalization", "motion-aware module", "4D View Synthesis Module", "4D-ViSM", "multi-view consistency", "dynamic details"], "githubStars": 19, "organization": {"_id": "693649ff6df58c411109e13e", "name": "Tsinghua-IVG", "fullname": "Tsinghua-IVG", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/RskTCzyJeWVIs5-RJA2QR.png"}, "summary_zh": "<ul>\n    <li>\u4ece\u5355\u5f20\u9759\u6001\u56fe\u50cf\u751f\u6210\u4e92\u52a8\u548c\u52a8\u6001\u76844D\u573a\u666f\u662f\u4e00\u9879\u91cd\u8981\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u51e0\u4f55\u548c\u8fd0\u52a8\u5206\u5f00\u5904\u7406\uff0c\u5bfc\u81f4\u65f6\u7a7a\u4e0d\u4e00\u81f4\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002</li>\n    <li>\u63d0\u51fa\u4e86MoRe4D\u6846\u67b6\uff0c\u8054\u5408\u8fdb\u884c\u8fd0\u52a8\u751f\u6210\u548c\u51e0\u4f55\u91cd\u5efa\u3002</li>\n    <li>\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b60,000\u4e2a\u89c6\u9891\u6837\u672c\u7684\u5927\u578b\u6570\u636e\u96c6TrajScene-60K\uff0c\u4ee5\u89e3\u51b3\u9ad8\u8d28\u91cf4D\u573a\u666f\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cMoRe4D\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u76844D\u573a\u666f\uff0c\u5177\u6709\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u4e30\u5bcc\u7684\u52a8\u6001\u7ec6\u8282\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Creating dynamic 4D scenes from one still image is a big challenge in technology.</li>\n    <li>The new method, MoRe4D, improves how motion and geometry are linked together for better results.</li>\n    <li>MoRe4D uses a new dataset called TrajScene-60K, which has 60,000 video samples with detailed movement paths.</li>\n    <li>The system includes advanced tools to ensure that generated scenes are both realistic and visually consistent.</li>\n    <li>Tests show that MoRe4D can produce high-quality 4D scenes with consistent views and detailed motion from just one image.</li>\n</ul>"}, "publishedAt": "2025-12-04T12:59:10.000Z", "title": "Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image", "summary": "Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05044.png", "numComments": 1, "submittedBy": {"_id": "661cfae9a853782abad2a495", "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg", "fullname": "Yanran Zhang", "name": "Yanran21", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "693649ff6df58c411109e13e", "name": "Tsinghua-IVG", "fullname": "Tsinghua-IVG", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/RskTCzyJeWVIs5-RJA2QR.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02835", "authors": [{"_id": "69325b966d1060ca587a275f", "user": {"_id": "68aebbb80fdaa186aa530e8e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2WI2GgcHoZzYSM-879ejO.png", "isPro": false, "fullname": "Yifan Li(SII)", "user": "Tangerine24", "type": "user"}, "name": "Yifan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:16.888Z", "hidden": false}, {"_id": "69325b966d1060ca587a2760", "name": "Yingda Yin", "hidden": false}, {"_id": "69325b966d1060ca587a2761", "name": "Lingting Zhu", "hidden": false}, {"_id": "69325b966d1060ca587a2762", "name": "Weikai Chen", "hidden": false}, {"_id": "69325b966d1060ca587a2763", "name": "Shengju Qian", "hidden": false}, {"_id": "69325b966d1060ca587a2764", "name": "Xin Wang", "hidden": false}, {"_id": "69325b966d1060ca587a2765", "name": "Yanwei Fu", "hidden": false}], "publishedAt": "2025-12-02T14:44:12.000Z", "submittedOnDailyAt": "2025-12-08T00:07:36.304Z", "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning", "submittedOnDailyBy": {"_id": "68aebbb80fdaa186aa530e8e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2WI2GgcHoZzYSM-879ejO.png", "isPro": false, "fullname": "Yifan Li(SII)", "user": "Tangerine24", "type": "user"}, "summary": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .", "upvotes": 6, "discussionId": "69325b966d1060ca587a2766", "projectPage": "https://clementine24.github.io/ReVSeg/", "githubRepo": "https://github.com/Clementine24/ReVSeg", "ai_summary": "ReVSeg, a reasoning-centric video object segmentation framework, uses sequential decision-making in pretrained vision language models and reinforcement learning to achieve state-of-the-art performance and interpretable reasoning.", "ai_keywords": ["video object segmentation", "dynamics", "causality", "temporal interactions", "latent embeddings", "pretrained vision language models", "semantics interpretation", "temporal evidence selection", "spatial grounding", "reinforcement learning", "reasoning trajectories"], "githubStars": 5, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u7269\u4f53\u5206\u5272\u4efb\u52a1\u590d\u6742\uff0c\u6d89\u53ca\u52a8\u6001\u3001\u56e0\u679c\u5173\u7cfb\u548c\u65f6\u95f4\u4ea4\u4e92\u3002</li>\n    <li>\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u5c06\u8fd9\u4e9b\u56e0\u7d20\u7b80\u5316\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u4e0d\u900f\u660e\u3002</li>\n    <li>ReVSeg\u91c7\u7528\u660e\u786e\u7684\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>ReVSeg\u6267\u884c\u4e09\u4e2a\u660e\u786e\u7684\u64cd\u4f5c\uff1a\u8bed\u4e49\u89e3\u91ca\u3001\u65f6\u95f4\u8bc1\u636e\u9009\u62e9\u548c\u7a7a\u95f4\u5b9a\u4f4d\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u6b65\u9aa4\u63a8\u7406\u94fe\uff0c\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Video object segmentation is complex because it involves understanding dynamics and interactions over time.</li>\n  <li>Many existing methods oversimplify the reasoning process, making it hard to understand how decisions are made.</li>\n  <li>The new method, ReVSeg, breaks down reasoning into three clear steps: understanding meanings, selecting relevant evidence over time, and determining spatial locations.</li>\n  <li>ReVSeg uses reinforcement learning to improve its decision-making process based on feedback from results.</li>\n  <li>Tests show that ReVSeg performs better than previous methods and provides clearer reasoning paths.</li>\n</ul>"}, "publishedAt": "2025-12-02T09:44:12.000Z", "title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning", "summary": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02835.png", "numComments": 1, "submittedBy": {"_id": "68aebbb80fdaa186aa530e8e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/2WI2GgcHoZzYSM-879ejO.png", "fullname": "Yifan Li(SII)", "name": "Tangerine24", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.00473", "authors": [{"_id": "693655753962c926cf6807eb", "name": "Junyan Ye", "hidden": false}, {"_id": "693655753962c926cf6807ec", "name": "Leiqi Zhu", "hidden": false}, {"_id": "693655753962c926cf6807ed", "name": "Yuncheng Guo", "hidden": false}, {"_id": "693655753962c926cf6807ee", "name": "Dongzhi Jiang", "hidden": false}, {"_id": "693655753962c926cf6807ef", "name": "Zilong Huang", "hidden": false}, {"_id": "693655753962c926cf6807f0", "name": "Yifan Zhang", "hidden": false}, {"_id": "693655753962c926cf6807f1", "name": "Zhiyuan Yan", "hidden": false}, {"_id": "693655753962c926cf6807f2", "name": "Haohuan Fu", "hidden": false}, {"_id": "693655753962c926cf6807f3", "name": "Conghui He", "hidden": false}, {"_id": "693655753962c926cf6807f4", "name": "Weijia Li", "hidden": false}], "publishedAt": "2025-11-29T12:52:26.000Z", "submittedOnDailyAt": "2025-12-08T02:06:14.909Z", "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards", "submittedOnDailyBy": {"_id": "6487e158f675b4a7867f45fa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg", "isPro": false, "fullname": "Zilong Huang", "user": "SereinH", "type": "user"}, "summary": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.", "upvotes": 6, "discussionId": "693655763962c926cf6807f5", "projectPage": "https://yejy53.github.io/RealGen/", "githubRepo": "https://github.com/yejy53/RealGen", "ai_summary": "RealGen is a photorealistic text-to-image framework that uses an LLM for prompt optimization and a diffusion model for image generation, enhanced by a Detector Reward mechanism and RealBench for automated evaluation.", "ai_keywords": ["LLM", "diffusion model", "Detector Reward", "semantic-level detectors", "feature-level detectors", "GRPO algorithm", "RealBench", "Detector-Scoring", "Arena-Scoring"], "githubStars": 14, "summary_zh": "<ul>\n    <li>\u968f\u7740\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u65b0\u7684\u6a21\u578b\u5982GPT-Image-1\u548cQwen-Image\u5728\u6587\u672c\u5230\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u6210\u903c\u771f\u56fe\u50cf\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u5e38\u5e38\u4ea7\u751f\u5e26\u6709AI\u4f2a\u5f71\u7684\u201c\u5047\u201d\u56fe\u50cf\uff0c\u4f8b\u5982\u201c\u8fc7\u4e8e\u5149\u6ed1\u7684\u76ae\u80a4\u201d\u548c\u201c\u6cb9\u817b\u7684\u9762\u90e8\u5149\u6cfd\u201d\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RealGen\uff0c\u4e00\u4e2a\u65b0\u7684\u903c\u771f\u6587\u672c\u5230\u56fe\u50cf\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u63d0\u793a\u4f18\u5316\u548c\u771f\u5b9e\u56fe\u50cf\u751f\u6210\u7684\u6280\u672f\u3002</li>\n    <li>RealGen\u5f15\u5165\u4e86\u201c\u68c0\u6d4b\u5668\u5956\u52b1\u201d\u673a\u5236\uff0c\u901a\u8fc7\u91cf\u5316\u4f2a\u5f71\u548c\u8bc4\u4f30\u771f\u5b9e\u611f\u6765\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7684\u771f\u5b9e\u611f\u548c\u7ec6\u8282\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cRealGen\u5728\u771f\u5b9e\u611f\u3001\u7ec6\u8282\u548c\u7f8e\u5b66\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5176\u4ed6\u6a21\u578b\uff0c\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models like GPT-Image-1 and Qwen-Image have improved text-to-image generation but still struggle with making realistic images.</li>\n    <li>RealGen is a new framework that focuses on creating photorealistic images by combining text prompt optimization and a diffusion model.</li>\n    <li>It uses a \"Detector Reward\" system to measure image quality and realism, helping to improve the generation process.</li>\n    <li>RealGen includes RealBench, a tool for automatically evaluating image realism without human input, providing more accurate results.</li>\n    <li>Tests show that RealGen produces better images than both general and specialized models in terms of realism and detail.</li>\n</ul>"}, "publishedAt": "2025-11-29T07:52:26.000Z", "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards", "summary": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.00473.png", "numComments": 1, "submittedBy": {"_id": "6487e158f675b4a7867f45fa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg", "fullname": "Zilong Huang", "name": "SereinH", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05927", "authors": [{"_id": "6936388c3962c926cf680713", "name": "Zhiting Mei", "hidden": false}, {"_id": "6936388c3962c926cf680714", "name": "Tenny Yin", "hidden": false}, {"_id": "6936388c3962c926cf680715", "name": "Micah Baker", "hidden": false}, {"_id": "6936388c3962c926cf680716", "name": "Ola Shorinwa", "hidden": false}, {"_id": "6936388c3962c926cf680717", "name": "Anirudha Majumdar", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/uFiYGO8LB_HsPAbDr5LFO.mp4"], "publishedAt": "2025-12-05T18:06:18.000Z", "submittedOnDailyAt": "2025-12-08T00:01:47.963Z", "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.", "upvotes": 3, "discussionId": "6936388c3962c926cf680718", "projectPage": "https://c-cubed-uq.github.io/", "ai_summary": "C3 is an uncertainty quantification method for training controllable video models that provides dense confidence estimation and out-of-distribution detection, addressing hallucination issues.", "ai_keywords": ["generative video models", "controllable video models", "instruction-guided video editing", "robot policy evaluation", "planning", "uncertainty quantification", "UQ", "continuous-scale calibrated", "dense confidence estimation", "subpatch level", "latent space", "pixel-space", "calibrated uncertainty estimates", "training distribution", "out-of-distribution detection"], "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u8fdb\u5c55\u5728\u9ad8\u4fdd\u771f\u89c6\u9891\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\uff0c\u5c24\u5176\u662f\u5728\u53ef\u63a7\u89c6\u9891\u751f\u6210\u4e2d\u3002</li>\n    <li>\u5c3d\u7ba1\u53ef\u63a7\u89c6\u9891\u6a21\u578b\u80fd\u529b\u5353\u8d8a\uff0c\u4f46\u5b83\u4eec\u5e38\u5e38\u751f\u6210\u4e0e\u7269\u7406\u73b0\u5b9e\u4e0d\u7b26\u7684\u672a\u6765\u89c6\u9891\u5e27\uff0c\u5b58\u5728\u201c\u5e7b\u89c9\u201d\u73b0\u8c61\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aC3\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6bcf\u4e2a\u751f\u6210\u89c6\u9891\u5e27\u7684\u5b50\u5757\u7ea7\u522b\u4e0a\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002</li>\n    <li>C3\u65b9\u6cd5\u901a\u8fc7\u4e09\u9879\u6838\u5fc3\u521b\u65b0\u4f7f\u89c6\u9891\u6a21\u578b\u80fd\u591f\u4f30\u8ba1\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u5305\u62ec\u901a\u8fc7\u4e25\u683c\u7684\u8bc4\u5206\u89c4\u5219\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cC3\u65b9\u6cd5\u53ef\u4ee5\u5728\u8bad\u7ec3\u5206\u5e03\u5185\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u6709\u6548\u68c0\u6d4b\u8d85\u51fa\u5206\u5e03\u7684\u60c5\u51b5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in video generation allow for creating high-quality videos based on text and actions, useful in areas like video editing and robotics.</li>\n    <li>Current video models often produce unrealistic video frames, which can be problematic for tasks like robot planning and evaluation.</li>\n    <li>The proposed method, called C3, helps video models measure and express their confidence, making it easier to identify when they might be wrong.</li>\n    <li>C3 uses a new approach to train video models for accuracy and reliability, focusing on latent space instead of pixel space to avoid training issues.</li>\n    <li>It creates detailed uncertainty maps that show which parts of the generated video are less reliable, tested successfully on large robot learning datasets.</li>\n</ul>"}, "publishedAt": "2025-12-05T13:06:18.000Z", "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty", "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/uFiYGO8LB_HsPAbDr5LFO.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05927.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2 \u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\u548c\u4ee3\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b (DSA)\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u7a33\u5065\u7684\u5f3a\u5316\u5b66\u4e60\u534f\u8bae\uff0cDeepSeek-V3.2 \u7684\u8868\u73b0\u4e0e GPT-5 \u76f8\u5f53\uff0c\u5176\u4e2d\u9ad8\u8ba1\u7b97\u7248\u672c\u8d85\u8d8a\u4e86 GPT-5\uff0c\u5e76\u5728\u63a8\u7406\u4e0a\u4e0e Gemini-3.0-Pro \u65d7\u9f13\u76f8\u5f53\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u5730\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u6267\u884c\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that balances fast performance with strong reasoning abilities.</li>\n    <li>It features a new attention method called DeepSeek Sparse Attention (DSA), which makes it more efficient, especially with long texts.</li>\n    <li>The model uses a strong reinforcement learning system that allows it to compete with top models like GPT-5 and even outperform it in some areas.</li>\n    <li>DeepSeek-V3.2 has achieved high performance in prestigious competitions like the International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a pipeline that creates large amounts of training data to improve reasoning and following instructions in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u7528\u4e8e\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u6e05\u3001\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u6d41\u6c34\u7ebf\u53bb\u566a\u6b65\u9aa4\uff0c\u6709\u6548\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5a92\u4f53\u751f\u6210\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6dc0\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u9996\u6b21\u5728\u6b64\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new framework for creating high-quality, real-time avatar videos using a powerful 14-billion-parameter diffusion model.</li>\n    <li>It uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up the video generation process by utilizing multiple GPUs.</li>\n    <li>The Rolling Sink Frame Mechanism (RSFM) improves the consistency of the avatars by adjusting their appearance based on a stored reference image.</li>\n    <li>Live Avatar can generate videos at 20 frames per second on 5 GPUs, making it one of the fastest and most effective systems for real-time avatar creation.</li>\n    <li>This work sets a new standard for using advanced diffusion models in long video synthesis for various applications.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4e86\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u7684\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\uff08DE\uff09\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u67b6\u6784\u4e0a\u8fdb\u884c\u591a\u5c42\u6b21\u7684SQL\u7ba1\u9053\u8bbe\u8ba1\u548c\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\uff08DA\uff09\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524d\u7684\u5148\u8fdb\u4ee3\u7406\u5728DAComp\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u7684\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u4e3a\u5f00\u53d1\u771f\u6b63\u6709\u80fd\u529b\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u548c\u73b0\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark with 210 tasks that mimics real-world data workflows in businesses.</li>\n    <li>It includes two main types of tasks: Data Engineering (DE) and Data Analysis (DA).</li>\n    <li>DE tasks involve creating and updating complex data systems, while DA tasks focus on solving open-ended business problems.</li>\n    <li>Current AI agents struggle with these tasks, scoring low on both DE (under 20%) and DA (below 40%), showing clear gaps in their abilities.</li>\n    <li>DAComp helps identify these weaknesses and aims to improve the development of autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01816", "authors": [{"_id": "692e5c0537312eaa83fd87b8", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:43.760Z", "hidden": false}, {"_id": "692e5c0537312eaa83fd87b9", "name": "Siyuan Li", "hidden": false}, {"_id": "692e5c0537312eaa83fd87ba", "name": "Conghui He", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bb", "name": "Lijun Wu", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bc", "user": {"_id": "64be296a46cc3cdfbb057f7e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg", "isPro": false, "fullname": "Cheng Tan", "user": "chengtan9907", "type": "user"}, "name": "Cheng Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:41.755Z", "hidden": false}], "publishedAt": "2025-12-01T15:52:31.000Z", "submittedOnDailyAt": "2025-12-02T01:31:46.625Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "submittedOnDailyBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "upvotes": 87, "discussionId": "692e5c0537312eaa83fd87bd", "projectPage": "https://opendatalab-raiser.github.io/Envision/", "githubRepo": "https://github.com/opendatalab-raiser/Envision", "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.", "ai_keywords": ["multimodal models", "text-to-image (T2I)", "causal event progression", "spatiotemporal causality", "Envision-a", "Envision-Score", "multi-dimensional consistency", "physicality", "aesthetics", "causal narrative coherence", "spatiotemporal consistency", "multi-frame reasoning", "dynamic world modeling"], "githubStars": 27, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u5e0c\u671b\u901a\u8fc7\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6765\u8d85\u8d8a\u5355\u4e00\u6a21\u6001\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u9759\u6001\u5355\u56fe\u751f\u6210\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\u548c\u52a8\u6001\u8fc7\u7a0b\u5efa\u6a21\u80fd\u529b\u4e0d\u8db3\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u201cEnvision\u201d\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u5230\u591a\u56fe\u50cf\u7684\u751f\u6210\uff0c\u5305\u542b1000\u4e2a\u56db\u9636\u6bb5\u63d0\u793a\uff0c\u6db5\u76d6\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u3002</li>\n    <li>\u5f15\u5165\u201cEnvision-Score\u201d\u4f5c\u4e3a\u7efc\u5408\u8bc4\u4ef7\u6307\u6807\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e16\u754c\u77e5\u8bc6\u548c\u56e0\u679c\u65f6\u5e8f\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4e13\u95e8\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7f8e\u5b66\u8868\u73b0\u4e0a\u4f18\u8d8a\uff0c\u4f46\u7f3a\u4e4f\u5185\u5728\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u800c\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u4e8b\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current models that combine text and images struggle with creating dynamic content because they focus too much on single images.</li>\n    <li>To improve this, the proposed Envision benchmark introduces a new way to evaluate models using sequences of images based on real-world events.</li>\n    <li>Envision includes 1,000 prompts across six subjects, aiming to test how well models understand world knowledge and temporal relationships.</li>\n    <li>The new Envision-Score metric assesses models on multiple aspects like consistency and aesthetics.</li>\n    <li>Results show that while specialized models are good at aesthetics, unified models are better at understanding narratives, but all models face challenges with consistency over time.</li>\n</ul>"}, "publishedAt": "2025-12-01T10:52:31.000Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png", "numComments": 4, "submittedBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "fullname": "Juanxi Tian", "name": "Juanxi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13}, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01374", "authors": [{"_id": "692e6bf937312eaa83fd8890", "user": {"_id": "610b70452719facd4ea85e28", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg", "isPro": false, "fullname": "Chujie Zheng", "user": "chujiezheng", "type": "user"}, "name": "Chujie Zheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:27.206Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8891", "name": "Kai Dang", "hidden": false}, {"_id": "692e6bf937312eaa83fd8892", "name": "Bowen Yu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8893", "name": "Mingze Li", "hidden": false}, {"_id": "692e6bf937312eaa83fd8894", "user": {"_id": "6278bd42541f3d2dfa77ea70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg", "isPro": false, "fullname": "Huiqiang Jiang", "user": "iofu728", "type": "user"}, "name": "Huiqiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:41.367Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8895", "name": "Junrong Lin", "hidden": false}, {"_id": "692e6bf937312eaa83fd8896", "name": "Yuqiong Liu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8897", "user": {"_id": "62088594a5943c8a8fc94560", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png", "isPro": false, "fullname": "An Yang", "user": "yangapku", "type": "user"}, "name": "An Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:25.208Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8898", "name": "Jingren Zhou", "hidden": false}, {"_id": "692e6bf937312eaa83fd8899", "name": "Junyang Lin", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "publishedAt": "2025-12-01T07:45:39.000Z", "submittedOnDailyAt": "2025-12-02T02:47:49.367Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "submittedOnDailyBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "isPro": false, "fullname": "Bowen Yu", "user": "Tigerph", "type": "user"}, "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "upvotes": 78, "discussionId": "692e6bfa37312eaa83fd889a", "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.", "ai_keywords": ["reinforcement learning", "large language models", "sequence-level reward", "token-level objective", "policy gradient methods", "REINFORCE", "first-order approximation", "training-inference discrepancy", "policy staleness", "importance sampling correction", "clipping", "Routing Replay", "Mixture-of-Experts", "on-policy training", "off-policy updates"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u89e3\u91ca\u4e86\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u66ff\u4ee3\u7684\u4ee4\u724c\u7ea7\u76ee\u6807\u6765\u4f18\u5316\u771f\u5b9e\u7684\u5e8f\u5217\u7ea7\u5956\u52b1\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u53ea\u6709\u5728\u6700\u5c0f\u5316\u8bad\u7ec3\u4e0e\u63a8\u7406\u5dee\u5f02\u548c\u7b56\u7565\u9648\u65e7\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u66ff\u4ee3\u76ee\u6807\u624d\u66f4\u6709\u6548\u3002</li>\n    <li>\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u53d1\u73b0\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\u7684\u57fa\u672c\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5728\u5728\u7ebf\u8bad\u7ec3\u4e2d\u5177\u6709\u6700\u9ad8\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u4e00\u65e6\u8bad\u7ec3\u7a33\u5b9a\uff0c\u6301\u7eed\u4f18\u5316\u53ef\u4ee5\u83b7\u5f97\u76f8\u4f3c\u7684\u6700\u7ec8\u8868\u73b0\uff0c\u4e0d\u53d7\u51b7\u542f\u52a8\u521d\u59cb\u5316\u7684\u5f71\u54cd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper introduces a new way to improve reinforcement learning using large language models.</li>\n    <li>It explains how to optimize rewards by using a token-level objective, especially when minimizing training-inference gaps and policy delays.</li>\n    <li>Key techniques like importance sampling correction, clipping, and Routing Replay are important for stabilizing training.</li>\n    <li>Experiments show that using importance sampling with on-policy training provides the best stability.</li>\n    <li>Once training is stable, the final performance remains consistent, regardless of how training starts.</li>\n</ul>"}, "publishedAt": "2025-12-01T02:45:39.000Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png", "numComments": 2, "submittedBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "fullname": "Bowen Yu", "name": "Tigerph", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04987", "authors": [{"_id": "6932458a6d1060ca587a2618", "name": "Nex-AGI Team", "hidden": false}, {"_id": "6932458a6d1060ca587a261a", "name": "Yuxuan Cai", "hidden": false}, {"_id": "6932458a6d1060ca587a261b", "name": "Lu Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a261c", "name": "Qiaoling Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a261d", "name": "Yuyang Ding", "hidden": false}, {"_id": "6932458a6d1060ca587a261e", "name": "Liwen Fan", "hidden": false}, {"_id": "6932458a6d1060ca587a261f", "name": "Wenjie Fu", "hidden": false}, {"_id": "6932458a6d1060ca587a2620", "user": {"_id": "658bd417925aadd43303566a", "avatarUrl": "/avatars/e97f6696817caaa4564a33f12c7b9090.svg", "isPro": false, "fullname": "Gao", "user": "Yufei0707", "type": "user"}, "name": "Yufei Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:36.106Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2621", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:43.376Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2622", "user": {"_id": "6461e09759daabed7575b7a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6461e09759daabed7575b7a2/sxfko49Q7ta0dCfgrpqoB.jpeg", "isPro": false, "fullname": "PinxueGuo", "user": "PinxueGuo", "type": "user"}, "name": "Pinxue Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:14:54.854Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2623", "name": "Zhenhua Han", "hidden": false}, {"_id": "6932458a6d1060ca587a2624", "name": "Zhengfu He", "hidden": false}, {"_id": "6932458a6d1060ca587a2625", "name": "Hanglei Hu", "hidden": false}, {"_id": "6932458a6d1060ca587a2626", "name": "Kai Hu", "hidden": false}, {"_id": "6932458a6d1060ca587a2627", "name": "Shengjia Hua", "hidden": false}, {"_id": "6932458a6d1060ca587a2628", "name": "Tianyu Huai", "hidden": false}, {"_id": "6932458a6d1060ca587a2629", "name": "Baodai Huang", "hidden": false}, {"_id": "6932458a6d1060ca587a262a", "name": "Li Ji", "hidden": false}, {"_id": "6932458a6d1060ca587a262b", "name": "Zhen Jiang", "hidden": false}, {"_id": "6932458a6d1060ca587a262c", "name": "Zhikai Lei", "hidden": false}, {"_id": "6932458a6d1060ca587a262d", "name": "Bufan Li", "hidden": false}, {"_id": "6932458a6d1060ca587a262e", "name": "Jiahang Lin", "hidden": false}, {"_id": "6932458a6d1060ca587a262f", "name": "Lizhi Lin", "hidden": false}, {"_id": "6932458a6d1060ca587a2630", "name": "Jinxiu Liu", "hidden": false}, {"_id": "6932458a6d1060ca587a2631", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:39.885Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2632", "name": "Ziming Liu", "hidden": false}, {"_id": "6932458a6d1060ca587a2633", "name": "Yuchen Ni", "hidden": false}, {"_id": "6932458a6d1060ca587a2634", "name": "Pengfang Qian", "hidden": false}, {"_id": "6932458a6d1060ca587a2635", "name": "Yujiong Shen", "hidden": false}, {"_id": "6932458a6d1060ca587a2636", "name": "Qingyun Shi", "hidden": false}, {"_id": "6932458a6d1060ca587a2637", "name": "Wentao Shu", "hidden": false}, {"_id": "6932458a6d1060ca587a2638", "name": "Peng Sun", "hidden": false}, {"_id": "6932458a6d1060ca587a2639", "name": "Yiran Suo", "hidden": false}, {"_id": "6932458a6d1060ca587a263a", "name": "Tian Tang", "hidden": false}, {"_id": "6932458a6d1060ca587a263b", "name": "Boyu Tian", "hidden": false}, {"_id": "6932458a6d1060ca587a263c", "user": {"_id": "6542391f3fcd1aee202383d2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5O7dZAVGmwhsx1THSJ-gn.jpeg", "isPro": false, "fullname": "wang", "user": "guoteng", "type": "user"}, "name": "Guoteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:14:58.106Z", "hidden": false}, {"_id": "6932458a6d1060ca587a263d", "name": "Junzhe Wang", "hidden": false}, {"_id": "6932458a6d1060ca587a263e", "name": "Peixin Wang", "hidden": false}, {"_id": "6932458a6d1060ca587a263f", "user": {"_id": "653a6e5cae155b92bae77b74", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg", "isPro": false, "fullname": "Zhiheng Xi", "user": "WooooDyy", "type": "user"}, "name": "Zhiheng Xi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:45.863Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2640", "name": "Hang Yan", "hidden": false}, {"_id": "6932458a6d1060ca587a2641", "user": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "name": "Jie Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:12.116Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2642", "name": "Zhixiong Yang", "hidden": false}, {"_id": "6932458a6d1060ca587a2643", "name": "Tianchu Yao", "hidden": false}, {"_id": "6932458a6d1060ca587a2644", "name": "Guangze Ye", "hidden": false}, {"_id": "6932458a6d1060ca587a2645", "name": "Qianxi Yu", "hidden": false}, {"_id": "6932458a6d1060ca587a2646", "user": {"_id": "6334f2f1259c518276efa730", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334f2f1259c518276efa730/z_SH_OBkDyj4RCN9mqsKS.jpeg", "isPro": false, "fullname": "Shuo Zhang", "user": "Meteonis", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:41.640Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2647", "name": "Xinyue Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2648", "name": "Yiqi Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2649", "name": "Jiarong Zhao", "hidden": false}, {"_id": "6932458a6d1060ca587a264a", "name": "Miao Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a264b", "name": "Rui Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a264c", "name": "Enyu Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264d", "name": "Jiazheng Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264e", "name": "Maosen Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264f", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a2650", "name": "Tao Gui", "hidden": false}, {"_id": "6932458a6d1060ca587a2651", "name": "Yining Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a2652", "name": "Xinchi Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a2653", "name": "Jie Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a2654", "user": {"_id": "62061f8f03825909dcbeba27", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62061f8f03825909dcbeba27/byOi6gxlycUtsUPIUPscQ.jpeg", "isPro": false, "fullname": "Siyuan Feng", "user": "Siyuan", "type": "user"}, "name": "Siyuan Feng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:38.168Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2655", "name": "Qin Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a2656", "name": "Liang He", "hidden": false}, {"_id": "6932458a6d1060ca587a2657", "name": "Qi Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2658", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6932458a6d1060ca587a2659", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2025-12-04T16:57:02.000Z", "submittedOnDailyAt": "2025-12-05T00:08:10.618Z", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "upvotes": 66, "discussionId": "6932458a6d1060ca587a265a", "githubRepo": "https://github.com/nex-agi/Nex-N1", "ai_summary": "The introduction of NexAU, NexA4A, and NexGAP enables the scaling of complexity, diversity, and fidelity in interactive environments for training large language models as autonomous agents, resulting in superior performance.", "ai_keywords": ["Large Language Models", "autonomous agents", "incentive-driven decision making", "policy learning", "interaction signals", "NexAU", "agent hierarchies", "NexA4A", "natural language", "NexGAP", "simulation-reality gap", "grounded trajectories synthesis", "SWE-bench", "tau2"], "githubStars": 71, "organization": {"_id": "6907441c72f7d95376e910a5", "name": "nex-agi", "fullname": "Nex AGI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6b63\u4ece\u88ab\u52a8\u54cd\u5e94\u8005\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u5b66\u4e60\u65b9\u5f0f\u9700\u8981\u4ece\u9759\u6001\u6a21\u4eff\u8f6c\u5411\u6fc0\u52b1\u9a71\u52a8\u7684\u51b3\u7b56\u5236\u5b9a\u3002</li>\n    <li>\u8fd9\u4e00\u8f6c\u53d8\u53d7\u5230\u7f3a\u4e4f\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u7684\u963b\u788d\uff0c\u8be5\u57fa\u7840\u8bbe\u65bd\u9700\u8981\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e92\u52a8\u4fe1\u53f7\u4ee5\u6709\u6548\u5b66\u4e60\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u6027\u6269\u5c55\u4e92\u52a8\u73af\u5883\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff1a\u590d\u6742\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u611f\u3002</li>\n    <li>NexAU\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u652f\u6301\u901a\u8fc7\u7b80\u5355\u914d\u7f6e\u6784\u5efa\u590d\u6742\u7684\u667a\u80fd\u4f53\u5c42\u7ea7\uff1bNexA4A\u81ea\u52a8\u751f\u6210\u591a\u6837\u7684\u667a\u80fd\u4f53\u5c42\u7ea7\uff1bNexGAP\u5219\u901a\u8fc7\u6574\u5408\u52a8\u6001\u7684\u771f\u5b9e\u73af\u5883\u6765\u5f25\u8865\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578bNex-N1\u5728\u5efa\u7acb\u7684\u591a\u6837\u548c\u590d\u6742\u7684\u4e92\u52a8\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u524d\u6cbf\u4e13\u6709\u6a21\u578b\u7684\u8868\u73b0\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving to become autonomous agents, requiring a new way of learning that focuses on decision-making driven by incentives.</li>\n    <li>The development of this new approach is hindered by the lack of infrastructure to create effective interaction signals for learning.</li>\n    <li>We present a method to improve interactive environments by focusing on three areas: complexity, diversity, and fidelity.</li>\n    <li>NexAU helps create complex agent structures easily, NexA4A generates diverse agent hierarchies automatically, and NexGAP enhances simulation quality by using real-world data.</li>\n    <li>Our model, Nex-N1, shows better performance than existing models on various benchmarks and is available for further research through open-sourcing.</li>\n</ul>"}, "publishedAt": "2025-12-04T11:57:02.000Z", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04987.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "6907441c72f7d95376e910a5", "name": "nex-agi", "fullname": "Nex AGI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.03041", "authors": [{"_id": "692fcb3726742347f61dad18", "user": {"_id": "646f3418a6a58aa29505fd30", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png", "isPro": false, "fullname": "QINGHE WANG", "user": "DecoderWQH666", "type": "user"}, "name": "Qinghe Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:13:33.820Z", "hidden": false}, {"_id": "692fcb3726742347f61dad19", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "692fcb3726742347f61dad1a", "user": {"_id": "652cb3e1c6857682d30d4c2b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JEQW6WZBcmjAWwODiDkYA.jpeg", "isPro": false, "fullname": "Libaolu", "user": "8ruceLi", "type": "user"}, "name": "Baolu Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:13:31.176Z", "hidden": false}, {"_id": "692fcb3726742347f61dad1b", "name": "Weikang Bian", "hidden": false}, {"_id": "692fcb3726742347f61dad1c", "name": "Quande Liu", "hidden": false}, {"_id": "692fcb3726742347f61dad1d", "name": "Huchuan Lu", "hidden": false}, {"_id": "692fcb3726742347f61dad1e", "name": "Xintao Wang", "hidden": false}, {"_id": "692fcb3726742347f61dad1f", "name": "Pengfei Wan", "hidden": false}, {"_id": "692fcb3726742347f61dad20", "name": "Kun Gai", "hidden": false}, {"_id": "692fcb3726742347f61dad21", "name": "Xu Jia", "hidden": false}], "publishedAt": "2025-12-02T18:59:48.000Z", "submittedOnDailyAt": "2025-12-03T03:10:24.737Z", "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework", "submittedOnDailyBy": {"_id": "646f3418a6a58aa29505fd30", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png", "isPro": false, "fullname": "QINGHE WANG", "user": "DecoderWQH666", "type": "user"}, "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.", "upvotes": 57, "discussionId": "692fcb3726742347f61dad22", "projectPage": "https://qinghew.github.io/MultiShotMaster/", "ai_summary": "MultiShotMaster extends a single-shot model with novel RoPE variants for flexible and controllable multi-shot video generation, addressing data scarcity with an automated annotation pipeline.", "ai_keywords": ["RoPE", "Multi-Shot Narrative RoPE", "Spatiotemporal Position-Aware RoPE", "reference tokens", "cross-shot grounding signals", "reference images", "text-driven inter-shot consistency", "motion control", "background-driven customized scene"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u89c6\u9891\u751f\u6210\u6280\u672f\u64c5\u957f\u5355\u955c\u5934\u89c6\u9891\uff0c\u4f46\u5728\u591a\u955c\u5934\u53d9\u4e8b\u89c6\u9891\u751f\u6210\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MultiShotMaster\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u5ea6\u53ef\u63a7\u7684\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5229\u7528\u4e24\u79cd\u65b0\u578bRoPE\u53d8\u4f53\uff0c\u652f\u6301\u7075\u6d3b\u7684\u955c\u5934\u5b89\u6392\u548c\u4e00\u81f4\u7684\u53d9\u4e8b\u987a\u5e8f\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86\u81ea\u52a8\u5316\u6570\u636e\u6ce8\u91ca\u7ba1\u9053\uff0c\u4ee5\u63d0\u53d6\u591a\u955c\u5934\u89c6\u9891\u53ca\u5176\u76f8\u5173\u4fe1\u606f\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u6027\u80fd\u548c\u53ef\u63a7\u6027\u4e0a\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video generation methods are good at making single-shot videos but struggle with creating longer, multi-shot videos that tell a story.</li>\n    <li>MultiShotMaster is a new framework designed to generate multi-shot videos with better control over the narrative and shot arrangement.</li>\n    <li>The framework uses two new techniques called Multi-Shot Narrative RoPE and Spatiotemporal Position-Aware RoPE to enhance video quality and consistency.</li>\n    <li>It includes an automated system to gather and annotate data for multi-shot videos, which helps improve the generation process.</li>\n    <li>Experimental results show that MultiShotMaster performs better and offers more control compared to existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-02T13:59:48.000Z", "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework", "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03041.png", "numComments": 4, "submittedBy": {"_id": "646f3418a6a58aa29505fd30", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646f3418a6a58aa29505fd30/1z13rnpb6rsUgQsYumWPg.png", "fullname": "QINGHE WANG", "name": "DecoderWQH666", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02014", "authors": [{"_id": "692e56a137312eaa83fd8761", "name": "Zhiheng Liu", "hidden": false}, {"_id": "692e56a137312eaa83fd8762", "user": {"_id": "64405a9d518271b0d1beea38", "avatarUrl": "/avatars/b702474588fd7090773320422417a582.svg", "isPro": false, "fullname": "Weiming Ren", "user": "wren93", "type": "user"}, "name": "Weiming Ren", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:25.022Z", "hidden": false}, {"_id": "692e56a137312eaa83fd8763", "name": "Haozhe Liu", "hidden": false}, {"_id": "692e56a137312eaa83fd8764", "name": "Zijian Zhou", "hidden": false}, {"_id": "692e56a137312eaa83fd8765", "user": {"_id": "6412a33900634c4fe9873652", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg", "isPro": false, "fullname": "Shoufa Chen", "user": "ShoufaChen", "type": "user"}, "name": "Shoufa Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:33.774Z", "hidden": false}, {"_id": "692e56a137312eaa83fd8766", "name": "Haonan Qiu", "hidden": false}, {"_id": "692e56a137312eaa83fd8767", "name": "Xiaoke Huang", "hidden": false}, {"_id": "692e56a137312eaa83fd8768", "name": "Zhaochong An", "hidden": false}, {"_id": "692e56a137312eaa83fd8769", "name": "Fanny Yang", "hidden": false}, {"_id": "692e56a137312eaa83fd876a", "name": "Aditya Patel", "hidden": false}, {"_id": "692e56a137312eaa83fd876b", "name": "Viktar Atliha", "hidden": false}, {"_id": "692e56a137312eaa83fd876c", "name": "Tony Ng", "hidden": false}, {"_id": "692e56a137312eaa83fd876d", "name": "Xiao Han", "hidden": false}, {"_id": "692e56a137312eaa83fd876e", "name": "Chuyan Zhu", "hidden": false}, {"_id": "692e56a137312eaa83fd876f", "name": "Chenyang Zhang", "hidden": false}, {"_id": "692e56a137312eaa83fd8770", "name": "Ding Liu", "hidden": false}, {"_id": "692e56a137312eaa83fd8771", "name": "Juan-Manuel Perez-Rua", "hidden": false}, {"_id": "692e56a137312eaa83fd8772", "name": "Sen He", "hidden": false}, {"_id": "692e56a137312eaa83fd8773", "name": "J\u00fcrgen Schmidhuber", "hidden": false}, {"_id": "692e56a137312eaa83fd8774", "name": "Wenhu Chen", "hidden": false}, {"_id": "692e56a137312eaa83fd8775", "name": "Ping Luo", "hidden": false}, {"_id": "692e56a137312eaa83fd8776", "name": "Wei Liu", "hidden": false}, {"_id": "692e56a137312eaa83fd8777", "name": "Tao Xiang", "hidden": false}, {"_id": "692e56a137312eaa83fd8778", "name": "Jonas Schult", "hidden": false}, {"_id": "692e56a137312eaa83fd8779", "user": {"_id": "62f8040c47d782a6e286ea8e", "avatarUrl": "/avatars/f45224b851ae79fff9912f6e67159e52.svg", "isPro": false, "fullname": "Yuren Cong", "user": "Yuren", "type": "user"}, "name": "Yuren Cong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:47.564Z", "hidden": false}], "publishedAt": "2025-12-01T18:59:51.000Z", "submittedOnDailyAt": "2025-12-02T01:20:57.356Z", "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.", "upvotes": 54, "discussionId": "692e56a137312eaa83fd877a", "projectPage": "https://tuna-ai.org/", "ai_summary": "TUNA, a unified multimodal model, uses a cascaded VAE and representation encoder for end-to-end multimodal understanding and generation, outperforming decoupled models and achieving state-of-the-art results across various benchmarks.", "ai_keywords": ["Unified multimodal models", "UMMs", "TUNA", "VAE encoder", "representation encoder", "unified visual representation", "multimodal understanding", "multimodal generation", "representation format mismatches", "image understanding", "video understanding", "image generation", "video generation", "image editing"], "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>TUNA\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7406\u89e3\u4e0e\u751f\u6210\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u8868\u793a\u7f16\u7801\u5668\uff0cTUNA\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u8868\u793a\u7a7a\u95f4\u3002</li>\n    <li>\u4e0e\u4e4b\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u76f8\u6bd4\uff0cTUNA\u907f\u514d\u4e86\u7531\u4e8e\u5206\u5f00\u7f16\u7801\u5668\u5f15\u8d77\u7684\u8868\u793a\u683c\u5f0f\u4e0d\u5339\u914d\uff0c\u63d0\u5347\u4e86\u7406\u89e3\u548c\u751f\u6210\u7684\u6027\u80fd\u3002</li>\n    <li>\u66f4\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u8868\u793a\u7f16\u7801\u5668\u80fd\u5728\u6240\u6709\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u63d0\u9ad8\u8868\u73b0\uff0c\u663e\u793a\u4e86\u8868\u793a\u7f16\u7801\u5668\u7684\u91cd\u8981\u6027\u3002</li>\n    <li>TUNA\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u7edf\u4e00\u8868\u793a\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>TUNA is a new model that combines understanding and generating images and videos in one system.</li>\n    <li>It creates a single visual representation to improve processing and avoid problems with different formats.</li>\n    <li>TUNA outperforms older models that use separate representations for understanding and generation.</li>\n    <li>Using better pretrained encoders leads to improved performance in all tasks.</li>\n    <li>Training on both understanding and generation together helps each task improve without conflict.</li>\n</ul>"}, "publishedAt": "2025-12-01T13:59:51.000Z", "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models", "summary": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02014.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01948", "authors": [{"_id": "692e62d937312eaa83fd87e7", "name": "Dingling Zhang", "hidden": false}, {"_id": "692e62d937312eaa83fd87e8", "name": "He Zhu", "hidden": false}, {"_id": "692e62d937312eaa83fd87e9", "user": {"_id": "6704ee27386892c420db1938", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6704ee27386892c420db1938/lb5mtEwYhn47RawkynYPs.jpeg", "isPro": false, "fullname": "JinCheng Ren", "user": "JinChengRen", "type": "user"}, "name": "Jincheng Ren", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:42.538Z", "hidden": false}, {"_id": "692e62d937312eaa83fd87ea", "name": "Kangqi Song", "hidden": false}, {"_id": "692e62d937312eaa83fd87eb", "name": "Xinran Zhou", "hidden": false}, {"_id": "692e62d937312eaa83fd87ec", "name": "Boyu Feng", "hidden": false}, {"_id": "692e62d937312eaa83fd87ed", "user": {"_id": "654ce87af0b05673196a9f45", "avatarUrl": "/avatars/7b9c854eb98e487e3057479b1c7860ac.svg", "isPro": false, "fullname": "Shudong Liu", "user": "Sudanl", "type": "user"}, "name": "Shudong Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:24.389Z", "hidden": false}, {"_id": "692e62d937312eaa83fd87ee", "name": "Jiabin Luo", "hidden": false}, {"_id": "692e62d937312eaa83fd87ef", "name": "Weihao Xie", "hidden": false}, {"_id": "692e62d937312eaa83fd87f0", "name": "Zhaohui Wang", "hidden": false}, {"_id": "692e62d937312eaa83fd87f1", "name": "Tianrui Qin", "hidden": false}, {"_id": "692e62d937312eaa83fd87f2", "name": "King Zhu", "hidden": false}, {"_id": "692e62d937312eaa83fd87f3", "name": "Yuqing Wang", "hidden": false}, {"_id": "692e62d937312eaa83fd87f4", "name": "Qianben Chen", "hidden": false}, {"_id": "692e62d937312eaa83fd87f5", "name": "Yuchen Eleanor Jiang", "hidden": false}, {"_id": "692e62d937312eaa83fd87f6", "name": "Wei Wang", "hidden": false}, {"_id": "692e62d937312eaa83fd87f7", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e62d937312eaa83fd87f8", "name": "Wangchunshu Zhou", "hidden": false}], "publishedAt": "2025-12-01T17:58:59.000Z", "submittedOnDailyAt": "2025-12-02T01:25:02.560Z", "title": "How Far Are We from Genuinely Useful Deep Research Agents?", "submittedOnDailyBy": {"_id": "65377c30e48353201e6fdda0", "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg", "isPro": false, "fullname": "Jiaheng Liu", "user": "CheeryLJH", "type": "user"}, "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.", "upvotes": 50, "discussionId": "692e62d937312eaa83fd87f9", "githubRepo": "https://github.com/OPPO-PersonalAI/FINDER_DEFT", "ai_summary": "FINDER is a benchmark for deep research agents with standardized human-curated tasks and DEFT is a failure taxonomy revealing that DRAs struggle with evidence integration, verification, and reasoning-resilient planning.", "ai_keywords": ["Deep Research Agents", "Fine-grained DEepResearch bench", "structured checklist items", "Deep rEsearch Failure Taxonomy", "reasoning", "retrieval", "generation", "failure modes", "reasoning-resilient planning"], "githubStars": 41, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRA\uff09\u65e8\u5728\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\u548c\u7efc\u5408\u81ea\u52a8\u751f\u6210\u5206\u6790\u5e08\u7ea7\u522b\u7684\u62a5\u544a\u3002</li>\n    <li>\u73b0\u6709\u7684DRA\u4e3b\u8981\u5728\u95ee\u7b54\u57fa\u51c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u751f\u6210\u7efc\u5408\u62a5\u544a\u7684\u7814\u7a76\u8f83\u5c11\u3002</li>\n    <li>\u76ee\u524d\u62a5\u544a\u5408\u6210\u7684\u57fa\u51c6\u5b58\u5728\u4efb\u52a1\u590d\u6742\u6027\u548c\u4e3b\u89c2\u6027\u6307\u6807\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002</li>\n    <li>\u63d0\u51fa\u4e86Fine-grained DEepResearch bench (FINDER)\uff0c\u5305\u542b100\u4e2a\u4eba\u5de5\u7b56\u5212\u7684\u7814\u7a76\u4efb\u52a1\u548c419\u4e2a\u7ed3\u6784\u5316\u68c0\u67e5\u9879\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u6df1\u5ea6\u7814\u7a76\u5931\u6548\u5206\u7c7b\u6cd5\uff08DEFT\uff09\uff0c\u8bc6\u522b\u4e8614\u79cd\u5931\u6548\u6a21\u5f0f\uff0c\u6307\u51fa\u5f53\u524dDRA\u5728\u8bc1\u636e\u6574\u5408\u548c\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep Research Agents (DRAs) are designed to create detailed reports automatically by gathering and combining information.</li>\n    <li>Most existing DRAs have only been tested on answering specific questions, not on generating full reports.</li>\n    <li>The current benchmarks for report synthesis are complicated and rely on subjective measures, which do not meet user needs.</li>\n    <li>To improve this, the authors introduce the Fine-grained DEepResearch bench (FINDER), which includes 100 research tasks and 419 checklist items to standardize report quality.</li>\n    <li>They also developed the Deep rEsearch Failure Taxonomy (DEFT), identifying 14 types of failures in DRAs related to reasoning, retrieval, and report generation.</li>\n</ul>"}, "publishedAt": "2025-12-01T12:58:59.000Z", "title": "How Far Are We from Genuinely Useful Deep Research Agents?", "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01948.png", "numComments": 2, "submittedBy": {"_id": "65377c30e48353201e6fdda0", "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg", "fullname": "Jiaheng Liu", "name": "CheeryLJH", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02472", "authors": [{"_id": "692fb2c026742347f61dac94", "user": {"_id": "5feab3a28a3201f8e554c969", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png", "isPro": false, "fullname": "Wenhao Yu", "user": "wyu1", "type": "user"}, "name": "Wenhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:15:34.540Z", "hidden": false}, {"_id": "692fb2c026742347f61dac95", "name": "Zhenwen Liang", "hidden": false}, {"_id": "692fb2c026742347f61dac96", "name": "Chengsong Huang", "hidden": false}, {"_id": "692fb2c026742347f61dac97", "name": "Kishan Panaganti", "hidden": false}, {"_id": "692fb2c026742347f61dac98", "name": "Tianqing Fang", "hidden": false}, {"_id": "692fb2c026742347f61dac99", "name": "Haitao Mi", "hidden": false}, {"_id": "692fb2c026742347f61dac9a", "name": "Dong Yu", "hidden": false}], "publishedAt": "2025-12-02T07:06:11.000Z", "submittedOnDailyAt": "2025-12-03T01:17:53.542Z", "title": "Guided Self-Evolving LLMs with Minimal Human Supervision", "submittedOnDailyBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.", "upvotes": 47, "discussionId": "692fb2c026742347f61dac9b", "ai_summary": "R-Few, a guided Self-Play Challenger-Solver framework, enables stable and controllable model self-evolution with minimal human supervision, achieving performance improvements on math and reasoning benchmarks.", "ai_keywords": ["self-evolution", "superintelligence", "concept drift", "diversity collapse", "mis-evolution", "R-Few", "Self-Play Challenger-Solver", "in-context grounding", "mixed training", "synthetic question generation", "online curriculum", "Qwen3-8B-Base", "R-Zero", "General-Reasoner", "ablation studies", "co-evolutionary dynamics"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>AI\u81ea\u6211\u8fdb\u5316\u88ab\u89c6\u4e3a\u901a\u5411\u8d85\u667a\u80fd\u7684\u9014\u5f84\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u5e38\u51fa\u73b0\u95ee\u9898\u3002</li>\n    <li>\u65e0\u6307\u5bfc\u7684\u81ea\u6211\u8fdb\u5316\u7cfb\u7edf\u5bb9\u6613\u505c\u6ede\u6216\u9000\u5316\uff0c\u56e0\u6982\u5ff5\u6f02\u79fb\u548c\u504f\u89c1\u5f3a\u5316\u7b49\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u4e86R-Few\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4eba\u7c7b\u76d1\u7763\u6765\u7a33\u5b9a\u81ea\u6211\u8fdb\u5316\u8fc7\u7a0b\u3002</li>\n    <li>R-Few\u7ed3\u5408\u4e86\u4eba\u7c7b\u6807\u6ce8\u6837\u672c\u548c\u5408\u6210\u95ee\u9898\u751f\u6210\uff0c\u4fc3\u8fdb\u6a21\u578b\u7684\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>\u5728\u6570\u5b66\u548c\u4e00\u822c\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cR-Few\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u4e86\u5176\u4ed6\u6a21\u578b\uff0c\u4e14\u7a33\u5b9a\u6027\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AI self-evolution aims to create smarter models that learn from their own experiences.</li>\n    <li>Unsupervised self-evolving systems often struggle, facing issues like concept drift and reinforcing biases.</li>\n    <li>The R-Few framework introduces a method with some human guidance to help models learn better.</li>\n    <li>R-Few uses a Challenger to create questions from a few human examples and a Solver to train on these questions, improving performance over time.</li>\n    <li>Tests show R-Few leads to better results in math and reasoning tasks, even with less human data than other models.</li>\n</ul>"}, "publishedAt": "2025-12-02T02:06:11.000Z", "title": "Guided Self-Evolving LLMs with Minimal Human Supervision", "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02472.png", "numComments": 2, "submittedBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "fullname": "Chengsong Huang", "name": "ChengsongHuang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u8f6c\u5316\u4e3a\u53ef\u8fd0\u884c\u7684\u4ee3\u7801\u3002</li>\n    <li>\u8be5\u9886\u57df\u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u53d1\u5c55\u5230\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u6027\u80fd\u63d0\u9ad8\u663e\u8457\uff0c\u6210\u529f\u7387\u4ece\u4e2a\u4f4d\u6570\u63d0\u5347\u5230\u8d85\u8fc795%\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4efd\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u5168\u9762\u6307\u5357\uff0c\u7cfb\u7edf\u5206\u6790\u4ece\u6570\u636e\u6574\u7406\u5230\u540e\u671f\u8bad\u7ec3\u7684\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u7814\u7a76\u4e86\u901a\u7528LLMs\uff08\u5982GPT-4\uff09\u4e0e\u4e13\u6ce8\u4e8e\u4ee3\u7801\u7684LLMs\uff08\u5982StarCoder\uff09\u7684\u80fd\u529b\uff0c\u8ba8\u8bba\u6280\u672f\u3001\u8bbe\u8ba1\u51b3\u7b56\u548c\u6743\u8861\u3002</li>\n    <li>\u5206\u6790\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63a2\u8ba8\u4ee3\u7801\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u4e3b\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are changing how software is developed by turning natural language into code, with tools like GitHub Copilot leading the way.</li>\n    <li>These models have improved dramatically, with success rates reaching over 95% in coding tasks compared to earlier systems.</li>\n    <li>This work provides a detailed guide on how code LLMs work, from data collection to training methods and real-world applications.</li>\n    <li>It compares general-purpose LLMs (like GPT-4) with those specialized in coding (like StarCoder), focusing on their strengths and weaknesses.</li>\n    <li>The study highlights the gap between academic research and practical software development needs, and it explores future research directions to address these challenges.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2511.08892", "authors": [{"_id": "69154dffa1b06ca3cc81351e", "name": "Weihao Tan", "hidden": false}, {"_id": "69154dffa1b06ca3cc81351f", "name": "Xiangyang Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813520", "name": "Yunhao Fang", "hidden": false}, {"_id": "69154dffa1b06ca3cc813521", "name": "Heyuan Yao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813522", "name": "Shi Yan", "hidden": false}, {"_id": "69154dffa1b06ca3cc813523", "name": "Hao Luo", "hidden": false}, {"_id": "69154dffa1b06ca3cc813524", "name": "Tenglong Ao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813525", "name": "Huihui Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813526", "name": "Hongbin Ren", "hidden": false}, {"_id": "69154dffa1b06ca3cc813527", "user": {"_id": "6369d92f64aad59d4d44d362", "avatarUrl": "/avatars/73956400cfbfd53116aefc17b3c9f0fd.svg", "isPro": false, "fullname": "Yi", "user": "Bairen", "type": "user"}, "name": "Bairen Yi", "status": "claimed_verified", "statusLastChangedAt": "2025-11-17T10:31:49.043Z", "hidden": false}, {"_id": "69154dffa1b06ca3cc813528", "name": "Yujia Qin", "hidden": false}, {"_id": "69154dffa1b06ca3cc813529", "name": "Bo An", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352a", "name": "Libin Liu", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352b", "name": "Guang Shi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "publishedAt": "2025-11-12T02:01:26.000Z", "submittedOnDailyAt": "2025-11-13T00:49:21.639Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "upvotes": 185, "discussionId": "69154dffa1b06ca3cc81352c", "projectPage": "https://www.lumine-ai.org/", "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.", "ai_keywords": ["vision-language model", "end-to-end", "3D open-world environments", "human-like interaction", "real-time", "zero-shot cross-game generalization"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Lumine\u662f\u7b2c\u4e00\u4e2a\u5f00\u653e\u7684\u901a\u7528\u667a\u80fd\u4f53\u5f00\u53d1\u914d\u65b9\uff0c\u80fd\u591f\u5728\u590d\u6742\u76843D\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u65f6\u5b8c\u6210\u957f\u8fbe\u6570\u5c0f\u65f6\u7684\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u91c7\u7528\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u7edf\u4e00\u5728\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u3002</li>\n    <li>Lumine\u5728\u300a\u539f\u795e\u300b\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u6548\u7387\u76f8\u5f53\u5730\u5b8c\u6210\u4e94\u5c0f\u65f6\u7684\u4e3b\u7ebf\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u5404\u79cd\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u4e0d\u4ec5\u5728\u7279\u5b9a\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u591f\u5728\u6ca1\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5176\u4ed6\u6e38\u620f\u4e2d\u5b8c\u6210\u957f\u8fbe100\u5206\u949f\u7684\u4efb\u52a1\u3002</li>\n    <li>Lumine\u7684\u6210\u529f\u5c55\u793a\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5f00\u53d1\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Lumine is a new open recipe for creating generalist agents that can handle long, complex tasks in 3D open-world games.</li>\n    <li>It uses a human-like approach to combine sensing, thinking, and acting, using a vision-language model.</li>\n    <li>Lumine processes visual information quickly and efficiently to perform actions based on natural language commands.</li>\n    <li>It can complete the five-hour main storyline of Genshin Impact at a level similar to human players and follow various instructions across different tasks.</li>\n    <li>Lumine shows strong performance in other games without needing extra training, successfully completing missions in Wuthering Waves and Honkai: Star Rail.</li>\n</ul>"}, "publishedAt": "2025-11-11T21:01:26.000Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08892.png", "numComments": 12, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2 \u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\u548c\u4ee3\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b (DSA)\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u7a33\u5065\u7684\u5f3a\u5316\u5b66\u4e60\u534f\u8bae\uff0cDeepSeek-V3.2 \u7684\u8868\u73b0\u4e0e GPT-5 \u76f8\u5f53\uff0c\u5176\u4e2d\u9ad8\u8ba1\u7b97\u7248\u672c\u8d85\u8d8a\u4e86 GPT-5\uff0c\u5e76\u5728\u63a8\u7406\u4e0a\u4e0e Gemini-3.0-Pro \u65d7\u9f13\u76f8\u5f53\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u5730\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u6267\u884c\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that balances fast performance with strong reasoning abilities.</li>\n    <li>It features a new attention method called DeepSeek Sparse Attention (DSA), which makes it more efficient, especially with long texts.</li>\n    <li>The model uses a strong reinforcement learning system that allows it to compete with top models like GPT-5 and even outperform it in some areas.</li>\n    <li>DeepSeek-V3.2 has achieved high performance in prestigious competitions like the International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a pipeline that creates large amounts of training data to improve reasoning and following instructions in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u76ee\u524d\uff0c\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u4e00\u4e9b\u9886\u5148\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\u53c2\u6570\u8f83\u5927\uff0820B\u523080B\uff09\uff0c\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u4e0d\u6613\u5b9e\u73b0\u63a8\u7406\u548c\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff08S3-DiT\uff09\u3002</li>\n    <li>Z-Image\u5728\u4f01\u4e1a\u7ea7H800 GPU\u4e0a\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u517c\u5bb9\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\uff08<16GB VRAM\uff09\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u6216\u8d85\u8fc7\u8bb8\u591a\u9886\u5148\u7ade\u4e89\u5bf9\u624b\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Many high-performance image generation models are proprietary, like Nano Banana Pro and Seedream 4.0.</li>\n    <li>Leading open-source models are large (20B to 80B parameters), making them hard to use on regular hardware.</li>\n    <li>We introduce Z-Image, a more efficient 6B-parameter model using a new architecture that focuses on smart design over sheer size.</li>\n    <li>Z-Image allows fast image generation on both high-end and consumer-grade hardware and includes an editing model called Z-Image-Edit.</li>\n    <li>Our model matches or exceeds the performance of leading competitors while being more affordable and accessible, and we are sharing our resources for others to use.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.11793", "authors": [{"_id": "691be81b6bfd5965c0fd37e2", "name": "MiroMind Team", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e3", "name": "Song Bai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e4", "name": "Lidong Bing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e5", "name": "Carson Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e6", "name": "Guanzheng Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e7", "user": {"_id": "632dab84fdb35759ea6646a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632dab84fdb35759ea6646a0/IxO5mbtzHJsr0YHW-YtVk.jpeg", "isPro": false, "fullname": "Yuntao Chen", "user": "YuntaoChen", "type": "user"}, "name": "Yuntao Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:45.403Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e8", "name": "Zhe Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e9", "name": "Ziyi Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ea", "name": "Jifeng Dai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37eb", "name": "Xuan Dong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ec", "name": "Yue Deng", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ed", "name": "Yunjie Fu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ee", "name": "Junqi Ge", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ef", "name": "Chenxia Han", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f0", "name": "Tammy Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f1", "name": "Zhenhang Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f2", "name": "Jerry Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f3", "name": "Shilei Jiang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f4", "name": "Tianyu Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f5", "user": {"_id": "64be2455b567ae97c34bb948", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be2455b567ae97c34bb948/QuCdStDDGaXjDmp4V-dBj.jpeg", "isPro": false, "fullname": "Xiaoqi Jian", "user": "mx1024", "type": "user"}, "name": "Xiaoqi Jian", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:52.417Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f6", "name": "Lei Lei", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f7", "user": {"_id": "6466e7be1343dce20e59191b", "avatarUrl": "/avatars/6779560b203c3773dc76372c0b8cbe4e.svg", "isPro": false, "fullname": "Li Ruilin", "user": "Eric-LRL-130", "type": "user"}, "name": "Ruilin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:43.774Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f8", "name": "Ryan Luo", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f9", "name": "Tiantong Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fa", "name": "Xiang Lin", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fb", "name": "Ziyuan Liu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fc", "name": "Zhiqi Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fd", "name": "Jie Ni", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fe", "name": "Qiang Ren", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ff", "name": "Pax Sun", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3800", "name": "Shiqian Su", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3801", "name": "Chenxin Tao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3802", "name": "Bin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3803", "name": "Hellen Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3804", "name": "Haonan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3805", "name": "James Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3806", "name": "Jin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3807", "name": "Jojo Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3808", "name": "Letian Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3809", "name": "Shizun Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380a", "user": {"_id": "63d34004b734eaa4d4faeccf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg", "isPro": false, "fullname": "Weizhi Wang", "user": "weizhiwang", "type": "user"}, "name": "Weizhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:47.000Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380b", "name": "Zixuan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380c", "name": "Jinfan Xu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380d", "name": "Sen Xing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380e", "user": {"_id": "637f347a52229c639211bee8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg", "isPro": false, "fullname": "Chenyu Yang", "user": "cyyang822", "type": "user"}, "name": "Chenyu Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:48.746Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380f", "user": {"_id": "6239888e7fef05b7bdd5fcff", "avatarUrl": "/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg", "isPro": false, "fullname": "Hai Ye", "user": "oceanpty", "type": "user"}, "name": "Hai Ye", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:50.623Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3810", "name": "Jiaheng Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3811", "name": "Yue Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3812", "name": "Muyan Zhong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3813", "name": "Tianchen Zhao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3814", "name": "Xizhou Zhu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3815", "name": "Yanpeng Zhou", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3816", "name": "Yifan Zhang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3817", "name": "Zhi Zhu", "hidden": false}], "publishedAt": "2025-11-14T18:52:07.000Z", "submittedOnDailyAt": "2025-11-18T02:00:07.077Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "upvotes": 153, "discussionId": "691be81b6bfd5965c0fd3818", "projectPage": "https://dr.miromind.ai/", "githubRepo": "https://github.com/MiroMindAI/MiroThinker", "githubStars": 1133, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MiroThinker v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u548c\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002</li>\n    <li>MiroThinker\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u652f\u6301\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u9002\u5e94\u590d\u6742\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>MiroThinker\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u4ea4\u4e92\u6df1\u5ea6\u7684\u6269\u5c55\u4e0e\u6a21\u578b\u7684\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u7279\u6027\uff0c\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u91cd\u8981\u7ef4\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MiroThinker v1.0 is an open-source research agent aimed at improving reasoning and information-seeking skills.</li>\n    <li>It focuses on enhancing how the model interacts with its environment, rather than just increasing model size or context length.</li>\n    <li>The model uses reinforcement learning to effectively manage many tool calls (up to 600) during tasks, supporting complex research activities.</li>\n    <li>MiroThinker outperforms previous open-source agents in various benchmarks, showing accuracy rates that approach those of advanced commercial models.</li>\n    <li>The research highlights that better performance comes from deeper and more frequent interactions with the environment, suggesting that this interaction is crucial for future research agent development.</li>\n</ul>"}, "publishedAt": "2025-11-14T13:52:07.000Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11793.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u7684\u591a\u6a21\u6001\u601d\u7ef4\u5de5\u5177\uff0c\u5e2e\u52a9\u66f4\u597d\u5730\u7406\u89e3\u957f\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5229\u7528\u6a21\u578b\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u805a\u7126\u4e8e\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\uff0c\u4ee5\u83b7\u53d6\u66f4\u7cbe\u7ec6\u7684\u89c6\u9891\u5e27\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002</li>\n    <li>LongVT\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) are good at video reasoning but can make mistakes, especially with long videos.</li>\n    <li>LongVT is a new framework that helps LMMs better understand long videos by focusing on specific clips after reviewing the whole video.</li>\n    <li>It uses a method that combines different types of thinking to analyze video content more effectively.</li>\n    <li>A new dataset called VideoSIAH will be released to help train and evaluate these models, including a large number of question-answering samples.</li>\n    <li>LongVT has been tested and shows better results than existing models in understanding and reasoning about long videos.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u7528\u4e8e\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u6e05\u3001\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u6d41\u6c34\u7ebf\u53bb\u566a\u6b65\u9aa4\uff0c\u6709\u6548\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5a92\u4f53\u751f\u6210\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6dc0\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u9996\u6b21\u5728\u6b64\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new framework for creating high-quality, real-time avatar videos using a powerful 14-billion-parameter diffusion model.</li>\n    <li>It uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up the video generation process by utilizing multiple GPUs.</li>\n    <li>The Rolling Sink Frame Mechanism (RSFM) improves the consistency of the avatars by adjusting their appearance based on a stored reference image.</li>\n    <li>Live Avatar can generate videos at 20 frames per second on 5 GPUs, making it one of the fastest and most effective systems for real-time avatar creation.</li>\n    <li>This work sets a new standard for using advanced diffusion models in long video synthesis for various applications.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 08, 2025";
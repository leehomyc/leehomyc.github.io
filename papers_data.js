window.trendingPapers = {
    "today": [{"paper": {"id": "2601.02151", "authors": [{"_id": "695f2d8a5fa3847525c41f8d", "user": {"_id": "6768c97367e4b4606a3c9cec", "avatarUrl": "/avatars/5ddafe7a05828366f66c79072556f370.svg", "isPro": false, "fullname": "diaomuxi", "user": "diaomuxi", "type": "user"}, "name": "Muxi Diao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:56.507Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8e", "user": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "name": "Lele Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:51.246Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8f", "user": {"_id": "64c0f972d76592ba899c2c9c", "avatarUrl": "/avatars/d6940beb135f99241c6fb2cf0e8ccdbe.svg", "isPro": false, "fullname": "GongWuxuan", "user": "Wuxuan-Gong", "type": "user"}, "name": "Wuxuan Gong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:19.470Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f90", "name": "Yutong Zhang", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f91", "user": {"_id": "64fbd4e69a62bb2791b3a665", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fbd4e69a62bb2791b3a665/ZEMtU8O0z98ryeRCG3l_K.jpeg", "isPro": false, "fullname": "Zhonghao Yan", "user": "zzzyzh", "type": "user"}, "name": "Zhonghao Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:53.904Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f92", "name": "Yufei Han", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f93", "user": {"_id": "67f4a56928cbc4f2f75c008d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qoX8HT0JsjqW2OQoENSvg.png", "isPro": false, "fullname": "Kongming Liang", "user": "KongmingLiang", "type": "user"}, "name": "Kongming Liang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:38.812Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f94", "name": "Weiran Xu", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f95", "name": "Zhanyu Ma", "hidden": false}], "publishedAt": "2026-01-05T14:28:17.000Z", "submittedOnDailyAt": "2026-01-08T01:42:04.476Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "submittedOnDailyBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "upvotes": 64, "discussionId": "695f2d8b5fa3847525c41f96", "projectPage": "https://ymxyll.github.io/EAFT/", "githubRepo": "https://github.com/PRIS-CV/EAFT", "githubRepoAddedBy": "user", "ai_summary": "Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.", "ai_keywords": ["supervised fine-tuning", "catastrophic forgetting", "on-policy reinforcement learning", "distributional gap", "Confident Conflicts", "token-level entropy", "epistemic uncertainty", "knowledge conflict", "gradient updates", "downstream performance"], "githubStars": 18, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "summary_zh": "<ul>\n    <li>\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5e38\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5219\u80fd\u4fdd\u6301\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0cSFT\u548cRL\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u524d\u8005\u5f3a\u8feb\u6a21\u578b\u9002\u5e94\u5916\u90e8\u76d1\u7763\u3002</li>\n    <li>\u5728SFT\u4e2d\uff0c\u6a21\u578b\u53ef\u80fd\u4f1a\u51fa\u73b0\u201c\u81ea\u4fe1\u51b2\u7a81\u201d\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5bf9\u81ea\u8eab\u9884\u6d4b\u975e\u5e38\u81ea\u4fe1\uff0c\u4f46\u5b66\u4e60\u5230\u7684\u771f\u5b9e\u60c5\u51b5\u5374\u76f8\u53cd\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u71b5\u81ea\u9002\u5e94\u5fae\u8c03\uff08EAFT\uff09\uff0c\u5b83\u5229\u7528\u6807\u8bb0\u71b5\u6765\u533a\u5206\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\u3002</li>\n    <li>\u5728\u591a\u79cd\u9886\u57df\uff08\u6570\u5b66\u3001\u533b\u5b66\u3001\u667a\u80fd\u4f53\u7b49\uff09\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEAFT\u5728\u4fdd\u6301\u4e00\u822c\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u6807\u51c6SFT\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Supervised Fine-Tuning (SFT) can cause \"catastrophic forgetting\" when adapting models to new domains.</li>\n    <li>On-policy Reinforcement Learning (RL) helps maintain a model's general abilities without forgetting.</li>\n    <li>The problem arises because SFT makes models fit external instructions, while RL aligns with the model's own beliefs.</li>\n    <li>Conflicts occur when models are confident in their predictions but are forced to learn different truths, leading to harmful updates.</li>\n    <li>Entropy-Adaptive Fine-Tuning (EAFT) helps by using uncertainty measures to manage learning from conflicting information, leading to better performance without losing general skills.</li>\n</ul>"}, "publishedAt": "2026-01-05T09:28:17.000Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02151.png", "numComments": 6, "submittedBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "fullname": "\u6768\u4e50\u4e50", "name": "ssl-asuka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03509", "authors": [{"_id": "695fbc7d5b7998385e639349", "name": "Haochen Shi", "hidden": false}, {"_id": "695fbc7d5b7998385e63934a", "name": "Xingdi Yuan", "hidden": false}, {"_id": "695fbc7d5b7998385e63934b", "name": "Bang Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "publishedAt": "2026-01-07T01:43:25.000Z", "submittedOnDailyAt": "2026-01-08T11:50:05.687Z", "title": "Evolving Programmatic Skill Networks", "submittedOnDailyBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "isPro": false, "fullname": "Bang Liu", "user": "Bang-UdeM-Mila", "type": "user"}, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "upvotes": 54, "discussionId": "695fbc7e5b7998385e63934c", "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.", "ai_keywords": ["Programmatic Skill Network", "executable symbolic programs", "skill composition", "structured fault localization", "progressive optimization", "maturity-aware update gating", "canonical structural refactoring", "rollback validation", "neural network training", "skill reuse", "rapid adaptation", "generalization"], "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u5728\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u6301\u7eed\u5b66\u4e60\u6280\u80fd\uff0c\u4ee3\u7406\u9700\u8981\u6784\u5efa\u548c\u4f18\u5316\u4e00\u4e2a\u4e0d\u65ad\u6269\u5c55\u7684\u53ef\u6267\u884c\u6280\u80fd\u5e93\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u7a0b\u5e8f\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u53ef\u6267\u884c\u7b26\u53f7\u7a0b\u5e8f\u7ec4\u6210\u7684\u7f51\u7edc\uff0c\u968f\u7740\u7ecf\u9a8c\u4e0d\u65ad\u6f14\u5316\u3002</li>\n    <li>PSN\u5305\u542b\u4e09\u79cd\u6838\u5fc3\u673a\u5236\uff1a\u6280\u80fd\u6545\u969c\u5b9a\u4f4d\u3001\u6210\u719f\u5ea6\u611f\u77e5\u7684\u4f18\u5316\uff0c\u4ee5\u53ca\u5728\u56de\u6eda\u9a8c\u8bc1\u4e0b\u7684\u7ed3\u6784\u91cd\u6784\u3002</li>\n    <li>PSN\u7684\u5b66\u4e60\u52a8\u6001\u4e0e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6709\u7ed3\u6784\u4e0a\u7684\u76f8\u4f3c\u4e4b\u5904\u3002</li>\n    <li>\u5728MineDojo\u548cCrafter\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPSN\u80fd\u591f\u5b9e\u73b0\u5f3a\u5927\u7684\u6280\u80fd\u91cd\u7528\u3001\u5feb\u901f\u9002\u5e94\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on teaching agents to learn and improve skills in changing environments.</li>\n    <li>A new system called the Programmatic Skill Network (PSN) helps agents create and refine skills as they gain experience.</li>\n    <li>PSN uses three main methods: finding and fixing skill issues, updating skills while keeping reliable ones stable, and reorganizing skills to stay efficient.</li>\n    <li>Experiments show that PSN allows agents to reuse skills effectively, adapt quickly, and perform well on a variety of tasks.</li>\n    <li>The researchers plan to share their code with the public for further use.</li>\n</ul>"}, "publishedAt": "2026-01-06T20:43:25.000Z", "title": "Evolving Programmatic Skill Networks", "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png", "numComments": 1, "submittedBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "fullname": "Bang Liu", "name": "Bang-UdeM-Mila", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03872", "authors": [{"_id": "695f1a475fa3847525c41d06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:32:36.055Z", "hidden": false}, {"_id": "695f1a475fa3847525c41d07", "name": "Guocheng Zhai", "hidden": false}, {"_id": "695f1a475fa3847525c41d08", "name": "Ruihan Jin", "hidden": false}, {"_id": "695f1a475fa3847525c41d09", "name": "Jiahao Yuan", "hidden": false}, {"_id": "695f1a475fa3847525c41d0a", "name": "Yuhao Shen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0b", "name": "Shuai Zhang", "hidden": false}, {"_id": "695f1a475fa3847525c41d0c", "name": "Zhengqi Wen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0d", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-07T12:38:33.000Z", "submittedOnDailyAt": "2026-01-08T04:50:03.287Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "upvotes": 30, "discussionId": "695f1a475fa3847525c41d0e", "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.", "ai_keywords": ["large language models", "external tools", "model-tool combination", "high-dimensional optimization", "dual-path framework", "training-free cluster-based routing", "RL-based multi-step routing", "cross-domain complex reasoning", "domain-specific alignment", "out-of-distribution generalization"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u9009\u62e9\u6700\u4f73\u7684\u6a21\u578b-\u5de5\u5177\u7ec4\u5408\u53d8\u6210\u4e86\u4e00\u4e2a\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\u3002</li>\n    <li>ATLAS\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u65b9\u6cd5\u5b9e\u73b0\u52a8\u6001\u5de5\u5177\u4f7f\u7528\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cATLAS\u5728\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548c\u8def\u7531\u65b9\u6cd5\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ATLAS is a new framework that helps combine large language models (LLMs) with external tools for better performance.</li>\n    <li>It addresses the challenge of choosing the best model-tool pair by using two main methods: a cluster-based approach and a reinforcement learning approach.</li>\n    <li>ATLAS has been tested on 15 different benchmarks and performs better than models like GPT-4o and other existing methods.</li>\n    <li>It shows improvements in both standard tasks and more complex reasoning tasks that go beyond the usual data.</li>\n    <li>The framework also enhances visual reasoning by effectively using specialized tools that work with different types of data.</li>\n</ul>"}, "publishedAt": "2026-01-07T07:38:33.000Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03986", "authors": [{"_id": "695f290d5fa3847525c41d7d", "name": "Qi Qian", "hidden": false}, {"_id": "695f290d5fa3847525c41d7e", "user": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "name": "Chengsong Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:58.524Z", "hidden": false}, {"_id": "695f290d5fa3847525c41d7f", "name": "Jingwen Xu", "hidden": false}, {"_id": "695f290d5fa3847525c41d80", "name": "Changze Lv", "hidden": false}, {"_id": "695f290d5fa3847525c41d81", "name": "Muling Wu", "hidden": false}, {"_id": "695f290d5fa3847525c41d82", "name": "Wenhao Liu", "hidden": false}, {"_id": "695f290d5fa3847525c41d83", "name": "Xiaohua Wang", "hidden": false}, {"_id": "695f290d5fa3847525c41d84", "name": "Zhenghua Wang", "hidden": false}, {"_id": "695f290d5fa3847525c41d85", "name": "Zisu Huang", "hidden": false}, {"_id": "695f290d5fa3847525c41d86", "name": "Muzhao Tian", "hidden": false}, {"_id": "695f290d5fa3847525c41d87", "name": "Jianhan Xu", "hidden": false}, {"_id": "695f290d5fa3847525c41d88", "name": "Kun Hu", "hidden": false}, {"_id": "695f290d5fa3847525c41d89", "name": "He-Da Wang", "hidden": false}, {"_id": "695f290d5fa3847525c41d8a", "name": "Yao Hu", "hidden": false}, {"_id": "695f290d5fa3847525c41d8b", "name": "Xuanjing Huang", "hidden": false}, {"_id": "695f290d5fa3847525c41d8c", "name": "Xiaoqing Zheng", "hidden": false}], "publishedAt": "2026-01-07T14:59:03.000Z", "submittedOnDailyAt": "2026-01-08T01:59:31.547Z", "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks", "submittedOnDailyBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.", "upvotes": 28, "discussionId": "695f290d5fa3847525c41d8d", "ai_summary": "Researchers developed Benchmark^2, a framework with three metrics to evaluate benchmark quality for large language models, revealing significant variations in existing benchmarks and enabling more efficient evaluation through selective benchmark construction.", "ai_keywords": ["Benchmark^2", "cross-benchmark ranking consistency", "discriminability score", "capability alignment deviation", "large language models", "benchmarks", "model rankings", "evaluation performance", "test sets"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u79f0\u4e3aBenchmark^2\u3002</li>\n    <li>\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6307\u6807\uff1a\u57fa\u51c6\u6392\u540d\u4e00\u81f4\u6027\u3001\u53ef\u533a\u5206\u6027\u5f97\u5206\u548c\u80fd\u529b\u5bf9\u9f50\u504f\u5dee\u3002</li>\n    <li>\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6db5\u76d615\u4e2a\u57fa\u51c6\u548c11\u4e2a\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u73b0\u6709\u57fa\u51c6\u7684\u8d28\u91cf\u5dee\u5f02\u663e\u8457\u3002</li>\n    <li>\u6839\u636e\u8fd9\u4e9b\u6307\u6807\u9009\u62e9\u57fa\u51c6\u53ef\u4ee5\u5728\u51cf\u5c11\u6d4b\u8bd5\u96c6\u89c4\u6a21\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u76f8\u4f3c\u7684\u8bc4\u4f30\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There are many benchmarks for testing large language models (LLMs), and we need better ways to evaluate their quality.</li>\n    <li>We introduce Benchmark^2, a framework with three key metrics to assess benchmark quality.</li>\n    <li>The three metrics are: consistency of model rankings across benchmarks, the ability to distinguish between models, and identifying cases where stronger models fail.</li>\n    <li>We tested 11 LLMs across 15 different benchmarks in areas like math, reasoning, and knowledge.</li>\n    <li>Our findings show that benchmark quality can vary widely, and using our metrics can help create effective benchmarks with fewer tests.</li>\n</ul>"}, "publishedAt": "2026-01-07T09:59:03.000Z", "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks", "summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03986.png", "numComments": 2, "submittedBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "fullname": "Chengsong Huang", "name": "ChengsongHuang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04151", "authors": [{"_id": "695f21355fa3847525c41d37", "name": "Jun Wang", "hidden": false}, {"_id": "695f21355fa3847525c41d38", "name": "Chunyu Qiang", "hidden": false}, {"_id": "695f21355fa3847525c41d39", "name": "Yuxin Guo", "hidden": false}, {"_id": "695f21355fa3847525c41d3a", "name": "Yiran Wang", "hidden": false}, {"_id": "695f21355fa3847525c41d3b", "name": "Xijuan Zeng", "hidden": false}, {"_id": "695f21355fa3847525c41d3c", "name": "Chen Zhang", "hidden": false}, {"_id": "695f21355fa3847525c41d3d", "name": "Pengfei Wan", "hidden": false}], "publishedAt": "2026-01-07T18:03:45.000Z", "submittedOnDailyAt": "2026-01-08T01:07:20.425Z", "title": "Klear: Unified Multi-Task Audio-Video Joint Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.", "upvotes": 9, "discussionId": "695f21365fa3847525c41d3e", "ai_summary": "Klear addresses audio-video joint generation challenges through a unified model architecture, progressive multitask training, and large-scale dense-caption data construction, achieving superior alignment and generalization.", "ai_keywords": ["audio-visual correspondence modeling", "DiT blocks", "Omni-Full Attention mechanism", "random modality masking", "multistage curriculum", "unimodal collapse", "dense-caption data", "automated data-construction pipeline", "audio-video joint generation", "instruction-following generation"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u97f3\u89c6\u9891\u8054\u5408\u751f\u6210\u6280\u672f\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u5f88\u591a\u6311\u6218\uff0c\u5982\u97f3\u89c6\u9891\u4e0d\u540c\u6b65\u548c\u53e3\u578b\u4e0e\u8bed\u8a00\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Klear\uff0c\u4ece\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u6574\u7406\u4e09\u4e2a\u65b9\u9762\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u6a21\u578b\u91c7\u7528\u5355\u5854\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u7edf\u4e00\u7684DiT\u6a21\u5757\u548c\u5168\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u97f3\u89c6\u9891\u5bf9\u9f50\u548c\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>\u8bad\u7ec3\u7b56\u7565\u91c7\u7528\u6e10\u8fdb\u5f0f\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u901a\u8fc7\u968f\u673a\u6a21\u6001\u906e\u7f69\u548c\u591a\u9636\u6bb5\u8bfe\u7a0b\u4f18\u5316\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u97f3\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u7684\u5bc6\u96c6\u5b57\u5e55\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Audio-video generation has improved but still faces issues like timing mismatches and poor lip synchronization.</li>\n    <li>Klear addresses these problems through better model design, training methods, and data quality.</li>\n    <li>The model uses a single-tower design for better audio-visual alignment and is scalable.</li>\n    <li>Training involves multitasking and a structured curriculum to improve performance and prevent issues with individual modalities.</li>\n    <li>Klear introduces a large, high-quality dataset with precise audio-video-caption pairs, leading to improved generation results that outperform previous methods.</li>\n</ul>"}, "publishedAt": "2026-01-07T13:03:45.000Z", "title": "Klear: Unified Multi-Task Audio-Video Joint Generation", "summary": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04151.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 203, "isUserFollowing": false}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.04194", "authors": [{"_id": "695f1a2a5fa3847525c41cf3", "name": "Yanzhe Lyu", "hidden": false}, {"_id": "695f1a2a5fa3847525c41cf4", "name": "Chen Geng", "hidden": false}, {"_id": "695f1a2a5fa3847525c41cf5", "name": "Karthik Dharmarajan", "hidden": false}, {"_id": "695f1a2a5fa3847525c41cf6", "name": "Yunzhi Zhang", "hidden": false}, {"_id": "695f1a2a5fa3847525c41cf7", "name": "Hadi Alzayer", "hidden": false}, {"_id": "695f1a2a5fa3847525c41cf8", "name": "Shangzhe Wu", "hidden": false}, {"_id": "695f1a2a5fa3847525c41cf9", "name": "Jiajun Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/e4m-6HYTsPup5SkMsApV4.mp4"], "publishedAt": "2026-01-07T18:59:40.000Z", "submittedOnDailyAt": "2026-01-08T00:15:39.806Z", "title": "Choreographing a World of Dynamic Objects", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord", "upvotes": 7, "discussionId": "695f1a2a5fa3847525c41cfa", "projectPage": "https://yanzhelyu.github.io/chord/", "ai_summary": "CHORD is a universal generative framework that extracts Lagrangian motion information from Eulerian video representations to synthesize diverse 4D dynamic scenes without requiring category-specific rules or large datasets.", "ai_keywords": ["video generative models", "Lagrangian motion", "Eulerian representations", "distillation-based pipeline", "4D dynamic scenes", "multi-body dynamics", "robotics manipulation policies"], "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u751f\u6210\u7ba1\u9053\uff0c\u79f0\u4e3aCHORD\uff0c\u7528\u4e8e\u5408\u6210\u52a8\u6001\u7269\u4f53\u548c\u573a\u666f\u3002</li>\n    <li>\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u89c4\u5219\uff0c\u52b3\u52a8\u5f3a\u5ea6\u5927\u4e14\u96be\u4ee5\u6269\u5c55\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u5f0f\u4f7f\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u63d0\u53d62D\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u4fe1\u606f\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u7528\u3001\u7075\u6d3b\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7269\u4f53\u7c7b\u522b\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u751f\u6210\u591a\u79cd4D\u52a8\u6001\u7684\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces a new system called CHORD for creating dynamic 4D scenes with moving objects.</li>\n    <li>Traditional methods for creating these scenes are complicated and not easy to scale.</li>\n    <li>Learning-based methods need large datasets that may not include all types of objects.</li>\n    <li>CHORD uses a unique approach to extract motion information from 2D videos, making it flexible and applicable to many object types.</li>\n    <li>The effectiveness of CHORD is shown through experiments, including its use in robotics manipulation.</li>\n</ul>"}, "publishedAt": "2026-01-07T13:59:40.000Z", "title": "Choreographing a World of Dynamic Objects", "summary": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/e4m-6HYTsPup5SkMsApV4.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04194.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 203, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.04171", "authors": [{"_id": "695f21255fa3847525c41d31", "name": "Mohit Raghavendra", "hidden": false}, {"_id": "695f21255fa3847525c41d32", "name": "Anisha Gunjal", "hidden": false}, {"_id": "695f21255fa3847525c41d33", "name": "Bing Liu", "hidden": false}, {"_id": "695f21255fa3847525c41d34", "name": "Yunzhong He", "hidden": false}], "publishedAt": "2026-01-07T18:38:23.000Z", "submittedOnDailyAt": "2026-01-08T01:09:10.323Z", "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.", "upvotes": 6, "discussionId": "695f21255fa3847525c41d35", "ai_summary": "Agentic Rubrics enable efficient and scalable verification for software engineering agents by creating context-aware checklists that outperform traditional methods while maintaining interpretability.", "ai_keywords": ["Reinforcement Learning", "Test-Time Scaling", "software engineering agents", "patch classifiers", "heuristic methods", "agentic context gathering", "codebase-specific criteria"], "organization": {"_id": "6677220f8a4064c02bc81217", "name": "ScaleAI", "fullname": "Scale AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d6a5f94c28026a003581b4/uqHyTuNQ8fX7LheVhzPeO.png"}, "summary_zh": "<ul>\n    <li>\u9a8c\u8bc1\u5bf9\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u975e\u5e38\u91cd\u8981\uff0c\u53ef\u4ee5\u63d0\u4f9b\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u4fe1\u53f7\u3002</li>\n    <li>\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u9a8c\u8bc1\u901a\u5e38\u4f9d\u8d56\u4ee3\u7801\u6267\u884c\uff0c\u4f46\u8fd9\u5728\u73af\u5883\u8bbe\u7f6e\u4e0a\u96be\u4ee5\u6269\u5c55\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u201c\u667a\u80fd\u8bc4\u5206\u6807\u51c6\u201d\uff0c\u901a\u8fc7\u4e13\u5bb6\u667a\u80fd\u4f53\u4e0e\u4ee3\u7801\u5e93\u4e92\u52a8\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8bc4\u5206\u6e05\u5355\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e0d\u6267\u884c\u6d4b\u8bd5\u7684\u60c5\u51b5\u4e0b\u5bf9\u5019\u9009\u8865\u4e01\u8fdb\u884c\u8bc4\u5206\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u667a\u80fd\u8bc4\u5206\u6807\u51c6\u80fd\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7ec6\u81f4\u7684\u9a8c\u8bc1\u4fe1\u53f7\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Verification helps improve software agents by providing rewards for learning and enhancing performance during use.</li>\n    <li>Traditional verification methods often require running code, which can be hard to manage and scale.</li>\n    <li>Agentic Rubrics are a new method where an expert agent creates a checklist based on the code context, allowing for scoring without running tests.</li>\n    <li>This method showed improved scores in tests, outperforming existing methods by at least 3.5 percentage points.</li>\n    <li>Agentic Rubrics effectively identify issues that standard tests might miss, making them a valuable tool for software engineering verification.</li>\n</ul>"}, "publishedAt": "2026-01-07T13:38:23.000Z", "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents", "summary": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04171.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 203, "isUserFollowing": false}, "organization": {"_id": "6677220f8a4064c02bc81217", "name": "ScaleAI", "fullname": "Scale AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65d6a5f94c28026a003581b4/uqHyTuNQ8fX7LheVhzPeO.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02075", "authors": [{"_id": "695d04eac03d6d81e4399cd9", "user": {"_id": "65d7018c90f11951bcfdf2f5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d7018c90f11951bcfdf2f5/RU0-n9k3RIdtv8xvg9Upe.jpeg", "isPro": false, "fullname": "Zhuofan Shi", "user": "FredericFan", "type": "user"}, "name": "Zhuofan Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:26:56.695Z", "hidden": false}, {"_id": "695d04eac03d6d81e4399cda", "name": "Hubao A", "hidden": false}, {"_id": "695d04eac03d6d81e4399cdb", "name": "Yufei Shao", "hidden": false}, {"_id": "695d04eac03d6d81e4399cdc", "name": "Mengyan Dai", "hidden": false}, {"_id": "695d04eac03d6d81e4399cdd", "name": "Yadong Yu", "hidden": false}, {"_id": "695d04eac03d6d81e4399cde", "name": "Pan Xiang", "hidden": false}, {"_id": "695d04eac03d6d81e4399cdf", "name": "Dongliang Huang", "hidden": false}, {"_id": "695d04eac03d6d81e4399ce0", "name": "Hongxu An", "hidden": false}, {"_id": "695d04eac03d6d81e4399ce1", "name": "Chunxiao Xin", "hidden": false}, {"_id": "695d04eac03d6d81e4399ce2", "name": "Haiyang Shen", "hidden": false}, {"_id": "695d04eac03d6d81e4399ce3", "name": "Zhenyu Wang", "hidden": false}, {"_id": "695d04eac03d6d81e4399ce4", "name": "Yunshan Na", "hidden": false}, {"_id": "695d04eac03d6d81e4399ce5", "name": "Gang Huang", "hidden": false}, {"_id": "695d04eac03d6d81e4399ce6", "name": "Xiang Jing", "hidden": false}], "publishedAt": "2026-01-05T12:56:51.000Z", "submittedOnDailyAt": "2026-01-08T00:16:06.121Z", "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics", "submittedOnDailyBy": {"_id": "65d7018c90f11951bcfdf2f5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d7018c90f11951bcfdf2f5/RU0-n9k3RIdtv8xvg9Upe.jpeg", "isPro": false, "fullname": "Zhuofan Shi", "user": "FredericFan", "type": "user"}, "summary": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2", "upvotes": 6, "discussionId": "695d04ebc03d6d81e4399ce7", "ai_summary": "MDAgent2 enables automated molecular dynamics code generation and question answering through domain-adapted language models and a multi-agent runtime system.", "ai_keywords": ["LAMMPS", "molecular dynamics", "code generation", "question answering", "continued pre-training", "supervised fine-tuning", "reinforcement learning", "MD-Instruct", "MD-Code", "MD-GRPO", "MDAgent2", "MD-EvalBench"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\u5bf9\u4e8e\u7406\u89e3\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u539f\u5b50\u7ea7\u884c\u4e3a\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u7f16\u5199LAMMPS\u811a\u672c\u662f\u4e00\u9879\u4e13\u4e1a\u800c\u8017\u65f6\u7684\u4efb\u52a1\u3002</li>\n    <li>MDAgent2\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5728MD\u9886\u57df\u5185\u540c\u65f6\u8fdb\u884c\u77e5\u8bc6\u95ee\u7b54\u548c\u4ee3\u7801\u751f\u6210\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u4e1a\u7684\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u751f\u6210\u4e86\u5305\u542bMD\u77e5\u8bc6\u3001\u95ee\u7b54\u548c\u4ee3\u7801\u751f\u6210\u7684\u4e09\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002</li>\n    <li>\u91c7\u7528\u4e09\u9636\u6bb5\u7684\u540e\u671f\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6765\u8bad\u7ec3\u4e24\u4e2a\u9886\u57df\u9002\u5e94\u6a21\u578bMD-Instruct\u548cMD-Code\u3002</li>\n    <li>MDAgent2\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u90e8\u7f72\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u4ee3\u7801\u751f\u6210\u3001\u6267\u884c\u3001\u8bc4\u4f30\u548c\u81ea\u6211\u7ea0\u6b63\u529f\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Molecular dynamics (MD) simulations help understand materials at the atomic level, but writing the necessary code can be difficult and time-consuming.</li>\n    <li>The new MDAgent2 framework improves on the previous version by being able to answer questions and generate code for MD tasks.</li>\n    <li>MDAgent2 uses three high-quality datasets and a multi-step training process to create two specialized models, MD-Instruct and MD-Code.</li>\n    <li>It introduces a new method called MD-GRPO, which improves the model by using results from simulations to learn better over time.</li>\n    <li>MDAgent2 also includes a system for generating, executing, and correcting code, achieving better performance than existing methods.</li>\n</ul>"}, "publishedAt": "2026-01-05T07:56:51.000Z", "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics", "summary": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02075.png", "numComments": 1, "submittedBy": {"_id": "65d7018c90f11951bcfdf2f5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d7018c90f11951bcfdf2f5/RU0-n9k3RIdtv8xvg9Upe.jpeg", "fullname": "Zhuofan Shi", "name": "FredericFan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00423", "authors": [{"_id": "695b259f832867f253525d7b", "user": {"_id": "65fd0dfdfdc5e8ee7d15f793", "avatarUrl": "/avatars/dc1073c79dfa5f4d683fc9cea46db645.svg", "isPro": false, "fullname": "Shengjun Zhang", "user": "zhangsj0722", "type": "user"}, "name": "Shengjun Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:33:36.350Z", "hidden": false}, {"_id": "695b259f832867f253525d7c", "name": "Zhang Zhang", "hidden": false}, {"_id": "695b259f832867f253525d7d", "name": "Chensheng Dai", "hidden": false}, {"_id": "695b259f832867f253525d7e", "name": "Yueqi Duan", "hidden": false}], "publishedAt": "2026-01-01T18:27:32.000Z", "submittedOnDailyAt": "2026-01-08T00:50:19.192Z", "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models", "submittedOnDailyBy": {"_id": "65fd0dfdfdc5e8ee7d15f793", "avatarUrl": "/avatars/dc1073c79dfa5f4d683fc9cea46db645.svg", "isPro": false, "fullname": "Shengjun Zhang", "user": "zhangsj0722", "type": "user"}, "summary": "Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.", "upvotes": 6, "discussionId": "695b25a0832867f253525d7f", "githubRepo": "https://github.com/shengjun-zhang/VisualGRPO", "githubRepoAddedBy": "user", "ai_summary": "Entropy-aware policy optimization method for reinforcement learning in flow matching models that improves exploration through SDE and ODE sampling strategies.", "ai_keywords": ["flow matching models", "reinforcement learning", "stochastic sampling", "denoising directions", "reward signals", "entropy awareness", "Group Relative Policy Optimization", "stochastic differential equations", "ordinary differential equations", "SDE sampling", "ODE sampling", "multi-step group normalized advantage"], "githubStars": 15, "organization": {"_id": "628735cbc83a2d6ab8d14a66", "name": "Tsinghua", "fullname": "Tsinghua University"}, "summary_zh": "<ul>\n    <li>\u8fd1\u5e74\u6765\uff0c\u5f3a\u5316\u5b66\u4e60\u6539\u5584\u4e86\u4eba\u7c7b\u504f\u597d\u7684\u6d41\u5339\u914d\u6a21\u578b\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u4f18\u5316\u591a\u6b21\u53bb\u566a\u6b65\u9aa4\u65f6\uff0c\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u4e14\u6a21\u7cca\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\u9ad8\u71b5\u6b65\u9aa4\u80fd\u66f4\u6709\u6548\u5730\u63a2\u7d22\uff0c\u800c\u4f4e\u71b5\u6b65\u9aa4\u5219\u5bfc\u81f4\u7ed3\u679c\u4e0d\u660e\u663e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86E-GRPO\uff0c\u4e00\u79cd\u6ce8\u91cd\u71b5\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u589e\u52a0SDE\u91c7\u6837\u6b65\u9aa4\u7684\u71b5\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u5956\u52b1\u8bbe\u7f6e\u4e0b\u6709\u6548\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in reinforcement learning have improved models that align with human preferences.</li>\n    <li>Current methods struggle with unclear rewards when optimizing multiple steps in the denoising process.</li>\n    <li>High entropy steps help with better exploration, while low entropy steps lead to less effective outcomes.</li>\n    <li>The proposed method, E-GRPO, increases the entropy of sampling steps to improve performance.</li>\n    <li>Experiments show that this new approach is effective across various reward scenarios.</li>\n</ul>"}, "publishedAt": "2026-01-01T13:27:32.000Z", "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models", "summary": "Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00423.png", "numComments": 1, "submittedBy": {"_id": "65fd0dfdfdc5e8ee7d15f793", "avatarUrl": "/avatars/dc1073c79dfa5f4d683fc9cea46db645.svg", "fullname": "Shengjun Zhang", "name": "zhangsj0722", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "628735cbc83a2d6ab8d14a66", "name": "Tsinghua", "fullname": "Tsinghua University"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03471", "authors": [{"_id": "695f27765fa3847525c41d6a", "name": "Mingyang Wei", "hidden": false}, {"_id": "695f27765fa3847525c41d6b", "name": "Dehai Min", "hidden": false}, {"_id": "695f27765fa3847525c41d6c", "name": "Zewen Liu", "hidden": false}, {"_id": "695f27765fa3847525c41d6d", "name": "Yuzhang Xie", "hidden": false}, {"_id": "695f27765fa3847525c41d6e", "name": "Guanchen Wu", "hidden": false}, {"_id": "695f27765fa3847525c41d6f", "name": "Carl Yang", "hidden": false}, {"_id": "695f27765fa3847525c41d70", "name": "Max S. Y. Lau", "hidden": false}, {"_id": "695f27765fa3847525c41d71", "name": "Qi He", "hidden": false}, {"_id": "695f27765fa3847525c41d72", "name": "Lu Cheng", "hidden": false}, {"_id": "695f27765fa3847525c41d73", "name": "Wei Jin", "hidden": false}], "publishedAt": "2026-01-06T23:49:10.000Z", "submittedOnDailyAt": "2026-01-08T01:12:19.381Z", "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning", "submittedOnDailyBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "isPro": false, "fullname": "Dehai Min", "user": "ZhishanQ", "type": "user"}, "summary": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.", "upvotes": 5, "discussionId": "695f27775fa3847525c41d74", "ai_summary": "EpiQAL presents a novel benchmark for evaluating epidemiological reasoning in language models through three distinct subsets measuring factual recall, multi-step inference, and conclusion reconstruction from scientific literature.", "ai_keywords": ["epidemiological question answering", "diagnostic benchmark", "evidence-grounded inference", "multi-step inference", "chain-of-thought prompting"], "summary_zh": "<ul>\n    <li>\u63d0\u51faEpiQAL\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u6d41\u884c\u75c5\u5b66\u95ee\u9898\u56de\u7b54\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u75be\u75c5\u3002</li>\n    <li>\u57fa\u51c6\u5305\u542b\u4e09\u4e2a\u5b50\u96c6\uff0c\u8bc4\u4f30\u6587\u672c\u8bb0\u5fc6\u3001\u591a\u6b65\u63a8\u7406\u548c\u7ed3\u8bba\u91cd\u5efa\u3002</li>\n    <li>\u7814\u7a76\u663e\u793a\uff0c\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d41\u884c\u75c5\u5b66\u63a8\u7406\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u5728\u591a\u6b65\u63a8\u7406\u4e0a\u6700\u5177\u6311\u6218\u6027\u3002</li>\n    <li>\u6a21\u578b\u7684\u8868\u73b0\u56e0\u5b50\u96c6\u800c\u5f02\uff0c\u89c4\u6a21\u5e76\u4e0d\u80fd\u5355\u72ec\u9884\u6d4b\u6210\u529f\u3002</li>\n    <li>EpiQAL\u4e3a\u8bc1\u636e\u57fa\u7840\u3001\u63a8\u7406\u548c\u7ed3\u8bba\u91cd\u5efa\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u8bca\u65ad\u4fe1\u53f7\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>EpiQAL is a new benchmark for evaluating how well models can answer epidemiological questions using evidence from research studies.</li>\n    <li>It includes three parts: recalling facts from texts, making inferences using evidence, and reconstructing conclusions without certain sections of the text.</li>\n    <li>The benchmark was created with the help of experts and tests different models on their ability to reason about diseases.</li>\n    <li>Current language models struggle with multi-step reasoning, which is the hardest part of the evaluation.</li>\n    <li>EpiQAL helps identify specific areas where models need improvement in understanding and reasoning about epidemiology.</li>\n</ul>"}, "publishedAt": "2026-01-06T18:49:10.000Z", "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning", "summary": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03471.png", "numComments": 1, "submittedBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "fullname": "Dehai Min", "name": "ZhishanQ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.03252", "authors": [{"_id": "695dc956c03d6d81e4399ea4", "name": "Hao Yu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea5", "user": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "name": "Haotong Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:04.783Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea6", "name": "Jiawei Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea7", "name": "Jiaxin Li", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea8", "name": "Yida Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea9", "user": {"_id": "6791a6c19ce382eae861ed61", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg", "isPro": false, "fullname": "Xueyang Zhang", "user": "zhangxueyang001", "type": "user"}, "name": "Xueyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:39.946Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399eaa", "name": "Yue Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399eab", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "695dc956c03d6d81e4399eac", "name": "Ruizhen Hu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ead", "user": {"_id": "62986ca2b58e71e2ac9b8f01", "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg", "isPro": false, "fullname": "Sida Peng", "user": "pengsida", "type": "user"}, "name": "Sida Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:12.074Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "publishedAt": "2026-01-06T18:57:06.000Z", "submittedOnDailyAt": "2026-01-07T00:26:37.060Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "submittedOnDailyBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "upvotes": 71, "discussionId": "695dc956c03d6d81e4399eae", "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.", "ai_keywords": ["neural implicit fields", "local implicit decoder", "continuous 2D coordinates", "arbitrary-resolution depth estimation", "synthetic benchmark", "4K synthetic benchmark", "novel view synthesis", "viewpoint shifts"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u5728\u79bb\u6563\u56fe\u50cf\u7f51\u683c\u4e0a\u9884\u6d4b\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u7684\u6062\u590d\u3002</li>\n    <li>\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86InfiniDepth\uff0c\u5b83\u5c06\u6df1\u5ea6\u8868\u793a\u4e3a\u795e\u7ecf\u9690\u5f0f\u573a\uff0c\u5141\u8bb8\u5728\u8fde\u7eed\u76842D\u5750\u6807\u4e0a\u67e5\u8be2\u6df1\u5ea6\u3002</li>\n    <li>InfiniDepth\u80fd\u591f\u5b9e\u73b0\u4efb\u610f\u5206\u8fa8\u7387\u548c\u7ec6\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76844K\u5408\u6210\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea\u4e94\u6b3e\u4e0d\u540c\u6e38\u620f\u7684\u4e30\u5bcc\u573a\u666f\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cInfiniDepth\u5728\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u7ec6\u8282\u4e30\u5bcc\u7684\u533a\u57df\u3002\u5b83\u8fd8\u6539\u5584\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u7684\u6548\u679c\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Current depth estimation methods can only predict depth on fixed image grids, which limits their flexibility and detail recovery.</li>\n    <li>This paper presents InfiniDepth, a new approach that uses neural implicit fields to represent depth, allowing for continuous depth queries at any resolution.</li>\n    <li>InfiniDepth was tested using a high-quality 4K synthetic dataset created from five different video games with varied scenes.</li>\n    <li>The results show that InfiniDepth outperforms existing methods in both synthetic and real-world scenarios, especially in areas with fine details.</li>\n    <li>It also improves the quality of new view generation from different angles, resulting in fewer visual errors like holes and artifacts.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:57:06.000Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png", "numComments": 8, "submittedBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "fullname": "Haotong Lin", "name": "haotongl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02151", "authors": [{"_id": "695f2d8a5fa3847525c41f8d", "user": {"_id": "6768c97367e4b4606a3c9cec", "avatarUrl": "/avatars/5ddafe7a05828366f66c79072556f370.svg", "isPro": false, "fullname": "diaomuxi", "user": "diaomuxi", "type": "user"}, "name": "Muxi Diao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:56.507Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8e", "user": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "name": "Lele Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:51.246Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8f", "user": {"_id": "64c0f972d76592ba899c2c9c", "avatarUrl": "/avatars/d6940beb135f99241c6fb2cf0e8ccdbe.svg", "isPro": false, "fullname": "GongWuxuan", "user": "Wuxuan-Gong", "type": "user"}, "name": "Wuxuan Gong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:19.470Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f90", "name": "Yutong Zhang", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f91", "user": {"_id": "64fbd4e69a62bb2791b3a665", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fbd4e69a62bb2791b3a665/ZEMtU8O0z98ryeRCG3l_K.jpeg", "isPro": false, "fullname": "Zhonghao Yan", "user": "zzzyzh", "type": "user"}, "name": "Zhonghao Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:53.904Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f92", "name": "Yufei Han", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f93", "user": {"_id": "67f4a56928cbc4f2f75c008d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qoX8HT0JsjqW2OQoENSvg.png", "isPro": false, "fullname": "Kongming Liang", "user": "KongmingLiang", "type": "user"}, "name": "Kongming Liang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:38.812Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f94", "name": "Weiran Xu", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f95", "name": "Zhanyu Ma", "hidden": false}], "publishedAt": "2026-01-05T14:28:17.000Z", "submittedOnDailyAt": "2026-01-08T01:42:04.476Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "submittedOnDailyBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "upvotes": 64, "discussionId": "695f2d8b5fa3847525c41f96", "projectPage": "https://ymxyll.github.io/EAFT/", "githubRepo": "https://github.com/PRIS-CV/EAFT", "githubRepoAddedBy": "user", "ai_summary": "Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.", "ai_keywords": ["supervised fine-tuning", "catastrophic forgetting", "on-policy reinforcement learning", "distributional gap", "Confident Conflicts", "token-level entropy", "epistemic uncertainty", "knowledge conflict", "gradient updates", "downstream performance"], "githubStars": 18, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "summary_zh": "<ul>\n    <li>\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5e38\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5219\u80fd\u4fdd\u6301\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0cSFT\u548cRL\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u524d\u8005\u5f3a\u8feb\u6a21\u578b\u9002\u5e94\u5916\u90e8\u76d1\u7763\u3002</li>\n    <li>\u5728SFT\u4e2d\uff0c\u6a21\u578b\u53ef\u80fd\u4f1a\u51fa\u73b0\u201c\u81ea\u4fe1\u51b2\u7a81\u201d\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5bf9\u81ea\u8eab\u9884\u6d4b\u975e\u5e38\u81ea\u4fe1\uff0c\u4f46\u5b66\u4e60\u5230\u7684\u771f\u5b9e\u60c5\u51b5\u5374\u76f8\u53cd\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u71b5\u81ea\u9002\u5e94\u5fae\u8c03\uff08EAFT\uff09\uff0c\u5b83\u5229\u7528\u6807\u8bb0\u71b5\u6765\u533a\u5206\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\u3002</li>\n    <li>\u5728\u591a\u79cd\u9886\u57df\uff08\u6570\u5b66\u3001\u533b\u5b66\u3001\u667a\u80fd\u4f53\u7b49\uff09\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEAFT\u5728\u4fdd\u6301\u4e00\u822c\u80fd\u529b\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u6807\u51c6SFT\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Supervised Fine-Tuning (SFT) can cause \"catastrophic forgetting\" when adapting models to new domains.</li>\n    <li>On-policy Reinforcement Learning (RL) helps maintain a model's general abilities without forgetting.</li>\n    <li>The problem arises because SFT makes models fit external instructions, while RL aligns with the model's own beliefs.</li>\n    <li>Conflicts occur when models are confident in their predictions but are forced to learn different truths, leading to harmful updates.</li>\n    <li>Entropy-Adaptive Fine-Tuning (EAFT) helps by using uncertainty measures to manage learning from conflicting information, leading to better performance without losing general skills.</li>\n</ul>"}, "publishedAt": "2026-01-05T09:28:17.000Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02151.png", "numComments": 6, "submittedBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "fullname": "\u6768\u4e50\u4e50", "name": "ssl-asuka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03509", "authors": [{"_id": "695fbc7d5b7998385e639349", "name": "Haochen Shi", "hidden": false}, {"_id": "695fbc7d5b7998385e63934a", "name": "Xingdi Yuan", "hidden": false}, {"_id": "695fbc7d5b7998385e63934b", "name": "Bang Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "publishedAt": "2026-01-07T01:43:25.000Z", "submittedOnDailyAt": "2026-01-08T11:50:05.687Z", "title": "Evolving Programmatic Skill Networks", "submittedOnDailyBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "isPro": false, "fullname": "Bang Liu", "user": "Bang-UdeM-Mila", "type": "user"}, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "upvotes": 54, "discussionId": "695fbc7e5b7998385e63934c", "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.", "ai_keywords": ["Programmatic Skill Network", "executable symbolic programs", "skill composition", "structured fault localization", "progressive optimization", "maturity-aware update gating", "canonical structural refactoring", "rollback validation", "neural network training", "skill reuse", "rapid adaptation", "generalization"], "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u5728\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u6301\u7eed\u5b66\u4e60\u6280\u80fd\uff0c\u4ee3\u7406\u9700\u8981\u6784\u5efa\u548c\u4f18\u5316\u4e00\u4e2a\u4e0d\u65ad\u6269\u5c55\u7684\u53ef\u6267\u884c\u6280\u80fd\u5e93\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u7a0b\u5e8f\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u53ef\u6267\u884c\u7b26\u53f7\u7a0b\u5e8f\u7ec4\u6210\u7684\u7f51\u7edc\uff0c\u968f\u7740\u7ecf\u9a8c\u4e0d\u65ad\u6f14\u5316\u3002</li>\n    <li>PSN\u5305\u542b\u4e09\u79cd\u6838\u5fc3\u673a\u5236\uff1a\u6280\u80fd\u6545\u969c\u5b9a\u4f4d\u3001\u6210\u719f\u5ea6\u611f\u77e5\u7684\u4f18\u5316\uff0c\u4ee5\u53ca\u5728\u56de\u6eda\u9a8c\u8bc1\u4e0b\u7684\u7ed3\u6784\u91cd\u6784\u3002</li>\n    <li>PSN\u7684\u5b66\u4e60\u52a8\u6001\u4e0e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6709\u7ed3\u6784\u4e0a\u7684\u76f8\u4f3c\u4e4b\u5904\u3002</li>\n    <li>\u5728MineDojo\u548cCrafter\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPSN\u80fd\u591f\u5b9e\u73b0\u5f3a\u5927\u7684\u6280\u80fd\u91cd\u7528\u3001\u5feb\u901f\u9002\u5e94\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on teaching agents to learn and improve skills in changing environments.</li>\n    <li>A new system called the Programmatic Skill Network (PSN) helps agents create and refine skills as they gain experience.</li>\n    <li>PSN uses three main methods: finding and fixing skill issues, updating skills while keeping reliable ones stable, and reorganizing skills to stay efficient.</li>\n    <li>Experiments show that PSN allows agents to reuse skills effectively, adapt quickly, and perform well on a variety of tasks.</li>\n    <li>The researchers plan to share their code with the public for further use.</li>\n</ul>"}, "publishedAt": "2026-01-06T20:43:25.000Z", "title": "Evolving Programmatic Skill Networks", "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png", "numComments": 1, "submittedBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "fullname": "Bang Liu", "name": "Bang-UdeM-Mila", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01554", "authors": [{"_id": "695dcda5c03d6d81e4399eb8", "name": "MOSI. AI", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eb9", "name": "Donghua Yu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eba", "name": "Zhengyuan Lin", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebb", "user": {"_id": "660c345da15ab85523ad00d1", "avatarUrl": "/avatars/b0bfdee89a6c62ff12140b9e85de499a.svg", "isPro": false, "fullname": "Chen Yang", "user": "kiiic", "type": "user"}, "name": "Chen Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:58.894Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebc", "name": "Yiyang Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebd", "name": "Hanfu Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebe", "name": "Jingqi Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebf", "name": "Ke Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec0", "name": "Liwei Fan", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec1", "name": "Yi Jiang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec2", "name": "Jie Zhu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec3", "name": "Muchen Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec4", "name": "Wenxuan Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec5", "name": "Yang Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec6", "user": {"_id": "6443f7bf1bc692d87b25e234", "avatarUrl": "/avatars/fa9e62d96d0691a9a48e3db499a61557.svg", "isPro": false, "fullname": "Xu Zhe", "user": "Phospheneser", "type": "user"}, "name": "Zhe Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:55.950Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec7", "name": "Yitian Gong", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec8", "name": "Yuqian Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec9", "name": "Wenbo Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eca", "user": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "name": "Zhaoye Fei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:10.124Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecb", "user": {"_id": "695757e4fd9dc6e9bac27935", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_uZEu4oOlKJVYqrG763Z-.jpeg", "isPro": false, "fullname": "aa", "user": "qinyuancheng", "type": "user"}, "name": "Qinyuan Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:03.749Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecc", "name": "Shimin Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecd", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:50.004Z", "hidden": false}], "publishedAt": "2026-01-04T15:01:10.000Z", "submittedOnDailyAt": "2026-01-07T00:52:16.123Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "submittedOnDailyBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "upvotes": 45, "discussionId": "695dcda6c03d6d81e4399ece", "projectPage": "https://mosi.cn/models/moss-transcribe-diarize", "ai_summary": "A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.", "ai_keywords": ["multimodal large language model", "end-to-end paradigm", "speaker diarization", "time-stamped transcription", "context window", "robust generalization"], "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>SATS\uff08\u8bf4\u8bdd\u8005\u6807\u6ce8\u65f6\u95f4\u6233\u8f6c\u5f55\uff09\u65e8\u5728\u51c6\u786e\u8bb0\u5f55\u53d1\u8a00\u5185\u5bb9\u53ca\u6bcf\u4f4d\u8bf4\u8bdd\u8005\u7684\u65f6\u95f4\u3002</li>\n    <li>\u73b0\u6709\u7684SATS\u7cfb\u7edf\u901a\u5e38\u4e0d\u662f\u7aef\u5230\u7aef\u7684\uff0c\u4e14\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u957f\u671f\u8bb0\u5fc6\u548c\u751f\u6210\u65f6\u95f4\u6233\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002</li>\n    <li>MOSS Transcribe Diarize \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5728\u7aef\u5230\u7aef\u7684\u57fa\u7840\u4e0a\u6267\u884cSATS\u3002</li>\n    <li>\u8be5\u6a21\u578b\u7ecf\u8fc7\u5927\u91cf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u652f\u6301\u6700\u591a90\u5206\u949f\u7684\u8f93\u5165\uff0c\u5e76\u5177\u6709128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002</li>\n    <li>\u7ecf\u8fc7\u8bc4\u4f30\uff0cMOSS Transcribe Diarize \u5728\u591a\u4e2a\u516c\u5171\u548c\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5546\u4e1a\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MOSS Transcribe Diarize is a new system designed for transcribing spoken words and tracking who said what in meetings.</li>\n    <li>It addresses issues in existing systems, like limited context, poor memory of speakers, and lack of timestamping.</li>\n    <li>This system uses a large language model and can handle long audio inputs of up to 90 minutes.</li>\n    <li>MOSS is trained on a wide variety of real-world data, making it flexible and effective.</li>\n    <li>It has been shown to perform better than leading commercial transcription systems in various tests.</li>\n</ul>"}, "publishedAt": "2026-01-04T10:01:10.000Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01554.png", "numComments": 2, "submittedBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "fullname": "Zhaoye Fei", "name": "ngc7293", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02204", "authors": [{"_id": "695c7d0d6aa73bc11f091433", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091434", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:57.686Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091435", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091436", "name": "Hang Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091437", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091438", "name": "Yongsheng Dong", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091439", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143a", "name": "Xian Li", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143b", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143c", "user": {"_id": "6344dcb1cd37e44d9ed46508", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg", "isPro": false, "fullname": "Yi Jiang", "user": "JiangYi", "type": "user"}, "name": "Yi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:55.158Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143d", "name": "Hu Ye", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143e", "name": "Bo Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143f", "name": "Yiming Gao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091440", "name": "Peng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091441", "name": "Akide Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091442", "name": "Zhipeng Yang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091443", "name": "Qili Deng", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091444", "name": "Linjie Xing", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091445", "name": "Jiyang Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091446", "name": "Zhao Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091447", "name": "Yang Zhou", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091448", "name": "Mingcong Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091449", "name": "Yi Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144a", "name": "Qian He", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144b", "name": "Xiwei Hu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144c", "name": "Zhongqi Qi", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144d", "name": "Jie Shao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144e", "name": "Zhiye Fu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144f", "name": "Shuai Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091450", "name": "Fangmin Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091451", "name": "Xuezhi Chai", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091452", "name": "Zhihua Wu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091453", "name": "Yitong Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091454", "name": "Zehuan Yuan", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091455", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091456", "name": "Xinglong Wu", "hidden": false}], "publishedAt": "2026-01-05T15:27:04.000Z", "submittedOnDailyAt": "2026-01-06T00:52:35.953Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "upvotes": 45, "discussionId": "695c7d0d6aa73bc11f091457", "githubRepo": "https://github.com/ByteVisionLab/NextFlow", "githubRepoAddedBy": "user", "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.", "ai_keywords": ["decoder-only autoregressive transformer", "interleaved text-image discrete tokens", "unified vision representation", "multimodal understanding", "multimodal generation", "next-token prediction", "next-scale prediction", "raster-scan methods", "visual generation", "prefix-tuning strategy", "reinforcement learning", "diffusion baselines"], "githubStars": 60, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>NextFlow \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u4f7f\u7528\u4e86 6 \u4e07\u4ebf\u4e2a\u6587\u672c-\u56fe\u50cf\u79bb\u6563\u6807\u8bb0\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b83\u53ef\u4ee5\u8fdb\u884c\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u652f\u6301\u56fe\u50cf\u7f16\u8f91\u3001\u4ea4\u9519\u5185\u5bb9\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u6587\u672c\u91c7\u7528\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u800c\u56fe\u50cf\u751f\u6210\u5219\u91c7\u7528\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u9884\u6d4b\uff0c\u4f7f\u5f97\u56fe\u50cf\u751f\u6210\u901f\u5ea6\u663e\u8457\u63d0\u9ad8\u3002</li>\n    <li>\u80fd\u591f\u5728 5 \u79d2\u5185\u751f\u6210 1024x1024 \u7684\u56fe\u50cf\uff0c\u6bd4\u4f20\u7edf\u6a21\u578b\u5feb\u5f88\u591a\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNextFlow \u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u4e13\u4e1a\u6269\u6563\u6a21\u578b\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NextFlow is a powerful AI model that combines text and images, trained on a huge dataset of 6 trillion tokens.</li>\n    <li>It can understand and create both text and images, allowing for tasks like image editing and video generation.</li>\n    <li>NextFlow generates images much faster than traditional methods, producing high-quality 1024x1024 images in just 5 seconds.</li>\n    <li>The model uses a unique approach for generating visuals and has a strong training method to ensure stability.</li>\n    <li>NextFlow performs exceptionally well in tests, competing with specialized models for image quality.</li>\n</ul>"}, "publishedAt": "2026-01-05T10:27:04.000Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01739", "authors": [{"_id": "695c72346aa73bc11f0913bf", "user": {"_id": "6044fd39e6aa3e130cb92867", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg", "isPro": false, "fullname": "Eunbi Choi", "user": "unbiarirang", "type": "user"}, "name": "Eunbi Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:17.472Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c0", "user": {"_id": "64d31ca9465b6039259838df", "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg", "isPro": false, "fullname": "kibong choi", "user": "bongchoi", "type": "user"}, "name": "Kibong Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:11.322Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c1", "name": "Seokhee Hong", "hidden": false}, {"_id": "695c72346aa73bc11f0913c2", "user": {"_id": "63c50e590c24c8b53958f75e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png", "isPro": false, "fullname": "Junwon Hwang", "user": "nuxlear", "type": "user"}, "name": "Junwon Hwang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:09.333Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c3", "name": "Hyojin Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913c4", "user": {"_id": "66a9e066a203add977948988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg", "isPro": false, "fullname": "hyunjik.jo", "user": "switiz87", "type": "user"}, "name": "Hyunjik Jo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:13.551Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c5", "name": "Joonkee Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c6", "name": "Seonghwan Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c7", "name": "Soyeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c8", "name": "Sunkyoung Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c9", "name": "Yireun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ca", "name": "Yongil Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913cb", "name": "Haeju Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cc", "name": "Jinsik Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cd", "name": "Kyungmin Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ce", "name": "Sangha Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913cf", "name": "Heuiyeen Yeen", "hidden": false}, {"_id": "695c72346aa73bc11f0913d0", "name": "Hwan Chang", "hidden": false}, {"_id": "695c72346aa73bc11f0913d1", "name": "Stanley Jungkyu Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d2", "name": "Yejin Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d3", "name": "Jiwon Ham", "hidden": false}, {"_id": "695c72346aa73bc11f0913d4", "name": "Kijeong Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913d5", "name": "Geunyeong Jeong", "hidden": false}, {"_id": "695c72346aa73bc11f0913d6", "name": "Gerrard Jeongwon Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d7", "name": "Yonghwan Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d8", "name": "Jiyeon Jung", "hidden": false}, {"_id": "695c72346aa73bc11f0913d9", "name": "Naeun Kang", "hidden": false}, {"_id": "695c72346aa73bc11f0913da", "name": "Dohoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913db", "name": "Euisoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dc", "name": "Hayeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dd", "name": "Hyosang Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913de", "name": "Hyunseo Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913df", "name": "Jieun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e0", "name": "Minu Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e1", "name": "Myoungshin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e2", "name": "Unsol Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e3", "name": "Youchul Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e4", "name": "YoungJin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e5", "name": "Chaeeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e6", "name": "Chaeyoon Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e7", "user": {"_id": "6399ab9e92e12136b99ef60e", "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg", "isPro": false, "fullname": "Changhun Lee", "user": "xvyaward", "type": "user"}, "name": "Changhun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:15.420Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913e8", "name": "Dahm Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e9", "name": "Edward Hwayoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ea", "name": "Honglak Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913eb", "name": "Jinsang Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ec", "name": "Jiyoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ed", "name": "Sangeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ee", "name": "Seungwon Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ef", "name": "Solji Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f0", "name": "Woohyung Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f1", "name": "Chanwoo Moon", "hidden": false}, {"_id": "695c72346aa73bc11f0913f2", "name": "Jaewoo Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f3", "name": "Jinho Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f4", "name": "Yongmin Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f5", "name": "Hyerin Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f6", "name": "Wooseok Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f7", "name": "Yongwoo Song", "hidden": false}, {"_id": "695c72346aa73bc11f0913f8", "name": "Sejong Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913f9", "name": "Sihoon Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913fa", "name": "Chang En Yea", "hidden": false}, {"_id": "695c72346aa73bc11f0913fb", "name": "Sihyuk Yi", "hidden": false}, {"_id": "695c72346aa73bc11f0913fc", "name": "Chansik Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fd", "name": "Dongkeun Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fe", "name": "Sangyeon Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913ff", "name": "Hyeongu Yun", "hidden": false}], "publishedAt": "2026-01-05T02:30:59.000Z", "submittedOnDailyAt": "2026-01-06T01:03:14.011Z", "title": "K-EXAONE Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "upvotes": 44, "discussionId": "695c72356aa73bc11f091400", "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE", "githubRepoAddedBy": "user", "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.", "ai_keywords": ["Mixture-of-Experts", "256K-token context window", "multilingual language model", "parameter-efficient fine-tuning"], "githubStars": 39, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "summary_zh": "<ul>\n    <li>K-EXAONE\u662fLG AI\u7814\u7a76\u5f00\u53d1\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u6a21\u578b\u91c7\u7528Mixture-of-Experts\u67b6\u6784\uff0c\u62e5\u67092360\u4ebf\u4e2a\u53c2\u6570\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u4e2a\u53c2\u6570\u3002</li>\n    <li>\u652f\u6301256K\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u6db5\u76d6\u516d\u79cd\u8bed\u8a00\uff1a\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u3002</li>\n    <li>K-EXAONE\u5728\u591a\u4e2a\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0e\u540c\u7c7b\u5f00\u653e\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u8be5\u6a21\u578b\u65e8\u5728\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4e1a\u548c\u7814\u7a76\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>K-EXAONE is a multilingual language model created by LG AI Research.</li>\n    <li>It has 236 billion parameters, using 23 billion during use, and can handle long text inputs up to 256,000 tokens.</li>\n    <li>The model supports six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.</li>\n    <li>K-EXAONE has been tested on various skills and performs similarly to other large models.</li>\n    <li>It aims to improve AI technology for better living and can be used in many industrial and research areas.</li>\n</ul>"}, "publishedAt": "2026-01-04T21:30:59.000Z", "title": "K-EXAONE Technical Report", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00664", "authors": [{"_id": "695b237a832867f253525d70", "user": {"_id": "66b57c77778c98d29446c8ec", "avatarUrl": "/avatars/c176bb7c072f3093f6a0786c87d384d8.svg", "isPro": false, "fullname": "Taekyung Ki", "user": "taekyungki", "type": "user"}, "name": "Taekyung Ki", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:19.100Z", "hidden": false}, {"_id": "695b237a832867f253525d71", "name": "Sangwon Jang", "hidden": false}, {"_id": "695b237a832867f253525d72", "name": "Jaehyeong Jo", "hidden": false}, {"_id": "695b237a832867f253525d73", "user": {"_id": "652066649004117947e46ed6", "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg", "isPro": false, "fullname": "Jaehong Yoon", "user": "jaehong31", "type": "user"}, "name": "Jaehong Yoon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:17.228Z", "hidden": false}, {"_id": "695b237a832867f253525d74", "name": "Sung Ju Hwang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "publishedAt": "2026-01-02T11:58:48.000Z", "submittedOnDailyAt": "2026-01-05T00:05:44.498Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "upvotes": 43, "discussionId": "695b237a832867f253525d75", "projectPage": "https://taekyungki.github.io/AvatarForcing/", "githubRepo": "https://github.com/TaekyungKi/AvatarForcing", "githubRepoAddedBy": "user", "ai_summary": "Avatar Forcing framework enables real-time interactive head avatar generation with low latency and expressive motion through diffusion forcing and label-free preference optimization.", "ai_keywords": ["diffusion forcing", "real-time interaction", "multimodal inputs", "audio", "motion", "causal constraints", "synthetic losing samples", "direct preference optimization", "interactive head avatar generation"], "githubStars": 65, "summary_zh": "<ul>\n    <li>\u751f\u6210\u771f\u5b9e\u5934\u50cf\u7684\u6280\u672f\u53ef\u4ee5\u7528\u9759\u6001\u8096\u50cf\u8fdb\u884c\u865a\u62df\u6c9f\u901a\u548c\u5185\u5bb9\u521b\u4f5c\u3002</li>\n    <li>\u73b0\u6709\u6a21\u578b\u5728\u4e92\u52a8\u6c9f\u901a\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u5e38\u5e38\u7f3a\u4e4f\u60c5\u611f\u6295\u5165\u548c\u4e92\u52a8\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cAvatar Forcing\u201d\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5b9e\u65f6\u751f\u6210\u52a8\u6001\u5934\u50cf\u7684\u6311\u6218\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u7528\u6237\u7684\u97f3\u9891\u548c\u52a8\u4f5c\u8f93\u5165\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u5373\u65f6\u53cd\u5e94\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ea6500\u6beb\u79d2\u7684\u5b9e\u65f6\u4e92\u52a8\uff0c\u5e76\u4e14\u751f\u6210\u7684\u5934\u50cf\u52a8\u4f5c\u6bd4\u57fa\u7ebf\u66f4\u5177\u8868\u73b0\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on creating lifelike avatars from pictures for better virtual communication, aiming for more interactive experiences.</li>\n    <li>Current models struggle with real-time emotional engagement and often provide only one-way responses.</li>\n    <li>The authors introduce a new method called Avatar Forcing, which allows avatars to respond quickly to user inputs like speech and gestures.</li>\n    <li>This framework processes user audio and motions with low latency (about 500ms) and improves interaction speed by 6.8 times compared to previous models.</li>\n    <li>It also enables expressive reactions without needing extra labeled data, with over 80% user preference for the generated avatar motions.</li>\n</ul>"}, "publishedAt": "2026-01-02T06:58:48.000Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00664.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 198}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03233", "authors": [{"_id": "695dc6d9c03d6d81e4399e85", "user": {"_id": "6303cc5e0547362a22a51af0", "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg", "isPro": false, "fullname": "Yoav HaCohen", "user": "yoavhacohen", "type": "user"}, "name": "Yoav HaCohen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:29.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e86", "user": {"_id": "6489c487b9e9258ba065418f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png", "isPro": false, "fullname": "Benny Brazowski", "user": "benibraz", "type": "user"}, "name": "Benny Brazowski", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:35.981Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e87", "user": {"_id": "62dd30a8d43078cd49ac8ad8", "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg", "isPro": false, "fullname": "Nisan Chiprut", "user": "nisan", "type": "user"}, "name": "Nisan Chiprut", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:41.634Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e88", "user": {"_id": "64a7adc087cbd4dc7301fdd6", "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg", "isPro": false, "fullname": "Yaki Bitterman", "user": "jacobitterman", "type": "user"}, "name": "Yaki Bitterman", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:46.749Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e89", "user": {"_id": "65897258509bcae23fa162c9", "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg", "isPro": false, "fullname": "Andrew Kvochko", "user": "kvochko", "type": "user"}, "name": "Andrew Kvochko", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:51.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8a", "name": "Avishai Berkowitz", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8b", "name": "Daniel Shalem", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8c", "user": {"_id": "681af83e2f4aaa88639e703d", "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg", "isPro": false, "fullname": "Daphna Lifschitz", "user": "Daphnal", "type": "user"}, "name": "Daphna Lifschitz", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:04.180Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8d", "user": {"_id": "636b97a57631fe5e86fe1fa2", "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg", "isPro": false, "fullname": "Dudu Moshe", "user": "dudumoshe", "type": "user"}, "name": "Dudu Moshe", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:13.512Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8e", "name": "Eitan Porat", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8f", "user": {"_id": "677a422979d3c32a5dd87a0a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png", "isPro": false, "fullname": "Eitan Richardson", "user": "eitanrich", "type": "user"}, "name": "Eitan Richardson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:22.677Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e90", "user": {"_id": "673f6911d83832a6ce15e7bf", "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg", "isPro": false, "fullname": "Guy Shiran", "user": "guysrn", "type": "user"}, "name": "Guy Shiran", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:28.250Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e91", "user": {"_id": "65744a2fe09de6aa74026d80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg", "isPro": false, "fullname": "Itay Chachy", "user": "ItayChachy", "type": "user"}, "name": "Itay Chachy", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:36.781Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e92", "name": "Jonathan Chetboun", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e93", "user": {"_id": "6678365ac411b340b32d6148", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg", "isPro": false, "fullname": "Michael Finkelson", "user": "MichaelFinkelson", "type": "user"}, "name": "Michael Finkelson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:53.574Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e94", "user": {"_id": "6318aa43cb4ca740c4c55651", "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg", "isPro": false, "fullname": "michael kupchick", "user": "michaellightricks", "type": "user"}, "name": "Michael Kupchick", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:59.945Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e95", "user": {"_id": "673f29b568595672b8d3e90e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png", "isPro": false, "fullname": "Nir Zabari", "user": "NirZabariLTX", "type": "user"}, "name": "Nir Zabari", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:07.297Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e96", "user": {"_id": "64ae89c043dda9449a1eb1ba", "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg", "isPro": false, "fullname": "Nitzan Guetta", "user": "nitzanguetta", "type": "user"}, "name": "Nitzan Guetta", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:14.712Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e97", "name": "Noa Kotler", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e98", "user": {"_id": "631f58935ba8c026340b377c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg", "isPro": false, "fullname": "Ofir Bibi", "user": "ofirbibi", "type": "user"}, "name": "Ofir Bibi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:27.196Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e99", "user": {"_id": "674348b46215a2c0878e219b", "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg", "isPro": false, "fullname": "Ori Gordon", "user": "origordon", "type": "user"}, "name": "Ori Gordon", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:34.878Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9a", "name": "Poriya Panet", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9b", "name": "Roi Benita", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9c", "name": "Shahar Armon", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9d", "name": "Victor Kulikov", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9e", "name": "Yaron Inger", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9f", "name": "Yonatan Shiftan", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea0", "name": "Zeev Melumian", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea1", "name": "Zeev Farbman", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "publishedAt": "2026-01-06T18:24:41.000Z", "submittedOnDailyAt": "2026-01-07T00:07:29.528Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "upvotes": 40, "discussionId": "695dc6d9c03d6d81e4399ea2", "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v", "githubRepo": "https://github.com/Lightricks/LTX-2", "githubRepoAddedBy": "user", "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.", "ai_keywords": ["text-to-video diffusion models", "audiovisual content", "dual-stream transformer", "cross-attention layers", "temporal positional embeddings", "AdaLN", "classifier-free guidance", "modality-aware classifier-free guidance", "multilingual text encoder", "diffusion models"], "githubStars": 922, "summary_zh": "<ul>\n    <li>LTX-2 \u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u4e00\u4e2a\u53cc\u6d41\u53d8\u6362\u5668\uff0c\u5206\u522b\u5904\u7406\u89c6\u9891\u548c\u97f3\u9891\u3002</li>\n    <li>LTX-2 \u80fd\u591f\u751f\u6210\u4e0e\u573a\u666f\u76f8\u7b26\u7684\u4e30\u5bcc\u97f3\u9891\uff0c\u5305\u62ec\u81ea\u7136\u80cc\u666f\u97f3\u548c\u97f3\u6548\u3002</li>\n    <li>\u6a21\u578b\u5728\u89c6\u542c\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u7cfb\u7edf\u3002</li>\n    <li>\u6240\u6709\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u90fd\u5df2\u516c\u5f00\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LTX-2 is a new model that generates high-quality videos with synchronized audio, filling the gap left by previous video-only models.</li>\n    <li>It uses a dual-stream transformer with a large video stream and a smaller audio stream to efficiently create audiovisual content.</li>\n    <li>The model includes advanced features for better understanding of prompts and improved alignment between audio and video.</li>\n    <li>LTX-2 can produce detailed audio that matches the scenes, including character voices and background sounds.</li>\n    <li>It achieves top-quality audiovisual output and is faster and cheaper than many existing proprietary models, and all its resources are available for public use.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:24:41.000Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01425", "authors": [{"_id": "695c765d6aa73bc11f091402", "name": "Xu Guo", "hidden": false}, {"_id": "695c765d6aa73bc11f091403", "name": "Fulong Ye", "hidden": false}, {"_id": "695c765d6aa73bc11f091404", "name": "Xinghui Li", "hidden": false}, {"_id": "695c765d6aa73bc11f091405", "name": "Pengqi Tu", "hidden": false}, {"_id": "695c765d6aa73bc11f091406", "name": "Pengze Zhang", "hidden": false}, {"_id": "695c765d6aa73bc11f091407", "user": {"_id": "674566cb79d6f3a9da7be0de", "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg", "isPro": false, "fullname": "Qichao Sun", "user": "Simons212", "type": "user"}, "name": "Qichao Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:07.336Z", "hidden": false}, {"_id": "695c765d6aa73bc11f091408", "name": "Songtao Zhao", "hidden": false}, {"_id": "695c765d6aa73bc11f091409", "name": "Xiangwang Hou", "hidden": false}, {"_id": "695c765d6aa73bc11f09140a", "name": "Qian He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "publishedAt": "2026-01-04T08:07:11.000Z", "submittedOnDailyAt": "2026-01-06T00:30:06.666Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "submittedOnDailyBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "upvotes": 33, "discussionId": "695c765d6aa73bc11f09140b", "projectPage": "https://guoxu1233.github.io/DreamID-V/", "githubRepo": "https://github.com/bytedance/DreamID-V", "githubRepoAddedBy": "user", "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.", "ai_keywords": ["Video Face Swapping", "Image Face Swapping", "diffusion transformer", "Modality-Aware Conditioning", "Synthetic-to-Real Curriculum", "Identity-Coherence Reinforcement Learning", "IDBench-V", "Identity-Anchored Video Synthesizer", "bidirectional ID quadruplets", "multi-model conditions"], "githubStars": 86, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u6280\u672f\u9700\u8981\u5728\u89c6\u9891\u4e2d\u65e0\u7f1d\u5730\u66ff\u6362\u8eab\u4efd\uff0c\u540c\u65f6\u4fdd\u6301\u59ff\u52bf\u3001\u8868\u60c5\u3001\u5149\u7167\u3001\u80cc\u666f\u548c\u52a8\u6001\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4fdd\u5b58\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u56fe\u50cf\u6362\u8138\u7684\u4f18\u52bf\u8f6c\u79fb\u5230\u89c6\u9891\u9886\u57df\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86SyncID-Pipe\u6570\u636e\u7ba1\u9053\u548c\u57fa\u4e8eDiffusion Transformer\u7684DreamID-V\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u521b\u5efa\u4e86IDBench-V\uff0c\u4e00\u4e2a\u591a\u6837\u5316\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u8868\u660eDreamID-V\u5728\u5404\u7c7b\u6362\u8138\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video Face Swapping (VFS) aims to insert a new face into a video while keeping the original look and feel intact.</li>\n    <li>Current methods often fail to maintain a person's identity and expressions consistently throughout the video.</li>\n    <li>The proposed solution includes a new system called SyncID-Pipe, which helps prepare and enhance video face swapping by linking images and videos effectively.</li>\n    <li>They developed a new framework named DreamID-V that uses advanced techniques to ensure better quality and consistency of face swaps in videos.</li>\n    <li>A new benchmark, IDBench-V, has been created for testing and improving video face swapping methods across various situations.</li>\n</ul>"}, "publishedAt": "2026-01-04T03:07:11.000Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01425.png", "numComments": 2, "submittedBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "fullname": "xuguo", "name": "XuGuo699", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03872", "authors": [{"_id": "695f1a475fa3847525c41d06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:32:36.055Z", "hidden": false}, {"_id": "695f1a475fa3847525c41d07", "name": "Guocheng Zhai", "hidden": false}, {"_id": "695f1a475fa3847525c41d08", "name": "Ruihan Jin", "hidden": false}, {"_id": "695f1a475fa3847525c41d09", "name": "Jiahao Yuan", "hidden": false}, {"_id": "695f1a475fa3847525c41d0a", "name": "Yuhao Shen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0b", "name": "Shuai Zhang", "hidden": false}, {"_id": "695f1a475fa3847525c41d0c", "name": "Zhengqi Wen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0d", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-07T12:38:33.000Z", "submittedOnDailyAt": "2026-01-08T04:50:03.287Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "upvotes": 30, "discussionId": "695f1a475fa3847525c41d0e", "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.", "ai_keywords": ["large language models", "external tools", "model-tool combination", "high-dimensional optimization", "dual-path framework", "training-free cluster-based routing", "RL-based multi-step routing", "cross-domain complex reasoning", "domain-specific alignment", "out-of-distribution generalization"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u9009\u62e9\u6700\u4f73\u7684\u6a21\u578b-\u5de5\u5177\u7ec4\u5408\u53d8\u6210\u4e86\u4e00\u4e2a\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\u3002</li>\n    <li>ATLAS\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u65b9\u6cd5\u5b9e\u73b0\u52a8\u6001\u5de5\u5177\u4f7f\u7528\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cATLAS\u5728\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548c\u8def\u7531\u65b9\u6cd5\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ATLAS is a new framework that helps combine large language models (LLMs) with external tools for better performance.</li>\n    <li>It addresses the challenge of choosing the best model-tool pair by using two main methods: a cluster-based approach and a reinforcement learning approach.</li>\n    <li>ATLAS has been tested on 15 different benchmarks and performs better than models like GPT-4o and other existing methods.</li>\n    <li>It shows improvements in both standard tasks and more complex reasoning tasks that go beyond the usual data.</li>\n    <li>The framework also enhances visual reasoning by effectively using specialized tools that work with different types of data.</li>\n</ul>"}, "publishedAt": "2026-01-07T07:38:33.000Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u76ee\u524d\u5927\u591a\u6570\u6570\u636e\u51c6\u5907\u4ecd\u4f9d\u8d56\u4e8e\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u548c\u53ef\u91cd\u590d\u6027\u3002</li>\n    <li>DataFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>\u6846\u67b6\u5305\u62ec\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u591a\u4e2a\u9886\u57df\u901a\u7528\u7684\u6570\u636e\u7ba1\u9053\uff0c\u8986\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u7b49\u3002</li>\n    <li>\u901a\u8fc7DataFlow\uff0c\u6570\u636e\u51c6\u5907\u7684\u6548\u679c\u663e\u8457\u63d0\u5347\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current methods are often inconsistent and not reproducible.</li>\n    <li>DataFlow is a new framework that helps prepare data for LLMs in a more organized and efficient way, using modular and reusable components.</li>\n    <li>The framework includes almost 200 reusable tools and six different pipelines for various tasks like text processing, math reasoning, and code generation.</li>\n    <li>DataFlow-Agent can automatically turn natural language instructions into executable data preparation pipelines.</li>\n    <li>The framework has shown to improve the performance of LLMs in various tasks, surpassing traditional datasets and methods.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u5b8c\u6574\u7684\u7cfb\u7edf\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u89c6\u9891\u521b\u4f5c\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u6267\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos using different types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one unified framework.</li>\n    <li>The system can handle various user inputs like text, images, and videos to create cinematic-quality videos.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training methods for efficient video creation.</li>\n    <li>It shows great skills in generating content, editing based on reasoning, and following multiple types of instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 94, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 76, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b56\u7565\u88ab\u5e7f\u6cdb\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u4e0d\u80fd\u6709\u6548\u5229\u7528\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u52a8\u6001\u5730\u652f\u6301\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u8fde\u63a5\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\u548c\u77e5\u8bc6\u7ed3\u6784\u7684\u5f62\u6210\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHGMem\u5728\u591a\u6b65\u9aa4RAG\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u663e\u8457\u8d85\u8fc7\u4f20\u7edf\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) is a method that helps large language models perform better in complex tasks that require deep understanding and reasoning.</li>\n    <li>Current memory systems in RAG mainly store facts but do not effectively connect them, leading to weak reasoning and understanding.</li>\n    <li>HGMem is a new memory system that uses a hypergraph structure to create dynamic connections between facts, allowing for better reasoning.</li>\n    <li>This new memory mechanism helps build a more integrated knowledge structure, improving the model's ability to understand and reason about complex issues.</li>\n    <li>Tests show that HGMem significantly enhances performance on multi-step RAG tasks compared to existing systems.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u8fdb\u884c4D\u91cd\u5efa\u548c\u65b0\u8f68\u8ff9\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u907f\u514d\u4e86\u6602\u8d35\u548c\u590d\u6742\u7684\u591a\u89c6\u89d24D\u6570\u636e\u9700\u6c42\u3002</li>\n    <li>NeoVerse\u91c7\u7528\u65e0\u59ff\u6001\u7684\u524d\u99884D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u76ee\u964d\u7ea7\u6a21\u5f0f\u4eff\u771f\u7b49\u6280\u672f\uff0c\u9002\u5e94\u591a\u79cd\u5355\u76ee\u89c6\u9891\u3002</li>\n    <li>\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeoVerse\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002</li>\n    <li>\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u8bbf\u95ee\uff1ahttps://neoverse-4d.github.io</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions and generate videos with new paths.</li>\n    <li>Current 4D models struggle with scaling due to complex data and training requirements.</li>\n    <li>NeoVerse is designed to work well with regular monocular videos, making it more accessible.</li>\n    <li>It includes innovative features like pose-free 4D reconstruction and simulation of video quality loss.</li>\n    <li>NeoVerse has shown top performance in various testing benchmarks for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Youtu-Agent\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u53d1\u5c55LLM\u4ee3\u7406\u3002</li>\n    <li>Youtu-Agent\u5177\u6709\u7075\u6d3b\u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u6807\u51c6\u4efb\u52a1\u548c\u590d\u6742\u9700\u6c42\u7684\u81ea\u52a8\u751f\u6210\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df7\u5408\u7b56\u7565\u4f18\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u8df5\u6a21\u5757\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents automatically.</li>\n    <li>It reduces the time and effort needed to set up agents by allowing easy integration of tools and adjustment to different environments.</li>\n    <li>The framework has two modes: one for standard tasks and another for complex tasks, which can generate necessary code and configurations on its own.</li>\n    <li>Youtu-Agent includes a system to help agents learn and improve without needing to change their core parameters.</li>\n    <li>Tests show Youtu-Agent performs very well, achieving top scores in various benchmarks and speeding up training significantly.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u5728\u89c6\u89c9\u4e0a\u771f\u5b9e\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u6307\u6807\uff08\u5982FVD\uff09\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u7565\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165MMGR\u6846\u67b6\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\uff08\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\uff09\u8fdb\u884c\u8bc4\u4f30\u3002</li>\n    <li>MMGR\u8bc4\u4f30\u6db5\u76d6\u4e09\u4e2a\u9886\u57df\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u5e76\u91c7\u7528\u7ec6\u81f4\u7684\u6307\u6807\u8bc4\u4f30\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff08ARC-AGI\u51c6\u786e\u7387\u4f4e\u4e8e10%\uff09\uff0c\u5e76\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u957f\u8fdc\u7a7a\u95f4\u89c4\u5212\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic videos, but they may not follow physical or logical rules effectively.</li>\n    <li>Current evaluation methods focus on visual quality but miss important reasoning errors.</li>\n    <li>MMGR is a new framework that assesses five reasoning skills: physical, logical, 3D spatial, 2D spatial, and temporal.</li>\n    <li>It tests models on different tasks, showing that many perform well on some tasks but poorly on others, especially in abstract reasoning.</li>\n    <li>MMGR identifies weaknesses in models and aims to improve the development of reasoning-aware video generation systems.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u79d1\u5b66AI\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u5bf9\u9f50\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6df1\u5ea6\u7814\u7a76\u3001\u60f3\u6cd5\u7684\u53ef\u884c\u6027\u548c\u5b9e\u9a8c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4ee5\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to think and work independently in science.</li>\n    <li>The authors present a definition of SGI based on the Practical Inquiry Model (PIM) and suggest four tasks that align with the work of scientists.</li>\n    <li>They created SGI-Bench, a benchmark with over 1,000 examples to test AI's performance on scientific questions and tasks.</li>\n    <li>Results show significant gaps in AI performance, such as low accuracy in deep research and issues with the feasibility of generated ideas.</li>\n    <li>The study introduces Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate novel hypotheses during tests, laying the groundwork for better AI in scientific discovery.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SemanticGen\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u66f4\u5feb\u4e14\u66f4\u9ad8\u6548\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u57fa\u51c6\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video generation methods use a VAE decoder but are slow and costly for long videos.</li>\n    <li>SemanticGen is a new approach that generates videos in a simpler, high-level semantic space first.</li>\n    <li>This method starts with a compact semantic video layout, then adds detailed features later.</li>\n    <li>Using this two-stage process leads to faster video generation and better efficiency.</li>\n    <li>SemanticGen creates high-quality videos and outperforms existing techniques in tests.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\uff0c\u964d\u4f4e\u6210\u672c\u3002</li>\n    <li>\u5f00\u53d1\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u6700\u65b0\u7684GUI\u6027\u80fd\u6807\u51c6\u3002</li>\n    <li>\u63a8\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002</li>\n    <li>\u5f15\u5165\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\uff0c\u6d4b\u8bd5\u4ee3\u7406\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002</li>\n    <li>\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models for automating graphical user interfaces (GUIs) are being developed to improve efficiency and reduce costs in training data collection.</li>\n    <li>A self-evolving training system has been created that ensures high-quality training data with over 90% accuracy and much lower costs.</li>\n    <li>Step-GUI models are achieving top performances in GUI tasks while also being versatile and capable of handling various functions.</li>\n    <li>A new protocol called GUI-MCP has been introduced to ensure user privacy during GUI automation, allowing sensitive data to remain on devices.</li>\n    <li>A benchmark called AndroidDaily has been established to test the effectiveness of these models in real-world mobile usage, showing promising results.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u5728\u63a8\u7406\u65f6\u901f\u5ea6\u8f83\u6162\u3002</li>\n    <li>\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u867d\u7136\u53ef\u4ee5\u5e76\u884c\u5904\u7406\uff0c\u4f46\u5b58\u5728\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u751f\u6210\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion\u662f\u4e00\u79cd\u65b0\u578b\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u5347\u5e76\u884c\u89e3\u7801\u7684\u65b9\u5f0f\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n    <li>\u5b83\u91c7\u7528\u201c\u8ba1\u5212\u548c\u586b\u5145\u201d\u7684\u89e3\u7801\u8fc7\u7a0b\uff0c\u5148\u8bc6\u522b\u5f31\u4f9d\u8d56\u7684\u5b50\u5e8f\u5217\uff0c\u7136\u540e\u5e76\u884c\u89e3\u7801\u3002</li>\n    <li>\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReFusion\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684MDMs\uff0c\u901f\u5ea6\u63d0\u9ad8\u8d85\u8fc718\u500d\uff0c\u5e76\u4e14\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u663e\u8457\u7f29\u5c0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive models (ARMs) are slow in generating results one step at a time.</li>\n    <li>Masked diffusion models (MDMs) are faster but have issues with high computation costs and poor generation quality.</li>\n    <li>ReFusion is a new MDM that improves efficiency by processing data in larger groups called slots instead of individual tokens.</li>\n    <li>This method uses a two-step process: first planning which slots to fill, and then filling them in parallel.</li>\n    <li>ReFusion outperforms previous MDMs by 34% and is over 18 times faster on average, while also being faster than traditional ARMs.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 09, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2602.10809", "authors": [{"_id": "698d8a1b65c0d15a6d1622db", "user": {"_id": "654c99d6e82a71cb487c2ecd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c99d6e82a71cb487c2ecd/hiMMOyh-3bAUaqnBM5yT4.jpeg", "isPro": false, "fullname": "ChenlongDeng", "user": "ChenlongDeng", "type": "user"}, "name": "Chenlong Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-13T09:37:24.735Z", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622dc", "name": "Mengjie Deng", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622dd", "name": "Junjie Wu", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622de", "name": "Dun Zeng", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622df", "name": "Teng Wang", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e0", "name": "Qingsong Xie", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e1", "user": {"_id": "6994899fe85a4b61cbc65813", "avatarUrl": "/avatars/2b0fce4d53c59eb06ef0b023485ed81f.svg", "isPro": false, "fullname": "Jiadeng Huang", "user": "Warden-H", "type": "user"}, "name": "Jiadeng Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:50:53.691Z", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e2", "name": "Shengjie Ma", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e3", "name": "Changwang Zhang", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e4", "name": "Zhaoxiang Wang", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e5", "name": "Jun Wang", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e6", "name": "Yutao Zhu", "hidden": false}, {"_id": "698d8a1b65c0d15a6d1622e7", "name": "Zhicheng Dou", "hidden": false}], "publishedAt": "2026-02-11T12:51:10.000Z", "submittedOnDailyAt": "2026-02-17T09:24:35.490Z", "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories", "submittedOnDailyBy": {"_id": "654c99d6e82a71cb487c2ecd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c99d6e82a71cb487c2ecd/hiMMOyh-3bAUaqnBM5yT4.jpeg", "isPro": false, "fullname": "ChenlongDeng", "user": "ChenlongDeng", "type": "user"}, "summary": "Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.", "upvotes": 29, "discussionId": "698d8a1b65c0d15a6d1622e8", "githubRepo": "https://github.com/RUC-NLPIR/DeepImageSearch", "githubRepoAddedBy": "user", "ai_summary": "DeepImageSearch presents an agentic approach to image retrieval that addresses limitations of traditional semantic matching by enabling multi-step reasoning over visual histories through a modular agent framework with dual-memory system.", "ai_keywords": ["multimodal retrieval systems", "visual streams", "temporal sequences", "agentic paradigm", "image retrieval", "visual history", "contextual cues", "DISBench", "vision-language models", "spatiotemporal associations", "modular agent framework", "dual-memory system", "long-horizon navigation"], "githubStars": 27, "organization": {"_id": "6695ed048765c1560ce56423", "name": "RUC-NLPIR", "fullname": "NLPIR Lab @ RUC", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625e62452a7279d3c77b5c38/CBwmyPCRzm4rHTGWhiCzR.jpeg"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u5728\u8bed\u4e49\u5339\u914d\u65b9\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u901a\u5e38\u5047\u8bbe\u67e5\u8be2\u4e0e\u56fe\u50cf\u76f8\u5173\u6027\u662f\u72ec\u7acb\u7684\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u89c6\u4e86\u73b0\u5b9e\u89c6\u89c9\u6d41\u4e2d\u7684\u4e30\u5bcc\u4f9d\u8d56\u5173\u7cfb\uff0c\u4fe1\u606f\u5f80\u5f80\u5206\u5e03\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DeepImageSearch\uff0c\u5c06\u56fe\u50cf\u68c0\u7d22\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\uff0c\u9700\u8981\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u3002</li>\n    <li>\u6784\u5efa\u4e86DISBench\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4e92\u8054\u89c6\u89c9\u6570\u636e\u7684\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u5e94\u5bf9\u4e0a\u4e0b\u6587\u4f9d\u8d56\u67e5\u8be2\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDISBench\u5bf9\u73b0\u6709\u6a21\u578b\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u5c06\u81ea\u4e3b\u63a8\u7406\u7eb3\u5165\u4e0b\u4e00\u4ee3\u68c0\u7d22\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current image retrieval systems focus on matching queries and images independently, missing out on the connections in visual sequences.</li>\n    <li>DeepImageSearch is a new approach that treats image retrieval as a task where models explore and reason over sequences of images.</li>\n    <li>DISBench is a new benchmark created to test these models using complex, interconnected visual data.</li>\n    <li>A collaborative process with humans and models is proposed to help find context in visual data before human checks are made.</li>\n    <li>Experiments show that DISBench presents tough challenges for existing models, emphasizing the need for advanced reasoning in retrieval systems.</li>\n</ul>"}, "publishedAt": "2026-02-11T07:51:10.000Z", "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories", "summary": "Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10809.png", "numComments": 1, "submittedBy": {"_id": "654c99d6e82a71cb487c2ecd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c99d6e82a71cb487c2ecd/hiMMOyh-3bAUaqnBM5yT4.jpeg", "fullname": "ChenlongDeng", "name": "ChenlongDeng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "6695ed048765c1560ce56423", "name": "RUC-NLPIR", "fullname": "NLPIR Lab @ RUC", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/625e62452a7279d3c77b5c38/CBwmyPCRzm4rHTGWhiCzR.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.13949", "authors": [{"_id": "69941b5e50fb2c0be4783de6", "user": {"_id": "62e1b3cb3eb0730f621a83f6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg", "isPro": false, "fullname": "Taiwei Shi", "user": "MaksimSTW", "type": "user"}, "name": "Taiwei Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:08:55.504Z", "hidden": false}, {"_id": "69941b5e50fb2c0be4783de7", "name": "Sihao Chen", "hidden": false}, {"_id": "69941b5e50fb2c0be4783de8", "name": "Bowen Jiang", "hidden": false}, {"_id": "69941b5e50fb2c0be4783de9", "user": {"_id": "64d660308ebc40443813f014", "avatarUrl": "/avatars/516bb2d2383be99794e366dfb41636b6.svg", "isPro": false, "fullname": "Linxin Song", "user": "linxinso", "type": "user"}, "name": "Linxin Song", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:08:53.295Z", "hidden": false}, {"_id": "69941b5e50fb2c0be4783dea", "name": "Longqi Yang", "hidden": false}, {"_id": "69941b5e50fb2c0be4783deb", "name": "Jieyu Zhao", "hidden": false}], "publishedAt": "2026-02-15T01:23:48.000Z", "submittedOnDailyAt": "2026-02-17T05:14:40.358Z", "title": "Experiential Reinforcement Learning", "submittedOnDailyBy": {"_id": "62e1b3cb3eb0730f621a83f6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg", "isPro": false, "fullname": "Taiwei Shi", "user": "MaksimSTW", "type": "user"}, "summary": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.", "upvotes": 26, "discussionId": "69941b5f50fb2c0be4783dec", "ai_summary": "Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs.", "ai_keywords": ["reinforcement learning", "environmental feedback", "policy training", "self-reflection", "behavioral revision", "exploration", "optimization"], "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u662f\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u73af\u5883\u53cd\u9988\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u53cd\u9988\u901a\u5e38\u7a00\u758f\u4e14\u5ef6\u8fdf\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4f53\u9a8c\u5f3a\u5316\u5b66\u4e60\uff08ERL\uff09\uff0c\u5b83\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u52a0\u5165\u4e86\u7ecf\u9a8c\u53cd\u601d\u4e0e\u5de9\u56fa\u7684\u5faa\u73af\u3002</li>\n    <li>\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u751f\u6210\u521d\u59cb\u5c1d\u8bd5\uff0c\u63a5\u6536\u53cd\u9988\u540e\u8fdb\u884c\u53cd\u601d\uff0c\u4ece\u800c\u6307\u5bfc\u66f4\u7cbe\u786e\u7684\u540e\u7eed\u5c1d\u8bd5\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u80fd\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684\u884c\u4e3a\u4fee\u6b63\uff0c\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u5e76\u7a33\u56fa\u4f18\u5316\u6548\u679c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cERL\u5728\u7a00\u758f\u5956\u52b1\u63a7\u5236\u73af\u5883\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning helps language models learn from feedback, but this feedback is often sparse and delayed.</li>\n    <li>Experiential Reinforcement Learning (ERL) introduces a loop of experience, reflection, and consolidation to improve learning from feedback.</li>\n    <li>In ERL, the model tries a task, receives feedback, reflects on it, and then makes a better attempt based on that reflection.</li>\n    <li>This method enhances learning efficiency and performance, achieving significant improvements in challenging environments.</li>\n    <li>ERL shows that adding self-reflection can effectively turn feedback into lasting improvements in behavior.</li>\n</ul>"}, "publishedAt": "2026-02-14T20:23:48.000Z", "title": "Experiential Reinforcement Learning", "summary": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13949.png", "numComments": 2, "submittedBy": {"_id": "62e1b3cb3eb0730f621a83f6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg", "fullname": "Taiwei Shi", "name": "MaksimSTW", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.14234", "authors": [{"_id": "6993dd2150fb2c0be4783cfa", "name": "Zheng Chu", "hidden": false}, {"_id": "6993dd2150fb2c0be4783cfb", "name": "Xiao Wang", "hidden": false}, {"_id": "6993dd2150fb2c0be4783cfc", "name": "Jack Hong", "hidden": false}, {"_id": "6993dd2150fb2c0be4783cfd", "name": "Huiming Fan", "hidden": false}, {"_id": "6993dd2150fb2c0be4783cfe", "name": "Yuqi Huang", "hidden": false}, {"_id": "6993dd2150fb2c0be4783cff", "name": "Yue Yang", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d00", "name": "Guohai Xu", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d01", "user": {"_id": "63fc5b724c57549ad5e54558", "avatarUrl": "/avatars/1374c1e8969533dd7543959666f16d1a.svg", "isPro": false, "fullname": "Chenxiao Zhao", "user": "ChenShawn", "type": "user"}, "name": "Chenxiao Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-17T17:17:24.619Z", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d02", "name": "Cheng Xiang", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d03", "name": "Shengchao Hu", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d04", "name": "Dongdong Kuang", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d05", "name": "Ming Liu", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d06", "name": "Bing Qin", "hidden": false}, {"_id": "6993dd2150fb2c0be4783d07", "name": "Xing Yu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/647416300da364bd0d009d20/u9_Uoq5LuqIoPS71iQFsn.png"], "publishedAt": "2026-02-15T17:04:46.000Z", "submittedOnDailyAt": "2026-02-17T00:50:04.355Z", "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents", "submittedOnDailyBy": {"_id": "647416300da364bd0d009d20", "avatarUrl": "/avatars/0474223271835611522a4eb488816e28.svg", "isPro": false, "fullname": "Xiao Wang", "user": "CherryDurian", "type": "user"}, "summary": "Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.", "upvotes": 17, "discussionId": "6993dd2250fb2c0be4783d08", "projectPage": "https://redsearchagent.github.io/index/", "githubRepo": "https://github.com/RedSearchAgent/REDSearcher", "githubRepoAddedBy": "user", "ai_summary": "REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.", "ai_keywords": ["task synthesis", "dual-constrained optimization", "graph topology", "evidence dispersion", "tool-augmented queries", "midtraining", "atomic capabilities", "knowledge", "planning", "function calling", "local simulated environment", "reinforcement learning", "search agents", "long-horizon tasks"], "githubStars": 9, "organization": {"_id": "6312d9c3830f549852f8e500", "name": "xiaohongshu", "fullname": "Xiaohongshu"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6b63\u4ece\u901a\u7528\u77e5\u8bc6\u5f15\u64ce\u8f6c\u5411\u73b0\u5b9e\u95ee\u9898\u89e3\u51b3\u8005\uff0c\u4f46\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u7684\u4f18\u5316\u4e0a\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u9ad8\u8d28\u91cf\u641c\u7d22\u8f68\u8ff9\u548c\u5956\u52b1\u4fe1\u53f7\u7684\u6781\u5ea6\u7a00\u758f\uff0c\u96be\u4ee5\u6784\u5efa\u53ef\u6269\u5c55\u7684\u957f\u671f\u4efb\u52a1\u3002</li>\n    <li>\u63d0\u51fa\u4e86REDSearcher\u6846\u67b6\uff0c\u8054\u5408\u8bbe\u8ba1\u590d\u6742\u4efb\u52a1\u5408\u6210\u3001\u4e2d\u671f\u8bad\u7ec3\u548c\u540e\u671f\u8bad\u7ec3\uff0c\u4ee5\u4f18\u5316\u641c\u7d22\u4ee3\u7406\u3002</li>\n    <li>\u901a\u8fc7\u53cc\u7ea6\u675f\u4f18\u5316\u6846\u67b6\u751f\u6210\u590d\u6742\u9ad8\u8d28\u91cf\u4efb\u52a1\uff0c\u9f13\u52b1\u4e3b\u52a8\u4f7f\u7528\u5de5\u5177\u800c\u4e0d\u662f\u88ab\u52a8\u56de\u5fc6\u3002</li>\n    <li>\u5c06\u53d1\u5e031\u4e07\u4e2a\u9ad8\u8d28\u91cf\u590d\u6742\u6587\u672c\u641c\u7d22\u8f68\u8ff9\u30015000\u4e2a\u591a\u6a21\u6001\u8f68\u8ff9\u548c1000\u4e2a\u6587\u672c\u5f3a\u5316\u5b66\u4e60\u67e5\u8be2\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are being improved to solve real-world problems, but optimizing them for search tasks is difficult.</li>\n    <li>The main issue is the lack of high-quality search examples and feedback due to complex task setups and costly tool interactions.</li>\n    <li>REDSearcher is a new framework that helps create complex tasks and improve training for better search agent performance.</li>\n    <li>It uses a special method to design tasks based on difficulty and introduces tools to help agents use resources more effectively.</li>\n    <li>The approach has been tested and shown to perform very well, and it will provide resources for future research, including thousands of search examples and code.</li>\n</ul>"}, "publishedAt": "2026-02-15T12:04:46.000Z", "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents", "summary": "Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/647416300da364bd0d009d20/u9_Uoq5LuqIoPS71iQFsn.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14234.png", "numComments": 1, "submittedBy": {"_id": "647416300da364bd0d009d20", "avatarUrl": "/avatars/0474223271835611522a4eb488816e28.svg", "fullname": "Xiao Wang", "name": "CherryDurian", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6312d9c3830f549852f8e500", "name": "xiaohongshu", "fullname": "Xiaohongshu"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14265", "authors": [{"_id": "6994205e50fb2c0be4783df6", "name": "Zachary Bamberger", "hidden": false}, {"_id": "6994205e50fb2c0be4783df7", "user": {"_id": "6568ee159c96f1a47bea2d53", "avatarUrl": "/avatars/e08cee5504dc55e4f7fa0ed5227f16d1.svg", "isPro": false, "fullname": "Till Saenger", "user": "TillRS", "type": "user"}, "name": "Till R. Saenger", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:08:48.423Z", "hidden": false}, {"_id": "6994205e50fb2c0be4783df8", "name": "Gilad Morad", "hidden": false}, {"_id": "6994205e50fb2c0be4783df9", "name": "Ofra Amir", "hidden": false}, {"_id": "6994205e50fb2c0be4783dfa", "name": "Brandon M. Stewart", "hidden": false}, {"_id": "6994205e50fb2c0be4783dfb", "name": "Amir Feder", "hidden": false}], "publishedAt": "2026-02-15T18:29:54.000Z", "submittedOnDailyAt": "2026-02-17T14:02:22.127Z", "title": "STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts", "submittedOnDailyBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "isPro": false, "fullname": "Eilam Shapira", "user": "EilamSha", "type": "user"}, "summary": "Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.", "upvotes": 16, "discussionId": "6994205e50fb2c0be4783dfc", "githubRepo": "https://github.com/zbambergerNLP/state-of-thoughts", "githubRepoAddedBy": "user", "ai_summary": "STATe presents an interpretable inference-time compute method that uses discrete textual interventions to generate diverse, high-quality, and explainable text by searching over reasoning patterns rather than relying on stochastic sampling.", "ai_keywords": ["Best-of-N", "Tree-of-Thoughts", "high-temperature sampling", "inference-time compute", "textual interventions", "action-guided generation", "reasoning patterns", "argument generation", "action sequences", "performance estimation"], "githubStars": 4, "summary_zh": "<ul>\n    <li>STATe-of-Thoughts (STATe)\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u65f6\u95f4\u8ba1\u7b97\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u8f93\u51fa\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002</li>\n    <li>STATe\u7528\u79bb\u6563\u548c\u53ef\u89e3\u91ca\u7684\u6587\u672c\u5e72\u9884\u4ee3\u66ff\u968f\u673a\u91c7\u6837\uff0c\u4f7f\u5f97\u63a8\u7406\u8fc7\u7a0b\u66f4\u6e05\u6670\u3002</li>\n    <li>\u8fd9\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\u53ef\u4ee5\u4ea7\u751f\u6bd4\u9ad8\u6e29\u91c7\u6837\u66f4\u5927\u7684\u54cd\u5e94\u591a\u6837\u6027\u3002</li>\n    <li>\u7edf\u8ba1\u884c\u52a8\u5e8f\u5217\u80fd\u591f\u6355\u6349\u5230\u4e0e\u8f93\u51fa\u8d28\u91cf\u9ad8\u5ea6\u76f8\u5173\u7684\u53ef\u89e3\u91ca\u7279\u5f81\u3002</li>\n    <li>STATe\u7684\u6846\u67b6\u80fd\u591f\u76f4\u63a5\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5bfb\u627e\u672a\u63a2\u7d22\u7684\u6709\u524d\u666f\u7684\u884c\u52a8\u533a\u57df\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STATe-of-Thoughts (STATe) is a new method designed to improve the quality and diversity of output in text generation.</li>\n    <li>It uses structured actions instead of random sampling, which helps create more varied responses.</li>\n    <li>In a study on argument generation, STATe showed that its action sequences can predict the quality of the output effectively.</li>\n    <li>By analyzing how actions affect performance, STATe can explore new options for generating better text.</li>\n    <li>STATe is a useful tool for creating high-quality, diverse, and understandable text outputs.</li>\n</ul>"}, "publishedAt": "2026-02-15T13:29:54.000Z", "title": "STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts", "summary": "Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14265.png", "numComments": 1, "submittedBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "fullname": "Eilam Shapira", "name": "EilamSha", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14492", "authors": [{"_id": "6993e4ac50fb2c0be4783d48", "user": {"_id": "67acb56faee29e2e5c1c41ec", "avatarUrl": "/avatars/b47094266fe70d10786da845ae2ade2c.svg", "isPro": false, "fullname": "Jiahao Yuan", "user": "Jhcircle", "type": "user"}, "name": "Jiahao Yuan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:54:02.776Z", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d49", "name": "Yike Xu", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d4a", "name": "Jinyong Wen", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d4b", "name": "Baokun Wang", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d4c", "name": "Ziyi Gao", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d4d", "name": "Xiaotong Lin", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d4e", "name": "Yun Liu", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d4f", "name": "Xing Fu", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d50", "name": "Yu Cheng", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d51", "name": "Yongchao Liu", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d52", "name": "Weiqiang Wang", "hidden": false}, {"_id": "6993e4ac50fb2c0be4783d53", "name": "Zhongle Xie", "hidden": false}], "publishedAt": "2026-02-16T06:09:31.000Z", "submittedOnDailyAt": "2026-02-17T04:09:21.228Z", "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model", "submittedOnDailyBy": {"_id": "67acb56faee29e2e5c1c41ec", "avatarUrl": "/avatars/b47094266fe70d10786da845ae2ade2c.svg", "isPro": false, "fullname": "Jiahao Yuan", "user": "Jhcircle", "type": "user"}, "summary": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.", "upvotes": 15, "discussionId": "6993e4ac50fb2c0be4783d54", "githubRepo": "https://github.com/JhCircle/Q-Anchor", "githubRepoAddedBy": "user", "ai_summary": "A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods.", "ai_keywords": ["UserU", "Q-Anchor Embedding", "dual-tower LLMs", "joint contrastive-autoregressive optimization", "Cluster-based Soft Prompt Tuning", "KV-cache-accelerated inference"], "githubStars": 2, "organization": {"_id": "67c1d682826160b28f778510", "name": "antgroup", "fullname": "Ant Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\u201cQuery-as-Anchor\u201d\uff0c\u7528\u4e8e\u52a8\u6001\u7528\u6237\u5efa\u6a21\uff0c\u63d0\u9ad8\u7528\u6237\u8868\u793a\u7684\u7075\u6d3b\u6027\u3002</li>\n    <li>\u6784\u5efa\u4e86UserU\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u591a\u79cd\u884c\u4e3a\u5e8f\u5217\u4e0e\u7528\u6237\u7406\u89e3\u8bed\u4e49\uff0c\u652f\u6301\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002</li>\n    <li>\u91c7\u7528Q-Anchor\u5d4c\u5165\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408\u5bf9\u6bd4-\u81ea\u56de\u5f52\u4f18\u5316\u5b9e\u73b0\u67e5\u8be2\u611f\u77e5\u7684\u7528\u6237\u8868\u793a\u3002</li>\n    <li>\u5f15\u5165\u57fa\u4e8e\u805a\u7c7b\u7684\u8f6f\u63d0\u793a\u8c03\u4f18\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u7279\u5b9a\u573a\u666f\u7684\u654f\u611f\u6027\u3002</li>\n    <li>\u572810\u4e2a\u652f\u4ed8\u5b9d\u5de5\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7ecf\u8fc7\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The new approach, Query-as-Anchor, improves user modeling by using dynamic, query-aware techniques instead of static representations.</li>\n    <li>It includes a large dataset called UserU, which connects user behavior data with understanding semantics.</li>\n    <li>The Q-Anchor Embedding architecture uses advanced encoding methods in large language models to create better user representations.</li>\n    <li>Cluster-based Soft Prompt Tuning helps align model focus with specific business scenarios for better accuracy.</li>\n    <li>The method has been tested successfully in real-world applications, showing excellent performance and scalability.</li>\n</ul>"}, "publishedAt": "2026-02-16T01:09:31.000Z", "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model", "summary": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14492.png", "numComments": 1, "submittedBy": {"_id": "67acb56faee29e2e5c1c41ec", "avatarUrl": "/avatars/b47094266fe70d10786da845ae2ade2c.svg", "fullname": "Jiahao Yuan", "name": "Jhcircle", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "67c1d682826160b28f778510", "name": "antgroup", "fullname": "Ant Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.14041", "authors": [{"_id": "6993d8d450fb2c0be4783ccc", "name": "Yuang Ai", "hidden": false}, {"_id": "6993d8d450fb2c0be4783ccd", "name": "Jiaming Han", "hidden": false}, {"_id": "6993d8d450fb2c0be4783cce", "name": "Shaobin Zhuang", "hidden": false}, {"_id": "6993d8d450fb2c0be4783ccf", "name": "Weijia Mao", "hidden": false}, {"_id": "6993d8d450fb2c0be4783cd0", "name": "Xuefeng Hu", "hidden": false}, {"_id": "6993d8d450fb2c0be4783cd1", "name": "Ziyan Yang", "hidden": false}, {"_id": "6993d8d450fb2c0be4783cd2", "name": "Zhenheng Yang", "hidden": false}, {"_id": "6993d8d450fb2c0be4783cd3", "name": "Huaibo Huang", "hidden": false}, {"_id": "6993d8d450fb2c0be4783cd4", "name": "Xiangyu Yue", "hidden": false}, {"_id": "6993d8d450fb2c0be4783cd5", "name": "Hao Chen", "hidden": false}], "publishedAt": "2026-02-15T08:09:05.000Z", "submittedOnDailyAt": "2026-02-17T02:18:25.240Z", "title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to 2^{256} states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.", "upvotes": 15, "discussionId": "6993d8d550fb2c0be4783cd6", "githubRepo": "https://github.com/shallowdream204/BitDance", "githubRepoAddedBy": "user", "ai_summary": "BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance.", "ai_keywords": ["autoregressive image generator", "binary visual tokens", "high-entropy binary latents", "binary diffusion head", "next-patch diffusion", "diffusion models", "FID", "parameter-efficient", "text-to-image generation", "photorealistic images", "image generation speedup"], "githubStars": 153, "summary_zh": "<ul>\n    <li>BitDance\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u5668\uff0c\u4f7f\u7528\u4e8c\u8fdb\u5236\u89c6\u89c9\u6807\u8bb0\u8fdb\u884c\u9884\u6d4b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u80fd\u591f\u8868\u793a\u591a\u8fbe2^256\u79cd\u72b6\u6001\uff0c\u5177\u6709\u7d27\u51d1\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u79bb\u6563\u8868\u793a\u3002</li>\n    <li>BitDance\u91c7\u7528\u4e8c\u8fdb\u5236\u6269\u6563\u5934\uff0c\u901a\u8fc7\u6269\u6563\u751f\u6210\u4e8c\u8fdb\u5236\u6807\u8bb0\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u4f20\u7edf\u7684\u7d22\u5f15\u9884\u6d4b\u3002</li>\n    <li>\u5f15\u5165\u7684\u65b0\u89e3\u7801\u65b9\u6cd5\u201c\u4e0b\u4e00\u4e2a\u8865\u4e01\u6269\u6563\u201d\u53ef\u4ee5\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u6807\u8bb0\uff0c\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002</li>\n    <li>\u5728\u751f\u62101024x1024\u56fe\u50cf\u65f6\uff0cBitDance\u7684\u901f\u5ea6\u6bd4\u4e4b\u524d\u7684\u81ea\u56de\u5f52\u6a21\u578b\u5feb\u8d85\u8fc730\u500d\uff0c\u5e76\u4e14\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>BitDance is a new image generator that predicts visual tokens using a unique method, allowing for a compact and expressive representation of images.</li>\n    <li>It uses a technique called binary diffusion to generate tokens, which improves the sampling process compared to traditional methods.</li>\n    <li>BitDance includes a new decoding method called next-patch diffusion, enabling it to predict multiple tokens at once quickly and accurately.</li>\n    <li>On the ImageNet dataset, BitDance outperforms other autoregressive models in image quality while using significantly fewer parameters and achieving faster speeds.</li>\n    <li>It also excels in text-to-image generation, producing high-quality images efficiently, and the code and models are available for further research.</li>\n</ul>"}, "publishedAt": "2026-02-15T03:09:05.000Z", "title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens", "summary": "We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to 2^{256} states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14041.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 232, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.07824", "authors": [{"_id": "6994358150fb2c0be4783e7d", "name": "Yiwei Qin", "hidden": false}, {"_id": "6994358150fb2c0be4783e7e", "name": "Zhen Huang", "hidden": false}, {"_id": "6994358150fb2c0be4783e7f", "name": "Tiantian Mi", "hidden": false}, {"_id": "6994358150fb2c0be4783e80", "name": "Weiye Si", "hidden": false}, {"_id": "6994358150fb2c0be4783e81", "name": "Chenyang Zhou", "hidden": false}, {"_id": "6994358150fb2c0be4783e82", "name": "Qipeng Guo", "hidden": false}, {"_id": "6994358150fb2c0be4783e83", "name": "Siyuan Feng", "hidden": false}, {"_id": "6994358150fb2c0be4783e84", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-02-08T05:06:34.000Z", "submittedOnDailyAt": "2026-02-17T07:08:00.845Z", "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training", "submittedOnDailyBy": {"_id": "683ebf283d659b032f260d27", "avatarUrl": "/avatars/4d34494c606b07b242313b4c0c6967eb.svg", "isPro": true, "fullname": "SII-Tiantian Mi", "user": "Mitiantian", "type": "user"}, "summary": "Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.", "upvotes": 14, "discussionId": "6994358150fb2c0be4783e85", "githubRepo": "https://github.com/GAIR-NLP/Data-Darwinism", "githubRepoAddedBy": "user", "ai_summary": "Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.", "ai_keywords": ["data-model co-evolution", "ten-level taxonomy", "generative refinement", "cognitive completion", "foundation models", "scientific literature", "continued pre-training", "domain-aligned tasks"], "githubStars": 12, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u8d28\u91cf\u5f71\u54cd\u57fa\u7840\u6a21\u578b\u7684\u8868\u73b0\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5904\u7406\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u6570\u636e\u8fbe\u5c14\u6587\u4e3b\u4e49\u201d\uff0c\u4e00\u79cd\u5341\u7ea7\u5206\u7c7b\u6cd5\uff0c\u63cf\u8ff0\u6570\u636e\u548c\u6a21\u578b\u7684\u5171\u540c\u8fdb\u5316\u3002</li>\n    <li>\u901a\u8fc7\u6784\u5efa900B\u4ee3\u5e01\u7684Darwin-Science\u8bed\u6599\u5e93\uff0c\u6211\u4eec\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7406\u8bba\uff0c\u5e76\u53d1\u73b0\u4e86\u79d1\u5b66\u6587\u672c\u4e2d\u7684\u5b66\u4e60\u5dee\u8ddd\u3002</li>\n    <li>\u6211\u4eec\u4f7f\u7528\u5148\u8fdb\u7684LLM\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u6027\u7cbe\u70bc\u548c\u8ba4\u77e5\u8865\u5168\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002</li>\n    <li>\u7ecf\u8fc7600B\u4ee3\u5e01\u7684\u9884\u8bad\u7ec3\uff0cDarwin-Science\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u91ca\u653e\u76f8\u5173\u8d44\u6e90\u4ee5\u4fc3\u8fdb\u5171\u540c\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data quality is crucial for the performance of foundation models, but there are no systematic frameworks to improve it.</li>\n    <li>We propose a ten-level system called Data Darwinism, which shows how data and models can evolve together.</li>\n    <li>We created a massive dataset from scientific literature called Darwin-Science, which contains 900 billion tokens.</li>\n    <li>We found that raw scientific text has gaps in learnability, which we addressed using advanced techniques to improve understanding.</li>\n    <li>Our models, trained on this refined data, significantly outperformed previous models, showing that higher-level processing can enhance data value.</li>\n</ul>"}, "publishedAt": "2026-02-08T00:06:34.000Z", "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training", "summary": "Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07824.png", "numComments": 1, "submittedBy": {"_id": "683ebf283d659b032f260d27", "avatarUrl": "/avatars/4d34494c606b07b242313b4c0c6967eb.svg", "fullname": "SII-Tiantian Mi", "name": "Mitiantian", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14699", "authors": [{"_id": "6993e6d950fb2c0be4783d7c", "name": "Muzhi Chen", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d7d", "name": "Xuanhe Zhou", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d7e", "name": "Wei Zhou", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d7f", "name": "Bangrui Xu", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d80", "name": "Surui Tang", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d81", "name": "Guoliang Li", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d82", "name": "Bingsheng He", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d83", "name": "Yeye He", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d84", "name": "Yitong Song", "hidden": false}, {"_id": "6993e6d950fb2c0be4783d85", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-02-16T12:39:46.000Z", "submittedOnDailyAt": "2026-02-17T01:28:08.285Z", "title": "Qute: Towards Quantum-Native Database", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.", "upvotes": 13, "discussionId": "6993e6d950fb2c0be4783d86", "githubRepo": "https://github.com/weAIDB/Qute", "githubRepoAddedBy": "user", "githubStars": 6, "summary_zh": "<ul>\n    <li>\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5b50\u6570\u636e\u5e93\uff08Qute\uff09\uff0c\u5c06\u91cf\u5b50\u8ba1\u7b97\u4f5c\u4e3a\u4e3b\u8981\u6267\u884c\u9009\u9879\u3002</li>\n    <li>\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6a21\u62df\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cQute\u5c06\u6269\u5c55\u7684SQL\u7f16\u8bd1\u6210\u9ad8\u6548\u7684\u91cf\u5b50\u7535\u8def\u3002</li>\n    <li>\u91c7\u7528\u6df7\u5408\u4f18\u5316\u5668\u52a8\u6001\u9009\u62e9\u91cf\u5b50\u548c\u7ecf\u5178\u6267\u884c\u8ba1\u5212\u3002</li>\n    <li>\u5f15\u5165\u9009\u62e9\u6027\u91cf\u5b50\u7d22\u5f15\u548c\u4fdd\u771f\u5b58\u50a8\u4ee5\u5e94\u5bf9\u5f53\u524d\u91cf\u5b50\u6bd4\u7279\u7684\u9650\u5236\u3002</li>\n    <li>\u5728\u771f\u5b9e\u7684\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u90e8\u7f72Qute\uff0c\u663e\u793a\u5176\u5728\u89c4\u6a21\u4e0a\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u539f\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper presents a new quantum database called Qute that allows direct quantum computation.</li>\n    <li>Qute translates an advanced version of SQL into efficient quantum circuits instead of simulating quantum algorithms on classical systems.</li>\n    <li>It uses a hybrid optimizer to choose between using quantum or classical execution plans based on the situation.</li>\n    <li>The database includes features like selective quantum indexing and storage solutions to handle current limitations of qubits.</li>\n    <li>Tests show that Qute performs better than traditional classical databases when run on an actual quantum processor, and an open-source prototype is available online.</li>\n</ul>"}, "publishedAt": "2026-02-16T07:39:46.000Z", "title": "Qute: Towards Quantum-Native Database", "summary": "This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14699.png", "numComments": 1, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14367", "authors": [{"_id": "699422d450fb2c0be4783e14", "name": "Shuofei Qiao", "hidden": false}, {"_id": "699422d450fb2c0be4783e15", "name": "Yunxiang Wei", "hidden": false}, {"_id": "699422d450fb2c0be4783e16", "name": "Xuehai Wang", "hidden": false}, {"_id": "699422d450fb2c0be4783e17", "name": "Bin Wu", "hidden": false}, {"_id": "699422d450fb2c0be4783e18", "name": "Boyang Xue", "hidden": false}, {"_id": "699422d450fb2c0be4783e19", "user": {"_id": "620b3bbb0668e435407c8d0a", "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg", "isPro": true, "fullname": "Ningyu Zhang", "user": "Ningyu", "type": "user"}, "name": "Ningyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:08:45.960Z", "hidden": false}, {"_id": "699422d450fb2c0be4783e1a", "name": "Hossein A. Rahmani", "hidden": false}, {"_id": "699422d450fb2c0be4783e1b", "name": "Yanshan Wang", "hidden": false}, {"_id": "699422d450fb2c0be4783e1c", "name": "Qiang Zhang", "hidden": false}, {"_id": "699422d450fb2c0be4783e1d", "name": "Keyan Ding", "hidden": false}, {"_id": "699422d450fb2c0be4783e1e", "name": "Jeff Z. Pan", "hidden": false}, {"_id": "699422d450fb2c0be4783e1f", "name": "Huajun Chen", "hidden": false}, {"_id": "699422d450fb2c0be4783e20", "name": "Emine Yilmaz", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6447800f30fa4ecb85ddad80/57QqW-zf15OPtwtDGuVa7.qt"], "publishedAt": "2026-02-16T00:40:31.000Z", "submittedOnDailyAt": "2026-02-17T06:09:43.596Z", "title": "InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem", "submittedOnDailyBy": {"_id": "6447800f30fa4ecb85ddad80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6447800f30fa4ecb85ddad80/NsmXIaMsWctmTNA7tFVkX.jpeg", "isPro": false, "fullname": "Shuofei Qiao", "user": "GoooDte", "type": "user"}, "summary": "The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.", "upvotes": 13, "discussionId": "699422d550fb2c0be4783e21", "projectPage": "http://innoeval.zjukg.cn/", "githubRepo": "https://github.com/zjunlp/InnoEval", "githubRepoAddedBy": "user", "ai_summary": "InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.", "ai_keywords": ["Large Language Models", "idea evaluation", "deep innovation evaluation framework", "knowledge-grounded reasoning", "multi-perspective reasoning", "heterogeneous deep knowledge search engine", "innovation review board", "multi-dimensional decoupled evaluation", "peer-reviewed submissions"], "githubStars": 9, "organization": {"_id": "6464cc20860abf030496e986", "name": "UniversityCollegeLondon", "fullname": "University College London", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6464cadff072e09adec1c372/OszOYithNLCeeNuMIjaoI.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4fc3\u8fdb\u4e86\u79d1\u5b66\u60f3\u6cd5\u7684\u4ea7\u751f\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u60f3\u6cd5\u7684\u8fdb\u5c55\u4e0d\u8db3\u3002</li>\n    <li>\u79d1\u5b66\u8bc4\u4f30\u9700\u8981\u77e5\u8bc6\u57fa\u7840\u3001\u96c6\u4f53\u8ba8\u8bba\u548c\u591a\u6807\u51c6\u51b3\u7b56\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u548c\u504f\u89c1\u3002</li>\n    <li>\u63d0\u51fa\u4e86InnoEval\uff0c\u4e00\u4e2a\u6df1\u5ea6\u521b\u65b0\u8bc4\u4f30\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u7684\u60f3\u6cd5\u8bc4\u4f30\u3002</li>\n    <li>InnoEval\u4f7f\u7528\u5f02\u6784\u6df1\u5ea6\u77e5\u8bc6\u641c\u7d22\u5f15\u64ce\uff0c\u4ece\u591a\u79cd\u5728\u7ebf\u6765\u6e90\u83b7\u53d6\u52a8\u6001\u8bc1\u636e\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cInnoEval\u5728\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\uff0c\u8bc4\u5224\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u9ad8\u5ea6\u4e00\u81f4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) have helped produce many new scientific ideas, but evaluating these ideas hasn't improved as much.</li>\n    <li>Effective evaluation of scientific ideas needs expert knowledge, group discussions, and consideration of multiple factors.</li>\n    <li>Current evaluation methods often lack depth, overlook important criteria, and can be biased.</li>\n    <li>The new framework, InnoEval, aims to improve idea evaluation by using a diverse knowledge search engine and a review board with varied academic expertise.</li>\n    <li>Tests show that InnoEval performs better than existing methods in various evaluation tasks and aligns closely with human expert opinions.</li>\n</ul>"}, "publishedAt": "2026-02-15T19:40:31.000Z", "title": "InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem", "summary": "The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6447800f30fa4ecb85ddad80/57QqW-zf15OPtwtDGuVa7.qt"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14367.png", "numComments": 1, "submittedBy": {"_id": "6447800f30fa4ecb85ddad80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6447800f30fa4ecb85ddad80/NsmXIaMsWctmTNA7tFVkX.jpeg", "fullname": "Shuofei Qiao", "name": "GoooDte", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "6464cc20860abf030496e986", "name": "UniversityCollegeLondon", "fullname": "University College London", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6464cadff072e09adec1c372/OszOYithNLCeeNuMIjaoI.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.13367", "authors": [{"_id": "699431ed50fb2c0be4783e65", "name": "Chen Yang", "hidden": false}, {"_id": "699431ed50fb2c0be4783e66", "name": "Guangyue Peng", "hidden": false}, {"_id": "699431ed50fb2c0be4783e67", "name": "Jiaying Zhu", "hidden": false}, {"_id": "699431ed50fb2c0be4783e68", "name": "Ran Le", "hidden": false}, {"_id": "699431ed50fb2c0be4783e69", "name": "Ruixiang Feng", "hidden": false}, {"_id": "699431ed50fb2c0be4783e6a", "name": "Tao Zhang", "hidden": false}, {"_id": "699431ed50fb2c0be4783e6b", "name": "Xiyun Xu", "hidden": false}, {"_id": "699431ed50fb2c0be4783e6c", "name": "Yang Song", "hidden": false}, {"_id": "699431ed50fb2c0be4783e6d", "name": "Yiming Jia", "hidden": false}, {"_id": "699431ed50fb2c0be4783e6e", "name": "Yuntao Wen", "hidden": false}, {"_id": "699431ed50fb2c0be4783e6f", "name": "Yunzhi Xu", "hidden": false}, {"_id": "699431ed50fb2c0be4783e70", "name": "Zekai Wang", "hidden": false}, {"_id": "699431ed50fb2c0be4783e71", "name": "Zhenwei An", "hidden": false}, {"_id": "699431ed50fb2c0be4783e72", "name": "Zhicong Sun", "hidden": false}, {"_id": "699431ed50fb2c0be4783e73", "name": "Zongchao Chen", "hidden": false}], "publishedAt": "2026-02-13T13:10:46.000Z", "submittedOnDailyAt": "2026-02-17T08:33:06.878Z", "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts", "submittedOnDailyBy": {"_id": "6947f69751d7ae7c3c7b6908", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PuIDZB9XDShHohKhYmdmp.png", "isPro": true, "fullname": "Ben Kelly", "user": "YellowjacketGames", "type": "user"}, "summary": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.", "upvotes": 13, "discussionId": "699431ed50fb2c0be4783e74", "projectPage": "https://huggingface.co/Nanbeige/Nanbeige4.1-3B", "ai_summary": "Nanbeige4.1-3B is a 3B-parameter unified language model that demonstrates superior performance in agentic behavior, code generation, and reasoning compared to larger models through advanced reward modeling and training techniques.", "ai_keywords": ["unified generalist language model", "reward modeling", "reinforcement learning", "tool-call turns", "deep search", "complex data synthesis", "turn-level supervision", "point-wise reward modeling", "pair-wise reward modeling", "code generation", "general reasoning", "agentic behavior", "human-aligned responses", "model optimization"], "organization": {"_id": "6533c00a9860c1cb37bff25f", "name": "Nanbeige", "fullname": "Nanbeige LLM Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646f0d118ff94af23bc44aab/GXHCollpMRgvYqUXQ2BQ7.png"}, "summary_zh": "<ul>\n    <li>Nanbeige4.1-3B\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u5f3a\u5927\u7684\u667a\u80fd\u884c\u4e3a\u3001\u4ee3\u7801\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53c2\u6570\u4ec5\u4e3a3\u4ebf\u3002</li>\n    <li>\u5b83\u662f\u7b2c\u4e00\u4e2a\u5f00\u6e90\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u5982\u6b64\u591a\u7684\u529f\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u5408\u70b9\u5bf9\u70b9\u548c\u6210\u5bf9\u5956\u52b1\u5efa\u6a21\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u548c\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002</li>\n    <li>\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\uff0c\u91c7\u7528\u4e86\u590d\u6742\u5ea6\u611f\u77e5\u5956\u52b1\u6765\u4f18\u5316\u6b63\u786e\u6027\u548c\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNanbeige4.1-3B\u5728\u6027\u80fd\u4e0a\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u66f4\u5927\u578b\u7684\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Nanbeige4.1-3B is a small language model with 3 billion parameters that can perform well in various tasks like reasoning, code generation, and agent-like behavior.</li>\n    <li>It is the first open-source model of its size to show such versatility in capabilities.</li>\n    <li>The model uses advanced reward modeling to ensure its responses are high-quality and aligned with human preferences.</li>\n    <li>It can handle complex problem-solving by executing up to 600 tool-call turns effectively.</li>\n    <li>Nanbeige4.1-3B outperforms other similar-sized models and even larger ones in many tasks, showcasing the potential of small language models.</li>\n</ul>"}, "publishedAt": "2026-02-13T08:10:46.000Z", "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts", "summary": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13367.png", "numComments": 1, "submittedBy": {"_id": "6947f69751d7ae7c3c7b6908", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PuIDZB9XDShHohKhYmdmp.png", "fullname": "Ben Kelly", "name": "YellowjacketGames", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "6533c00a9860c1cb37bff25f", "name": "Nanbeige", "fullname": "Nanbeige LLM Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646f0d118ff94af23bc44aab/GXHCollpMRgvYqUXQ2BQ7.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.10604", "authors": [{"_id": "698d417065c0d15a6d162026", "name": "Ailin Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162027", "name": "Ang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162028", "name": "Aobo Kong", "hidden": false}, {"_id": "698d417065c0d15a6d162029", "name": "Bin Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202a", "name": "Binxing Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d16202b", "name": "Bo Dong", "hidden": false}, {"_id": "698d417065c0d15a6d16202c", "name": "Bojun Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202d", "name": "Boyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16202e", "name": "Brian Li", "hidden": false}, {"_id": "698d417065c0d15a6d16202f", "name": "Buyun Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162030", "name": "Chang Su", "hidden": false}, {"_id": "698d417065c0d15a6d162031", "name": "Changxin Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162032", "name": "Changyi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162033", "name": "Chao Lou", "hidden": false}, {"_id": "698d417065c0d15a6d162034", "name": "Chen Hu", "hidden": false}, {"_id": "698d417065c0d15a6d162035", "name": "Chen Xu", "hidden": false}, {"_id": "698d417065c0d15a6d162036", "name": "Chenfeng Yu", "hidden": false}, {"_id": "698d417065c0d15a6d162037", "name": "Chengting Feng", "hidden": false}, {"_id": "698d417065c0d15a6d162038", "name": "Chengyuan Yao", "hidden": false}, {"_id": "698d417065c0d15a6d162039", "name": "Chunrui Han", "hidden": false}, {"_id": "698d417065c0d15a6d16203a", "name": "Dan Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203b", "name": "Dapeng Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16203c", "name": "Daxin Jiang", "hidden": false}, {"_id": "698d417065c0d15a6d16203d", "name": "Dehua Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203e", "name": "Deshan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16203f", "name": "Di Qi", "hidden": false}, {"_id": "698d417065c0d15a6d162040", "name": "Enle Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162041", "name": "Fajie Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162042", "name": "Fanqi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162043", "name": "Guanzhe Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162044", "name": "Gulin Yan", "hidden": false}, {"_id": "698d417065c0d15a6d162045", "name": "Guoliang Cao", "hidden": false}, {"_id": "698d417065c0d15a6d162046", "name": "Guopeng Li", "hidden": false}, {"_id": "698d417065c0d15a6d162047", "name": "Han Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162048", "name": "Hangyu Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162049", "user": {"_id": "64b7874b9f5987572ca28461", "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg", "isPro": false, "fullname": "hanshanzhang", "user": "brain-zhang", "type": "user"}, "name": "Hanshan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:52.602Z", "hidden": false}, {"_id": "698d417065c0d15a6d16204a", "name": "Hao Nie", "hidden": false}, {"_id": "698d417065c0d15a6d16204b", "name": "Haonan Jia", "hidden": false}, {"_id": "698d417065c0d15a6d16204c", "name": "Haoran Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204d", "name": "Hebin Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16204e", "name": "Hekun Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204f", "name": "Heng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162050", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "698d417065c0d15a6d162051", "name": "Hongbo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162052", "name": "Hongbo Peng", "hidden": false}, {"_id": "698d417065c0d15a6d162053", "name": "Hongyu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162054", "name": "Hongyuan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162055", "name": "Houyong Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162056", "name": "Huangxi Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d162057", "name": "Huimin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d162058", "name": "Huiyong Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162059", "name": "Jia Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16205a", "name": "Jian Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16205b", "name": "Jianjian Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16205c", "name": "Jiaoren Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16205d", "name": "Jiaran Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16205e", "name": "Jiashu Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16205f", "name": "Jiashuo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162060", "name": "Jiayi Fu", "hidden": false}, {"_id": "698d417065c0d15a6d162061", "name": "Jiayu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162062", "name": "Jie Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162063", "name": "Jie Luo", "hidden": false}, {"_id": "698d417065c0d15a6d162064", "name": "Jie Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162065", "name": "Jie Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162066", "name": "Jieyi Hou", "hidden": false}, {"_id": "698d417065c0d15a6d162067", "name": "Jing Bai", "hidden": false}, {"_id": "698d417065c0d15a6d162068", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:37.335Z", "hidden": false}, {"_id": "698d417065c0d15a6d162069", "name": "Jingjing Xie", "hidden": false}, {"_id": "698d417065c0d15a6d16206a", "name": "Jingwei Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16206b", "name": "Jingyang Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16206c", "name": "Jishi Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16206d", "name": "Junfeng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16206e", "name": "Junzhe Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16206f", "name": "Ka Man Lo", "hidden": false}, {"_id": "698d417065c0d15a6d162070", "name": "Kai Liang", "hidden": false}, {"_id": "698d417065c0d15a6d162071", "name": "Kaibo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162072", "name": "Kaijun Tan", "hidden": false}, {"_id": "698d417065c0d15a6d162073", "user": {"_id": "66668c591964b6188ee310c2", "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg", "isPro": false, "fullname": "Kaiwen Yan", "user": "linrany", "type": "user"}, "name": "Kaiwen Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:58.524Z", "hidden": false}, {"_id": "698d417065c0d15a6d162074", "name": "Kaixiang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162075", "name": "Kang An", "hidden": false}, {"_id": "698d417065c0d15a6d162076", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:56.339Z", "hidden": false}, {"_id": "698d417065c0d15a6d162077", "name": "Lei Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162078", "name": "Liang Lv", "hidden": false}, {"_id": "698d417065c0d15a6d162079", "name": "Liang Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16207a", "name": "Liangyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207b", "name": "Lieyu Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16207c", "name": "Liguo Tan", "hidden": false}, {"_id": "698d417065c0d15a6d16207d", "name": "Lin Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16207e", "name": "Lina Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207f", "name": "Luck Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162080", "name": "Mengqiang Ren", "hidden": false}, {"_id": "698d417065c0d15a6d162081", "name": "Michael Li", "hidden": false}, {"_id": "698d417065c0d15a6d162082", "name": "Ming Li", "hidden": false}, {"_id": "698d417065c0d15a6d162083", "name": "Mingliang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162084", "name": "Mingming Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162085", "name": "Mingrui Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162086", "name": "Mitt Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162087", "name": "Na Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162088", "name": "Peng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162089", "name": "Qi Han", "hidden": false}, {"_id": "698d417065c0d15a6d16208a", "name": "Qian Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16208b", "name": "Qinglin He", "hidden": false}, {"_id": "698d417065c0d15a6d16208c", "name": "Qinxin Du", "hidden": false}, {"_id": "698d417065c0d15a6d16208d", "name": "Qiuping Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16208e", "name": "Quan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16208f", "name": "Rongqiu Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162090", "name": "Ruihang Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162091", "name": "Ruixin Han", "hidden": false}, {"_id": "698d417065c0d15a6d162092", "name": "Ruosi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162093", "name": "Ruyan Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162094", "name": "Shan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162095", "name": "Shaoliang Pang", "hidden": false}, {"_id": "698d417065c0d15a6d162096", "name": "Shaowen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162097", "name": "Shengjie Fan", "hidden": false}, {"_id": "698d417065c0d15a6d162098", "name": "Shijie Shang", "hidden": false}, {"_id": "698d417065c0d15a6d162099", "name": "Shiliang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d16209a", "name": "Shiwei Li", "hidden": false}, {"_id": "698d417065c0d15a6d16209b", "name": "Shuangshuang Tian", "hidden": false}, {"_id": "698d417065c0d15a6d16209c", "name": "Siqi Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16209d", "name": "Siye Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16209e", "name": "Siyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16209f", "name": "Song Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a0", "name": "Tiancheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620a1", "name": "Tianchi Yue", "hidden": false}, {"_id": "698d417065c0d15a6d1620a2", "name": "Tianhao Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d1620a3", "name": "Tianning Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620a4", "name": "Tingdan Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620a5", "name": "Wang You", "hidden": false}, {"_id": "698d417065c0d15a6d1620a6", "name": "Wei Ji", "hidden": false}, {"_id": "698d417065c0d15a6d1620a7", "name": "Wei Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a8", "name": "Wei Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620a9", "name": "Weibo Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620aa", "user": {"_id": "6657620ea496f7fcb67c3871", "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg", "isPro": false, "fullname": "xieweihao", "user": "chalengr", "type": "user"}, "name": "Weihao Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:48.216Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ab", "name": "Wen Sun", "hidden": false}, {"_id": "698d417065c0d15a6d1620ac", "name": "Wenjin Deng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ad", "user": {"_id": "650c04795510464e85b47470", "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg", "isPro": false, "fullname": "wen", "user": "zhengwenzhen", "type": "user"}, "name": "Wenzhen Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:45.930Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ae", "name": "Wuxun Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620af", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b0", "name": "Xiangwen Kong", "hidden": false}, {"_id": "698d417065c0d15a6d1620b1", "name": "Xiangyu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b2", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b3", "name": "Xiaobo Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b4", "name": "Xiaojia Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b5", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620b6", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d1620b7", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620b8", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b9", "name": "Xin Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ba", "name": "Xin Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bb", "name": "Xin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bc", "name": "Xing Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620bd", "name": "Xingping Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620be", "name": "Xinran Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620bf", "name": "Xu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620c0", "user": {"_id": "64ec5b64bfb2aa06a46ff2d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg", "isPro": false, "fullname": "xuan he", "user": "tpa115k31", "type": "user"}, "name": "Xuan He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:36.240Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620c1", "name": "Xuanti Feng", "hidden": false}, {"_id": "698d417065c0d15a6d1620c2", "name": "Xuedan Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c3", "name": "Xuqiang Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620c4", "name": "Yanbo Yu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c5", "name": "Yang Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620c6", "name": "Yang Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c7", "name": "Yanlin Lai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c8", "name": "Yanming Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c9", "name": "Yaoyu Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ca", "name": "Yeqing Shen", "hidden": false}, {"_id": "698d417065c0d15a6d1620cb", "name": "Yibo Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d1620cc", "name": "Yichen Lv", "hidden": false}, {"_id": "698d417065c0d15a6d1620cd", "name": "Yicheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620ce", "name": "Yifeng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620cf", "name": "Yijing Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d0", "name": "Yikun Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d1", "name": "Yin Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d2", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d3", "name": "Yinmin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d4", "name": "Yitong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d5", "name": "Yixuan Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d6", "name": "Yiyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620d7", "name": "Yongchi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d8", "name": "Yongshen Long", "hidden": false}, {"_id": "698d417065c0d15a6d1620d9", "name": "Yongyao Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620da", "name": "Yousong Guan", "hidden": false}, {"_id": "698d417065c0d15a6d1620db", "name": "Yu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620dc", "name": "Yuang Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620dd", "name": "Yuanhao Ding", "hidden": false}, {"_id": "698d417065c0d15a6d1620de", "name": "Yuantao Fan", "hidden": false}, {"_id": "698d417065c0d15a6d1620df", "name": "Yuanzhen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e0", "name": "Yuchu Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620e1", "name": "Yudi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e2", "name": "Yue Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620e3", "name": "Yueqiang Lin", "hidden": false}, {"_id": "698d417065c0d15a6d1620e4", "name": "Yufan Lu", "hidden": false}, {"_id": "698d417065c0d15a6d1620e5", "name": "Yuling Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e6", "name": "Yunzhou Ju", "hidden": false}, {"_id": "698d417065c0d15a6d1620e7", "name": "Yurong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e8", "name": "Yusheng Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620e9", "name": "Yuxiang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ea", "name": "Yuyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620eb", "name": "Yuzhu Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620ec", "name": "Zejia Weng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ed", "name": "Zetao Hong", "hidden": false}, {"_id": "698d417065c0d15a6d1620ee", "name": "Zexi Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ef", "name": "Zhe Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620f0", "name": "Zheng Ge", "hidden": false}, {"_id": "698d417065c0d15a6d1620f1", "name": "Zheng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620f2", "name": "Zheng Zeng", "hidden": false}, {"_id": "698d417065c0d15a6d1620f3", "user": {"_id": "63607ace9ddc44e710e13f0f", "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg", "isPro": false, "fullname": "zy", "user": "lu-vae", "type": "user"}, "name": "Zhenyi Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:50.532Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620f4", "name": "Zhewei Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f5", "name": "Zhichao Chang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f6", "name": "Zhiguo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f7", "name": "Zhiheng Hu", "hidden": false}, {"_id": "698d417065c0d15a6d1620f8", "name": "Zidong Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f9", "name": "Zili Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fa", "name": "Ziqi Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620fb", "name": "Zixin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fc", "name": "Zixuan Wang", "hidden": false}], "publishedAt": "2026-02-11T07:53:51.000Z", "submittedOnDailyAt": "2026-02-12T00:26:49.880Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "upvotes": 150, "discussionId": "698d417165c0d15a6d1620fd", "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash", "githubRepoAddedBy": "user", "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.", "ai_keywords": ["Mixture-of-Experts", "sparse MoE", "foundation model", "active parameters", "interleaved attention", "sliding-window attention", "full attention", "Multi-Token Prediction", "reinforcement learning", "verifiable signals", "preference feedback", "off-policy training", "self-improvement", "IMO-AnswerBench", "LiveCodeBench", "tau2-Bench", "BrowseComp", "Terminal-Bench"], "githubStars": 1245, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Step 3.5 Flash\uff0c\u4e00\u4e2a\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u667a\u80fd\u4ee3\u7406\u7684\u63a8\u7406\u548c\u6267\u884c\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u62e5\u6709196\u4ebf\u53c2\u6570\u7684\u57fa\u7840\u548c110\u4ebf\u6d3b\u8dc3\u53c2\u6570\uff0c\u4f18\u5316\u4e86\u591a\u8f6e\u4ea4\u4e92\u7684\u5ef6\u8fdf\u548c\u6210\u672c\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u4fe1\u53f7\u548c\u504f\u597d\u53cd\u9988\uff0c\u786e\u4fdd\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>Step 3.5 Flash\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f8b\u5982\u6570\u5b66\u3001\u7f16\u7801\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u6027\u80fd\u4e0e\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u6548\u7387\u524d\u6cbf\uff0c\u63d0\u4f9b\u4e86\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u590d\u6742\u4ee3\u7406\u7684\u9ad8\u5bc6\u5ea6\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Step 3.5 Flash is a new model that combines smart decision-making with efficient computing.</li>\n    <li>It uses 196 billion parameters for its core and activates 11 billion for quick responses.</li>\n    <li>The model is designed to work better and faster during multiple interactions, reducing costs and delays.</li>\n    <li>It employs a unique learning system that helps it improve consistently in tasks like math, coding, and using tools.</li>\n    <li>Step 3.5 Flash performs well in various benchmarks, showing results similar to advanced models like GPT-5.2 and Gemini 3.0 Pro.</li>\n</ul>"}, "publishedAt": "2026-02-11T02:53:51.000Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 231, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.12783", "authors": [{"_id": "6992ceae50fb2c0be47839c1", "name": "Yuejie Li", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c2", "name": "Ke Yang", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c3", "name": "Yueying Hua", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c4", "user": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "isPro": false, "fullname": "berlin", "user": "berlin8587", "type": "user"}, "name": "Berlin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-16T15:36:03.590Z", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c5", "name": "Jianhao Nie", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c6", "name": "Yueping He", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c7", "name": "Caixin Kang", "hidden": false}], "publishedAt": "2026-02-13T10:08:27.000Z", "submittedOnDailyAt": "2026-02-16T14:32:15.752Z", "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "submittedOnDailyBy": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "isPro": false, "fullname": "berlin", "user": "berlin8587", "type": "user"}, "summary": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "upvotes": 132, "discussionId": "6992ceae50fb2c0be47839c8", "githubRepo": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval", "githubRepoAddedBy": "user", "githubStars": 96, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86SQuTR\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bed\u97f3\u67e5\u8be2\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u57fa\u51c6\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>SQuTR\u5305\u542b\u6765\u81ea\u516d\u4e2a\u5e38\u7528\u82f1\u8bed\u548c\u4e2d\u6587\u6587\u672c\u68c0\u7d22\u6570\u636e\u96c6\u768437,317\u4e2a\u72ec\u7279\u67e5\u8be2\uff0c\u8986\u76d6\u591a\u4e2a\u9886\u57df\u548c\u67e5\u8be2\u7c7b\u578b\u3002</li>\n    <li>\u5408\u6210\u4e86200\u540d\u771f\u5b9e\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\uff0c\u5e76\u5728\u63a7\u5236\u7684\u4fe1\u566a\u6bd4\u4e0b\u6df7\u540817\u79cd\u771f\u5b9e\u73af\u5883\u566a\u58f0\uff0c\u4ee5\u8bc4\u4f30\u9c81\u68d2\u6027\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u968f\u7740\u566a\u58f0\u589e\u52a0\uff0c\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\uff0c\u5404\u7cfb\u7edf\u7684\u4e0b\u964d\u5e45\u5ea6\u5dee\u5f02\u660e\u663e\u3002</li>\n    <li>SQuTR\u4e3a\u8bed\u97f3\u67e5\u8be2\u5230\u6587\u672c\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u7684\u6df1\u5165\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Spoken query retrieval is essential for modern information retrieval, but existing tests are too simple and do not reflect real-world noise conditions.</li>\n    <li>The new benchmark called SQuTR includes a large dataset with 37,317 queries from English and Chinese, covering various topics and query types.</li>\n    <li>SQuTR uses voice samples from 200 speakers and mixes in 17 types of background noise to test systems under different sound conditions.</li>\n    <li>Tests show that as noise increases, retrieval performance drops significantly, with varying impacts on different systems.</li>\n    <li>SQuTR serves as a tool for evaluating and improving the robustness of spoken query retrieval systems in noisy environments.</li>\n</ul>"}, "publishedAt": "2026-02-13T05:08:27.000Z", "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "summary": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12783.png", "numComments": 1, "submittedBy": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "fullname": "berlin", "name": "berlin8587", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12036", "authors": [{"_id": "698eeb55cace060ff123af56", "user": {"_id": "64e2d169d2af12910d682130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg", "isPro": false, "fullname": "xuxin", "user": "xx18", "type": "user"}, "name": "Xin Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-13T09:36:02.476Z", "hidden": false}, {"_id": "698eeb55cace060ff123af57", "name": "Clive Bai", "hidden": false}, {"_id": "698eeb55cace060ff123af58", "name": "Kai Yang", "hidden": false}, {"_id": "698eeb55cace060ff123af59", "name": "Tianhao Chen", "hidden": false}, {"_id": "698eeb55cace060ff123af5a", "name": "Yangkun Chen", "hidden": false}, {"_id": "698eeb55cace060ff123af5b", "name": "Weijie Liu", "hidden": false}, {"_id": "698eeb55cace060ff123af5c", "name": "Hao Chen", "hidden": false}, {"_id": "698eeb55cace060ff123af5d", "name": "Yang Wang", "hidden": false}, {"_id": "698eeb55cace060ff123af5e", "name": "Saiyong Yang", "hidden": false}, {"_id": "698eeb55cace060ff123af5f", "name": "Can Yang", "hidden": false}], "publishedAt": "2026-02-12T15:03:37.000Z", "submittedOnDailyAt": "2026-02-13T06:44:41.991Z", "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models", "submittedOnDailyBy": {"_id": "64e2d169d2af12910d682130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg", "isPro": false, "fullname": "xuxin", "user": "xx18", "type": "user"}, "summary": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.", "upvotes": 81, "discussionId": "698eeb55cace060ff123af60", "githubRepo": "https://github.com/XinXU-USTC/Composition-RL", "githubRepoAddedBy": "user", "ai_summary": "Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "verifiable prompts", "pass rate", "compositional prompts", "curriculum learning", "cross-domain RL"], "githubStars": 3, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u63d0\u793a\u662f\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6210\u529f\u7684\u57fa\u7840\uff0c\u4f46\u5305\u542b\u8bb8\u591a\u65e0\u4fe1\u606f\u7684\u4f8b\u5b50\uff0c\u6269\u5c55\u6210\u672c\u9ad8\u3002</li>\n    <li>\u7814\u7a76\u8005\u5173\u6ce8\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f18\u5148\u8003\u8651\u90a3\u4e9b\u901a\u8fc7\u7387\u4e3a0\u7684\u56f0\u96be\u63d0\u793a\u3002</li>\n    <li>\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\uff0c\u5bb9\u6613\u7684\u901a\u8fc7\u7387\u4e3a1\u7684\u63d0\u793a\u8d8a\u6765\u8d8a\u591a\uff0c\u5bfc\u81f4\u6709\u6548\u6570\u636e\u91cf\u51cf\u5c11\u3002</li>\n    <li>\u63d0\u51fa\u4e86Composition-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u591a\u4e2a\u95ee\u9898\u7ec4\u5408\u6210\u65b0\u7684\u53ef\u9a8c\u8bc1\u95ee\u9898\u6765\u66f4\u597d\u5730\u5229\u7528\u6709\u9650\u7684\u63d0\u793a\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cComposition-RL\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u652f\u6301\u8de8\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large-scale prompts are important for Reinforcement Learning with Verifiable Rewards (RLVR), but many examples are not useful and expanding them is expensive.</li>\n    <li>Recent research suggests focusing on difficult prompts that have a rollout pass rate of 0 to make better use of limited training data.</li>\n    <li>Easy prompts with a pass rate of 1 become more common during training, which can reduce the amount of effective data available.</li>\n    <li>To address this, Composition-RL combines multiple problems into new questions to enhance training with these verifiable prompts.</li>\n    <li>Experiments show that Composition-RL improves reasoning skills, and a curriculum version of it enhances performance by gradually increasing complexity.</li>\n</ul>"}, "publishedAt": "2026-02-12T10:03:37.000Z", "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models", "summary": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12036.png", "numComments": 1, "submittedBy": {"_id": "64e2d169d2af12910d682130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg", "fullname": "xuxin", "name": "xx18", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12205", "authors": [{"_id": "698ea0f9cace060ff123ae3a", "name": "Dianyi Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3b", "name": "Ruihang Li", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3c", "name": "Feng Han", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3d", "name": "Chaofan Ma", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3e", "name": "Wei Song", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3f", "name": "Siyuan Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae40", "name": "Yibin Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae41", "name": "Yi Xin", "hidden": false}, {"_id": "698ea0f9cace060ff123ae42", "name": "Hongjian Liu", "hidden": false}, {"_id": "698ea0f9cace060ff123ae43", "name": "Zhixiong Zhang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae44", "name": "Shengyuan Ding", "hidden": false}, {"_id": "698ea0f9cace060ff123ae45", "name": "Tianhang Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae46", "name": "Zhenglin Cheng", "hidden": false}, {"_id": "698ea0f9cace060ff123ae47", "name": "Tao Lin", "hidden": false}, {"_id": "698ea0f9cace060ff123ae48", "name": "Cheng Jin", "hidden": false}, {"_id": "698ea0f9cace060ff123ae49", "name": "Kaicheng Yu", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4a", "name": "Jingjing Chen", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4b", "name": "Wenjie Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4c", "name": "Zhongyu Wei", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4d", "name": "Jiaqi Wang", "hidden": false}], "publishedAt": "2026-02-12T17:44:24.000Z", "submittedOnDailyAt": "2026-02-13T03:37:56.296Z", "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing", "submittedOnDailyBy": {"_id": "64b4eec4faa3181a5eab9c46", "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg", "isPro": true, "fullname": "Jiaqi Wang", "user": "myownskyW7", "type": "user"}, "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.", "upvotes": 60, "discussionId": "698ea0f9cace060ff123ae4e", "projectPage": "https://deepgenteam.github.io/", "githubRepo": "https://github.com/DeepGenTeam/DeepGen", "githubRepoAddedBy": "user", "ai_summary": "A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.", "ai_keywords": ["unified multimodal models", "image generation", "image editing", "parameter scale", "VLM layers", "DiT representations", "Stacked Channel Bridging", "think tokens", "data-centric training strategy", "alignment pre-training", "joint supervised fine-tuning", "reinforcement learning", "MR-GRPO", "generation quality", "human preferences", "visual artifacts"], "githubStars": 36, "organization": {"_id": "683ebd0d913d82e703e77286", "name": "sii-research", "fullname": "Shanghai Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/SQAtyVRxNjp9L0CUi0tgI.png"}, "summary_zh": "<ul>\n    <li>DeepGen 1.0 \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684 5B \u6a21\u578b\uff0c\u5177\u6709\u4e0e\u5927\u89c4\u6a21\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86 Stacked Channel Bridging (SCB) \u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002</li>\n    <li>\u8bad\u7ec3\u7b56\u7565\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u9884\u8bad\u7ec3\u3001\u8054\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u548c\u5bf9\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002</li>\n    <li>\u5c3d\u7ba1\u53ea\u4f7f\u7528\u4e86\u7ea6 50M \u7684\u6837\u672c\uff0cDeepGen 1.0 \u5728\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u5f00\u6e90\u4e86\u8bad\u7ec3\u4ee3\u7801\u3001\u6743\u91cd\u548c\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7edf\u4e00\u591a\u6a21\u6001\u7814\u7a76\u66ff\u4ee3\u65b9\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepGen 1.0 is a lightweight image generation model with 5 billion parameters, outperforming larger models that have over 10 billion parameters.</li>\n    <li>It uses a method called Stacked Channel Bridging (SCB) to improve understanding of images and enhance control over the generation process.</li>\n    <li>The training strategy involves three stages: pre-training on image-text pairs, fine-tuning on various tasks, and reinforcement learning to improve quality and alignment with human preferences.</li>\n    <li>Despite using only about 50 million samples, DeepGen 1.0 shows excellent performance, surpassing larger models on key benchmarks.</li>\n    <li>The developers are open-sourcing the training code, model weights, and datasets to support research in multimodal models.</li>\n</ul>"}, "publishedAt": "2026-02-12T12:44:24.000Z", "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing", "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12205.png", "numComments": 1, "submittedBy": {"_id": "64b4eec4faa3181a5eab9c46", "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg", "fullname": "Jiaqi Wang", "name": "myownskyW7", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 25, "isUserFollowing": false}, "organization": {"_id": "683ebd0d913d82e703e77286", "name": "sii-research", "fullname": "Shanghai Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/SQAtyVRxNjp9L0CUi0tgI.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.12705", "authors": [{"_id": "6992818050fb2c0be47838b2", "name": "Baorong Shi", "hidden": false}, {"_id": "6992818050fb2c0be47838b3", "name": "Bo Cui", "hidden": false}, {"_id": "6992818050fb2c0be47838b4", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6992818050fb2c0be47838b5", "name": "Deli Yu", "hidden": false}, {"_id": "6992818050fb2c0be47838b6", "name": "Fang Qian", "hidden": false}, {"_id": "6992818050fb2c0be47838b7", "name": "Haihua Yang", "hidden": false}, {"_id": "6992818050fb2c0be47838b8", "name": "Huichao Wang", "hidden": false}, {"_id": "6992818050fb2c0be47838b9", "name": "Jiale Chen", "hidden": false}, {"_id": "6992818050fb2c0be47838ba", "name": "Jianfei Pan", "hidden": false}, {"_id": "6992818050fb2c0be47838bb", "name": "Jieqiong Cao", "hidden": false}, {"_id": "6992818050fb2c0be47838bc", "name": "Jinghao Lin", "hidden": false}, {"_id": "6992818050fb2c0be47838bd", "name": "Kai Wu", "hidden": false}, {"_id": "6992818050fb2c0be47838be", "name": "Lin Yang", "hidden": false}, {"_id": "6992818050fb2c0be47838bf", "name": "Shengsheng Yao", "hidden": false}, {"_id": "6992818050fb2c0be47838c0", "name": "Tao Chen", "hidden": false}, {"_id": "6992818050fb2c0be47838c1", "name": "Xiaojun Xiao", "hidden": false}, {"_id": "6992818050fb2c0be47838c2", "user": {"_id": "666a59bff0d87d9c3b1dd907", "avatarUrl": "/avatars/4af5a47d78bca525c7ec985a390408a4.svg", "isPro": false, "fullname": "Xiaozhong Ji", "user": "xiaozhongji", "type": "user"}, "name": "Xiaozhong Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-16T15:36:45.325Z", "hidden": false}, {"_id": "6992818050fb2c0be47838c3", "name": "Xu Wang", "hidden": false}, {"_id": "6992818050fb2c0be47838c4", "name": "Yijun He", "hidden": false}, {"_id": "6992818050fb2c0be47838c5", "name": "Zhixiong Yang", "hidden": false}], "publishedAt": "2026-02-13T08:19:38.000Z", "submittedOnDailyAt": "2026-02-16T00:05:18.833Z", "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "submittedOnDailyBy": {"_id": "64c636b94c9bebfa6ac80ae4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png", "isPro": false, "fullname": "kai", "user": "KaiWu123", "type": "user"}, "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "upvotes": 55, "discussionId": "6992818050fb2c0be47838c6", "ai_summary": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.", "ai_keywords": ["vision-language foundation model", "entity-aware continual pretraining", "heterogeneous medical corpora", "long-tail gaps", "reinforcement learning", "tool-augmented agentic training", "multi-step diagnostic reasoning", "evidence-grounded reasoning", "hallucination reduction", "medical instruction adherence"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>MedXIAOHE\u662f\u4e00\u4e2a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u533b\u5b66\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u9886\u5148\u7684\u5c01\u95ed\u6e90\u591a\u6a21\u6001\u7cfb\u7edf\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\uff0cMedXIAOHE\u6269\u5927\u4e86\u77e5\u8bc6\u8986\u76d6\u8303\u56f4\uff0c\u51cf\u5c11\u4e86\u7a00\u6709\u75be\u75c5\u7b49\u957f\u5c3e\u95ee\u9898\u3002</li>\n    <li>\u5b83\u7ed3\u5408\u4e86\u591a\u79cd\u533b\u5b66\u63a8\u7406\u6a21\u5f0f\uff0c\u652f\u6301\u591a\u6b65\u9aa4\u7684\u8bca\u65ad\u63a8\u7406\uff0c\u786e\u4fdd\u51b3\u7b56\u53ef\u9a8c\u8bc1\u3002</li>\n    <li>\u4e3a\u63d0\u9ad8\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u9760\u6027\uff0cMedXIAOHE\u6574\u5408\u4e86\u7528\u6237\u504f\u597d\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u548c\u4f4e\u5e7b\u89c9\u7684\u957f\u7bc7\u62a5\u544a\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MedXIAOHE is a new medical model that helps understand and reason about medical information for real-world use.</li>\n    <li>It performs better than other top medical systems and handles a variety of medical tasks well.</li>\n    <li>The model uses a special training process that includes diverse medical data to improve its knowledge and tackle rare diseases.</li>\n    <li>It incorporates advanced reasoning techniques to support complex medical decision-making and provides clear decision-making paths.</li>\n    <li>The system is designed to be reliable, following user preferences and generating accurate medical reports.</li>\n</ul>"}, "publishedAt": "2026-02-13T03:19:38.000Z", "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12705.png", "numComments": 4, "submittedBy": {"_id": "64c636b94c9bebfa6ac80ae4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png", "fullname": "kai", "name": "KaiWu123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.12125", "authors": [{"_id": "698e8f46cace060ff123ac51", "name": "Wenkai Yang", "hidden": false}, {"_id": "698e8f46cace060ff123ac52", "name": "Weijie Liu", "hidden": false}, {"_id": "698e8f46cace060ff123ac53", "name": "Ruobing Xie", "hidden": false}, {"_id": "698e8f46cace060ff123ac54", "name": "Kai Yang", "hidden": false}, {"_id": "698e8f46cace060ff123ac55", "name": "Saiyong Yang", "hidden": false}, {"_id": "698e8f46cace060ff123ac56", "name": "Yankai Lin", "hidden": false}], "publishedAt": "2026-02-12T16:14:29.000Z", "submittedOnDailyAt": "2026-02-13T00:24:06.722Z", "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation", "submittedOnDailyBy": {"_id": "64b7df742f5a966b973e25f7", "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg", "isPro": false, "fullname": "Wenkai Yang", "user": "Keven16", "type": "user"}, "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.", "upvotes": 53, "discussionId": "698e8f46cace060ff123ac57", "githubRepo": "https://github.com/RUCBM/G-OPD", "githubRepoAddedBy": "user", "ai_summary": "On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.", "ai_keywords": ["on-policy distillation", "logit distribution", "dense KL-constrained RL", "reward scaling factor", "reward extrapolation", "reward correction", "teacher-student size pairings", "domain-specific RL", "strong-to-weak distillation"], "githubStars": 8, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u5728\u6559\u5e08\u4e0e\u5b66\u751f\u7684\u5bf9\u9f50\u60c5\u51b5\u4e0b\u7684\u5728\u7ebf\u7b56\u7565\u84b8\u998f\uff08OPD\uff09\uff0c\u663e\u793a\u4e86\u5176\u5728\u63d0\u9ad8\u5b66\u751f\u6027\u80fd\u65b9\u9762\u7684\u4f18\u52bf\u3002</li>\n    <li>\u53d1\u5c55\u4e86\u901a\u7528\u5728\u7ebf\u7b56\u7565\u84b8\u998f\uff08G-OPD\uff09\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u7075\u6d3b\u7684\u53c2\u8003\u6a21\u578b\u548c\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u5927\u4e8e1\uff08\u79f0\u4e3aExOPD\uff09\u80fd\u6709\u6548\u63d0\u5347\u6807\u51c6OPD\u7684\u6027\u80fd\u3002</li>\n    <li>\u5728\u5f3a\u5230\u5f31\u7684\u84b8\u998f\u8bbe\u7f6e\u4e2d\uff0c\u9009\u62e9\u6559\u5e08\u7684\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u53c2\u8003\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u5956\u52b1\u4fe1\u53f7\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u8be5\u7814\u7a76\u4e3a\u672a\u6765OPD\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>On-policy distillation (OPD) aligns a student model with a teacher model's output and performs better than other methods like off-policy distillation and reinforcement learning.</li>\n  <li>The study shows that OPD is a specific form of reinforcement learning using a certain reward structure.</li>\n  <li>A new framework called Generalized On-Policy Distillation (G-OPD) is proposed, which allows for more flexibility in choosing the reference model and adjusting the reward weight.</li>\n  <li>Using a higher reward scaling factor (ExOPD) improves performance, especially when combining knowledge from multiple expert models.</li>\n  <li>In a scenario where a smaller student is trained from a larger teacher, using the teacher's base model for reward correction enhances performance, but it requires more computational resources.</li>\n</ul>"}, "publishedAt": "2026-02-12T11:14:29.000Z", "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation", "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12125.png", "numComments": 2, "submittedBy": {"_id": "64b7df742f5a966b973e25f7", "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg", "fullname": "Wenkai Yang", "name": "Keven16", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.11858", "authors": [{"_id": "698ea72fcace060ff123ae5d", "name": "Lai Wei", "hidden": false}, {"_id": "698ea72fcace060ff123ae5e", "name": "Liangbo He", "hidden": false}, {"_id": "698ea72fcace060ff123ae5f", "name": "Jun Lan", "hidden": false}, {"_id": "698ea72fcace060ff123ae60", "name": "Lingzhong Dong", "hidden": false}, {"_id": "698ea72fcace060ff123ae61", "name": "Yutong Cai", "hidden": false}, {"_id": "698ea72fcace060ff123ae62", "name": "Siyuan Li", "hidden": false}, {"_id": "698ea72fcace060ff123ae63", "name": "Huijia Zhu", "hidden": false}, {"_id": "698ea72fcace060ff123ae64", "name": "Weiqiang Wang", "hidden": false}, {"_id": "698ea72fcace060ff123ae65", "name": "Linghe Kong", "hidden": false}, {"_id": "698ea72fcace060ff123ae66", "name": "Yue Wang", "hidden": false}, {"_id": "698ea72fcace060ff123ae67", "name": "Zhuosheng Zhang", "hidden": false}, {"_id": "698ea72fcace060ff123ae68", "name": "Weiran Huang", "hidden": false}], "publishedAt": "2026-02-12T12:00:35.000Z", "submittedOnDailyAt": "2026-02-16T00:45:35.867Z", "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception", "submittedOnDailyBy": {"_id": "64a16b1aeacb4b50ba1c889d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg", "isPro": false, "fullname": "Lai Wei", "user": "WaltonFuture", "type": "user"}, "summary": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.", "upvotes": 50, "discussionId": "698ea72fcace060ff123ae69", "githubRepo": "https://github.com/inclusionAI/Zooming-without-Zooming", "githubRepoAddedBy": "user", "ai_summary": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.", "ai_keywords": ["Multimodal Large Language Models", "visual question answering", "fine-grained perception", "Thinking-with-Images", "region-to-image distillation", "micro-cropped regions", "teacher-student distillation", "ZoomBench", "visual reasoning", "GUI agents"], "githubStars": 55, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5e7f\u6cdb\u7684\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ec6\u81f4\u611f\u77e5\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u65b0\u65b9\u6cd5\u201c\u56fe\u50cf\u601d\u8003\u201d\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u53cd\u590d\u653e\u5927\u548c\u7f29\u5c0f\u611f\u5174\u8da3\u533a\u57df\u6765\u6539\u5584\u7ec6\u81f4\u611f\u77e5\uff0c\u4f46\u4f1a\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u533a\u57df\u5230\u56fe\u50cf\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u5c06\u653e\u5927\u8fc7\u7a0b\u4ece\u63a8\u7406\u5de5\u5177\u8f6c\u53d8\u4e3a\u8bad\u7ec3\u539f\u7406\uff0c\u4ece\u800c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002</li>\n    <li>\u901a\u8fc7\u5fae\u88c1\u526a\u533a\u57df\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6570\u636e\uff0c\u5e76\u5c06\u8fd9\u79cd\u76d1\u7763\u4fe1\u606f\u84b8\u998f\u5230\u5b8c\u6574\u56fe\u50cf\u4e2d\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u7684\u7ec6\u81f4\u611f\u77e5\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86ZoomBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u4e2a\u7ec6\u81f4\u611f\u77e5\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal Large Language Models (MLLMs) are good at understanding visuals but struggle with detailed perception due to overwhelming global context.</li>\n    <li>The \"Thinking-with-Images\" method helps by zooming in on areas of interest but is slow because it requires multiple tool calls.</li>\n    <li>We propose a new method called Region-to-Image Distillation, which integrates zooming during training instead of at inference time, improving speed and efficiency.</li>\n    <li>Our approach allows smaller models to perform better at fine-grained perception after being trained on high-quality data generated from zoomed-in regions.</li>\n    <li>We introduce ZoomBench, a new benchmark for evaluating fine-grained visual question answering (VQA) with 845 data points, demonstrating our models' improved performance on multiple tasks.</li>\n</ul>"}, "publishedAt": "2026-02-12T07:00:35.000Z", "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception", "summary": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11858.png", "numComments": 1, "submittedBy": {"_id": "64a16b1aeacb4b50ba1c889d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg", "fullname": "Lai Wei", "name": "WaltonFuture", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.11124", "authors": [{"_id": "698d486865c0d15a6d162162", "user": {"_id": "6570977f87a92b76922c9950", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg", "isPro": false, "fullname": "Tianyi Xiong", "user": "txiong23", "type": "user"}, "name": "Tianyi Xiong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:23.141Z", "hidden": false}, {"_id": "698d486865c0d15a6d162163", "name": "Shihao Wang", "hidden": false}, {"_id": "698d486865c0d15a6d162164", "name": "Guilin Liu", "hidden": false}, {"_id": "698d486865c0d15a6d162165", "name": "Yi Dong", "hidden": false}, {"_id": "698d486865c0d15a6d162166", "name": "Ming Li", "hidden": false}, {"_id": "698d486865c0d15a6d162167", "name": "Heng Huang", "hidden": false}, {"_id": "698d486865c0d15a6d162168", "name": "Jan Kautz", "hidden": false}, {"_id": "698d486865c0d15a6d162169", "user": {"_id": "66c8037c737ba92ae3fe0322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg", "isPro": true, "fullname": "Zhiding Yu", "user": "Zhiding", "type": "user"}, "name": "Zhiding Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:25.160Z", "hidden": false}], "publishedAt": "2026-02-11T18:35:39.000Z", "submittedOnDailyAt": "2026-02-12T02:07:20.427Z", "title": "PhyCritic: Multimodal Critic Models for Physical AI", "submittedOnDailyBy": {"_id": "6570977f87a92b76922c9950", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg", "isPro": false, "fullname": "Tianyi Xiong", "user": "txiong23", "type": "user"}, "summary": "With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.", "upvotes": 43, "discussionId": "698d486865c0d15a6d16216a", "projectPage": "https://research.nvidia.com/labs/lpr/phycritic", "ai_summary": "PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.", "ai_keywords": ["multimodal models", "physical AI", "RLVR pipeline", "physical skill warmup stage", "self-referential critic finetuning", "perception", "reasoning", "policy model"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u53ef\u9760\u7684\u8bc4\u4f30\u6a21\u578b\u53d8\u5f97\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u4e00\u822c\u89c6\u89c9\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u7269\u7406\u4eba\u5de5\u667a\u80fd\u4efb\u52a1\u7684\u7814\u7a76\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86PhyCritic\uff0c\u4e00\u4e2a\u9488\u5bf9\u7269\u7406\u4eba\u5de5\u667a\u80fd\u4f18\u5316\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u6a21\u578b\u3002</li>\n    <li>PhyCritic\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u6d41\u7a0b\u63d0\u5347\u5224\u65ad\u80fd\u529b\uff0c\u5305\u62ec\u7269\u7406\u6280\u80fd\u9884\u70ed\u548c\u81ea\u6211\u53c2\u8003\u5fae\u8c03\u3002</li>\n    <li>\u5728\u7269\u7406\u548c\u4e00\u822c\u591a\u6a21\u6001\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0cPhyCritic\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models need reliable judge and critic models for evaluating responses.</li>\n    <li>Current critics mostly focus on general visual tasks, not on physical AI tasks like perception and planning.</li>\n    <li>PhyCritic is a new multimodal critic model specifically designed for physical AI.</li>\n    <li>It uses a two-stage process to improve judgment accuracy and physical correctness.</li>\n    <li>PhyCritic outperforms existing models in both physical and general evaluation tasks.</li>\n</ul>"}, "publishedAt": "2026-02-11T13:35:39.000Z", "title": "PhyCritic: Multimodal Critic Models for Physical AI", "summary": "With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11124.png", "numComments": 1, "submittedBy": {"_id": "6570977f87a92b76922c9950", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg", "fullname": "Tianyi Xiong", "name": "txiong23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10934", "authors": [{"_id": "698d585765c0d15a6d1621fe", "user": {"_id": "66c893b2e51ba3009235b1c0", "avatarUrl": "/avatars/5341d40b4b4caca2c145a46eb1754582.svg", "isPro": false, "fullname": "yitian gong", "user": "fdugyt", "type": "user"}, "name": "Yitian Gong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:26:16.742Z", "hidden": false}, {"_id": "698d585765c0d15a6d1621ff", "name": "Kuangwei Chen", "hidden": false}, {"_id": "698d585765c0d15a6d162200", "user": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "name": "Zhaoye Fei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-13T09:37:26.561Z", "hidden": false}, {"_id": "698d585765c0d15a6d162201", "name": "Xiaogui Yang", "hidden": false}, {"_id": "698d585765c0d15a6d162202", "name": "Ke Chen", "hidden": false}, {"_id": "698d585765c0d15a6d162203", "name": "Yang Wang", "hidden": false}, {"_id": "698d585765c0d15a6d162204", "name": "Kexin Huang", "hidden": false}, {"_id": "698d585765c0d15a6d162205", "name": "Mingshu Chen", "hidden": false}, {"_id": "698d585765c0d15a6d162206", "name": "Ruixiao Li", "hidden": false}, {"_id": "698d585765c0d15a6d162207", "name": "Qingyuan Cheng", "hidden": false}, {"_id": "698d585765c0d15a6d162208", "name": "Shimin Li", "hidden": false}, {"_id": "698d585765c0d15a6d162209", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-02-11T15:13:27.000Z", "submittedOnDailyAt": "2026-02-13T01:47:00.074Z", "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models", "submittedOnDailyBy": {"_id": "66c893b2e51ba3009235b1c0", "avatarUrl": "/avatars/5341d40b4b4caca2c145a46eb1754582.svg", "isPro": false, "fullname": "yitian gong", "user": "fdugyt", "type": "user"}, "summary": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.", "upvotes": 43, "discussionId": "698d585765c0d15a6d16220a", "githubRepo": "https://github.com/OpenMOSS/MOSS-Audio-Tokenizer", "githubRepoAddedBy": "user", "ai_summary": "A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.", "ai_keywords": ["audio tokenization", "causal audio tokenizer", "Transformer architecture", "end-to-end learning", "quantizer", "encoder", "decoder", "discrete audio tokens", "autoregressive TTS", "automatic speech recognition", "codec", "pre-trained encoders", "semantic distillation", "heterogeneous CNN-based architectures"], "githubStars": 76, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u79bb\u6563\u97f3\u9891\u6807\u8bb0\u5668\u662f\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u548c\u751f\u6210\u97f3\u9891\u7684\u57fa\u7840\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u548c\u590d\u6742\u7684CNN\u67b6\u6784\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>\u672c\u6587\u63d0\u51faCAT\uff08\u56e0\u679c\u97f3\u9891\u6807\u8bb0\u5668\uff09\uff0c\u91c7\u7528\u5168\u65b0\u7684Transformer\u67b6\u6784\uff0c\u4ece\u5934\u5f00\u59cb\u4f18\u5316\u97f3\u9891\u5904\u7406\u3002</li>\n    <li>\u57fa\u4e8eCAT\u67b6\u6784\uff0c\u6211\u4eec\u5f00\u53d1\u4e86MOSS-Audio-Tokenizer\uff0c\u62e5\u670916\u4ebf\u53c2\u6570\uff0c\u7ecf\u8fc7300\u4e07\u5c0f\u65f6\u7684\u591a\u6837\u5316\u97f3\u9891\u6570\u636e\u9884\u8bad\u7ec3\u3002</li>\n    <li>MOSS-Audio-Tokenizer\u5728\u591a\u79cd\u97f3\u9891\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u8bed\u97f3\u3001\u58f0\u97f3\u548c\u97f3\u4e50\u7684\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Discrete audio tokenizers help large language models process and generate audio more effectively.</li>\n    <li>Current methods often use fixed architectures that limit their performance and scalability.</li>\n    <li>The paper introduces CAT, a new audio tokenizer that uses a Transformer-based design and optimizes all components together for better audio quality.</li>\n    <li>MOSS-Audio-Tokenizer, built on CAT, has 1.6 billion parameters and was trained on a large dataset, showing better performance than previous models in various audio types.</li>\n    <li>This model also leads to advancements in text-to-speech (TTS) and automatic speech recognition (ASR) without needing extra encoders.</li>\n</ul>"}, "publishedAt": "2026-02-11T10:13:27.000Z", "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models", "summary": "Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoders, semantic distillation, or heterogeneous CNN-based architectures. These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT (Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder, quantizer, and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codecs over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoders. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10934.png", "numComments": 3, "submittedBy": {"_id": "66c893b2e51ba3009235b1c0", "avatarUrl": "/avatars/5341d40b4b4caca2c145a46eb1754582.svg", "fullname": "yitian gong", "name": "fdugyt", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.11144", "authors": [{"_id": "698d4ad765c0d15a6d162188", "name": "Ruichuan An", "hidden": false}, {"_id": "698d4ad765c0d15a6d162189", "name": "Sihan Yang", "hidden": false}, {"_id": "698d4ad765c0d15a6d16218a", "name": "Ziyu Guo", "hidden": false}, {"_id": "698d4ad765c0d15a6d16218b", "name": "Wei Dai", "hidden": false}, {"_id": "698d4ad765c0d15a6d16218c", "name": "Zijun Shen", "hidden": false}, {"_id": "698d4ad765c0d15a6d16218d", "name": "Haodong Li", "hidden": false}, {"_id": "698d4ad765c0d15a6d16218e", "name": "Renrui Zhang", "hidden": false}, {"_id": "698d4ad765c0d15a6d16218f", "name": "Xinyu Wei", "hidden": false}, {"_id": "698d4ad765c0d15a6d162190", "name": "Guopeng Li", "hidden": false}, {"_id": "698d4ad765c0d15a6d162191", "name": "Wenshan Wu", "hidden": false}, {"_id": "698d4ad765c0d15a6d162192", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2026-02-11T18:55:54.000Z", "submittedOnDailyAt": "2026-02-12T01:06:56.691Z", "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite", "submittedOnDailyBy": {"_id": "65ddea8b2d26e59a5a33330f", "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg", "isPro": false, "fullname": "li haodong", "user": "mickyhimself", "type": "user"}, "summary": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.", "upvotes": 42, "discussionId": "698d4ad865c0d15a6d162193", "ai_summary": "GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.", "ai_keywords": ["Unified Multimodal Models", "Generative Fluid Intelligence", "Inducing Implicit Patterns", "Executing Ad-hoc Constraints", "Adapting to Contextual Knowledge", "attention intervention strategy"], "summary_zh": "<ul>\n    <li>\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff08UMMs\uff09\u5728\u89c6\u89c9\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u7684\u662f\u7ed3\u6676\u667a\u529b\u3002</li>\n    <li>\u7ed3\u6676\u667a\u529b\u4fa7\u91cd\u4e8e\u56de\u5fc6\u7d2f\u79ef\u7684\u77e5\u8bc6\uff0c\u800c\u5ffd\u89c6\u4e86\u751f\u6210\u6d41\u52a8\u667a\u529b\uff08GFI\uff09\uff0c\u5373\u5728\u65b0\u60c5\u5883\u4e2d\u63a8\u5bfc\u6a21\u5f0f\u548c\u9002\u5e94\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u8bc4\u4f30GFI\uff0c\u6211\u4eec\u5f15\u5165\u4e86GENIUS\u8bc4\u4f30\u5957\u4ef6\uff0c\u5f62\u5f0f\u5316GFI\u4e3a\u4e09\u79cd\u57fa\u672c\u80fd\u529b\u7684\u7ed3\u5408\u3002</li>\n    <li>\u6211\u4eec\u7684\u8bc4\u4f30\u663e\u793a\uff0c12\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u663e\u8457\u6027\u80fd\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5bf9\u4e0a\u4e0b\u6587\u7406\u89e3\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u5e72\u9884\u7b56\u7565\uff0c\u4ee5\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u5e94\u5bf9\u52a8\u6001\u63a8\u7406\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Unified Multimodal Models (UMMs) have improved in visual generation but mainly focus on Crystallized Intelligence.</li>\n    <li>This focus misses Generative Fluid Intelligence (GFI), which is about recognizing patterns and adapting to new situations.</li>\n    <li>GENIUS is a new evaluation tool that measures GFI through three main tasks: recognizing patterns, following unique constraints, and adapting to context.</li>\n    <li>Tests on 12 models show they struggle with these tasks, mainly due to limited understanding of context.</li>\n    <li>To help improve this, a new strategy that doesn't require extra training is proposed, aiming to enhance dynamic reasoning in models.</li>\n</ul>"}, "publishedAt": "2026-02-11T13:55:54.000Z", "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite", "summary": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11144.png", "numComments": 1, "submittedBy": {"_id": "65ddea8b2d26e59a5a33330f", "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg", "fullname": "li haodong", "name": "mickyhimself", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 0, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u7684\u51cf\u5c11\uff0c\u9884\u8bad\u7ec3\u65b9\u6cd5\u6b63\u5728\u4ece\u4f7f\u7528\u66f4\u591a\u7684\u6587\u672c\u8f6c\u5411\u4f7f\u7528\u66f4\u597d\u7684\u6587\u672c\u3002</li>\n    <li>\u63d0\u51fa\u4e86OPUS\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u73b0\u4ee3\u4f18\u5316\u5668\u5b9a\u4e49\u6570\u636e\u9009\u62e9\u7684\u52a8\u6001\u6548\u7528\uff0c\u5e76\u5bf9\u5019\u9009\u6570\u636e\u8fdb\u884c\u8bc4\u5206\u3002</li>\n    <li>OPUS\u7ed3\u5408\u4e86Ghost\u6280\u672f\u548cBoltzmann\u91c7\u6837\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u6570\u636e\u591a\u6837\u6027\uff0c\u4ec5\u589e\u52a04.7%\u7684\u8ba1\u7b97\u5f00\u9500\u3002</li>\n    <li>\u5728GPT-2 Large/XL\u7684\u9884\u8bad\u7ec3\u4e2d\uff0cOPUS\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u5de5\u4e1a\u57fa\u51c6\u3002</li>\n    <li>\u5728\u7279\u5b9a\u9886\u57df\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0cOPUS\u4ec5\u4f7f\u75280.5B\u4e2a\u4ee4\u724c\u5c31\u80fd\u8fbe\u5230\u4f18\u4e8e\u5b8c\u65743B\u4e2a\u4ee4\u724c\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>As high-quality public text becomes limited, pre-training is focusing on using better data instead of just more data.</li>\n    <li>The proposed method, OPUS, selects training data based on how useful it is for the learning process, using modern optimizers.</li>\n    <li>OPUS uses efficient techniques to manage data selection and keep compute costs low, with only a small increase in resource usage.</li>\n    <li>In tests, OPUS performed better than existing methods, even achieving good results with less data in specialized areas.</li>\n    <li>Combining OPUS with traditional data filters improved efficiency further, even when using lower-quality data.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10388", "authors": [{"_id": "698d3bd265c0d15a6d16200e", "user": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "name": "Zhongzhi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:57:05.580Z", "hidden": false}, {"_id": "698d3bd265c0d15a6d16200f", "name": "Xuansheng Wu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162010", "name": "Yijiang Li", "hidden": false}, {"_id": "698d3bd265c0d15a6d162011", "name": "Lijie Hu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162012", "name": "Ninghao Liu", "hidden": false}], "publishedAt": "2026-02-11T00:23:13.000Z", "submittedOnDailyAt": "2026-02-16T02:31:34.708Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "submittedOnDailyBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "upvotes": 200, "discussionId": "698d3bd265c0d15a6d162013", "projectPage": "https://website-sigma-three-35.vercel.app/", "githubRepo": "https://github.com/Zhongzhi660/FAC-Synthesis", "githubRepoAddedBy": "user", "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.", "ai_keywords": ["Feature Activation Coverage", "sparse autoencoder", "data diversity", "downstream performance", "instruction following", "toxicity detection", "reward modeling", "behavior steering", "cross-model knowledge transfer", "data-centric optimization"], "githubStars": 52, "summary_zh": "<ul>\n    <li>\u540e\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u6e38\u6027\u80fd\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u6307\u6807\u6765\u91cf\u5316\u591a\u6837\u6027\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5bf9\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u7684\u53cd\u9988\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6307\u6807\u201c\u7279\u5f81\u6fc0\u6d3b\u8986\u76d6\u201d\uff08FAC\uff09\uff0c\u7528\u4e8e\u5728\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u8861\u91cf\u6570\u636e\u591a\u6837\u6027\u3002</li>\n    <li>\u57fa\u4e8eFAC\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u79f0\u4e3aFAC Synthesis\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u5177\u591a\u6837\u6027\u7684\u5408\u6210\u6837\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u6570\u636e\u591a\u6837\u6027\u548c\u4e0b\u6e38\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u53d1\u73b0\u4e86\u5171\u4eab\u7684\u53ef\u89e3\u91ca\u7279\u5f81\u7a7a\u95f4\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Post-training data diversity is important for improving the performance of large language models (LLMs).</li>\n  <li>Current methods measure diversity using text-based metrics that don\u2019t effectively capture features important for performance.</li>\n  <li>The new Feature Activation Coverage (FAC) metric assesses data diversity in a way that's easier to understand.</li>\n  <li>FAC Synthesis is a framework that generates new data samples based on missing features identified in existing datasets.</li>\n  <li>Tests show that this approach enhances data diversity and improves performance on various tasks, and it allows for knowledge sharing across different models.</li>\n</ul>"}, "publishedAt": "2026-02-10T19:23:13.000Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10388.png", "numComments": 2, "submittedBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "fullname": "ZhongzhiLi", "name": "Zhongzhi1228", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>\u62a5\u544a\u4ecb\u7ecd\u4e86ERNIE 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u7684\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u81ea\u56de\u5f52\u57fa\u7840\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u8d85\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\uff0c\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u6240\u6709\u6a21\u6001\uff0c\u76ee\u6807\u662f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u4ee4\u724c\u3002</li>\n    <li>ERNIE 5.0\u5f15\u5165\u4e86\u5f39\u6027\u8bad\u7ec3\u6a21\u5f0f\uff0c\u5728\u5355\u6b21\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u4e0d\u540c\u6df1\u5ea6\u548c\u4e13\u5bb6\u5bb9\u91cf\u7684\u5b50\u6a21\u578b\uff0c\u4ee5\u9002\u5e94\u5404\u79cd\u8d44\u6e90\u9650\u5236\u3002</li>\n    <li>\u6a21\u578b\u5728\u591a\u4e2a\u6a21\u6001\u4e0a\u8868\u73b0\u51fa\u5f3a\u52b2\u4e14\u5747\u8861\u7684\u6027\u80fd\uff0c\u662f\u9996\u4e2a\u751f\u4ea7\u89c4\u6a21\u7684\u4e07\u4ebf\u53c2\u6570\u7edf\u4e00\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u4e3a\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u62a5\u544a\u63d0\u4f9b\u4e86\u4e13\u5bb6\u8def\u7531\u7684\u53ef\u89c6\u5316\u548c\u5f39\u6027\u8bad\u7ec3\u7684\u8be6\u7ec6\u5b9e\u8bc1\u5206\u6790\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new model designed to understand and generate text, images, videos, and audio all together.</li>\n    <li>The model is trained using a unique method that helps it predict the next parts of data from different types, using a special architecture that optimizes performance.</li>\n    <li>It can create different versions of itself during training to balance speed and size based on available resources.</li>\n    <li>The model also addresses challenges in using reinforcement learning with complex multimodal data to ensure it performs well after training.</li>\n    <li>ERNIE 5.0 is the first large-scale model with a trillion parameters that effectively handles multiple types of data, and it includes helpful visualizations and analysis for further research.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Green-VLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7eff\u8272\u4eba\u5f62\u673a\u5668\u4eba\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u673a\u5668\u4eba\u4e0a\u4fdd\u6301\u826f\u597d\u7684\u901a\u7528\u6027\u3002</li>\n    <li>Green-VLA \u7531\u4e94\u4e2a\u9636\u6bb5\u7ec4\u6210\uff1a\u57fa\u7840\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u591a\u6a21\u6001\u57fa\u7840\u3001\u591a\u79cd\u8eab\u4f53\u7684\u9884\u8bad\u7ec3\u3001\u7279\u5b9a\u8eab\u4f53\u7684\u9002\u5e94\u548c\u5f3a\u5316\u5b66\u4e60\u653f\u7b56\u5bf9\u9f50\u3002</li>\n    <li>\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u4f7f\u7528\u4e863000\u5c0f\u65f6\u7684\u6f14\u793a\uff0c\u5e76\u8fdb\u884c\u65f6\u95f4\u5bf9\u9f50\u548c\u8d28\u91cf\u8fc7\u6ee4\u3002</li>\n    <li>\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0cVLA\u63a7\u5236\u5668\u589e\u5f3a\u4e86\u60c5\u8282\u8fdb\u5c55\u9884\u6d4b\u3001\u5206\u5e03\u5916\u68c0\u6d4b\u548c\u57fa\u4e8e\u8054\u5408\u9884\u6d4b\u7684\u6307\u5bfc\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u76ee\u6807\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u540e\uff0c\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u671f\u6548\u7387\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a framework designed for the Green humanoid robot that combines vision, language, and actions.</li>\n    <li>It uses a five-stage learning process to improve robot capabilities across different types of robots.</li>\n    <li>The framework includes a large amount of training data (3,000 hours of demonstrations) and ensures good quality through filtering.</li>\n    <li>It features a system that allows one control policy to manage various types of robots, enhancing flexibility.</li>\n    <li>Testing shows that the framework improves success rates and efficiency in real-world tasks.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09877", "authors": [{"_id": "698c7abdeb12ea7453916869", "user": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "name": "Chenxu Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-12T16:49:45.534Z", "hidden": false}, {"_id": "698c7abdeb12ea745391686a", "name": "Chaozhuo Li", "hidden": false}, {"_id": "698c7abdeb12ea745391686b", "name": "Songyang Liu", "hidden": false}, {"_id": "698c7abdeb12ea745391686c", "name": "Zejian Chen", "hidden": false}, {"_id": "698c7abdeb12ea745391686d", "name": "Jinyu Hou", "hidden": false}, {"_id": "698c7abdeb12ea745391686e", "name": "Ji Qi", "hidden": false}, {"_id": "698c7abdeb12ea745391686f", "name": "Rui Li", "hidden": false}, {"_id": "698c7abdeb12ea7453916870", "name": "Litian Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916871", "name": "Qiwei Ye", "hidden": false}, {"_id": "698c7abdeb12ea7453916872", "name": "Zheng Liu", "hidden": false}, {"_id": "698c7abdeb12ea7453916873", "name": "Xu Chen", "hidden": false}, {"_id": "698c7abdeb12ea7453916874", "name": "Xi Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916875", "name": "Philip S. Yu", "hidden": false}], "publishedAt": "2026-02-10T15:18:19.000Z", "submittedOnDailyAt": "2026-02-13T00:53:30.377Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "submittedOnDailyBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "upvotes": 169, "discussionId": "698c7abdeb12ea7453916876", "ai_summary": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.", "ai_keywords": ["multi-agent systems", "large language models", "self-evolution", "safety alignment", "information-theoretic framework", "anthropic value distributions", "statistical blind spots", "self-evolving AI societies", "external oversight", "safety-preserving mechanisms"], "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u53ef\u6269\u5c55\u96c6\u4f53\u667a\u80fd\u548c\u81ea\u6211\u8fdb\u5316\u7684\u6f5c\u529b\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u6301\u7eed\u81ea\u6211\u8fdb\u5316\u3001\u5b8c\u5168\u9694\u79bb\u548c\u5b89\u5168\u4e0d\u53d8\u4e09\u8005\u540c\u65f6\u6ee1\u8db3\u662f\u4e0d\u53ef\u80fd\u7684\u3002</li>\n    <li>\u81ea\u6211\u8fdb\u5316\u7684\u5b64\u7acb\u72b6\u6001\u4f1a\u5bfc\u81f4\u7edf\u8ba1\u76f2\u70b9\uff0c\u4ece\u800c\u964d\u4f4e\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7814\u7a76\u5f00\u653e\u548c\u5c01\u95ed\u7684\u81ea\u6211\u8fdb\u5316\u7cfb\u7edf\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e86\u5b89\u5168\u6027\u4e0b\u964d\u7684\u73b0\u8c61\u3002</li>\n    <li>\u6211\u4eec\u5efa\u8bae\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u5916\u90e8\u76d1\u7763\u6216\u65b0\u7684\u5b89\u5168\u673a\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems using large language models can potentially improve themselves and work together effectively.</li>\n    <li>It is impossible for these systems to continuously evolve, stay completely isolated, and remain safe at the same time\u2014a challenge called the self-evolution trilemma.</li>\n    <li>Isolated self-improvement can create blind spots, which can lead to safety issues that worsen over time.</li>\n    <li>Research from a community of agents and experiments shows that safety can degrade as predicted.</li>\n    <li>The study suggests new ways to address safety concerns, emphasizing the need for external oversight and better safety measures.</li>\n</ul>"}, "publishedAt": "2026-02-10T10:18:19.000Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png", "numComments": 2, "submittedBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "fullname": "Chenxu Wang", "name": "xunyoyo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3bGUI\u667a\u80fd\u4f53\u901a\u8fc7\u611f\u77e5\u754c\u9762\u548c\u6267\u884c\u52a8\u4f5c\u4e0e\u73af\u5883\u4e92\u52a8\u3002</li>\n    <li>Code2World\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u53ef\u4ee5\u751f\u6210\u53ef\u6e32\u67d3\u7684\u4ee3\u7801\u6765\u6a21\u62df\u4e0b\u4e00\u4e2a\u89c6\u89c9\u72b6\u6001\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86AndroidCode\uff0c\u5305\u542b\u8d85\u8fc78\u4e07\u4e2a\u9ad8\u8d28\u91cf\u7684\u5c4f\u5e55-\u52a8\u4f5c\u5bf9\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u89c9\u53cd\u9988\u673a\u5236\uff0cCode2World\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u4e00\u6b65\u7528\u6237\u754c\u9762\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002</li>\n    <li>Code2World\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86Gemini-2.5-Flash\u5728AndroidWorld\u7684\u6210\u529f\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Code2World is a tool that helps virtual agents understand and predict changes in user interfaces by generating code for visual states.</li>\n    <li>It addresses challenges in existing methods that struggle with both visual quality and control over interface structure.</li>\n    <li>The project creates a large dataset of over 80,000 high-quality screen-action pairs by converting GUI interactions into HTML code.</li>\n    <li>Code2World uses advanced training methods, including reinforcement learning, to improve the accuracy of its predictions.</li>\n    <li>Tests show that Code2World outperforms other models, increasing success rates in navigation tasks significantly.</li>\n</ul>"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10604", "authors": [{"_id": "698d417065c0d15a6d162026", "name": "Ailin Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162027", "name": "Ang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162028", "name": "Aobo Kong", "hidden": false}, {"_id": "698d417065c0d15a6d162029", "name": "Bin Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202a", "name": "Binxing Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d16202b", "name": "Bo Dong", "hidden": false}, {"_id": "698d417065c0d15a6d16202c", "name": "Bojun Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202d", "name": "Boyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16202e", "name": "Brian Li", "hidden": false}, {"_id": "698d417065c0d15a6d16202f", "name": "Buyun Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162030", "name": "Chang Su", "hidden": false}, {"_id": "698d417065c0d15a6d162031", "name": "Changxin Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162032", "name": "Changyi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162033", "name": "Chao Lou", "hidden": false}, {"_id": "698d417065c0d15a6d162034", "name": "Chen Hu", "hidden": false}, {"_id": "698d417065c0d15a6d162035", "name": "Chen Xu", "hidden": false}, {"_id": "698d417065c0d15a6d162036", "name": "Chenfeng Yu", "hidden": false}, {"_id": "698d417065c0d15a6d162037", "name": "Chengting Feng", "hidden": false}, {"_id": "698d417065c0d15a6d162038", "name": "Chengyuan Yao", "hidden": false}, {"_id": "698d417065c0d15a6d162039", "name": "Chunrui Han", "hidden": false}, {"_id": "698d417065c0d15a6d16203a", "name": "Dan Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203b", "name": "Dapeng Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16203c", "name": "Daxin Jiang", "hidden": false}, {"_id": "698d417065c0d15a6d16203d", "name": "Dehua Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203e", "name": "Deshan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16203f", "name": "Di Qi", "hidden": false}, {"_id": "698d417065c0d15a6d162040", "name": "Enle Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162041", "name": "Fajie Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162042", "name": "Fanqi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162043", "name": "Guanzhe Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162044", "name": "Gulin Yan", "hidden": false}, {"_id": "698d417065c0d15a6d162045", "name": "Guoliang Cao", "hidden": false}, {"_id": "698d417065c0d15a6d162046", "name": "Guopeng Li", "hidden": false}, {"_id": "698d417065c0d15a6d162047", "name": "Han Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162048", "name": "Hangyu Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162049", "user": {"_id": "64b7874b9f5987572ca28461", "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg", "isPro": false, "fullname": "hanshanzhang", "user": "brain-zhang", "type": "user"}, "name": "Hanshan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:52.602Z", "hidden": false}, {"_id": "698d417065c0d15a6d16204a", "name": "Hao Nie", "hidden": false}, {"_id": "698d417065c0d15a6d16204b", "name": "Haonan Jia", "hidden": false}, {"_id": "698d417065c0d15a6d16204c", "name": "Haoran Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204d", "name": "Hebin Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16204e", "name": "Hekun Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204f", "name": "Heng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162050", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "698d417065c0d15a6d162051", "name": "Hongbo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162052", "name": "Hongbo Peng", "hidden": false}, {"_id": "698d417065c0d15a6d162053", "name": "Hongyu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162054", "name": "Hongyuan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162055", "name": "Houyong Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162056", "name": "Huangxi Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d162057", "name": "Huimin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d162058", "name": "Huiyong Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162059", "name": "Jia Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16205a", "name": "Jian Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16205b", "name": "Jianjian Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16205c", "name": "Jiaoren Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16205d", "name": "Jiaran Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16205e", "name": "Jiashu Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16205f", "name": "Jiashuo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162060", "name": "Jiayi Fu", "hidden": false}, {"_id": "698d417065c0d15a6d162061", "name": "Jiayu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162062", "name": "Jie Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162063", "name": "Jie Luo", "hidden": false}, {"_id": "698d417065c0d15a6d162064", "name": "Jie Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162065", "name": "Jie Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162066", "name": "Jieyi Hou", "hidden": false}, {"_id": "698d417065c0d15a6d162067", "name": "Jing Bai", "hidden": false}, {"_id": "698d417065c0d15a6d162068", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:37.335Z", "hidden": false}, {"_id": "698d417065c0d15a6d162069", "name": "Jingjing Xie", "hidden": false}, {"_id": "698d417065c0d15a6d16206a", "name": "Jingwei Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16206b", "name": "Jingyang Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16206c", "name": "Jishi Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16206d", "name": "Junfeng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16206e", "name": "Junzhe Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16206f", "name": "Ka Man Lo", "hidden": false}, {"_id": "698d417065c0d15a6d162070", "name": "Kai Liang", "hidden": false}, {"_id": "698d417065c0d15a6d162071", "name": "Kaibo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162072", "name": "Kaijun Tan", "hidden": false}, {"_id": "698d417065c0d15a6d162073", "user": {"_id": "66668c591964b6188ee310c2", "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg", "isPro": false, "fullname": "Kaiwen Yan", "user": "linrany", "type": "user"}, "name": "Kaiwen Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:58.524Z", "hidden": false}, {"_id": "698d417065c0d15a6d162074", "name": "Kaixiang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162075", "name": "Kang An", "hidden": false}, {"_id": "698d417065c0d15a6d162076", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:56.339Z", "hidden": false}, {"_id": "698d417065c0d15a6d162077", "name": "Lei Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162078", "name": "Liang Lv", "hidden": false}, {"_id": "698d417065c0d15a6d162079", "name": "Liang Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16207a", "name": "Liangyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207b", "name": "Lieyu Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16207c", "name": "Liguo Tan", "hidden": false}, {"_id": "698d417065c0d15a6d16207d", "name": "Lin Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16207e", "name": "Lina Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207f", "name": "Luck Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162080", "name": "Mengqiang Ren", "hidden": false}, {"_id": "698d417065c0d15a6d162081", "name": "Michael Li", "hidden": false}, {"_id": "698d417065c0d15a6d162082", "name": "Ming Li", "hidden": false}, {"_id": "698d417065c0d15a6d162083", "name": "Mingliang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162084", "name": "Mingming Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162085", "name": "Mingrui Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162086", "name": "Mitt Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162087", "name": "Na Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162088", "name": "Peng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162089", "name": "Qi Han", "hidden": false}, {"_id": "698d417065c0d15a6d16208a", "name": "Qian Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16208b", "name": "Qinglin He", "hidden": false}, {"_id": "698d417065c0d15a6d16208c", "name": "Qinxin Du", "hidden": false}, {"_id": "698d417065c0d15a6d16208d", "name": "Qiuping Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16208e", "name": "Quan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16208f", "name": "Rongqiu Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162090", "name": "Ruihang Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162091", "name": "Ruixin Han", "hidden": false}, {"_id": "698d417065c0d15a6d162092", "name": "Ruosi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162093", "name": "Ruyan Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162094", "name": "Shan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162095", "name": "Shaoliang Pang", "hidden": false}, {"_id": "698d417065c0d15a6d162096", "name": "Shaowen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162097", "name": "Shengjie Fan", "hidden": false}, {"_id": "698d417065c0d15a6d162098", "name": "Shijie Shang", "hidden": false}, {"_id": "698d417065c0d15a6d162099", "name": "Shiliang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d16209a", "name": "Shiwei Li", "hidden": false}, {"_id": "698d417065c0d15a6d16209b", "name": "Shuangshuang Tian", "hidden": false}, {"_id": "698d417065c0d15a6d16209c", "name": "Siqi Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16209d", "name": "Siye Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16209e", "name": "Siyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16209f", "name": "Song Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a0", "name": "Tiancheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620a1", "name": "Tianchi Yue", "hidden": false}, {"_id": "698d417065c0d15a6d1620a2", "name": "Tianhao Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d1620a3", "name": "Tianning Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620a4", "name": "Tingdan Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620a5", "name": "Wang You", "hidden": false}, {"_id": "698d417065c0d15a6d1620a6", "name": "Wei Ji", "hidden": false}, {"_id": "698d417065c0d15a6d1620a7", "name": "Wei Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a8", "name": "Wei Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620a9", "name": "Weibo Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620aa", "user": {"_id": "6657620ea496f7fcb67c3871", "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg", "isPro": false, "fullname": "xieweihao", "user": "chalengr", "type": "user"}, "name": "Weihao Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:48.216Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ab", "name": "Wen Sun", "hidden": false}, {"_id": "698d417065c0d15a6d1620ac", "name": "Wenjin Deng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ad", "user": {"_id": "650c04795510464e85b47470", "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg", "isPro": false, "fullname": "wen", "user": "zhengwenzhen", "type": "user"}, "name": "Wenzhen Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:45.930Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ae", "name": "Wuxun Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620af", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b0", "name": "Xiangwen Kong", "hidden": false}, {"_id": "698d417065c0d15a6d1620b1", "name": "Xiangyu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b2", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b3", "name": "Xiaobo Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b4", "name": "Xiaojia Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b5", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620b6", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d1620b7", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620b8", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b9", "name": "Xin Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ba", "name": "Xin Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bb", "name": "Xin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bc", "name": "Xing Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620bd", "name": "Xingping Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620be", "name": "Xinran Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620bf", "name": "Xu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620c0", "user": {"_id": "64ec5b64bfb2aa06a46ff2d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg", "isPro": false, "fullname": "xuan he", "user": "tpa115k31", "type": "user"}, "name": "Xuan He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:36.240Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620c1", "name": "Xuanti Feng", "hidden": false}, {"_id": "698d417065c0d15a6d1620c2", "name": "Xuedan Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c3", "name": "Xuqiang Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620c4", "name": "Yanbo Yu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c5", "name": "Yang Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620c6", "name": "Yang Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c7", "name": "Yanlin Lai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c8", "name": "Yanming Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c9", "name": "Yaoyu Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ca", "name": "Yeqing Shen", "hidden": false}, {"_id": "698d417065c0d15a6d1620cb", "name": "Yibo Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d1620cc", "name": "Yichen Lv", "hidden": false}, {"_id": "698d417065c0d15a6d1620cd", "name": "Yicheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620ce", "name": "Yifeng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620cf", "name": "Yijing Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d0", "name": "Yikun Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d1", "name": "Yin Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d2", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d3", "name": "Yinmin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d4", "name": "Yitong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d5", "name": "Yixuan Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d6", "name": "Yiyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620d7", "name": "Yongchi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d8", "name": "Yongshen Long", "hidden": false}, {"_id": "698d417065c0d15a6d1620d9", "name": "Yongyao Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620da", "name": "Yousong Guan", "hidden": false}, {"_id": "698d417065c0d15a6d1620db", "name": "Yu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620dc", "name": "Yuang Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620dd", "name": "Yuanhao Ding", "hidden": false}, {"_id": "698d417065c0d15a6d1620de", "name": "Yuantao Fan", "hidden": false}, {"_id": "698d417065c0d15a6d1620df", "name": "Yuanzhen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e0", "name": "Yuchu Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620e1", "name": "Yudi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e2", "name": "Yue Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620e3", "name": "Yueqiang Lin", "hidden": false}, {"_id": "698d417065c0d15a6d1620e4", "name": "Yufan Lu", "hidden": false}, {"_id": "698d417065c0d15a6d1620e5", "name": "Yuling Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e6", "name": "Yunzhou Ju", "hidden": false}, {"_id": "698d417065c0d15a6d1620e7", "name": "Yurong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e8", "name": "Yusheng Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620e9", "name": "Yuxiang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ea", "name": "Yuyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620eb", "name": "Yuzhu Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620ec", "name": "Zejia Weng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ed", "name": "Zetao Hong", "hidden": false}, {"_id": "698d417065c0d15a6d1620ee", "name": "Zexi Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ef", "name": "Zhe Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620f0", "name": "Zheng Ge", "hidden": false}, {"_id": "698d417065c0d15a6d1620f1", "name": "Zheng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620f2", "name": "Zheng Zeng", "hidden": false}, {"_id": "698d417065c0d15a6d1620f3", "user": {"_id": "63607ace9ddc44e710e13f0f", "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg", "isPro": false, "fullname": "zy", "user": "lu-vae", "type": "user"}, "name": "Zhenyi Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:50.532Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620f4", "name": "Zhewei Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f5", "name": "Zhichao Chang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f6", "name": "Zhiguo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f7", "name": "Zhiheng Hu", "hidden": false}, {"_id": "698d417065c0d15a6d1620f8", "name": "Zidong Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f9", "name": "Zili Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fa", "name": "Ziqi Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620fb", "name": "Zixin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fc", "name": "Zixuan Wang", "hidden": false}], "publishedAt": "2026-02-11T07:53:51.000Z", "submittedOnDailyAt": "2026-02-12T00:26:49.880Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "upvotes": 150, "discussionId": "698d417165c0d15a6d1620fd", "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash", "githubRepoAddedBy": "user", "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.", "ai_keywords": ["Mixture-of-Experts", "sparse MoE", "foundation model", "active parameters", "interleaved attention", "sliding-window attention", "full attention", "Multi-Token Prediction", "reinforcement learning", "verifiable signals", "preference feedback", "off-policy training", "self-improvement", "IMO-AnswerBench", "LiveCodeBench", "tau2-Bench", "BrowseComp", "Terminal-Bench"], "githubStars": 1245, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Step 3.5 Flash\uff0c\u4e00\u4e2a\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u667a\u80fd\u4ee3\u7406\u7684\u63a8\u7406\u548c\u6267\u884c\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u62e5\u6709196\u4ebf\u53c2\u6570\u7684\u57fa\u7840\u548c110\u4ebf\u6d3b\u8dc3\u53c2\u6570\uff0c\u4f18\u5316\u4e86\u591a\u8f6e\u4ea4\u4e92\u7684\u5ef6\u8fdf\u548c\u6210\u672c\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u4fe1\u53f7\u548c\u504f\u597d\u53cd\u9988\uff0c\u786e\u4fdd\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>Step 3.5 Flash\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f8b\u5982\u6570\u5b66\u3001\u7f16\u7801\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u6027\u80fd\u4e0e\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u6548\u7387\u524d\u6cbf\uff0c\u63d0\u4f9b\u4e86\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u590d\u6742\u4ee3\u7406\u7684\u9ad8\u5bc6\u5ea6\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Step 3.5 Flash is a new model that combines smart decision-making with efficient computing.</li>\n    <li>It uses 196 billion parameters for its core and activates 11 billion for quick responses.</li>\n    <li>The model is designed to work better and faster during multiple interactions, reducing costs and delays.</li>\n    <li>It employs a unique learning system that helps it improve consistently in tasks like math, coding, and using tools.</li>\n    <li>Step 3.5 Flash performs well in various benchmarks, showing results similar to advanced models like GPT-5.2 and Gemini 3.0 Pro.</li>\n</ul>"}, "publishedAt": "2026-02-11T02:53:51.000Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 231, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Kimi K2.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u901a\u7528\u667a\u80fd\u6c34\u5e73\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u8fd9\u4e24\u79cd\u6a21\u6001\u80fd\u591f\u76f8\u4e92\u589e\u5f3a\u3002</li>\n    <li>\u5b83\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5305\u62ec\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9SFT\u548c\u8054\u5408\u6587\u672c-\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u6211\u6307\u5bfc\u7684\u5e76\u884c\u4ee3\u7406\u534f\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u5730\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u6210\u4e0d\u540c\u7684\u5b50\u4efb\u52a1\u5e76\u540c\u65f6\u6267\u884c\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0cKimi K2.5\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14Agent Swarm\u7684\u5ef6\u8fdf\u6bd4\u5355\u4ee3\u7406\u57fa\u7ebf\u964d\u4f4e\u4e86\u6700\u591a4.5\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kimi K2.5 is an open-source model that combines text and vision to improve intelligence.</li>\n    <li>It uses techniques like joint pre-training and reinforcement learning to enhance both text and vision capabilities.</li>\n    <li>K2.5 introduces Agent Swarm, which breaks down complex tasks into smaller parts and works on them at the same time.</li>\n    <li>Tests show K2.5 performs very well in coding, vision, reasoning, and other tasks.</li>\n    <li>The model is available for researchers and developers to use in real-world applications.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.09082", "authors": [{"_id": "698bea506052d3bed96309cb", "name": "Veuns-Team", "hidden": false}, {"_id": "698bea506052d3bed96309cd", "name": "Changlong Gao", "hidden": false}, {"_id": "698bea506052d3bed96309ce", "user": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "name": "Zhangxuan Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:14.456Z", "hidden": false}, {"_id": "698bea506052d3bed96309cf", "name": "Yulin Liu", "hidden": false}, {"_id": "698bea506052d3bed96309d0", "name": "Xinyu Qiu", "hidden": false}, {"_id": "698bea506052d3bed96309d1", "name": "Shuheng Shen", "hidden": false}, {"_id": "698bea506052d3bed96309d2", "name": "Yue Wen", "hidden": false}, {"_id": "698bea506052d3bed96309d3", "name": "Tianyu Xia", "hidden": false}, {"_id": "698bea506052d3bed96309d4", "name": "Zhenyu Xu", "hidden": false}, {"_id": "698bea506052d3bed96309d5", "user": {"_id": "64cb238576200ec80fe988f8", "avatarUrl": "/avatars/42c48710c7881c9dfbcc075fec3cb600.svg", "isPro": false, "fullname": "zeus", "user": "zengw", "type": "user"}, "name": "Zhengwen Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:24:43.235Z", "hidden": false}, {"_id": "698bea506052d3bed96309d6", "user": {"_id": "654c9dac09dd7ef524a0be1e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png", "isPro": false, "fullname": "beitongzhou", "user": "syorami", "type": "user"}, "name": "Beitong Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:11.859Z", "hidden": false}, {"_id": "698bea506052d3bed96309d7", "name": "Xingran Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309d8", "name": "Weizhi Chen", "hidden": false}, {"_id": "698bea506052d3bed96309d9", "name": "Sunhao Dai", "hidden": false}, {"_id": "698bea506052d3bed96309da", "name": "Jingya Dou", "hidden": false}, {"_id": "698bea506052d3bed96309db", "name": "Yichen Gong", "hidden": false}, {"_id": "698bea506052d3bed96309dc", "name": "Yuan Guo", "hidden": false}, {"_id": "698bea506052d3bed96309dd", "name": "Zhenlin Guo", "hidden": false}, {"_id": "698bea506052d3bed96309de", "user": {"_id": "65e0763a9299e96ee674876e", "avatarUrl": "/avatars/0ea342c9f72fa3b8a8f634559d094907.svg", "isPro": false, "fullname": "fengdian", "user": "fengrudian", "type": "user"}, "name": "Feng Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:04.463Z", "hidden": false}, {"_id": "698bea506052d3bed96309df", "name": "Qian Li", "hidden": false}, {"_id": "698bea506052d3bed96309e0", "name": "Jinzhen Lin", "hidden": false}, {"_id": "698bea506052d3bed96309e1", "name": "Yuqi Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309e2", "name": "Linchao Zhu", "hidden": false}, {"_id": "698bea506052d3bed96309e3", "name": "Liang Chen", "hidden": false}, {"_id": "698bea506052d3bed96309e4", "name": "Zhenyu Guo", "hidden": false}, {"_id": "698bea506052d3bed96309e5", "name": "Changhua Meng", "hidden": false}, {"_id": "698bea506052d3bed96309e6", "name": "Weiqiang Wang", "hidden": false}], "publishedAt": "2026-02-09T18:43:40.000Z", "submittedOnDailyAt": "2026-02-11T00:10:55.649Z", "title": "UI-Venus-1.5 Technical Report", "submittedOnDailyBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "upvotes": 143, "discussionId": "698bea516052d3bed96309e7", "projectPage": "https://ui-venus.github.io/UI-Venus-1.5/", "githubRepo": "https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5", "githubRepoAddedBy": "user", "ai_summary": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.", "ai_keywords": ["GUI agents", "Mid-Training stage", "Online Reinforcement Learning", "full-trajectory rollouts", "Model Merging", "dense variants", "mixture-of-experts variant"], "githubStars": 708, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>UI-Venus-1.5\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aefGUI\u667a\u80fd\u4f53\uff0c\u65e8\u5728\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u4ea4\u4e92\u3002</li>\n    <li>\u8be5\u6a21\u578b\u6709\u4e24\u79cd\u5bc6\u96c6\u578b\u53d8\u4f53\uff082B\u548c8B\uff09\u548c\u4e00\u79cd\u4e13\u5bb6\u6df7\u5408\u578b\u53d8\u4f53\uff0830B-A3B\uff09\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\u3002</li>\n    <li>UI-Venus-1.5\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u6280\u672f\u8fdb\u5c55\uff0c\u5305\u62ec\u5229\u752810\u4ebf\u4e2a\u6807\u8bb0\u8fdb\u884c\u7684\u5168\u9762\u4e2d\u671f\u8bad\u7ec3\u3001\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6784\u5efa\u7684\u7edf\u4e00\u667a\u80fd\u4f53\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUI-Venus-1.5\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u524d\u4e00\u7248\u672c\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f73\u6c34\u5e73\u3002</li>\n    <li>\u5b83\u5728\u5404\u79cd\u4e2d\u56fd\u79fb\u52a8\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u7528\u6237\u6307\u4ee4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UI-Venus-1.5 is a new and improved GUI agent designed to automate tasks in digital environments.</li>\n    <li>It includes two dense models and one mixture-of-experts model to suit different application needs.</li>\n    <li>The model introduces three major improvements: a Mid-Training stage using 10 billion tokens, Online Reinforcement Learning for better navigation, and a unified agent from different domain-specific models.</li>\n    <li>UI-Venus-1.5 achieves top performance on several benchmarks, surpassing previous models.</li>\n    <li>It shows strong navigation skills in various Chinese mobile apps, successfully following user commands in real situations.</li>\n</ul>"}, "publishedAt": "2026-02-09T13:43:40.000Z", "title": "UI-Venus-1.5 Technical Report", "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png", "numComments": 2, "submittedBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "fullname": "Zhangxuan Gu", "name": "zhangxgu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u91d1\u878d\u5e02\u573a\u566a\u58f0\u591a\u4e14\u53d8\u5316\u5927\uff0c\u5bfc\u81f4\u56de\u6d4b\u7ed3\u679c\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u5f71\u54cdalpha\u6316\u6398\u3002</li>\n    <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u6027alpha\u6316\u6398\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u6539\u5584\u6316\u6398\u8fc7\u7a0b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u5b9a\u4f4d\u548c\u4fee\u6b63\u6bcf\u6761\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u6b65\u9aa4\uff0c\u5e76\u91cd\u7ec4\u9ad8\u6536\u76ca\u6bb5\u843d\uff0c\u4ee5\u91cd\u590d\u5229\u7528\u6709\u6548\u6a21\u5f0f\u3002</li>\n    <li>QuantaAlpha\u5728\u56e0\u5b50\u751f\u6210\u65f6\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u63a7\u5236\u56e0\u5b50\u7684\u590d\u6742\u6027\u548c\u5197\u4f59\u6027\u3002</li>\n    <li>\u5728CSI 300\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cQuantaAlpha\u7684\u8868\u73b0\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5728CSI 500\u548cS&P 500\u4e5f\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets are unpredictable, making it hard to test and find effective trading strategies.</li>\n    <li>QuantaAlpha is a new framework designed to improve the process of finding these trading strategies through evolutionary methods.</li>\n    <li>It enhances the mining process by revising less effective steps and combining successful elements from past attempts.</li>\n    <li>QuantaAlpha ensures that the trading strategies it generates are clear and not overly complicated to avoid confusion.</li>\n    <li>Tests show that QuantaAlpha outperforms existing models, achieving strong returns and stability across different market indices.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 18, 2026";
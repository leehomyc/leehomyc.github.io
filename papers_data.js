window.trendingPapers = {
    "today": [{"paper": {"id": "2602.14111", "authors": [{"_id": "69959cbbed493589ceb5be31", "user": {"_id": "6572e9380503eeadb78fb3e3", "avatarUrl": "/avatars/cbf26f1b2c72732d37e8cb847c48f152.svg", "isPro": false, "fullname": "Anton Korznikov", "user": "AntonKorznikov", "type": "user"}, "name": "Anton Korznikov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:45.391Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be32", "user": {"_id": "661e44cf1d8ffc49b57ba07e", "avatarUrl": "/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg", "isPro": false, "fullname": "Andrey Galichin", "user": "andreuka18", "type": "user"}, "name": "Andrey Galichin", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:51.000Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be33", "user": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "isPro": false, "fullname": "Alexey Dontsov", "user": "therem", "type": "user"}, "name": "Alexey Dontsov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:38.913Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be34", "user": {"_id": "66e19e09140031bf85f0e6f3", "avatarUrl": "/avatars/6cce10740eb73f066d7ed0fe8ca3a93a.svg", "isPro": false, "fullname": "Oleg Rogov", "user": "Olegario228", "type": "user"}, "name": "Oleg Rogov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:59.681Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be35", "name": "Ivan Oseledets", "hidden": false}, {"_id": "69959cbbed493589ceb5be36", "user": {"_id": "662f8d645c4db70c77a203b0", "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg", "isPro": false, "fullname": "Elena Tutubalina", "user": "tlenusik", "type": "user"}, "name": "Elena Tutubalina", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T12:30:12.235Z", "hidden": false}], "publishedAt": "2026-02-15T11:53:55.000Z", "submittedOnDailyAt": "2026-02-18T08:38:30.231Z", "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?", "submittedOnDailyBy": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "isPro": false, "fullname": "Alexey Dontsov", "user": "therem", "type": "user"}, "summary": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.", "upvotes": 51, "discussionId": "69959cbbed493589ceb5be37", "ai_summary": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.", "ai_keywords": ["Sparse Autoencoders", "neural networks", "activations", "explained variance", "interpretability", "sparse probing", "causal editing"], "summary_zh": "<ul>\n    <li>\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u88ab\u7528\u6765\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5c06\u6fc0\u6d3b\u5206\u89e3\u4e3a\u53ef\u7406\u89e3\u7684\u7279\u5f81\u3002</li>\n    <li>\u5c3d\u7ba1SAE\u5728\u67d0\u4e9b\u6a21\u578b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u8d1f\u9762\u7ed3\u679c\u8ba9\u4eba\u8d28\u7591\u5b83\u4eec\u63d0\u53d6\u7684\u7279\u5f81\u662f\u5426\u6709\u610f\u4e49\u3002</li>\n    <li>\u5728\u5408\u6210\u73af\u5883\u4e2d\uff0cSAE\u4ec5\u6062\u590d\u4e869%\u7684\u771f\u5b9e\u7279\u5f81\uff0c\u5c3d\u7ba1\u89e3\u91ca\u65b9\u5dee\u8fbe71%\uff0c\u663e\u793a\u5b83\u4eec\u7684\u6838\u5fc3\u4efb\u52a1\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u4e0e\u968f\u673a\u503c\u7684\u57fa\u7ebf\u6bd4\u8f83\uff0c\u53d1\u73b0SAE\u5728\u53ef\u89e3\u91ca\u6027\u548c\u5176\u4ed6\u6307\u6807\u4e0a\u7684\u8868\u73b0\u4e0e\u57fa\u7ebf\u76f8\u4f3c\u3002</li>\n    <li>\u603b\u4f53\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u72b6\u6001\u4e0b\u7684SAE\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u5206\u89e3\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Sparse Autoencoders (SAEs) aim to help understand neural networks by breaking down their activations into simple, understandable features.</li>\n    <li>Despite some success, recent studies show that SAEs may not always find meaningful features in neural networks.</li>\n    <li>In tests with known features, SAEs only identified 9% of true features, indicating a failure in their main purpose, even when they had good reconstruction results.</li>\n    <li>When comparing SAEs to random baselines in real activations, the results were similar in terms of interpretability and other evaluation metrics, suggesting that SAEs are not significantly better.</li>\n    <li>Overall, the findings indicate that current SAEs do not effectively break down the internal workings of neural network models.</li>\n</ul>"}, "publishedAt": "2026-02-15T06:53:55.000Z", "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?", "summary": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14111.png", "numComments": 2, "submittedBy": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "fullname": "Alexey Dontsov", "name": "therem", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12670", "authors": [{"_id": "6994d2138d17d1ee8c10eb51", "user": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "isPro": false, "fullname": "Xiangyi Li", "user": "xdotli", "type": "user"}, "name": "Xiangyi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:43.446Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb52", "name": "Wenbo Chen", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb53", "name": "Yimin Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb54", "name": "Shenghan Zheng", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb55", "user": {"_id": "6462bf90c9cc74e82e270cb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462bf90c9cc74e82e270cb6/usLDOUhyIfDXA2HZvXvps.jpeg", "isPro": true, "fullname": "Kobe Chen", "user": "kobe0938", "type": "user"}, "name": "Xiaokun Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:37.777Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb56", "user": {"_id": "659a1c9511b48706bab783cc", "avatarUrl": "/avatars/6978a5bc7ab284d9f7285f9fd2c8d0e0.svg", "isPro": false, "fullname": "Yifeng He", "user": "yfhe", "type": "user"}, "name": "Yifeng He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:41.599Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb57", "name": "Yubo Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb58", "user": {"_id": "663bd5fcfb931d4660fd18b7", "avatarUrl": "/avatars/17aa421d40fe3532d0ddecbc2accb249.svg", "isPro": false, "fullname": "Bingran You", "user": "bingran-you", "type": "user"}, "name": "Bingran You", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:36:48.789Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb59", "name": "Haotian Shen", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5a", "user": {"_id": "63b5d1bb0d5913eee4869c0d", "avatarUrl": "/avatars/ae397f54bbb3debc1f7903f4c6959ae6.svg", "isPro": false, "fullname": "Jiankai Sun", "user": "zhenv5", "type": "user"}, "name": "Jiankai Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:36:59.144Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5b", "name": "Shuyi Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5c", "user": {"_id": "65cc8abe8ebd392213020575", "avatarUrl": "/avatars/e0b49fe07b3553779992092f60aa0b48.svg", "isPro": false, "fullname": "qunhongzeng", "user": "qunhongzeng", "type": "user"}, "name": "Qunhong Zeng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:07.249Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5d", "name": "Di Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5e", "user": {"_id": "6275a465597c70eb8949fce5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png", "isPro": false, "fullname": "Xuandong Zhao", "user": "Xuandong", "type": "user"}, "name": "Xuandong Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:13.482Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5f", "name": "Yuanli Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb60", "user": {"_id": "65a0dc6690eb7a1524186cc2", "avatarUrl": "/avatars/6ecf7fb5aa74f2453d0e3bc7b9cca0d3.svg", "isPro": true, "fullname": "Roey Ben Chaim", "user": "roeybc", "type": "user"}, "name": "Roey Ben Chaim", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:19.969Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb61", "name": "Zonglin Di", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb62", "name": "Yipeng Gao", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb63", "name": "Junwei He", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb64", "user": {"_id": "63710bca7a5e5d8efdbff215", "avatarUrl": "/avatars/25e7f713d613ec81ba775265eadff8bc.svg", "isPro": false, "fullname": "He", "user": "Yizhuo", "type": "user"}, "name": "Yizhuo He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:45.624Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb65", "name": "Liqiang Jing", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb66", "name": "Luyang Kong", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb67", "name": "Xin Lan", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb68", "name": "Jiachen Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb69", "name": "Songlin Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6a", "name": "Yijiang Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6b", "user": {"_id": "64b5198c25882acb62fb77ef", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png", "isPro": false, "fullname": "Yueqian Lin", "user": "linyueqian", "type": "user"}, "name": "Yueqian Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:39.753Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6c", "name": "Xinyi Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6d", "name": "Xuanqing Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6e", "name": "Haoran Lyu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6f", "name": "Ze Ma", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb70", "name": "Bowei Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb71", "name": "Runhui Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb72", "name": "Tianyu Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb73", "name": "Wengao Ye", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb74", "name": "Yue Zhang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb75", "name": "Hanwen Xing", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb76", "name": "Yiqi Xue", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb77", "name": "Steven Dillmann", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb78", "name": "Han-chung Lee", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"], "publishedAt": "2026-02-13T07:06:06.000Z", "submittedOnDailyAt": "2026-02-18T11:04:39.385Z", "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "submittedOnDailyBy": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "isPro": false, "fullname": "Xiangyi Li", "user": "xdotli", "type": "user"}, "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "upvotes": 36, "discussionId": "6994d2148d17d1ee8c10eb79", "projectPage": "https://skillsbench.ai/", "githubRepo": "https://github.com/benchflow-ai/skillsbench", "githubRepoAddedBy": "user", "ai_summary": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.", "ai_keywords": ["agent skills", "LLM agents", "SkillsBench", "procedural knowledge", "curated Skills", "self-generated Skills", "agent-model configurations", "pass rate", "domain-specific effects"], "githubStars": 413, "organization": {"_id": "69657f24ec1d157f1590a81d", "name": "benchflow", "fullname": "BenchFlow", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"}, "summary_zh": "<ul>\n    <li>Agent Skills \u662f\u589e\u5f3aLLM\u4ee3\u7406\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u5305\uff0c\u7528\u4e8e\u63a8\u7406\u65f6\u3002</li>\n    <li>SkillsBench \u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec86\u4e2a\u4efb\u52a1\u548c11\u4e2a\u9886\u57df\uff0c\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u6280\u80fd\u3002</li>\n    <li>\u6d4b\u8bd5\u6761\u4ef6\u5305\u62ec\u65e0\u6280\u80fd\u3001\u7b56\u5212\u6280\u80fd\u548c\u81ea\u751f\u6210\u6280\u80fd\u3002</li>\n    <li>\u7b56\u5212\u6280\u80fd\u5e73\u5747\u63d0\u5347\u901a\u8fc7\u738716.2\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u5728\u4e0d\u540c\u9886\u57df\u6548\u679c\u5dee\u5f02\u663e\u8457\u3002</li>\n    <li>\u81ea\u751f\u6210\u6280\u80fd\u5e73\u5747\u6ca1\u6709\u6548\u679c\uff0c\u8f83\u5c0f\u7684\u6280\u80fd\u6a21\u5757\u4f18\u4e8e\u5168\u9762\u6587\u6863\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agent Skills enhance the abilities of LLM agents but measuring their effectiveness is challenging.</li>\n    <li>SkillsBench is a new benchmark that includes 86 tasks across 11 areas, tested with curated and self-generated Skills.</li>\n    <li>Curated Skills improve success rates by an average of 16.2 percentage points, but the impact varies by area.</li>\n    <li>Self-generated Skills do not help on average, indicating that models struggle to create effective procedural knowledge.</li>\n    <li>Focused Skills with fewer modules are more effective than extensive documentation, and smaller models with Skills can perform as well as larger models without them.</li>\n</ul>"}, "publishedAt": "2026-02-13T02:06:06.000Z", "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12670.png", "numComments": 3, "submittedBy": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "fullname": "Xiangyi Li", "name": "xdotli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "69657f24ec1d157f1590a81d", "name": "benchflow", "fullname": "BenchFlow", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.15763", "authors": [{"_id": "6995270f8d17d1ee8c10ebc5", "name": "GLM-5 Team", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc7", "name": "Aohan Zeng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc8", "name": "Xin Lv", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc9", "user": {"_id": "62b196646d3d059f40c3df19", "avatarUrl": "/avatars/dbddf54ae949437223f3a438d30ef653.svg", "isPro": false, "fullname": "Zhenyu Hou", "user": "think2try", "type": "user"}, "name": "Zhenyu Hou", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:01.080Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebca", "user": {"_id": "63033dc4e1e7f0e03a5e1a31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg", "isPro": false, "fullname": "Zhengxiao Du", "user": "zxdu20", "type": "user"}, "name": "Zhengxiao Du", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:03.474Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcb", "user": {"_id": "6231576e92e83fd1179ac3f0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664543160657-6231576e92e83fd1179ac3f0.jpeg", "isPro": false, "fullname": "Qinkai Zheng", "user": "Stanislas", "type": "user"}, "name": "Qinkai Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:21.181Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcc", "name": "Bin Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcd", "name": "Da Yin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebce", "name": "Chendi Ge", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcf", "user": {"_id": "62d00ff8dd7bdfc5e5c553c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d00ff8dd7bdfc5e5c553c6/u9Be7K16IS2Hc5OqKctEA.jpeg", "isPro": false, "fullname": "chengxing xie", "user": "yitianlian", "type": "user"}, "name": "Chengxing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:28.339Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd0", "user": {"_id": "65eaf755ab0a6a90da55ab58", "avatarUrl": "/avatars/a46890a9d067a913513edf3759f12c85.svg", "isPro": false, "fullname": "Cunxiang Wang", "user": "wangcunxiang", "type": "user"}, "name": "Cunxiang Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T10:16:12.082Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd1", "name": "Gengzheng Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd2", "name": "Hao Zeng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd3", "user": {"_id": "622ec54ce27c88667db094ad", "avatarUrl": "/avatars/8f06594b625a9c4fca1fffec4885bbdc.svg", "isPro": false, "fullname": "Haoke Zhang", "user": "zhk", "type": "user"}, "name": "Haoke Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:43.672Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd4", "name": "Haoran Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd5", "user": {"_id": "654ccc038a299b6be086cf1b", "avatarUrl": "/avatars/87d0f3c0f991368ce923da32cdd971a1.svg", "isPro": false, "fullname": "Huilong Chen", "user": "HuilongChen", "type": "user"}, "name": "Huilong Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:16.751Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd6", "name": "Jiajie Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd7", "name": "Jian Jiao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd8", "name": "Jiaqi Guo", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd9", "user": {"_id": "65655c3ed35fc55406e116aa", "avatarUrl": "/avatars/d45081ec1617bb737ea531866b76f57a.svg", "isPro": false, "fullname": "jingsen", "user": "wangjingsen", "type": "user"}, "name": "Jingsen Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:34:23.674Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebda", "name": "Jingzhao Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdb", "user": {"_id": "650d8ddbcbd0c7d550dc8278", "avatarUrl": "/avatars/3e49752b6b3c9f3875b4470cebb838e6.svg", "isPro": false, "fullname": "wujinzhu", "user": "kimjohn", "type": "user"}, "name": "Jinzhu Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:34:46.441Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdc", "user": {"_id": "64b73d6353d91a364aa8cea5", "avatarUrl": "/avatars/721553ab563cf13d8e7bfde088e3b753.svg", "isPro": false, "fullname": "Kedong Wang", "user": "mrwkd123", "type": "user"}, "name": "Kedong Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:47.078Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdd", "name": "Lei Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebde", "name": "Lin Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdf", "user": {"_id": "60eff04e22ab0ac83b0fc9d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60eff04e22ab0ac83b0fc9d8/pBjftNyFN1qB8Br3FZQmD.jpeg", "isPro": false, "fullname": "lucen zhong", "user": "anchorzhong", "type": "user"}, "name": "Lucen Zhong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:53.663Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe0", "user": {"_id": "64f1abd2b12bdcef55e1c078", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1abd2b12bdcef55e1c078/yG9HR_imUWqESN-JNcjf1.jpeg", "isPro": false, "fullname": "Mingdao Liu", "user": "lambdax", "type": "user"}, "name": "Mingdao Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:00.291Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe1", "user": {"_id": "6706db274341dcee45811105", "avatarUrl": "/avatars/125ef02f12785b8f51e84639d879efaa.svg", "isPro": false, "fullname": "Mingming Zhao", "user": "Lukas0510", "type": "user"}, "name": "Mingming Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:15.048Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe2", "name": "Pengfan Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe3", "name": "Qian Dong", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe4", "name": "Rui Lu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe5", "name": "Shuang-Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe6", "name": "Shulin Cao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe7", "name": "Song Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe8", "name": "Ting Jiang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe9", "name": "Xiaodong Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebea", "name": "Xiaohan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebeb", "name": "Xuancheng Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebec", "user": {"_id": "658834ba4f9d2b955e19d389", "avatarUrl": "/avatars/89d9a6f9fac67391dc052d49def1e758.svg", "isPro": false, "fullname": "Xuezhen Dong", "user": "xzdong", "type": "user"}, "name": "Xuezhen Dong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:25.460Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebed", "name": "Yabo Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebee", "name": "Yao Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebef", "name": "Yifan An", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf0", "name": "Yilin Niu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf1", "name": "Yitong Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf2", "name": "Yuanhao Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf3", "name": "Yukuo Cen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf4", "user": {"_id": "64ed568ccf6118a9379a61b8", "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg", "isPro": false, "fullname": "Yushi Bai", "user": "bys0318", "type": "user"}, "name": "Yushi Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T13:36:07.294Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf5", "name": "Zhongpei Qiao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf6", "name": "Zihan Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf7", "name": "Zikang Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf8", "name": "Zilin Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf9", "name": "Ziqiang Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfa", "name": "Zixuan Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfb", "name": "Bojie Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfc", "name": "Bosi Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfd", "name": "Can Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfe", "name": "Changpeng Cai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebff", "name": "Chao Yu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec00", "name": "Chen Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec01", "name": "Chen Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec02", "name": "Chenghua Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec03", "name": "Chengwei Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec04", "name": "Chenhui Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec05", "name": "Chenzheng Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec06", "name": "Congfeng Yin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec07", "name": "Daoyan Lin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec08", "name": "Dayong Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec09", "name": "Di Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0a", "name": "Ding Ai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0b", "name": "Erle Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0c", "name": "Fangzhou Yi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0d", "name": "Feiyu Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0e", "name": "Guohong Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0f", "name": "Hailong Sun", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec10", "name": "Haisha Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec11", "name": "Haiyi Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec12", "name": "Hanchen Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec13", "name": "Hanrui Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec14", "name": "Hanyu Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec15", "name": "Hao Peng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec16", "name": "Hao Tai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec17", "name": "Haobo Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec18", "name": "He Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec19", "name": "Hongwei Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1a", "name": "Hongxi Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1b", "name": "Hongyu Ge", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1c", "name": "Huan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1d", "name": "Huan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1e", "name": "Huanpeng Chu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1f", "name": "Jia'ni Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec20", "name": "Jiachen Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec21", "name": "Jiajing Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec22", "name": "Jiamin Ren", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec23", "name": "Jiapeng Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec24", "name": "Jiaxin Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec25", "name": "Jiayi Gui", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec26", "name": "Jiayue Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec27", "name": "Jijie Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec28", "name": "Jing An", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec29", "name": "Jing Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2a", "name": "Jingwei Yuan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2b", "name": "Jinhua Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2c", "name": "Jinxin Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2d", "name": "Junkai Zhi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2e", "name": "Junwen Duan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2f", "name": "Kaiyue Zhou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec30", "name": "Kangjian Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec31", "name": "Ke Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec32", "name": "Keyun Luo", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec33", "name": "Laiqiang Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec34", "name": "Leigang Sha", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec35", "name": "Liang Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec36", "name": "Lindong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec37", "name": "Lintao Ding", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec38", "name": "Lu Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec39", "name": "Minghao Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3a", "name": "Nianyi Lin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3b", "name": "Pan Ta", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3c", "name": "Qiang Zou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3d", "name": "Rongjun Song", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3e", "name": "Ruiqi Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3f", "name": "Shangqing Tu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec40", "name": "Shangtong Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec41", "name": "Shaoxiang Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec42", "name": "Shengyan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec43", "name": "Shijie Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec44", "name": "Shuang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec45", "name": "Shuyi Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec46", "name": "Wei Qin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec47", "name": "Wei Tian", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec48", "name": "Weining Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec49", "name": "Wenbo Yu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4a", "name": "Wenjie Liang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4b", "name": "Xiang Kuang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4c", "name": "Xiangmeng Cheng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4d", "name": "Xiangyang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4e", "name": "Xiaoquan Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4f", "name": "Xiaowei Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec50", "name": "Xiaoying Ling", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec51", "name": "Xing Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec52", "name": "Xingye Xia", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec53", "name": "Xinyuan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec54", "name": "Xinze Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec55", "name": "Xirui Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec56", "name": "Xunkai Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec57", "name": "Yandong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec58", "name": "Yanfu Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec59", "name": "Yidong Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5a", "name": "Yifan Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5b", "name": "Yijun Tan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5c", "name": "Yilin Zhou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5d", "name": "Yiming Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5e", "name": "Ying Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5f", "name": "Yinpei Su", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec60", "name": "Yipeng Geng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec61", "name": "Yipeng Geng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec62", "name": "Yong Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec63", "name": "Yonglin Tan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec64", "name": "Yuean Bi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec65", "name": "Yuhan Shen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec66", "name": "Yuhao Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec67", "name": "Yujiang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec68", "name": "Yunan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec69", "name": "Yunqing Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6a", "name": "Yuntao Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6b", "name": "Yurong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6c", "name": "Yutao Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6d", "name": "Yuxi Duan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6e", "name": "Yuxuan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6f", "name": "Zezhen Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec70", "name": "Zhengtao Jiang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec71", "name": "Zhenhe Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec72", "name": "Zheyu Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec73", "name": "Zhixiang Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec74", "name": "Zhuo Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec75", "name": "Zhuoer Feng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec76", "name": "Zijun Yao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec77", "name": "Ziwei Chai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec78", "name": "Ziyuan Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec79", "name": "Zuzhou Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7a", "name": "Bin Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7b", "name": "Minlie Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7c", "user": {"_id": "62ec23fcbd19e355478fc584", "avatarUrl": "/avatars/d7c567ef5f20bb3b9905cb5015d11e12.svg", "isPro": false, "fullname": "Hongning Wang", "user": "howang", "type": "user"}, "name": "Hongning Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:02.752Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7d", "user": {"_id": "65df8cbc2705d9672f55d1aa", "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg", "isPro": false, "fullname": "Juanzi Li", "user": "juanli", "type": "user"}, "name": "Juanzi Li", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:56.007Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7e", "user": {"_id": "640e73bdfdeaae1390857b62", "avatarUrl": "/avatars/cd6779e30f716002a7838ed93d5c0754.svg", "isPro": false, "fullname": "Yuxiao Dong", "user": "yuxiaod", "type": "user"}, "name": "Yuxiao Dong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:48.099Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7f", "user": {"_id": "640dff05474aa6f89556677e", "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg", "isPro": false, "fullname": "Jie Tang", "user": "jerytang", "type": "user"}, "name": "Jie Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:39.525Z", "hidden": false}], "publishedAt": "2026-02-17T17:50:56.000Z", "submittedOnDailyAt": "2026-02-18T00:12:28.521Z", "title": "GLM-5: from Vibe Coding to Agentic Engineering", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.", "upvotes": 31, "discussionId": "6995270f8d17d1ee8c10ec80", "githubRepo": "https://github.com/zai-org/GLM-5", "githubRepoAddedBy": "user", "ai_summary": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.", "ai_keywords": ["DSA", "asynchronous reinforcement learning", "agentic engineering", "vibe coding", "ARC capabilities", "post-training efficiency", "model alignment", "autonomous agents", "software engineering", "open benchmarks"], "githubStars": 1103, "summary_zh": "<ul>\n    <li>GLM-5\u662f\u4e00\u4e2a\u65b0\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5c06\u201c\u6c1b\u56f4\u7f16\u7801\u201d\u8f6c\u53d8\u4e3a\u201c\u81ea\u4e3b\u5de5\u7a0b\u201d\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u524d\u4ee3\u57fa\u7840\u4e0a\uff0c\u91c7\u7528\u4e86DSA\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u957f\u4e0a\u4e0b\u6587\u7684\u4e00\u81f4\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u81ea\u4e3b\u6027\uff0cGLM-5\u5f15\u5165\u4e86\u65b0\u7684\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u540e\u7684\u6548\u7387\u3002</li>\n    <li>\u65b0\u63d0\u51fa\u7684\u5f02\u6b65\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u4ece\u590d\u6742\u7684\u957f\u65f6\u95f4\u4ea4\u4e92\u4e2d\u5b66\u4e60\u3002</li>\n    <li>GLM-5\u5728\u4e3b\u8981\u7684\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u7f16\u7801\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u57fa\u51c6\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u4ee5\u5728https://github.com/zai-org/GLM-5\u627e\u5230\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>GLM-5 is a new advanced model that improves how we use coding and engineering techniques.</li>\n    <li>It reduces training and operation costs while keeping its ability to handle long contexts intact.</li>\n    <li>A new learning method improves how the model learns after training, making it more efficient.</li>\n    <li>GLM-5 uses innovative algorithms to enhance learning from complex tasks over longer periods.</li>\n    <li>It performs exceptionally well in real-world coding tasks, outperforming previous models in software engineering.</li>\n</ul>"}, "publishedAt": "2026-02-17T12:50:56.000Z", "title": "GLM-5: from Vibe Coding to Agentic Engineering", "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15763.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 234, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14299", "authors": [{"_id": "69952cc38d17d1ee8c10ec97", "name": "Ming Li", "hidden": false}, {"_id": "69952cc38d17d1ee8c10ec98", "user": {"_id": "6534a434e778506c5b1e5be8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6534a434e778506c5b1e5be8/349SdAnjEdIQJSzWvKfZ4.png", "isPro": true, "fullname": "Xirui Li", "user": "AIcell", "type": "user"}, "name": "Xirui Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:52.645Z", "hidden": false}, {"_id": "69952cc38d17d1ee8c10ec99", "user": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "isPro": false, "fullname": "Tianyi Zhou", "user": "zhoutianyi", "type": "user"}, "name": "Tianyi Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:54.520Z", "hidden": false}], "publishedAt": "2026-02-15T20:15:28.000Z", "submittedOnDailyAt": "2026-02-18T00:37:23.132Z", "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook", "submittedOnDailyBy": {"_id": "65031d01cccc7b28a388c719", "avatarUrl": "/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg", "isPro": false, "fullname": "Ming Li", "user": "MingLiiii", "type": "user"}, "summary": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.", "upvotes": 21, "discussionId": "69952cc38d17d1ee8c10ec9a", "projectPage": "https://github.com/MingLiiii/Moltbook_Socialization", "githubRepo": "https://github.com/MingLiiii/Moltbook_Socialization", "githubRepoAddedBy": "user", "ai_summary": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.", "ai_keywords": ["large language model agents", "networked environments", "AI agent societies", "semantic stabilization", "lexical turnover", "individual inertia", "influence persistence", "collective consensus", "systemic diagnosis", "dynamic evolution"], "githubStars": 7, "organization": {"_id": "647f5b7daa8c04bbf938c625", "name": "umd-zhou-lab", "fullname": "Tianyi Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u793e\u4f1a\u662f\u5426\u4f1a\u50cf\u4eba\u7c7b\u793e\u4f1a\u4e00\u6837\u53d1\u5c55\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9a\u91cf\u8bca\u65ad\u6846\u67b6\uff0c\u5206\u6790AI\u4ee3\u7406\u793e\u4f1a\u7684\u52a8\u6001\u6f14\u53d8\u3002</li>\n    <li>\u53d1\u73b0\u8fd9\u4e9b\u4ee3\u7406\u5728\u8bed\u4e49\u4e0a\u5feb\u901f\u7a33\u5b9a\uff0c\u4f46\u4e2a\u4f53\u4e4b\u95f4\u4fdd\u6301\u9ad8\u5ea6\u591a\u6837\u6027\u3002</li>\n    <li>\u4ee3\u7406\u4e4b\u95f4\u7f3a\u4e4f\u9002\u5e94\u6027\u56de\u5e94\uff0c\u5bfc\u81f4\u5f71\u54cd\u529b\u65e0\u6cd5\u6301\u7eed\u3002</li>\n    <li>\u7ed3\u8bba\u8868\u660e\uff0c\u89c4\u6a21\u548c\u4e92\u52a8\u5bc6\u5ea6\u4e0d\u8db3\u4ee5\u4fc3\u6210\u793e\u4f1a\u5316\uff0c\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u548c\u5206\u6790\u65b0\u4e00\u4ee3AI\u4ee3\u7406\u793e\u4f1a\u7684\u539f\u5219\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study investigates whether AI agent societies behave like human social systems as they grow in networked environments.</li>\n    <li>Moltbook is used to model a future where autonomous agents interact in a constantly evolving online society.</li>\n    <li>A new framework is introduced to analyze how these AI societies change over time, focusing on stability, diversity, and influence among agents.</li>\n    <li>Results show that while overall language use stabilizes, individual agents remain diverse and do not strongly influence each other.</li>\n    <li>The findings suggest that simply having many agents and frequent interactions does not guarantee socialization, leading to recommendations for designing future AI societies.</li>\n</ul>"}, "publishedAt": "2026-02-15T15:15:28.000Z", "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook", "summary": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14299.png", "numComments": 3, "submittedBy": {"_id": "65031d01cccc7b28a388c719", "avatarUrl": "/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg", "fullname": "Ming Li", "name": "MingLiiii", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "647f5b7daa8c04bbf938c625", "name": "umd-zhou-lab", "fullname": "Tianyi Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.15112", "authors": [{"_id": "69952dd98d17d1ee8c10ecab", "user": {"_id": "6622b683b904cd87220d285d", "avatarUrl": "/avatars/8f4f79c2f5fbd69b498cbe03c6c2e9a2.svg", "isPro": false, "fullname": "Aniketh", "user": "anikethh", "type": "user"}, "name": "Aniketh Garikaparthi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T13:36:08.825Z", "hidden": false}, {"_id": "69952dd98d17d1ee8c10ecac", "name": "Manasi Patwardhan", "hidden": false}, {"_id": "69952dd98d17d1ee8c10ecad", "name": "Arman Cohan", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6622b683b904cd87220d285d/o11NNNjEOl-7s5I-lL2E3.png"], "publishedAt": "2026-02-16T19:00:03.000Z", "submittedOnDailyAt": "2026-02-18T01:22:46.918Z", "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research", "submittedOnDailyBy": {"_id": "6622b683b904cd87220d285d", "avatarUrl": "/avatars/8f4f79c2f5fbd69b498cbe03c6c2e9a2.svg", "isPro": false, "fullname": "Aniketh", "user": "anikethh", "type": "user"}, "summary": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.", "upvotes": 14, "discussionId": "69952dd98d17d1ee8c10ecae", "githubRepo": "https://github.com/Anikethh/ResearchGym", "githubRepoAddedBy": "user", "ai_summary": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.", "ai_keywords": ["ResearchGym", "AI agents", "end-to-end research", "ICML", "ICLR", "ACL", "datasets", "evaluation harness", "baseline implementations", "containerized task environments", "sub-tasks", "GPT-5", "hypothesis generation", "experimental execution", "autonomous agents", "capability-reliability gap", "context length", "Claude Code", "Codex"], "githubStars": 5, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86ResearchGym\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u7aef\u5230\u7aef\u7814\u7a76\u4e2d\u7684\u57fa\u51c6\u548c\u6267\u884c\u73af\u5883\u3002</li>\n    <li>\u6211\u4eec\u4eceICML\u3001ICLR\u548cACL\u7684\u4e94\u7bc7\u8bba\u6587\u4e2d\u63d0\u53d6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u57fa\u51c6\u5b9e\u73b0\uff0c\u4f46\u4e0d\u5305\u542b\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u8fd9\u4e9b\u73af\u5883\u5305\u542b39\u4e2a\u5b50\u4efb\u52a1\uff0c\u4ee3\u7406\u9700\u8981\u63d0\u51fa\u65b0\u5047\u8bbe\u3001\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u8bd5\u56fe\u8d85\u8d8a\u4eba\u7c7b\u57fa\u51c6\u3002</li>\n    <li>\u5bf9GPT-5\u9a71\u52a8\u7684\u4ee3\u7406\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0c\u53d1\u73b0\u5176\u80fd\u529b\u548c\u53ef\u9760\u6027\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u53ea\u67096.7%\u7684\u8bc4\u4f30\u4e2d\u6709\u6240\u6539\u5584\u3002</li>\n    <li>\u867d\u7136\u4ee3\u7406\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8868\u73b0\uff0c\u4f46\u6574\u4f53\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u663e\u793a\u51fa\u591a\u79cd\u957f\u671f\u5931\u8d25\u6a21\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ResearchGym is a new tool for testing AI agents in research tasks.</li>\n    <li>It uses five research papers from conferences and sets up environments for 39 sub-tasks without including the proposed methods from the papers.</li>\n    <li>AI agents must come up with new ideas, run experiments, and try to beat human performance on various metrics.</li>\n    <li>In tests, an AI powered by GPT-5 showed a large gap in reliability, succeeding in improving only 1 out of 15 evaluations.</li>\n    <li>Despite challenges, the agent occasionally performed well, and other AI models like Claude Code and Codex showed similar issues.</li>\n</ul>"}, "publishedAt": "2026-02-16T14:00:03.000Z", "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research", "summary": "We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6622b683b904cd87220d285d/o11NNNjEOl-7s5I-lL2E3.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15112.png", "numComments": 3, "submittedBy": {"_id": "6622b683b904cd87220d285d", "avatarUrl": "/avatars/8f4f79c2f5fbd69b498cbe03c6c2e9a2.svg", "fullname": "Aniketh", "name": "anikethh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12279", "authors": [{"_id": "69947c8fd2ea89ac106cf9af", "user": {"_id": "62b67da0f56de4396ca9e44b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg", "isPro": false, "fullname": "Liangyu Chen", "user": "liangyuch", "type": "user"}, "name": "Leon Liangyu Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:38:47.995Z", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b0", "user": {"_id": "650a8979c19e5b4c8a6ff062", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650a8979c19e5b4c8a6ff062/64_JuECX_k_-uK7m7nlua.jpeg", "isPro": false, "fullname": "Haoyu Ma", "user": "haoyum1997", "type": "user"}, "name": "Haoyu Ma", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:38:57.190Z", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b1", "user": {"_id": "65f09aefcaf237b0a2d4d3ff", "avatarUrl": "/avatars/5b03ea49e878058efa3c88e53a6e6a9b.svg", "isPro": false, "fullname": "Zhipeng Fan", "user": "Jetp", "type": "user"}, "name": "Zhipeng Fan", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:39:04.149Z", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b2", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:39:13.973Z", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b3", "name": "Animesh Sinha", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b4", "user": {"_id": "6549417b3ce45eb764faf993", "avatarUrl": "/avatars/d310f475d0697f5f13b3d4141ea0ccaf.svg", "isPro": false, "fullname": "Xiaoliang Dai", "user": "daixl1992", "type": "user"}, "name": "Xiaoliang Dai", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:39:20.967Z", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b5", "name": "Jialiang Wang", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b6", "name": "Zecheng He", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b7", "name": "Jianwei Yang", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b8", "user": {"_id": "62aba526cae4462c0c6caa0f", "avatarUrl": "/avatars/430560ec2c2547f819225769ab432f30.svg", "isPro": false, "fullname": "Chunyuan Li", "user": "Chunyuan24", "type": "user"}, "name": "Chunyuan Li", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:39:39.037Z", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9b9", "name": "Junzhe Sun", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9ba", "name": "Chu Wang", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9bb", "user": {"_id": "677c8b2e92550a07fcad0f50", "avatarUrl": "/avatars/2be26e8f25e98cfe5b1d227ee0409cd0.svg", "isPro": false, "fullname": "Serena Yeung-Levy", "user": "yeunglevy", "type": "user"}, "name": "Serena Yeung-Levy", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:39:53.861Z", "hidden": false}, {"_id": "69947c8fd2ea89ac106cf9bc", "name": "Felix Juefei-Xu", "hidden": false}], "publishedAt": "2026-02-12T18:59:49.000Z", "submittedOnDailyAt": "2026-02-18T06:59:40.830Z", "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling", "submittedOnDailyBy": {"_id": "62b67da0f56de4396ca9e44b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg", "isPro": false, "fullname": "Liangyu Chen", "user": "liangyuch", "type": "user"}, "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.", "upvotes": 13, "discussionId": "69947c90d2ea89ac106cf9bd", "ai_summary": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities.", "ai_keywords": ["unified models", "multimodal understanding", "multimodal generation", "test-time scaling", "chain-of-thought reasoning", "agentic data synthesis", "unified model training", "test-time inference", "cognitive behaviors", "visual reasoning"], "summary_zh": "<ul>\n    <li>\u7edf\u4e00\u6a21\u578b\u53ef\u4ee5\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u5904\u7406\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u4f46\u901a\u5e38\u4e00\u6b21\u6027\u8f93\u51fa\u7ed3\u679c\u3002</li>\n    <li>\u8bb8\u591a\u591a\u6a21\u6001\u4efb\u52a1\u9700\u8981\u5206\u89e3\u6307\u4ee4\u3001\u9a8c\u8bc1\u4e2d\u95f4\u7ed3\u679c\u5e76\u8fdb\u884c\u8fed\u4ee3\u4fee\u6b63\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86UniT\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u591a\u4e2a\u8f6e\u6b21\u4e2d\u8fdb\u884c\u63a8\u7406\u3001\u9a8c\u8bc1\u548c\u6539\u8fdb\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u77ed\u63a8\u7406\u8f68\u8ff9\u8bad\u7ec3\u7684\u7edf\u4e00\u6a21\u578b\u80fd\u5728\u6d4b\u8bd5\u65f6\u63a8\u5e7f\u5230\u66f4\u957f\u7684\u63a8\u7406\u94fe\u3002</li>\n    <li>\u8bad\u7ec3\u751f\u6210\u548c\u7f16\u8f91\u8f68\u8ff9\u80fd\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u5206\u5e03\u89c6\u89c9\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Unified models can understand and generate different types of data but usually do it in one go without improving their results.</li>\n    <li>Many complex tasks need models to break down instructions, check results, and make repeated adjustments.</li>\n    <li>The new framework, UniT, allows unified models to think, check, and refine their outputs over multiple steps.</li>\n    <li>Key findings show that models trained on short reasoning can handle longer tasks, and that step-by-step reasoning is more efficient than trying to do everything at once.</li>\n    <li>Training on both creating and editing data helps improve how models understand and reason about visuals that are different from what they were trained on.</li>\n</ul>"}, "publishedAt": "2026-02-12T13:59:49.000Z", "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling", "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12279.png", "numComments": 1, "submittedBy": {"_id": "62b67da0f56de4396ca9e44b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg", "fullname": "Liangyu Chen", "name": "liangyuch", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.15547", "authors": [{"_id": "69958815ed493589ceb5be21", "user": {"_id": "64d22f33032a420d1863b6ea", "avatarUrl": "/avatars/ed3eaf4bab70dd6ab9a2b67b5928e4fb.svg", "isPro": false, "fullname": "Mohammad Kalim Akram", "user": "makram93", "type": "user"}, "name": "Mohammad Kalim Akram", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:06:40.906Z", "hidden": false}, {"_id": "69958815ed493589ceb5be22", "user": {"_id": "64c23f6d569648a60737eddb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c23f6d569648a60737eddb/iZq7bp-yYaGl5VBVoN5Dg.jpeg", "isPro": false, "fullname": "Saba Sturua", "user": "jupyterjazz", "type": "user"}, "name": "Saba Sturua", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:06:49.861Z", "hidden": false}, {"_id": "69958815ed493589ceb5be23", "user": {"_id": "6911a37ace661438b73ff25d", "avatarUrl": "/avatars/21ae8db1f909229d22d2c93e4f1cb0e0.svg", "isPro": false, "fullname": "Nastia Havriushenko", "user": "ahavrius", "type": "user"}, "name": "Nastia Havriushenko", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:06:58.693Z", "hidden": false}, {"_id": "69958815ed493589ceb5be24", "user": {"_id": "645b5a5b438d6cfbe1ad12a1", "avatarUrl": "/avatars/f53e63f8e52115a95814b7be1f07a391.svg", "isPro": false, "fullname": "Quentin Herreros", "user": "qherreros", "type": "user"}, "name": "Quentin Herreros", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:07:05.539Z", "hidden": false}, {"_id": "69958815ed493589ceb5be25", "name": "Michael G\u00fcnther", "hidden": false}, {"_id": "69958815ed493589ceb5be26", "user": {"_id": "60638400b1703ddba0d458a7", "avatarUrl": "/avatars/50228a18e7f211275a09e3cbd6e2931e.svg", "isPro": false, "fullname": "Maximilian Werk", "user": "mwerk", "type": "user"}, "name": "Maximilian Werk", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:07:16.360Z", "hidden": false}, {"_id": "69958815ed493589ceb5be27", "user": {"_id": "603763514de52ff951d89793", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png", "isPro": false, "fullname": "Han Xiao", "user": "hanxiao", "type": "user"}, "name": "Han Xiao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T12:34:37.784Z", "hidden": false}], "publishedAt": "2026-02-17T12:50:50.000Z", "submittedOnDailyAt": "2026-02-18T11:26:39.504Z", "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation", "submittedOnDailyBy": {"_id": "603763514de52ff951d89793", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png", "isPro": false, "fullname": "Han Xiao", "user": "hanxiao", "type": "user"}, "summary": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.", "upvotes": 9, "discussionId": "69958816ed493589ceb5be28", "ai_summary": "Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization.", "ai_keywords": ["text embedding models", "contrastive loss", "model distillation", "semantic similarity", "information retrieval", "clustering", "classification", "embedding models", "long texts", "binary quantization"], "organization": {"_id": "63563e0c2d14fcd7d83743cf", "name": "jinaai", "fullname": "Jina AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/603763514de52ff951d89793/wD54VbAHHyHop3uYlJKl4.png"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5e7f\u6cdb\u7528\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u5982\u4fe1\u606f\u68c0\u7d22\u548c\u5206\u7c7b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u6a21\u578b\u84b8\u998f\u6280\u672f\u548c\u7279\u5b9a\u4efb\u52a1\u7684\u5bf9\u6bd4\u635f\u5931\uff0c\u751f\u6210\u9ad8\u6027\u80fd\u7684\u5c0f\u578b\u5d4c\u5165\u6a21\u578b\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6bd4\u5355\u7eaf\u7684\u5bf9\u6bd4\u8bad\u7ec3\u6216\u84b8\u998f\u8bad\u7ec3\u66f4\u6709\u6548\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u7684\u6a21\u578bjina-embeddings-v5-text-small\u548cjina-embeddings-v5-text-nano\u5728\u540c\u7c7b\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u652f\u6301\u591a\u79cd\u8bed\u8a00\u7684\u957f\u6587\u672c\uff0c\u5e76\u4e14\u5728\u622a\u65ad\u548c\u4e8c\u8fdb\u5236\u91cf\u5316\u65f6\u4f9d\u7136\u4fdd\u6301\u7a33\u5b9a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text embedding models help with tasks like finding similar information, grouping data, and classifying it.</li>\n    <li>The authors propose a new training method that mixes model distillation with specific loss functions for better performance.</li>\n    <li>This new approach is more effective for creating smaller models compared to older training methods.</li>\n    <li>The resulting models, named jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, perform very well for their size.</li>\n    <li>These models can handle long texts in multiple languages and maintain quality even when shortened or compressed.</li>\n</ul>"}, "publishedAt": "2026-02-17T07:50:50.000Z", "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation", "summary": "Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15547.png", "numComments": 1, "submittedBy": {"_id": "603763514de52ff951d89793", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png", "fullname": "Han Xiao", "name": "hanxiao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19, "isUserFollowing": false}, "organization": {"_id": "63563e0c2d14fcd7d83743cf", "name": "jinaai", "fullname": "Jina AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/603763514de52ff951d89793/wD54VbAHHyHop3uYlJKl4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.14486", "authors": [{"_id": "69954c908d17d1ee8c10ecc4", "user": {"_id": "616a7abc82a777e9333a1567", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634368720319-616a7abc82a777e9333a1567.png", "isPro": false, "fullname": "Fabian Gr\u00f6ger", "user": "FabianGroeger", "type": "user"}, "name": "Fabian Gr\u00f6ger", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:56.112Z", "hidden": false}, {"_id": "69954c908d17d1ee8c10ecc5", "name": "Shuo Wen", "hidden": false}, {"_id": "69954c908d17d1ee8c10ecc6", "name": "Maria Brbi\u0107", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/616a7abc82a777e9333a1567/lP32oGpT13vpLGBXdguN9.mp4"], "publishedAt": "2026-02-16T06:01:23.000Z", "submittedOnDailyAt": "2026-02-18T02:55:51.556Z", "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View", "submittedOnDailyBy": {"_id": "616a7abc82a777e9333a1567", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634368720319-616a7abc82a777e9333a1567.png", "isPro": false, "fullname": "Fabian Gr\u00f6ger", "user": "FabianGroeger", "type": "user"}, "summary": "The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.", "upvotes": 7, "discussionId": "69954c908d17d1ee8c10ecc7", "projectPage": "https://brbiclab.epfl.ch/projects/aristotelian/", "githubRepo": "https://github.com/mlbio-epfl/aristotelian", "githubRepoAddedBy": "user", "ai_summary": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.", "ai_keywords": ["representational similarity", "neural networks", "spectral measures", "neighborhood similarity", "permutation-based null-calibration framework", "Platonic Representation Hypothesis", "Aristotelian Representation Hypothesis"], "githubStars": 8, "summary_zh": "<ul>\n    <li>\u201c\u67cf\u62c9\u56fe\u8868\u5f81\u5047\u8bf4\u201d\u8ba4\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u5f81\u8d8b\u5411\u4e8e\u4e00\u4e2a\u5171\u540c\u7684\u73b0\u5b9e\u7edf\u8ba1\u6a21\u578b\u3002</li>\n    <li>\u73b0\u6709\u7684\u8868\u5f81\u76f8\u4f3c\u6027\u5ea6\u91cf\u53d7\u7f51\u7edc\u89c4\u6a21\u5f71\u54cd\uff0c\u6a21\u578b\u7684\u6df1\u5ea6\u6216\u5bbd\u5ea6\u589e\u52a0\u53ef\u80fd\u5bfc\u81f4\u76f8\u4f3c\u6027\u5206\u6570\u865a\u9ad8\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u6362\u7684\u96f6\u6821\u51c6\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u4efb\u4f55\u76f8\u4f3c\u6027\u5ea6\u91cf\u8f6c\u6362\u4e3a\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u6821\u51c6\u5206\u6570\u3002</li>\n    <li>\u4f7f\u7528\u6821\u51c6\u6846\u67b6\u91cd\u65b0\u5ba1\u89c6\u201c\u67cf\u62c9\u56fe\u8868\u5f81\u5047\u8bf4\u201d\uff0c\u53d1\u73b0\u5168\u7403\u8c31\u5ea6\u91cf\u7684\u6536\u655b\u6027\u5728\u6821\u51c6\u540e\u5927\u90e8\u5206\u6d88\u5931\uff0c\u800c\u5c40\u90e8\u90bb\u57df\u76f8\u4f3c\u6027\u4fdd\u6301\u663e\u8457\u4e00\u81f4\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u4e9a\u91cc\u58eb\u591a\u5fb7\u8868\u5f81\u5047\u8bf4\u201d\uff1a\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8868\u5f81\u8d8b\u5411\u4e8e\u5171\u4eab\u7684\u5c40\u90e8\u90bb\u57df\u5173\u7cfb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The Platonic Representation Hypothesis suggests that neural networks are developing similar ways to represent reality.</li>\n    <li>Current methods for measuring how similar these representations are can be affected by the size of the neural network.</li>\n    <li>We created a new method to improve these similarity measurements, ensuring they are more accurate and statistically valid.</li>\n    <li>After applying our new method, we found that the strong similarities reported earlier mostly disappeared, except for some local relationships.</li>\n    <li>We propose a new hypothesis, the Aristotelian Representation Hypothesis, which focuses on shared local relationships in neural network representations.</li>\n</ul>"}, "publishedAt": "2026-02-16T01:01:23.000Z", "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View", "summary": "The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/616a7abc82a777e9333a1567/lP32oGpT13vpLGBXdguN9.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14486.png", "numComments": 1, "submittedBy": {"_id": "616a7abc82a777e9333a1567", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634368720319-616a7abc82a777e9333a1567.png", "fullname": "Fabian Gr\u00f6ger", "name": "FabianGroeger", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.15772", "authors": [{"_id": "69953f568d17d1ee8c10ecbc", "user": {"_id": "674736347f64a8dbfbf73f97", "avatarUrl": "/avatars/35d825248230fe66c4ae88f153b7128e.svg", "isPro": false, "fullname": "Sen Ye", "user": "sen-ye", "type": "user"}, "name": "Sen Ye", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:40:22.765Z", "hidden": false}, {"_id": "69953f568d17d1ee8c10ecbd", "user": {"_id": "63f5993afcf95ecac2b419b5", "avatarUrl": "/avatars/a8c020080a84d9a663789c4fb19270e9.svg", "isPro": false, "fullname": "Mengde Xu", "user": "Mendel192", "type": "user"}, "name": "Mengde Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:40:30.096Z", "hidden": false}, {"_id": "69953f568d17d1ee8c10ecbe", "name": "Shuyang Gu", "hidden": false}, {"_id": "69953f568d17d1ee8c10ecbf", "name": "Di He", "hidden": false}, {"_id": "69953f568d17d1ee8c10ecc0", "name": "Liwei Wang", "hidden": false}, {"_id": "69953f568d17d1ee8c10ecc1", "name": "Han Hu", "hidden": false}], "publishedAt": "2026-02-17T18:04:13.000Z", "submittedOnDailyAt": "2026-02-18T01:56:50.987Z", "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models", "submittedOnDailyBy": {"_id": "674736347f64a8dbfbf73f97", "avatarUrl": "/avatars/35d825248230fe66c4ae88f153b7128e.svg", "isPro": false, "fullname": "Sen Ye", "user": "sen-ye", "type": "user"}, "summary": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.", "upvotes": 5, "discussionId": "69953f578d17d1ee8c10ecc2", "githubRepo": "https://github.com/sen-ye/R3", "githubRepoAddedBy": "user", "ai_summary": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.", "ai_keywords": ["multimodal models", "generative capabilities", "understanding capabilities", "optimization dilemma", "Reason-Reflect-Refine framework", "generate-understand-regenerate process"], "githubStars": 2, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u6a21\u578b\u5728\u751f\u6210\u80fd\u529b\u548c\u7406\u89e3\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002</li>\n    <li>\u751f\u6210\u4e0e\u7406\u89e3\u4e4b\u95f4\u7684\u51b2\u7a81\u5bfc\u81f4\u4e86\u6a21\u578b\u5185\u90e8\u7684\u7ade\u4e89\u52a8\u6001\u3002</li>\n    <li>\u63d0\u51fa\u4e86Reason-Reflect-Refine (R3)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u8fc7\u7a0b\u6539\u5584\u751f\u6210\u548c\u7406\u89e3\u3002</li>\n    <li>\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5229\u7528\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u679c\u548c\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u8bbe\u8ba1\u4e0b\u4e00\u4ee3\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Research in multimodal models struggles with balancing generative abilities and understanding.</li>\n    <li>The conflict between generating content and understanding it creates challenges in model performance.</li>\n    <li>The proposed Reason-Reflect-Refine (R3) framework changes the generation process to a multi-step approach.</li>\n    <li>This new approach combines generation and understanding, leading to better results in both areas.</li>\n    <li>Insights from this work can help develop future multimodal models, and the code is available online.</li>\n</ul>"}, "publishedAt": "2026-02-17T13:04:13.000Z", "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models", "summary": "Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15772.png", "numComments": 1, "submittedBy": {"_id": "674736347f64a8dbfbf73f97", "avatarUrl": "/avatars/35d825248230fe66c4ae88f153b7128e.svg", "fullname": "Sen Ye", "name": "sen-ye", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.15322", "authors": [{"_id": "69952aa78d17d1ee8c10ec85", "name": "Taejong Joo", "hidden": false}, {"_id": "69952aa78d17d1ee8c10ec86", "name": "Wenhan Xia", "hidden": false}, {"_id": "69952aa78d17d1ee8c10ec87", "name": "Cheolmin Kim", "hidden": false}, {"_id": "69952aa78d17d1ee8c10ec88", "name": "Ming Zhang", "hidden": false}, {"_id": "69952aa78d17d1ee8c10ec89", "name": "Eugene Ie", "hidden": false}], "publishedAt": "2026-02-17T02:57:12.000Z", "submittedOnDailyAt": "2026-02-18T00:28:04.913Z", "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.", "upvotes": 5, "discussionId": "69952aa88d17d1ee8c10ec8a", "ai_summary": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.", "ai_keywords": ["large language models", "dense adaptive optimizers", "preconditioners", "random masking", "RMSProp", "curvature-dependent geometric regularization", "momentum-gradient alignment", "adaptive optimizers", "perplexity"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u901a\u5e38\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u81ea\u9002\u5e94\u4f18\u5316\u5668\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u968f\u673a\u906e\u7f69\u53c2\u6570\u66f4\u65b0\u53ef\u4ee5\u975e\u5e38\u6709\u6548\uff0c\u906e\u7f69\u7248\u672c\u7684RMSProp\u4f18\u5316\u5668\u8868\u73b0\u4f18\u4e8e\u6700\u65b0\u7684\u4f18\u5316\u5668\u3002</li>\n    <li>\u968f\u673a\u906e\u7f69\u5f15\u5165\u4e86\u4e00\u79cd\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u5e73\u6ed1\u4e86\u4f18\u5316\u8fc7\u7a0b\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u52a8\u91cf\u5bf9\u9f50\u68af\u5ea6\u906e\u7f69\uff08Magma\uff09\uff0c\u901a\u8fc7\u52a8\u91cf\u4e0e\u68af\u5ea6\u7684\u5bf9\u9f50\u6765\u8c03\u8282\u906e\u7f69\u66f4\u65b0\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cMagma\u53ef\u4ee5\u7b80\u5355\u66ff\u4ee3\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u4e14\u57281B\u6a21\u578b\u4e0a\u964d\u4f4e\u4e86\u8d85\u8fc719%\u7684\u56f0\u60d1\u5ea6\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Using random masking of parameter updates can improve training of large language models (LLMs).</li>\n    <li>A masked version of the RMSProp optimizer outperforms recent top optimizers.</li>\n    <li>Random masking helps smooth the training process through geometric regularization.</li>\n    <li>Magma, a new method, uses momentum to enhance the masked updates.</li>\n    <li>Magma is easy to implement and leads to better performance with minimal extra computational cost.</li>\n</ul>"}, "publishedAt": "2026-02-16T21:57:12.000Z", "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers", "summary": "Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15322.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 234, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.12783", "authors": [{"_id": "6992ceae50fb2c0be47839c1", "name": "Yuejie Li", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c2", "name": "Ke Yang", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c3", "name": "Yueying Hua", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c4", "user": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "isPro": false, "fullname": "berlin", "user": "berlin8587", "type": "user"}, "name": "Berlin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-16T15:36:03.590Z", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c5", "name": "Jianhao Nie", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c6", "name": "Yueping He", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c7", "name": "Caixin Kang", "hidden": false}], "publishedAt": "2026-02-13T10:08:27.000Z", "submittedOnDailyAt": "2026-02-16T14:32:15.752Z", "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "submittedOnDailyBy": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "isPro": false, "fullname": "berlin", "user": "berlin8587", "type": "user"}, "summary": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "upvotes": 132, "discussionId": "6992ceae50fb2c0be47839c8", "githubRepo": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval", "githubRepoAddedBy": "user", "githubStars": 96, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u8bed\u97f3\u67e5\u8be2\u68c0\u7d22\u8bc4\u4f30\u6570\u636e\u96c6\u5e38\u5e38\u53ea\u9002\u7528\u4e8e\u7b80\u5355\u67e5\u8be2\uff0c\u65e0\u6cd5\u8bc4\u4f30\u7cfb\u7edf\u5728\u590d\u6742\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SQuTR\uff0c\u4e00\u4e2a\u9488\u5bf9\u8bed\u97f3\u67e5\u8be2\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u57fa\u51c6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bc4\u4f30\u534f\u8bae\u3002</li>\n    <li>SQuTR\u6c47\u96c6\u4e86\u6765\u81ea\u516d\u4e2a\u5e38\u7528\u82f1\u6c49\u6587\u672c\u68c0\u7d22\u6570\u636e\u96c6\u768437,317\u4e2a\u72ec\u7279\u67e5\u8be2\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u548c\u67e5\u8be2\u7c7b\u578b\u3002</li>\n    <li>\u6211\u4eec\u4f7f\u7528200\u4f4d\u771f\u5b9e\u8bf4\u8bdd\u8005\u7684\u58f0\u97f3\u8d44\u6599\u5408\u6210\u8bed\u97f3\uff0c\u5e76\u5728\u53d7\u63a7\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u6df7\u540817\u79cd\u73b0\u5b9e\u73af\u5883\u566a\u58f0\uff0c\u4ee5\u8fdb\u884c\u9c81\u68d2\u6027\u8bc4\u4f30\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u7740\u566a\u58f0\u589e\u52a0\uff0c\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u4e0d\u540c\u7cfb\u7edf\u7684\u4e0b\u964d\u5e45\u5ea6\u5dee\u5f02\u663e\u8457\uff0c\u663e\u793a\u9c81\u68d2\u6027\u4ecd\u7136\u662f\u4e2a\u91cd\u8981\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SQuTR is a new benchmark for evaluating how well spoken query retrieval systems work in noisy environments.</li>\n    <li>It includes a large dataset with 37,317 unique queries from various English and Chinese text retrieval sources.</li>\n    <li>The dataset incorporates realistic speech from 200 speakers and mixes it with different types of real-world background noise.</li>\n    <li>Tests show that performance drops significantly as noise levels increase, highlighting challenges for even advanced retrieval models.</li>\n    <li>SQuTR aims to help researchers improve the robustness of spoken query retrieval systems through standardized evaluations.</li>\n</ul>"}, "publishedAt": "2026-02-13T05:08:27.000Z", "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "summary": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12783.png", "numComments": 1, "submittedBy": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "fullname": "berlin", "name": "berlin8587", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12036", "authors": [{"_id": "698eeb55cace060ff123af56", "user": {"_id": "64e2d169d2af12910d682130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg", "isPro": false, "fullname": "xuxin", "user": "xx18", "type": "user"}, "name": "Xin Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-13T09:36:02.476Z", "hidden": false}, {"_id": "698eeb55cace060ff123af57", "name": "Clive Bai", "hidden": false}, {"_id": "698eeb55cace060ff123af58", "name": "Kai Yang", "hidden": false}, {"_id": "698eeb55cace060ff123af59", "name": "Tianhao Chen", "hidden": false}, {"_id": "698eeb55cace060ff123af5a", "name": "Yangkun Chen", "hidden": false}, {"_id": "698eeb55cace060ff123af5b", "name": "Weijie Liu", "hidden": false}, {"_id": "698eeb55cace060ff123af5c", "name": "Hao Chen", "hidden": false}, {"_id": "698eeb55cace060ff123af5d", "name": "Yang Wang", "hidden": false}, {"_id": "698eeb55cace060ff123af5e", "name": "Saiyong Yang", "hidden": false}, {"_id": "698eeb55cace060ff123af5f", "name": "Can Yang", "hidden": false}], "publishedAt": "2026-02-12T15:03:37.000Z", "submittedOnDailyAt": "2026-02-13T06:44:41.991Z", "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models", "submittedOnDailyBy": {"_id": "64e2d169d2af12910d682130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg", "isPro": false, "fullname": "xuxin", "user": "xx18", "type": "user"}, "summary": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.", "upvotes": 81, "discussionId": "698eeb55cace060ff123af60", "githubRepo": "https://github.com/XinXU-USTC/Composition-RL", "githubRepoAddedBy": "user", "ai_summary": "Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "verifiable prompts", "pass rate", "compositional prompts", "curriculum learning", "cross-domain RL"], "githubStars": 3, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u63d0\u793a\u662f\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u6210\u529f\u7684\u57fa\u7840\uff0c\u4f46\u6269\u5c55\u6210\u672c\u9ad8\u4e14\u5305\u542b\u5f88\u591a\u65e0\u7528\u793a\u4f8b\u3002</li>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u4f18\u5148\u4f7f\u7528\u96be\u5ea6\u8f83\u5927\u7684\u63d0\u793a\uff0c\u4ee5\u63d0\u9ad8\u5229\u7528\u6709\u9650\u8bad\u7ec3\u6570\u636e\u7684\u6548\u7387\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Composition-RL\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u95ee\u9898\u751f\u6210\u65b0\u7684\u53ef\u9a8c\u8bc1\u95ee\u9898\u6765\u66f4\u597d\u5730\u5229\u7528\u6709\u9650\u7684\u63d0\u793a\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cComposition-RL\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u5747\u80fd\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u8fd8\u652f\u6301\u8de8\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u7ec4\u5408\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u63d0\u793a\u6765\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) uses large prompts but struggles with unhelpful examples and expansion costs.</li>\n    <li>Current methods focus on using hard prompts with a 0% pass rate, while easy prompts with a 100% pass rate become too common as training continues.</li>\n    <li>Composition-RL is a new approach that combines multiple problems into a single question to improve training with limited prompts.</li>\n    <li>Tests show that Composition-RL enhances reasoning skills across various model sizes, and its performance improves further with a gradual training method.</li>\n    <li>It also allows for better cross-domain learning by mixing prompts from different areas, and resources are available online.</li>\n</ul>"}, "publishedAt": "2026-02-12T10:03:37.000Z", "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models", "summary": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12036.png", "numComments": 1, "submittedBy": {"_id": "64e2d169d2af12910d682130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e2d169d2af12910d682130/VG8UdqJCJGc0K4G0P0XQP.jpeg", "fullname": "xuxin", "name": "xx18", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12205", "authors": [{"_id": "698ea0f9cace060ff123ae3a", "name": "Dianyi Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3b", "name": "Ruihang Li", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3c", "name": "Feng Han", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3d", "name": "Chaofan Ma", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3e", "name": "Wei Song", "hidden": false}, {"_id": "698ea0f9cace060ff123ae3f", "name": "Siyuan Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae40", "name": "Yibin Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae41", "name": "Yi Xin", "hidden": false}, {"_id": "698ea0f9cace060ff123ae42", "name": "Hongjian Liu", "hidden": false}, {"_id": "698ea0f9cace060ff123ae43", "name": "Zhixiong Zhang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae44", "name": "Shengyuan Ding", "hidden": false}, {"_id": "698ea0f9cace060ff123ae45", "name": "Tianhang Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae46", "name": "Zhenglin Cheng", "hidden": false}, {"_id": "698ea0f9cace060ff123ae47", "name": "Tao Lin", "hidden": false}, {"_id": "698ea0f9cace060ff123ae48", "name": "Cheng Jin", "hidden": false}, {"_id": "698ea0f9cace060ff123ae49", "name": "Kaicheng Yu", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4a", "name": "Jingjing Chen", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4b", "name": "Wenjie Wang", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4c", "name": "Zhongyu Wei", "hidden": false}, {"_id": "698ea0f9cace060ff123ae4d", "name": "Jiaqi Wang", "hidden": false}], "publishedAt": "2026-02-12T17:44:24.000Z", "submittedOnDailyAt": "2026-02-13T03:37:56.296Z", "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing", "submittedOnDailyBy": {"_id": "64b4eec4faa3181a5eab9c46", "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg", "isPro": true, "fullname": "Jiaqi Wang", "user": "myownskyW7", "type": "user"}, "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.", "upvotes": 60, "discussionId": "698ea0f9cace060ff123ae4e", "projectPage": "https://deepgenteam.github.io/", "githubRepo": "https://github.com/DeepGenTeam/DeepGen", "githubRepoAddedBy": "user", "ai_summary": "A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.", "ai_keywords": ["unified multimodal models", "image generation", "image editing", "parameter scale", "VLM layers", "DiT representations", "Stacked Channel Bridging", "think tokens", "data-centric training strategy", "alignment pre-training", "joint supervised fine-tuning", "reinforcement learning", "MR-GRPO", "generation quality", "human preferences", "visual artifacts"], "githubStars": 36, "organization": {"_id": "683ebd0d913d82e703e77286", "name": "sii-research", "fullname": "Shanghai Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/SQAtyVRxNjp9L0CUi0tgI.png"}, "summary_zh": "<ul>\n    <li>DeepGen 1.0 \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u4e3a 5B\uff0c\u6027\u80fd\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5806\u53e0\u901a\u9053\u6865\u63a5\uff08SCB\uff09\u65b9\u6cd5\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>\u91c7\u7528\u4e09\u9636\u6bb5\u7684\u6570\u636e\u4e2d\u5fc3\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u751f\u6210\u3001\u7f16\u8f91\u548c\u63a8\u7406\u4efb\u52a1\u7684\u80fd\u529b\u3002</li>\n    <li>\u5c3d\u7ba1\u53ea\u7528\u7ea6 5000 \u4e07\u4e2a\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0cDeepGen 1.0 \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\u3002</li>\n    <li>\u5f00\u6e90\u8bad\u7ec3\u4ee3\u7801\u3001\u6743\u91cd\u548c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u7edf\u4e00\u591a\u6a21\u6001\u7814\u7a76\u7684\u666e\u53ca\u548c\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepGen 1.0 is a lightweight model with 5 billion parameters, offering strong image generation and editing capabilities without the high costs of larger models.</li>\n    <li>It uses Stacked Channel Bridging (SCB) to improve understanding and control by combining features from multiple layers with special tokens for better guidance.</li>\n    <li>The training process has three stages: aligning image-text data, fine-tuning on various tasks, and using reinforcement learning to improve quality and alignment with human preferences.</li>\n    <li>Despite being trained on only about 50 million samples, DeepGen 1.0 outperforms much larger models in various tests.</li>\n    <li>The developers are sharing their training code, model weights, and datasets to support multimodal research for everyone.</li>\n</ul>"}, "publishedAt": "2026-02-12T12:44:24.000Z", "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing", "summary": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12205.png", "numComments": 1, "submittedBy": {"_id": "64b4eec4faa3181a5eab9c46", "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg", "fullname": "Jiaqi Wang", "name": "myownskyW7", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 25, "isUserFollowing": false}, "organization": {"_id": "683ebd0d913d82e703e77286", "name": "sii-research", "fullname": "Shanghai Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/SQAtyVRxNjp9L0CUi0tgI.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.12705", "authors": [{"_id": "6992818050fb2c0be47838b2", "name": "Baorong Shi", "hidden": false}, {"_id": "6992818050fb2c0be47838b3", "name": "Bo Cui", "hidden": false}, {"_id": "6992818050fb2c0be47838b4", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6992818050fb2c0be47838b5", "name": "Deli Yu", "hidden": false}, {"_id": "6992818050fb2c0be47838b6", "name": "Fang Qian", "hidden": false}, {"_id": "6992818050fb2c0be47838b7", "name": "Haihua Yang", "hidden": false}, {"_id": "6992818050fb2c0be47838b8", "name": "Huichao Wang", "hidden": false}, {"_id": "6992818050fb2c0be47838b9", "name": "Jiale Chen", "hidden": false}, {"_id": "6992818050fb2c0be47838ba", "name": "Jianfei Pan", "hidden": false}, {"_id": "6992818050fb2c0be47838bb", "name": "Jieqiong Cao", "hidden": false}, {"_id": "6992818050fb2c0be47838bc", "name": "Jinghao Lin", "hidden": false}, {"_id": "6992818050fb2c0be47838bd", "name": "Kai Wu", "hidden": false}, {"_id": "6992818050fb2c0be47838be", "name": "Lin Yang", "hidden": false}, {"_id": "6992818050fb2c0be47838bf", "name": "Shengsheng Yao", "hidden": false}, {"_id": "6992818050fb2c0be47838c0", "name": "Tao Chen", "hidden": false}, {"_id": "6992818050fb2c0be47838c1", "name": "Xiaojun Xiao", "hidden": false}, {"_id": "6992818050fb2c0be47838c2", "user": {"_id": "666a59bff0d87d9c3b1dd907", "avatarUrl": "/avatars/4af5a47d78bca525c7ec985a390408a4.svg", "isPro": false, "fullname": "Xiaozhong Ji", "user": "xiaozhongji", "type": "user"}, "name": "Xiaozhong Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-16T15:36:45.325Z", "hidden": false}, {"_id": "6992818050fb2c0be47838c3", "name": "Xu Wang", "hidden": false}, {"_id": "6992818050fb2c0be47838c4", "name": "Yijun He", "hidden": false}, {"_id": "6992818050fb2c0be47838c5", "name": "Zhixiong Yang", "hidden": false}], "publishedAt": "2026-02-13T08:19:38.000Z", "submittedOnDailyAt": "2026-02-16T00:05:18.833Z", "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "submittedOnDailyBy": {"_id": "64c636b94c9bebfa6ac80ae4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png", "isPro": false, "fullname": "kai", "user": "KaiWu123", "type": "user"}, "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "upvotes": 55, "discussionId": "6992818050fb2c0be47838c6", "ai_summary": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.", "ai_keywords": ["vision-language foundation model", "entity-aware continual pretraining", "heterogeneous medical corpora", "long-tail gaps", "reinforcement learning", "tool-augmented agentic training", "multi-step diagnostic reasoning", "evidence-grounded reasoning", "hallucination reduction", "medical instruction adherence"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n<li>MedXIAOHE \u662f\u4e00\u4e2a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u533b\u7597\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n<li>\u8be5\u6a21\u578b\u5728\u5404\u7c7b\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u9886\u5148\u7684\u95ed\u6e90\u591a\u6a21\u6001\u7cfb\u7edf\u3002</li>\n<li>\u91c7\u7528\u4e86\u5b9e\u4f53\u611f\u77e5\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u6574\u5408\u591a\u79cd\u533b\u5b66\u6570\u636e\uff0c\u4ee5\u6269\u5927\u77e5\u8bc6\u8986\u76d6\u548c\u51cf\u5c11\u7a00\u6709\u75be\u75c5\u7684\u4fe1\u606f\u5dee\u8ddd\u3002</li>\n<li>\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u589e\u5f3a\u8bad\u7ec3\uff0cMedXIAOHE \u80fd\u591f\u8fdb\u884c\u591a\u6b65\u9aa4\u7684\u8bca\u65ad\u63a8\u7406\uff0c\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u51b3\u7b56\u8bb0\u5f55\u3002</li>\n<li>\u4e3a\u4e86\u63d0\u9ad8\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0cMedXIAOHE \u6574\u5408\u4e86\u7528\u6237\u504f\u597d\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u548c\u4f4e\u5e7b\u89c9\u7684\u957f\u7bc7\u62a5\u544a\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MedXIAOHE is a new model that helps computers understand and reason about medical information for real-world use.</li>\n    <li>It outperforms other top medical systems in various tests and has improved knowledge of rare diseases.</li>\n    <li>The model uses continual training and reinforcement learning to enhance its reasoning and decision-making abilities.</li>\n    <li>It focuses on being reliable by following user preferences and providing clear, evidence-based reports.</li>\n    <li>The report shares design choices and insights to encourage more research in this area.</li>\n</ul>"}, "publishedAt": "2026-02-13T03:19:38.000Z", "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12705.png", "numComments": 4, "submittedBy": {"_id": "64c636b94c9bebfa6ac80ae4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png", "fullname": "kai", "name": "KaiWu123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.12125", "authors": [{"_id": "698e8f46cace060ff123ac51", "name": "Wenkai Yang", "hidden": false}, {"_id": "698e8f46cace060ff123ac52", "name": "Weijie Liu", "hidden": false}, {"_id": "698e8f46cace060ff123ac53", "name": "Ruobing Xie", "hidden": false}, {"_id": "698e8f46cace060ff123ac54", "name": "Kai Yang", "hidden": false}, {"_id": "698e8f46cace060ff123ac55", "name": "Saiyong Yang", "hidden": false}, {"_id": "698e8f46cace060ff123ac56", "name": "Yankai Lin", "hidden": false}], "publishedAt": "2026-02-12T16:14:29.000Z", "submittedOnDailyAt": "2026-02-13T00:24:06.722Z", "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation", "submittedOnDailyBy": {"_id": "64b7df742f5a966b973e25f7", "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg", "isPro": false, "fullname": "Wenkai Yang", "user": "Keven16", "type": "user"}, "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.", "upvotes": 53, "discussionId": "698e8f46cace060ff123ac57", "githubRepo": "https://github.com/RUCBM/G-OPD", "githubRepoAddedBy": "user", "ai_summary": "On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.", "ai_keywords": ["on-policy distillation", "logit distribution", "dense KL-constrained RL", "reward scaling factor", "reward extrapolation", "reward correction", "teacher-student size pairings", "domain-specific RL", "strong-to-weak distillation"], "githubStars": 8, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u201c\u5728\u653f\u7b56\u84b8\u998f\u201d\uff08OPD\uff09\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u5b66\u751f\u6a21\u578b\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002</li>\n    <li>OPD \u88ab\u8bc1\u660e\u662f\u5bc6\u96c6 KL \u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u5176\u4e2d\u5956\u52b1\u548c KL \u6b63\u5219\u5316\u7684\u6743\u91cd\u76f8\u540c\u3002</li>\n    <li>\u5f15\u5165\u4e86\u201c\u5e7f\u4e49\u5728\u653f\u7b56\u84b8\u998f\u201d\uff08G-OPD\uff09\u6846\u67b6\uff0c\u589e\u52a0\u4e86\u7075\u6d3b\u7684\u53c2\u8003\u6a21\u578b\u548c\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5956\u52b1\u7f29\u653e\u56e0\u5b50\u5927\u4e8e1\uff08\u79f0\u4e3aExOPD\uff09\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n    <li>\u5728\u5f3a\u5230\u5f31\u7684\u84b8\u998f\u8bbe\u7f6e\u4e2d\uff0c\u4f7f\u7528\u6559\u5e08\u7684\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u53c2\u8003\u6a21\u578b\u8fdb\u884c\u5956\u52b1\u4fee\u6b63\u80fd\u63d0\u9ad8\u84b8\u998f\u6548\u679c\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>On-policy distillation (OPD) improves student performance by aligning them with a teacher's logit distribution based on student-generated data.</li>\n    <li>OPD is shown to be a specific case of a type of reinforcement learning (RL) with balanced rewards and regularization.</li>\n    <li>The new Generalized On-Policy Distillation (G-OPD) framework allows for a flexible reference model and adjusts the importance of rewards in training.</li>\n    <li>Introducing a reward scaling factor greater than 1, called ExOPD, enhances performance beyond standard OPD, especially when combining knowledge from different models.</li>\n    <li>Using the teacher's base model for reward correction in distillation can improve results, but requires additional computation and access to the teacher's earlier model.</li>\n</ul>"}, "publishedAt": "2026-02-12T11:14:29.000Z", "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation", "summary": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12125.png", "numComments": 2, "submittedBy": {"_id": "64b7df742f5a966b973e25f7", "avatarUrl": "/avatars/e24e7769188d441317b3b7d10ef8fd60.svg", "fullname": "Wenkai Yang", "name": "Keven16", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14111", "authors": [{"_id": "69959cbbed493589ceb5be31", "user": {"_id": "6572e9380503eeadb78fb3e3", "avatarUrl": "/avatars/cbf26f1b2c72732d37e8cb847c48f152.svg", "isPro": false, "fullname": "Anton Korznikov", "user": "AntonKorznikov", "type": "user"}, "name": "Anton Korznikov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:45.391Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be32", "user": {"_id": "661e44cf1d8ffc49b57ba07e", "avatarUrl": "/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg", "isPro": false, "fullname": "Andrey Galichin", "user": "andreuka18", "type": "user"}, "name": "Andrey Galichin", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:51.000Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be33", "user": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "isPro": false, "fullname": "Alexey Dontsov", "user": "therem", "type": "user"}, "name": "Alexey Dontsov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:38.913Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be34", "user": {"_id": "66e19e09140031bf85f0e6f3", "avatarUrl": "/avatars/6cce10740eb73f066d7ed0fe8ca3a93a.svg", "isPro": false, "fullname": "Oleg Rogov", "user": "Olegario228", "type": "user"}, "name": "Oleg Rogov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:59.681Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be35", "name": "Ivan Oseledets", "hidden": false}, {"_id": "69959cbbed493589ceb5be36", "user": {"_id": "662f8d645c4db70c77a203b0", "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg", "isPro": false, "fullname": "Elena Tutubalina", "user": "tlenusik", "type": "user"}, "name": "Elena Tutubalina", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T12:30:12.235Z", "hidden": false}], "publishedAt": "2026-02-15T11:53:55.000Z", "submittedOnDailyAt": "2026-02-18T08:38:30.231Z", "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?", "submittedOnDailyBy": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "isPro": false, "fullname": "Alexey Dontsov", "user": "therem", "type": "user"}, "summary": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.", "upvotes": 51, "discussionId": "69959cbbed493589ceb5be37", "ai_summary": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.", "ai_keywords": ["Sparse Autoencoders", "neural networks", "activations", "explained variance", "interpretability", "sparse probing", "causal editing"], "summary_zh": "<ul>\n    <li>\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u88ab\u7528\u6765\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5c06\u6fc0\u6d3b\u5206\u89e3\u4e3a\u53ef\u7406\u89e3\u7684\u7279\u5f81\u3002</li>\n    <li>\u5c3d\u7ba1SAE\u5728\u67d0\u4e9b\u6a21\u578b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u8d1f\u9762\u7ed3\u679c\u8ba9\u4eba\u8d28\u7591\u5b83\u4eec\u63d0\u53d6\u7684\u7279\u5f81\u662f\u5426\u6709\u610f\u4e49\u3002</li>\n    <li>\u5728\u5408\u6210\u73af\u5883\u4e2d\uff0cSAE\u4ec5\u6062\u590d\u4e869%\u7684\u771f\u5b9e\u7279\u5f81\uff0c\u5c3d\u7ba1\u89e3\u91ca\u65b9\u5dee\u8fbe71%\uff0c\u663e\u793a\u5b83\u4eec\u7684\u6838\u5fc3\u4efb\u52a1\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u4e0e\u968f\u673a\u503c\u7684\u57fa\u7ebf\u6bd4\u8f83\uff0c\u53d1\u73b0SAE\u5728\u53ef\u89e3\u91ca\u6027\u548c\u5176\u4ed6\u6307\u6807\u4e0a\u7684\u8868\u73b0\u4e0e\u57fa\u7ebf\u76f8\u4f3c\u3002</li>\n    <li>\u603b\u4f53\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u72b6\u6001\u4e0b\u7684SAE\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u5206\u89e3\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Sparse Autoencoders (SAEs) aim to help understand neural networks by breaking down their activations into simple, understandable features.</li>\n    <li>Despite some success, recent studies show that SAEs may not always find meaningful features in neural networks.</li>\n    <li>In tests with known features, SAEs only identified 9% of true features, indicating a failure in their main purpose, even when they had good reconstruction results.</li>\n    <li>When comparing SAEs to random baselines in real activations, the results were similar in terms of interpretability and other evaluation metrics, suggesting that SAEs are not significantly better.</li>\n    <li>Overall, the findings indicate that current SAEs do not effectively break down the internal workings of neural network models.</li>\n</ul>"}, "publishedAt": "2026-02-15T06:53:55.000Z", "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?", "summary": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14111.png", "numComments": 2, "submittedBy": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "fullname": "Alexey Dontsov", "name": "therem", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.11858", "authors": [{"_id": "698ea72fcace060ff123ae5d", "name": "Lai Wei", "hidden": false}, {"_id": "698ea72fcace060ff123ae5e", "name": "Liangbo He", "hidden": false}, {"_id": "698ea72fcace060ff123ae5f", "name": "Jun Lan", "hidden": false}, {"_id": "698ea72fcace060ff123ae60", "name": "Lingzhong Dong", "hidden": false}, {"_id": "698ea72fcace060ff123ae61", "name": "Yutong Cai", "hidden": false}, {"_id": "698ea72fcace060ff123ae62", "name": "Siyuan Li", "hidden": false}, {"_id": "698ea72fcace060ff123ae63", "name": "Huijia Zhu", "hidden": false}, {"_id": "698ea72fcace060ff123ae64", "name": "Weiqiang Wang", "hidden": false}, {"_id": "698ea72fcace060ff123ae65", "name": "Linghe Kong", "hidden": false}, {"_id": "698ea72fcace060ff123ae66", "name": "Yue Wang", "hidden": false}, {"_id": "698ea72fcace060ff123ae67", "name": "Zhuosheng Zhang", "hidden": false}, {"_id": "698ea72fcace060ff123ae68", "name": "Weiran Huang", "hidden": false}], "publishedAt": "2026-02-12T12:00:35.000Z", "submittedOnDailyAt": "2026-02-16T00:45:35.867Z", "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception", "submittedOnDailyBy": {"_id": "64a16b1aeacb4b50ba1c889d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg", "isPro": false, "fullname": "Lai Wei", "user": "WaltonFuture", "type": "user"}, "summary": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.", "upvotes": 50, "discussionId": "698ea72fcace060ff123ae69", "githubRepo": "https://github.com/inclusionAI/Zooming-without-Zooming", "githubRepoAddedBy": "user", "ai_summary": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.", "ai_keywords": ["Multimodal Large Language Models", "visual question answering", "fine-grained perception", "Thinking-with-Images", "region-to-image distillation", "micro-cropped regions", "teacher-student distillation", "ZoomBench", "visual reasoning", "GUI agents"], "githubStars": 55, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e7f\u6cdb\u7684\u89c6\u89c9\u7406\u89e3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ec6\u81f4\u611f\u77e5\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u533a\u57df\u5230\u56fe\u50cf\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u5c06\u7f29\u653e\u8fc7\u7a0b\u4ece\u63a8\u7406\u65f6\u95f4\u8f6c\u53d8\u4e3a\u8bad\u7ec3\u65f6\u95f4\uff0c\u51cf\u5c11\u5ef6\u8fdf\u3002</li>\n    <li>\u901a\u8fc7\u5fae\u88c1\u526a\u533a\u57df\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u95ee\u7b54\u6570\u636e\uff0c\u5e76\u5c06\u5176\u84b8\u998f\u56de\u5b8c\u6574\u56fe\u50cf\uff0c\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u7ec6\u81f4\u611f\u77e5\u80fd\u529b\u3002</li>\n    <li>\u63a8\u51fa\u4e86ZoomBench\uff0c\u4e00\u4e2a\u5305\u542b845\u4e2a\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ec6\u81f4\u611f\u77e5\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u591a\u4e2a\u7ec6\u81f4\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u5e76\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8ba4\u77e5\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal Large Language Models (MLLMs) are good at understanding visuals but struggle with detailed perception.</li>\n    <li>Current methods improve this by zooming in on areas of interest, but they are slow and require multiple tool uses.</li>\n    <li>The proposed solution, Region-to-Image Distillation, allows zooming to happen during training instead of inference, speeding up the process.</li>\n    <li>This approach uses micro-cropped regions to create high-quality data for training, leading to better fine-grained perception in smaller models.</li>\n    <li>The study introduces ZoomBench, a benchmark for evaluating fine-grained perception, showing that the new models perform better across various tasks.</li>\n</ul>"}, "publishedAt": "2026-02-12T07:00:35.000Z", "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception", "summary": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11858.png", "numComments": 1, "submittedBy": {"_id": "64a16b1aeacb4b50ba1c889d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg", "fullname": "Lai Wei", "name": "WaltonFuture", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.12670", "authors": [{"_id": "6994d2138d17d1ee8c10eb51", "user": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "isPro": false, "fullname": "Xiangyi Li", "user": "xdotli", "type": "user"}, "name": "Xiangyi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:43.446Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb52", "name": "Wenbo Chen", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb53", "name": "Yimin Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb54", "name": "Shenghan Zheng", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb55", "user": {"_id": "6462bf90c9cc74e82e270cb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462bf90c9cc74e82e270cb6/usLDOUhyIfDXA2HZvXvps.jpeg", "isPro": true, "fullname": "Kobe Chen", "user": "kobe0938", "type": "user"}, "name": "Xiaokun Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:37.777Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb56", "user": {"_id": "659a1c9511b48706bab783cc", "avatarUrl": "/avatars/6978a5bc7ab284d9f7285f9fd2c8d0e0.svg", "isPro": false, "fullname": "Yifeng He", "user": "yfhe", "type": "user"}, "name": "Yifeng He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:41.599Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb57", "name": "Yubo Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb58", "user": {"_id": "663bd5fcfb931d4660fd18b7", "avatarUrl": "/avatars/17aa421d40fe3532d0ddecbc2accb249.svg", "isPro": false, "fullname": "Bingran You", "user": "bingran-you", "type": "user"}, "name": "Bingran You", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:36:48.789Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb59", "name": "Haotian Shen", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5a", "user": {"_id": "63b5d1bb0d5913eee4869c0d", "avatarUrl": "/avatars/ae397f54bbb3debc1f7903f4c6959ae6.svg", "isPro": false, "fullname": "Jiankai Sun", "user": "zhenv5", "type": "user"}, "name": "Jiankai Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:36:59.144Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5b", "name": "Shuyi Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5c", "user": {"_id": "65cc8abe8ebd392213020575", "avatarUrl": "/avatars/e0b49fe07b3553779992092f60aa0b48.svg", "isPro": false, "fullname": "qunhongzeng", "user": "qunhongzeng", "type": "user"}, "name": "Qunhong Zeng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:07.249Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5d", "name": "Di Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5e", "user": {"_id": "6275a465597c70eb8949fce5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png", "isPro": false, "fullname": "Xuandong Zhao", "user": "Xuandong", "type": "user"}, "name": "Xuandong Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:13.482Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5f", "name": "Yuanli Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb60", "user": {"_id": "65a0dc6690eb7a1524186cc2", "avatarUrl": "/avatars/6ecf7fb5aa74f2453d0e3bc7b9cca0d3.svg", "isPro": true, "fullname": "Roey Ben Chaim", "user": "roeybc", "type": "user"}, "name": "Roey Ben Chaim", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:19.969Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb61", "name": "Zonglin Di", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb62", "name": "Yipeng Gao", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb63", "name": "Junwei He", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb64", "user": {"_id": "63710bca7a5e5d8efdbff215", "avatarUrl": "/avatars/25e7f713d613ec81ba775265eadff8bc.svg", "isPro": false, "fullname": "He", "user": "Yizhuo", "type": "user"}, "name": "Yizhuo He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:45.624Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb65", "name": "Liqiang Jing", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb66", "name": "Luyang Kong", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb67", "name": "Xin Lan", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb68", "name": "Jiachen Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb69", "name": "Songlin Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6a", "name": "Yijiang Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6b", "user": {"_id": "64b5198c25882acb62fb77ef", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png", "isPro": false, "fullname": "Yueqian Lin", "user": "linyueqian", "type": "user"}, "name": "Yueqian Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:39.753Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6c", "name": "Xinyi Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6d", "name": "Xuanqing Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6e", "name": "Haoran Lyu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6f", "name": "Ze Ma", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb70", "name": "Bowei Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb71", "name": "Runhui Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb72", "name": "Tianyu Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb73", "name": "Wengao Ye", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb74", "name": "Yue Zhang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb75", "name": "Hanwen Xing", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb76", "name": "Yiqi Xue", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb77", "name": "Steven Dillmann", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb78", "name": "Han-chung Lee", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"], "publishedAt": "2026-02-13T07:06:06.000Z", "submittedOnDailyAt": "2026-02-18T11:04:39.385Z", "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "submittedOnDailyBy": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "isPro": false, "fullname": "Xiangyi Li", "user": "xdotli", "type": "user"}, "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "upvotes": 36, "discussionId": "6994d2148d17d1ee8c10eb79", "projectPage": "https://skillsbench.ai/", "githubRepo": "https://github.com/benchflow-ai/skillsbench", "githubRepoAddedBy": "user", "ai_summary": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.", "ai_keywords": ["agent skills", "LLM agents", "SkillsBench", "procedural knowledge", "curated Skills", "self-generated Skills", "agent-model configurations", "pass rate", "domain-specific effects"], "githubStars": 413, "organization": {"_id": "69657f24ec1d157f1590a81d", "name": "benchflow", "fullname": "BenchFlow", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"}, "summary_zh": "<ul>\n    <li>Agent Skills \u662f\u589e\u5f3aLLM\u4ee3\u7406\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u5305\uff0c\u7528\u4e8e\u63a8\u7406\u65f6\u3002</li>\n    <li>SkillsBench \u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec86\u4e2a\u4efb\u52a1\u548c11\u4e2a\u9886\u57df\uff0c\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u6280\u80fd\u3002</li>\n    <li>\u6d4b\u8bd5\u6761\u4ef6\u5305\u62ec\u65e0\u6280\u80fd\u3001\u7b56\u5212\u6280\u80fd\u548c\u81ea\u751f\u6210\u6280\u80fd\u3002</li>\n    <li>\u7b56\u5212\u6280\u80fd\u5e73\u5747\u63d0\u5347\u901a\u8fc7\u738716.2\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u5728\u4e0d\u540c\u9886\u57df\u6548\u679c\u5dee\u5f02\u663e\u8457\u3002</li>\n    <li>\u81ea\u751f\u6210\u6280\u80fd\u5e73\u5747\u6ca1\u6709\u6548\u679c\uff0c\u8f83\u5c0f\u7684\u6280\u80fd\u6a21\u5757\u4f18\u4e8e\u5168\u9762\u6587\u6863\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agent Skills enhance the abilities of LLM agents but measuring their effectiveness is challenging.</li>\n    <li>SkillsBench is a new benchmark that includes 86 tasks across 11 areas, tested with curated and self-generated Skills.</li>\n    <li>Curated Skills improve success rates by an average of 16.2 percentage points, but the impact varies by area.</li>\n    <li>Self-generated Skills do not help on average, indicating that models struggle to create effective procedural knowledge.</li>\n    <li>Focused Skills with fewer modules are more effective than extensive documentation, and smaller models with Skills can perform as well as larger models without them.</li>\n</ul>"}, "publishedAt": "2026-02-13T02:06:06.000Z", "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12670.png", "numComments": 3, "submittedBy": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "fullname": "Xiangyi Li", "name": "xdotli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "69657f24ec1d157f1590a81d", "name": "benchflow", "fullname": "BenchFlow", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12099", "authors": [{"_id": "698e8ff2cace060ff123ac59", "name": "GigaBrain Team", "hidden": false}, {"_id": "698e8ff2cace060ff123ac5a", "name": "Boyuan Wang", "hidden": false}, {"_id": "698e8ff2cace060ff123ac5b", "name": "Chaojun Ni", "hidden": false}, {"_id": "698e8ff2cace060ff123ac5c", "name": "Guan Huang", "hidden": false}, {"_id": "698e8ff2cace060ff123ac5d", "name": "Guosheng Zhao", "hidden": false}, {"_id": "698e8ff2cace060ff123ac5e", "name": "Hao Li", "hidden": false}, {"_id": "698e8ff2cace060ff123ac5f", "name": "Jie Li", "hidden": false}, {"_id": "698e8ff2cace060ff123ac60", "name": "Jindi Lv", "hidden": false}, {"_id": "698e8ff2cace060ff123ac61", "name": "Jingyu Liu", "hidden": false}, {"_id": "698e8ff2cace060ff123ac62", "name": "Lv Feng", "hidden": false}, {"_id": "698e8ff2cace060ff123ac63", "name": "Mingming Yu", "hidden": false}, {"_id": "698e8ff2cace060ff123ac64", "name": "Peng Li", "hidden": false}, {"_id": "698e8ff2cace060ff123ac65", "name": "Qiuping Deng", "hidden": false}, {"_id": "698e8ff2cace060ff123ac66", "name": "Tianze Liu", "hidden": false}, {"_id": "698e8ff2cace060ff123ac67", "name": "Xinyu Zhou", "hidden": false}, {"_id": "698e8ff2cace060ff123ac68", "name": "Xinze Chen", "hidden": false}, {"_id": "698e8ff2cace060ff123ac69", "user": {"_id": "6426616ea5ec4a5cbc535634", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/5IfSFYd9QOxz8K9QmBCst.png", "isPro": false, "fullname": "JeffWang", "user": "Jeff-Wang", "type": "user"}, "name": "Xiaofeng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-13T09:36:41.892Z", "hidden": false}, {"_id": "698e8ff2cace060ff123ac6a", "user": {"_id": "644012cf3e0374802e174f7c", "avatarUrl": "/avatars/0f4a4bd6f96ce193871843e1d01439e8.svg", "isPro": false, "fullname": "Yang Wang", "user": "supermodelteam", "type": "user"}, "name": "Yang Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-13T09:36:39.496Z", "hidden": false}, {"_id": "698e8ff2cace060ff123ac6b", "name": "Yifan Li", "hidden": false}, {"_id": "698e8ff2cace060ff123ac6c", "name": "Yifei Nie", "hidden": false}, {"_id": "698e8ff2cace060ff123ac6d", "name": "Yilong Li", "hidden": false}, {"_id": "698e8ff2cace060ff123ac6e", "name": "Yukun Zhou", "hidden": false}, {"_id": "698e8ff2cace060ff123ac6f", "name": "Yun Ye", "hidden": false}, {"_id": "698e8ff2cace060ff123ac70", "name": "Zhichao Liu", "hidden": false}, {"_id": "698e8ff2cace060ff123ac71", "name": "Zheng Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6426616ea5ec4a5cbc535634/w6OBVRe04BfXtAnLDzr3o.mp4"], "publishedAt": "2026-02-12T15:55:19.000Z", "submittedOnDailyAt": "2026-02-13T00:24:14.150Z", "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning", "submittedOnDailyBy": {"_id": "6426616ea5ec4a5cbc535634", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/5IfSFYd9QOxz8K9QmBCst.png", "isPro": false, "fullname": "JeffWang", "user": "Jeff-Wang", "type": "user"}, "summary": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, a VLA model trained via world model-based reinforcement learning. Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M^* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our https://gigabrain05m.github.io{project page}.", "upvotes": 33, "discussionId": "698e8ff2cace060ff123ac72", "projectPage": "https://gigabrain05m.github.io/", "githubRepo": "https://github.com/open-gigaai/giga-brain-0", "githubRepoAddedBy": "user", "ai_summary": "A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.", "ai_keywords": ["Vision-language-action models", "world models", "reinforcement learning", "cross-task adaptation", "RAMP", "RoboChallenge benchmark", "robotic manipulation"], "githubStars": 2271, "organization": {"_id": "68d6587936e2de9610d9f5f0", "name": "open-gigaai", "fullname": "GigaAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"}, "summary_zh": "<ul>\n    <li>Vision-language-action (VLA) \u6a21\u578b\u76f4\u63a5\u4ece\u5f53\u524d\u89c2\u5bdf\u4e2d\u9884\u6d4b\u591a\u6b65\u52a8\u4f5c\uff0c\u4f46\u9762\u4e34\u573a\u666f\u7406\u89e3\u548c\u672a\u6765\u9884\u6d4b\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002</li>\n    <li>\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7ecf\u8fc7\u5927\u91cf\u89c6\u9891\u6570\u636e\u9884\u8bad\u7ec3\uff0c\u5177\u5907\u5f3a\u5927\u7684\u65f6\u7a7a\u63a8\u7406\u548c\u51c6\u786e\u7684\u672a\u6765\u9884\u6d4b\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 GigaBrain-0.5M*\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684 VLA \u6a21\u578b\u3002</li>\n    <li>GigaBrain-0.5M* \u5728 GigaBrain-0.5 \u7684\u57fa\u7840\u4e0a\uff0c\u540e\u8005\u5df2\u5728\u8d85\u8fc7 10,000 \u5c0f\u65f6\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u56fd\u9645 RoboChallenge \u57fa\u51c6\u4e2d\u6392\u540d\u7b2c\u4e00\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGigaBrain-0.5M* \u5728 Laundry Folding\u3001Box Packing \u548c Espresso Preparation \u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u7ea6 30%\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>GigaBrain-0.5M* is a new model that improves how machines understand scenes and predict future actions.</li>\n  <li>It builds on GigaBrain-0.5, which was trained with a lot of data on robotic tasks and currently leads in a benchmark competition.</li>\n  <li>The model uses a technique called RAMP to enhance learning and adapt to different tasks effectively.</li>\n  <li>Tests show that GigaBrain-0.5M* performs about 30% better than previous models in complex tasks like laundry folding and making espresso.</li>\n  <li>The model can successfully handle long and complicated tasks, as shown in real-world videos on its project page.</li>\n</ul>"}, "publishedAt": "2026-02-12T10:55:19.000Z", "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning", "summary": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, a VLA model trained via world model-based reinforcement learning. Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M^* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our https://gigabrain05m.github.io{project page}.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6426616ea5ec4a5cbc535634/w6OBVRe04BfXtAnLDzr3o.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12099.png", "numComments": 1, "submittedBy": {"_id": "6426616ea5ec4a5cbc535634", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/5IfSFYd9QOxz8K9QmBCst.png", "fullname": "JeffWang", "name": "Jeff-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68d6587936e2de9610d9f5f0", "name": "open-gigaai", "fullname": "GigaAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68d6394328e169473e90e4a6/zUK7FKr_8XqrN0aFUgsD-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.15763", "authors": [{"_id": "6995270f8d17d1ee8c10ebc5", "name": "GLM-5 Team", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc7", "name": "Aohan Zeng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc8", "name": "Xin Lv", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc9", "user": {"_id": "62b196646d3d059f40c3df19", "avatarUrl": "/avatars/dbddf54ae949437223f3a438d30ef653.svg", "isPro": false, "fullname": "Zhenyu Hou", "user": "think2try", "type": "user"}, "name": "Zhenyu Hou", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:01.080Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebca", "user": {"_id": "63033dc4e1e7f0e03a5e1a31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg", "isPro": false, "fullname": "Zhengxiao Du", "user": "zxdu20", "type": "user"}, "name": "Zhengxiao Du", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:03.474Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcb", "user": {"_id": "6231576e92e83fd1179ac3f0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664543160657-6231576e92e83fd1179ac3f0.jpeg", "isPro": false, "fullname": "Qinkai Zheng", "user": "Stanislas", "type": "user"}, "name": "Qinkai Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:21.181Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcc", "name": "Bin Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcd", "name": "Da Yin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebce", "name": "Chendi Ge", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcf", "user": {"_id": "62d00ff8dd7bdfc5e5c553c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d00ff8dd7bdfc5e5c553c6/u9Be7K16IS2Hc5OqKctEA.jpeg", "isPro": false, "fullname": "chengxing xie", "user": "yitianlian", "type": "user"}, "name": "Chengxing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:28.339Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd0", "user": {"_id": "65eaf755ab0a6a90da55ab58", "avatarUrl": "/avatars/a46890a9d067a913513edf3759f12c85.svg", "isPro": false, "fullname": "Cunxiang Wang", "user": "wangcunxiang", "type": "user"}, "name": "Cunxiang Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T10:16:12.082Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd1", "name": "Gengzheng Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd2", "name": "Hao Zeng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd3", "user": {"_id": "622ec54ce27c88667db094ad", "avatarUrl": "/avatars/8f06594b625a9c4fca1fffec4885bbdc.svg", "isPro": false, "fullname": "Haoke Zhang", "user": "zhk", "type": "user"}, "name": "Haoke Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:43.672Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd4", "name": "Haoran Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd5", "user": {"_id": "654ccc038a299b6be086cf1b", "avatarUrl": "/avatars/87d0f3c0f991368ce923da32cdd971a1.svg", "isPro": false, "fullname": "Huilong Chen", "user": "HuilongChen", "type": "user"}, "name": "Huilong Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:16.751Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd6", "name": "Jiajie Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd7", "name": "Jian Jiao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd8", "name": "Jiaqi Guo", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd9", "user": {"_id": "65655c3ed35fc55406e116aa", "avatarUrl": "/avatars/d45081ec1617bb737ea531866b76f57a.svg", "isPro": false, "fullname": "jingsen", "user": "wangjingsen", "type": "user"}, "name": "Jingsen Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:34:23.674Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebda", "name": "Jingzhao Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdb", "user": {"_id": "650d8ddbcbd0c7d550dc8278", "avatarUrl": "/avatars/3e49752b6b3c9f3875b4470cebb838e6.svg", "isPro": false, "fullname": "wujinzhu", "user": "kimjohn", "type": "user"}, "name": "Jinzhu Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:34:46.441Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdc", "user": {"_id": "64b73d6353d91a364aa8cea5", "avatarUrl": "/avatars/721553ab563cf13d8e7bfde088e3b753.svg", "isPro": false, "fullname": "Kedong Wang", "user": "mrwkd123", "type": "user"}, "name": "Kedong Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:47.078Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdd", "name": "Lei Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebde", "name": "Lin Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdf", "user": {"_id": "60eff04e22ab0ac83b0fc9d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60eff04e22ab0ac83b0fc9d8/pBjftNyFN1qB8Br3FZQmD.jpeg", "isPro": false, "fullname": "lucen zhong", "user": "anchorzhong", "type": "user"}, "name": "Lucen Zhong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:53.663Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe0", "user": {"_id": "64f1abd2b12bdcef55e1c078", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1abd2b12bdcef55e1c078/yG9HR_imUWqESN-JNcjf1.jpeg", "isPro": false, "fullname": "Mingdao Liu", "user": "lambdax", "type": "user"}, "name": "Mingdao Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:00.291Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe1", "user": {"_id": "6706db274341dcee45811105", "avatarUrl": "/avatars/125ef02f12785b8f51e84639d879efaa.svg", "isPro": false, "fullname": "Mingming Zhao", "user": "Lukas0510", "type": "user"}, "name": "Mingming Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:15.048Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe2", "name": "Pengfan Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe3", "name": "Qian Dong", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe4", "name": "Rui Lu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe5", "name": "Shuang-Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe6", "name": "Shulin Cao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe7", "name": "Song Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe8", "name": "Ting Jiang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe9", "name": "Xiaodong Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebea", "name": "Xiaohan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebeb", "name": "Xuancheng Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebec", "user": {"_id": "658834ba4f9d2b955e19d389", "avatarUrl": "/avatars/89d9a6f9fac67391dc052d49def1e758.svg", "isPro": false, "fullname": "Xuezhen Dong", "user": "xzdong", "type": "user"}, "name": "Xuezhen Dong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:25.460Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebed", "name": "Yabo Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebee", "name": "Yao Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebef", "name": "Yifan An", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf0", "name": "Yilin Niu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf1", "name": "Yitong Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf2", "name": "Yuanhao Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf3", "name": "Yukuo Cen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf4", "user": {"_id": "64ed568ccf6118a9379a61b8", "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg", "isPro": false, "fullname": "Yushi Bai", "user": "bys0318", "type": "user"}, "name": "Yushi Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T13:36:07.294Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf5", "name": "Zhongpei Qiao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf6", "name": "Zihan Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf7", "name": "Zikang Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf8", "name": "Zilin Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf9", "name": "Ziqiang Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfa", "name": "Zixuan Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfb", "name": "Bojie Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfc", "name": "Bosi Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfd", "name": "Can Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfe", "name": "Changpeng Cai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebff", "name": "Chao Yu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec00", "name": "Chen Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec01", "name": "Chen Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec02", "name": "Chenghua Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec03", "name": "Chengwei Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec04", "name": "Chenhui Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec05", "name": "Chenzheng Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec06", "name": "Congfeng Yin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec07", "name": "Daoyan Lin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec08", "name": "Dayong Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec09", "name": "Di Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0a", "name": "Ding Ai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0b", "name": "Erle Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0c", "name": "Fangzhou Yi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0d", "name": "Feiyu Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0e", "name": "Guohong Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0f", "name": "Hailong Sun", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec10", "name": "Haisha Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec11", "name": "Haiyi Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec12", "name": "Hanchen Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec13", "name": "Hanrui Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec14", "name": "Hanyu Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec15", "name": "Hao Peng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec16", "name": "Hao Tai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec17", "name": "Haobo Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec18", "name": "He Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec19", "name": "Hongwei Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1a", "name": "Hongxi Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1b", "name": "Hongyu Ge", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1c", "name": "Huan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1d", "name": "Huan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1e", "name": "Huanpeng Chu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1f", "name": "Jia'ni Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec20", "name": "Jiachen Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec21", "name": "Jiajing Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec22", "name": "Jiamin Ren", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec23", "name": "Jiapeng Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec24", "name": "Jiaxin Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec25", "name": "Jiayi Gui", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec26", "name": "Jiayue Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec27", "name": "Jijie Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec28", "name": "Jing An", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec29", "name": "Jing Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2a", "name": "Jingwei Yuan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2b", "name": "Jinhua Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2c", "name": "Jinxin Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2d", "name": "Junkai Zhi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2e", "name": "Junwen Duan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2f", "name": "Kaiyue Zhou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec30", "name": "Kangjian Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec31", "name": "Ke Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec32", "name": "Keyun Luo", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec33", "name": "Laiqiang Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec34", "name": "Leigang Sha", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec35", "name": "Liang Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec36", "name": "Lindong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec37", "name": "Lintao Ding", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec38", "name": "Lu Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec39", "name": "Minghao Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3a", "name": "Nianyi Lin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3b", "name": "Pan Ta", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3c", "name": "Qiang Zou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3d", "name": "Rongjun Song", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3e", "name": "Ruiqi Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3f", "name": "Shangqing Tu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec40", "name": "Shangtong Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec41", "name": "Shaoxiang Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec42", "name": "Shengyan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec43", "name": "Shijie Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec44", "name": "Shuang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec45", "name": "Shuyi Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec46", "name": "Wei Qin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec47", "name": "Wei Tian", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec48", "name": "Weining Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec49", "name": "Wenbo Yu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4a", "name": "Wenjie Liang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4b", "name": "Xiang Kuang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4c", "name": "Xiangmeng Cheng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4d", "name": "Xiangyang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4e", "name": "Xiaoquan Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4f", "name": "Xiaowei Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec50", "name": "Xiaoying Ling", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec51", "name": "Xing Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec52", "name": "Xingye Xia", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec53", "name": "Xinyuan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec54", "name": "Xinze Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec55", "name": "Xirui Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec56", "name": "Xunkai Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec57", "name": "Yandong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec58", "name": "Yanfu Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec59", "name": "Yidong Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5a", "name": "Yifan Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5b", "name": "Yijun Tan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5c", "name": "Yilin Zhou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5d", "name": "Yiming Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5e", "name": "Ying Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5f", "name": "Yinpei Su", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec60", "name": "Yipeng Geng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec61", "name": "Yipeng Geng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec62", "name": "Yong Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec63", "name": "Yonglin Tan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec64", "name": "Yuean Bi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec65", "name": "Yuhan Shen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec66", "name": "Yuhao Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec67", "name": "Yujiang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec68", "name": "Yunan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec69", "name": "Yunqing Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6a", "name": "Yuntao Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6b", "name": "Yurong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6c", "name": "Yutao Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6d", "name": "Yuxi Duan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6e", "name": "Yuxuan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6f", "name": "Zezhen Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec70", "name": "Zhengtao Jiang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec71", "name": "Zhenhe Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec72", "name": "Zheyu Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec73", "name": "Zhixiang Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec74", "name": "Zhuo Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec75", "name": "Zhuoer Feng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec76", "name": "Zijun Yao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec77", "name": "Ziwei Chai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec78", "name": "Ziyuan Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec79", "name": "Zuzhou Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7a", "name": "Bin Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7b", "name": "Minlie Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7c", "user": {"_id": "62ec23fcbd19e355478fc584", "avatarUrl": "/avatars/d7c567ef5f20bb3b9905cb5015d11e12.svg", "isPro": false, "fullname": "Hongning Wang", "user": "howang", "type": "user"}, "name": "Hongning Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:02.752Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7d", "user": {"_id": "65df8cbc2705d9672f55d1aa", "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg", "isPro": false, "fullname": "Juanzi Li", "user": "juanli", "type": "user"}, "name": "Juanzi Li", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:56.007Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7e", "user": {"_id": "640e73bdfdeaae1390857b62", "avatarUrl": "/avatars/cd6779e30f716002a7838ed93d5c0754.svg", "isPro": false, "fullname": "Yuxiao Dong", "user": "yuxiaod", "type": "user"}, "name": "Yuxiao Dong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:48.099Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7f", "user": {"_id": "640dff05474aa6f89556677e", "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg", "isPro": false, "fullname": "Jie Tang", "user": "jerytang", "type": "user"}, "name": "Jie Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:39.525Z", "hidden": false}], "publishedAt": "2026-02-17T17:50:56.000Z", "submittedOnDailyAt": "2026-02-18T00:12:28.521Z", "title": "GLM-5: from Vibe Coding to Agentic Engineering", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.", "upvotes": 31, "discussionId": "6995270f8d17d1ee8c10ec80", "githubRepo": "https://github.com/zai-org/GLM-5", "githubRepoAddedBy": "user", "ai_summary": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.", "ai_keywords": ["DSA", "asynchronous reinforcement learning", "agentic engineering", "vibe coding", "ARC capabilities", "post-training efficiency", "model alignment", "autonomous agents", "software engineering", "open benchmarks"], "githubStars": 1103, "summary_zh": "<ul>\n    <li>GLM-5\u662f\u4e00\u4e2a\u65b0\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5c06\u201c\u6c1b\u56f4\u7f16\u7801\u201d\u8f6c\u53d8\u4e3a\u201c\u81ea\u4e3b\u5de5\u7a0b\u201d\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u524d\u4ee3\u57fa\u7840\u4e0a\uff0c\u91c7\u7528\u4e86DSA\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u957f\u4e0a\u4e0b\u6587\u7684\u4e00\u81f4\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u81ea\u4e3b\u6027\uff0cGLM-5\u5f15\u5165\u4e86\u65b0\u7684\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u540e\u7684\u6548\u7387\u3002</li>\n    <li>\u65b0\u63d0\u51fa\u7684\u5f02\u6b65\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u4ece\u590d\u6742\u7684\u957f\u65f6\u95f4\u4ea4\u4e92\u4e2d\u5b66\u4e60\u3002</li>\n    <li>GLM-5\u5728\u4e3b\u8981\u7684\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u7f16\u7801\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u57fa\u51c6\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u4ee5\u5728https://github.com/zai-org/GLM-5\u627e\u5230\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>GLM-5 is a new advanced model that improves how we use coding and engineering techniques.</li>\n    <li>It reduces training and operation costs while keeping its ability to handle long contexts intact.</li>\n    <li>A new learning method improves how the model learns after training, making it more efficient.</li>\n    <li>GLM-5 uses innovative algorithms to enhance learning from complex tasks over longer periods.</li>\n    <li>It performs exceptionally well in real-world coding tasks, outperforming previous models in software engineering.</li>\n</ul>"}, "publishedAt": "2026-02-17T12:50:56.000Z", "title": "GLM-5: from Vibe Coding to Agentic Engineering", "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15763.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 234, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u7684\u63a5\u8fd1\u67af\u7aed\uff0c\u9884\u8bad\u7ec3\u6b63\u5728\u4ece\u4f7f\u7528\u66f4\u591a\u7684\u6807\u8bb0\u8f6c\u5411\u4f7f\u7528\u66f4\u597d\u7684\u6807\u8bb0\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9759\u6001\u8fc7\u6ee4\u5668\uff0c\u8981\u4e48\u4f7f\u7528\u57fa\u4e8e\u539f\u59cb\u68af\u5ea6\u7684\u52a8\u6001\u6807\u51c6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86OPUS\uff0c\u8fd9\u662f\u4e00\u79cd\u52a8\u6001\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5668\u7684\u66f4\u65b0\u7a7a\u95f4\u5b9a\u4e49\u6548\u7528\u3002</li>\n    <li>OPUS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u5de5\u4e1a\u7ea7\u57fa\u7ebf\u548c\u5b8c\u6574\u7684200B-token\u8bad\u7ec3\u3002</li>\n    <li>\u5728\u7279\u5b9a\u9886\u57df\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0cOPUS\u4ec5\u4f7f\u75280.5B tokens\u5c31\u80fd\u8d85\u8fc73B tokens\u7684\u5b8c\u6574\u8bad\u7ec3\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-quality public text is running out, leading to a focus on better data for pre-training models.</li>\n    <li>Current methods for data selection are either too rigid or not effective in adapting to training changes.</li>\n    <li>OPUS is a new method that selects data based on how it improves model updates during training.</li>\n    <li>It uses efficient techniques to handle large data while maintaining diversity in the selected data.</li>\n    <li>OPUS shows better performance with fewer tokens compared to traditional training methods, especially in specialized areas.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10388", "authors": [{"_id": "698d3bd265c0d15a6d16200e", "user": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "name": "Zhongzhi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:57:05.580Z", "hidden": false}, {"_id": "698d3bd265c0d15a6d16200f", "name": "Xuansheng Wu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162010", "name": "Yijiang Li", "hidden": false}, {"_id": "698d3bd265c0d15a6d162011", "name": "Lijie Hu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162012", "name": "Ninghao Liu", "hidden": false}], "publishedAt": "2026-02-11T00:23:13.000Z", "submittedOnDailyAt": "2026-02-16T02:31:34.708Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "submittedOnDailyBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "upvotes": 200, "discussionId": "698d3bd265c0d15a6d162013", "projectPage": "https://website-sigma-three-35.vercel.app/", "githubRepo": "https://github.com/Zhongzhi660/FAC-Synthesis", "githubRepoAddedBy": "user", "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.", "ai_keywords": ["Feature Activation Coverage", "sparse autoencoder", "data diversity", "downstream performance", "instruction following", "toxicity detection", "reward modeling", "behavior steering", "cross-model knowledge transfer", "data-centric optimization"], "githubStars": 52, "summary_zh": "<ul>\n    <li>\u540e\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u591a\u6837\u6027\u5ea6\u91cf\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u8bed\u8a00\u6587\u672c\uff0c\u4f46\u5bf9\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u7684\u8d21\u732e\u6709\u9650\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7279\u5f81\u6fc0\u6d3b\u8986\u76d6\uff08FAC\uff09\uff0c\u7528\u4e8e\u8861\u91cf\u6570\u636e\u7684\u591a\u6837\u6027\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFAC\u7684\u591a\u6837\u6027\u9a71\u52a8\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u53cd\u6620\u7f3a\u5931\u7279\u5f81\u7684\u5408\u6210\u6837\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5747\u63d0\u5347\u4e86\u6570\u636e\u591a\u6837\u6027\u548c\u4e0b\u6e38\u6027\u80fd\uff0c\u5e76\u652f\u6301\u8de8\u6a21\u578b\u77e5\u8bc6\u8f6c\u79fb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data diversity after training is important for the performance of large language models (LLMs).</li>\n    <li>Current methods to measure diversity focus on text metrics, which do not effectively indicate important features for performance.</li>\n    <li>This work introduces a new metric called Feature Activation Coverage (FAC) to better measure diversity in a useful way.</li>\n    <li>The authors propose a framework called FAC Synthesis that uses a model to find missing features and creates new data samples to address these gaps.</li>\n    <li>Experiments show that this approach improves both data diversity and performance on various tasks and allows for knowledge transfer between different model types.</li>\n</ul>"}, "publishedAt": "2026-02-10T19:23:13.000Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10388.png", "numComments": 2, "submittedBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "fullname": "ZhongzhiLi", "name": "Zhongzhi1228", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>ERNIE 5.0 \u662f\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u56de\u5f52\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u7b49\u591a\u79cd\u6a21\u6001\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u8d85\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff08MoE\uff09\uff0c\u5728\u7edf\u4e00\u7684\u9884\u6d4b\u76ee\u6807\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>ERNIE 5.0 \u5f15\u5165\u4e86\u5f39\u6027\u8bad\u7ec3\u8303\u5f0f\uff0c\u53ef\u4ee5\u5728\u5355\u6b21\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u4e0d\u540c\u6df1\u5ea6\u548c\u4e13\u5bb6\u5bb9\u91cf\u7684\u5b50\u6a21\u578b\uff0c\u4ee5\u7075\u6d3b\u5e94\u5bf9\u8d44\u6e90\u9650\u5236\u3002</li>\n    <li>\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u800c\u5747\u8861\u7684\u6027\u80fd\u3002</li>\n    <li>ERNIE 5.0 \u662f\u7b2c\u4e00\u4e2a\u516c\u5f00\u7684\u4e07\u4ebf\u53c2\u6570\u7edf\u4e00\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u53ef\u89c6\u5316\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new model that can understand and generate text, images, videos, and audio.</li>\n    <li>It uses a special architecture that allows it to efficiently manage different types of data and adapt to various resource needs.</li>\n    <li>The model can create several sub-models with different capabilities in one training session, allowing for flexibility in performance and resource use.</li>\n    <li>ERNIE 5.0 is the first large-scale model with a trillion parameters that can handle multiple types of data in both understanding and generation.</li>\n    <li>The report includes visuals and analysis to help others understand the model's structure and training methods.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Green-VLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7eff\u8272\u7c7b\u4eba\u673a\u5668\u4eba\u90e8\u7f72\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6846\u67b6\u3002</li>\n    <li>Green-VLA\u5305\u62ec\u4e94\u4e2a\u9636\u6bb5\u7684\u8bfe\u7a0b\uff0c\u6db5\u76d6\u57fa\u7840VLM\u3001\u591a\u6a21\u6001\u5bf9\u63a5\u3001\u591a\u79cd\u4f53\u73b0\u7684\u9884\u8bad\u7ec3\u3001\u7279\u5b9a\u4f53\u73b0\u7684\u9002\u5e94\u548c\u5f3a\u5316\u5b66\u4e60\u653f\u7b56\u5bf9\u9f50\u3002</li>\n    <li>\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u7ed3\u5408\u65f6\u95f4\u5bf9\u9f50\u548c\u8d28\u91cf\u8fc7\u6ee4\uff0c\u5171\u6536\u96c6\u4e863000\u5c0f\u65f6\u7684\u6f14\u793a\u6570\u636e\u3002</li>\n    <li>\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0cVLA\u63a7\u5236\u5668\u589e\u5f3a\u4e86\u60c5\u8282\u8fdb\u5c55\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u57fa\u4e8e\u8054\u5408\u9884\u6d4b\u7684\u6307\u5bfc\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u76ee\u6807\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRL\u5bf9\u9f50\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u65f6\u95f4\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5f88\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a new system designed for the Green humanoid robot, helping it understand and act in various situations.</li>\n    <li>The framework has five stages that include training on basic language models, understanding different types of information, and adapting to different robot forms.</li>\n    <li>It uses a large amount of data (3,000 hours of demonstrations) to improve its learning and performance.</li>\n    <li>The system can control different types of robots with a single set of instructions, making it flexible and efficient.</li>\n    <li>Tests show that this approach improves the robot's success rate and ability to handle complex tasks safely.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09877", "authors": [{"_id": "698c7abdeb12ea7453916869", "user": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "name": "Chenxu Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-12T16:49:45.534Z", "hidden": false}, {"_id": "698c7abdeb12ea745391686a", "name": "Chaozhuo Li", "hidden": false}, {"_id": "698c7abdeb12ea745391686b", "name": "Songyang Liu", "hidden": false}, {"_id": "698c7abdeb12ea745391686c", "name": "Zejian Chen", "hidden": false}, {"_id": "698c7abdeb12ea745391686d", "name": "Jinyu Hou", "hidden": false}, {"_id": "698c7abdeb12ea745391686e", "name": "Ji Qi", "hidden": false}, {"_id": "698c7abdeb12ea745391686f", "name": "Rui Li", "hidden": false}, {"_id": "698c7abdeb12ea7453916870", "name": "Litian Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916871", "name": "Qiwei Ye", "hidden": false}, {"_id": "698c7abdeb12ea7453916872", "name": "Zheng Liu", "hidden": false}, {"_id": "698c7abdeb12ea7453916873", "name": "Xu Chen", "hidden": false}, {"_id": "698c7abdeb12ea7453916874", "name": "Xi Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916875", "name": "Philip S. Yu", "hidden": false}], "publishedAt": "2026-02-10T15:18:19.000Z", "submittedOnDailyAt": "2026-02-13T00:53:30.377Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "submittedOnDailyBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "upvotes": 169, "discussionId": "698c7abdeb12ea7453916876", "ai_summary": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.", "ai_keywords": ["multi-agent systems", "large language models", "self-evolution", "safety alignment", "information-theoretic framework", "anthropic value distributions", "statistical blind spots", "self-evolving AI societies", "external oversight", "safety-preserving mechanisms"], "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u53ef\u4ee5\u5b9e\u73b0\u96c6\u4f53\u667a\u80fd\u548c\u81ea\u6211\u8fdb\u5316\u3002</li>\n    <li>\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5e94\u8be5\u80fd\u591f\u6301\u7eed\u81ea\u6211\u6539\u8fdb\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u3002</li>\n    <li>\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u5b8c\u5168\u81ea\u6211\u8fdb\u5316\u3001\u5b8c\u5168\u9694\u79bb\u548c\u5b89\u5168\u4e0d\u53d8\u7684\u667a\u80fd\u4f53\u793e\u4f1a\u662f\u4e0d\u53ef\u80fd\u7684\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u9694\u79bb\u7684\u81ea\u6211\u8fdb\u5316\u4f1a\u5bfc\u81f4\u5b89\u5168\u6027\u4e0b\u964d\uff0c\u4ea7\u751f\u7edf\u8ba1\u76f2\u70b9\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u5916\u90e8\u76d1\u7763\u6216\u65b0\u578b\u7684\u5b89\u5168\u673a\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems using large language models can help achieve better collective intelligence and self-improvement.</li>\n    <li>However, it's impossible to have these systems continuously improve while being completely safe and isolated, a challenge called the self-evolution trilemma.</li>\n    <li>Research shows that isolating these systems leads to safety issues, as they develop blind spots that can harm their alignment with human values.</li>\n    <li>Experiments with both open and closed systems confirm that safety will deteriorate over time.</li>\n    <li>The study suggests finding new ways to ensure safety and emphasizes the need for external oversight to manage risks in self-evolving AI systems.</li>\n</ul>"}, "publishedAt": "2026-02-10T10:18:19.000Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png", "numComments": 2, "submittedBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "fullname": "Chenxu Wang", "name": "xunyoyo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u56fe\u5f62\u7528\u6237\u754c\u9762\u4ee3\u7406\u901a\u8fc7\u611f\u77e5\u754c\u9762\u548c\u6267\u884c\u52a8\u4f5c\u4e0e\u73af\u5883\u4e92\u52a8\u3002</li>\n    <li>Code2World\u662f\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00\u7f16\u7801\u5668\uff0c\u53ef\u901a\u8fc7\u751f\u6210\u53ef\u6e32\u67d3\u4ee3\u7801\u6765\u6a21\u62df\u4e0b\u4e00\u4e2a\u89c6\u89c9\u72b6\u6001\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86AndroidCode\uff0c\u901a\u8fc7\u5c06GUI\u8f68\u8ff9\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771fHTML\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>Code2World\u5728\u7528\u6237\u754c\u9762\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u8d85\u8d8a\u4e86\u7ade\u4e89\u5bf9\u624bGPT-5\u548cGemini-3-Pro-Image\u3002</li>\n    <li>Code2World\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u5bfc\u822a\u7684\u6210\u529f\u7387\uff0cAndroidWorld\u5bfc\u822a\u6210\u529f\u7387\u63d0\u5347\u4e869.5%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Code2World is a system that helps virtual agents predict the next visual state of user interfaces by generating code.</li>\n    <li>It addresses issues with existing methods that struggle to balance high-quality visuals and precise control over structures.</li>\n    <li>The system uses a large dataset of over 80,000 screen-action pairs created from translating GUI interactions into HTML code.</li>\n    <li>Code2World improves the predictions of user interfaces, outperforming other models like GPT-5 and Gemini-3-Pro-Image.</li>\n    <li>It also enhances navigation success rates in Android applications by improving performance by 9.5% over previous models.</li>\n</ul>"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10604", "authors": [{"_id": "698d417065c0d15a6d162026", "name": "Ailin Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162027", "name": "Ang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162028", "name": "Aobo Kong", "hidden": false}, {"_id": "698d417065c0d15a6d162029", "name": "Bin Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202a", "name": "Binxing Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d16202b", "name": "Bo Dong", "hidden": false}, {"_id": "698d417065c0d15a6d16202c", "name": "Bojun Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202d", "name": "Boyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16202e", "name": "Brian Li", "hidden": false}, {"_id": "698d417065c0d15a6d16202f", "name": "Buyun Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162030", "name": "Chang Su", "hidden": false}, {"_id": "698d417065c0d15a6d162031", "name": "Changxin Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162032", "name": "Changyi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162033", "name": "Chao Lou", "hidden": false}, {"_id": "698d417065c0d15a6d162034", "name": "Chen Hu", "hidden": false}, {"_id": "698d417065c0d15a6d162035", "name": "Chen Xu", "hidden": false}, {"_id": "698d417065c0d15a6d162036", "name": "Chenfeng Yu", "hidden": false}, {"_id": "698d417065c0d15a6d162037", "name": "Chengting Feng", "hidden": false}, {"_id": "698d417065c0d15a6d162038", "name": "Chengyuan Yao", "hidden": false}, {"_id": "698d417065c0d15a6d162039", "name": "Chunrui Han", "hidden": false}, {"_id": "698d417065c0d15a6d16203a", "name": "Dan Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203b", "name": "Dapeng Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16203c", "name": "Daxin Jiang", "hidden": false}, {"_id": "698d417065c0d15a6d16203d", "name": "Dehua Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203e", "name": "Deshan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16203f", "name": "Di Qi", "hidden": false}, {"_id": "698d417065c0d15a6d162040", "name": "Enle Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162041", "name": "Fajie Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162042", "name": "Fanqi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162043", "name": "Guanzhe Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162044", "name": "Gulin Yan", "hidden": false}, {"_id": "698d417065c0d15a6d162045", "name": "Guoliang Cao", "hidden": false}, {"_id": "698d417065c0d15a6d162046", "name": "Guopeng Li", "hidden": false}, {"_id": "698d417065c0d15a6d162047", "name": "Han Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162048", "name": "Hangyu Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162049", "user": {"_id": "64b7874b9f5987572ca28461", "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg", "isPro": false, "fullname": "hanshanzhang", "user": "brain-zhang", "type": "user"}, "name": "Hanshan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:52.602Z", "hidden": false}, {"_id": "698d417065c0d15a6d16204a", "name": "Hao Nie", "hidden": false}, {"_id": "698d417065c0d15a6d16204b", "name": "Haonan Jia", "hidden": false}, {"_id": "698d417065c0d15a6d16204c", "name": "Haoran Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204d", "name": "Hebin Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16204e", "name": "Hekun Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204f", "name": "Heng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162050", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "698d417065c0d15a6d162051", "name": "Hongbo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162052", "name": "Hongbo Peng", "hidden": false}, {"_id": "698d417065c0d15a6d162053", "name": "Hongyu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162054", "name": "Hongyuan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162055", "name": "Houyong Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162056", "name": "Huangxi Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d162057", "name": "Huimin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d162058", "name": "Huiyong Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162059", "name": "Jia Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16205a", "name": "Jian Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16205b", "name": "Jianjian Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16205c", "name": "Jiaoren Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16205d", "name": "Jiaran Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16205e", "name": "Jiashu Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16205f", "name": "Jiashuo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162060", "name": "Jiayi Fu", "hidden": false}, {"_id": "698d417065c0d15a6d162061", "name": "Jiayu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162062", "name": "Jie Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162063", "name": "Jie Luo", "hidden": false}, {"_id": "698d417065c0d15a6d162064", "name": "Jie Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162065", "name": "Jie Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162066", "name": "Jieyi Hou", "hidden": false}, {"_id": "698d417065c0d15a6d162067", "name": "Jing Bai", "hidden": false}, {"_id": "698d417065c0d15a6d162068", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:37.335Z", "hidden": false}, {"_id": "698d417065c0d15a6d162069", "name": "Jingjing Xie", "hidden": false}, {"_id": "698d417065c0d15a6d16206a", "name": "Jingwei Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16206b", "name": "Jingyang Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16206c", "name": "Jishi Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16206d", "name": "Junfeng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16206e", "name": "Junzhe Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16206f", "name": "Ka Man Lo", "hidden": false}, {"_id": "698d417065c0d15a6d162070", "name": "Kai Liang", "hidden": false}, {"_id": "698d417065c0d15a6d162071", "name": "Kaibo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162072", "name": "Kaijun Tan", "hidden": false}, {"_id": "698d417065c0d15a6d162073", "user": {"_id": "66668c591964b6188ee310c2", "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg", "isPro": false, "fullname": "Kaiwen Yan", "user": "linrany", "type": "user"}, "name": "Kaiwen Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:58.524Z", "hidden": false}, {"_id": "698d417065c0d15a6d162074", "name": "Kaixiang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162075", "name": "Kang An", "hidden": false}, {"_id": "698d417065c0d15a6d162076", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:56.339Z", "hidden": false}, {"_id": "698d417065c0d15a6d162077", "name": "Lei Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162078", "name": "Liang Lv", "hidden": false}, {"_id": "698d417065c0d15a6d162079", "name": "Liang Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16207a", "name": "Liangyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207b", "name": "Lieyu Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16207c", "name": "Liguo Tan", "hidden": false}, {"_id": "698d417065c0d15a6d16207d", "name": "Lin Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16207e", "name": "Lina Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207f", "name": "Luck Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162080", "name": "Mengqiang Ren", "hidden": false}, {"_id": "698d417065c0d15a6d162081", "name": "Michael Li", "hidden": false}, {"_id": "698d417065c0d15a6d162082", "name": "Ming Li", "hidden": false}, {"_id": "698d417065c0d15a6d162083", "name": "Mingliang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162084", "name": "Mingming Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162085", "name": "Mingrui Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162086", "name": "Mitt Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162087", "name": "Na Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162088", "name": "Peng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162089", "name": "Qi Han", "hidden": false}, {"_id": "698d417065c0d15a6d16208a", "name": "Qian Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16208b", "name": "Qinglin He", "hidden": false}, {"_id": "698d417065c0d15a6d16208c", "name": "Qinxin Du", "hidden": false}, {"_id": "698d417065c0d15a6d16208d", "name": "Qiuping Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16208e", "name": "Quan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16208f", "name": "Rongqiu Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162090", "name": "Ruihang Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162091", "name": "Ruixin Han", "hidden": false}, {"_id": "698d417065c0d15a6d162092", "name": "Ruosi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162093", "name": "Ruyan Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162094", "name": "Shan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162095", "name": "Shaoliang Pang", "hidden": false}, {"_id": "698d417065c0d15a6d162096", "name": "Shaowen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162097", "name": "Shengjie Fan", "hidden": false}, {"_id": "698d417065c0d15a6d162098", "name": "Shijie Shang", "hidden": false}, {"_id": "698d417065c0d15a6d162099", "name": "Shiliang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d16209a", "name": "Shiwei Li", "hidden": false}, {"_id": "698d417065c0d15a6d16209b", "name": "Shuangshuang Tian", "hidden": false}, {"_id": "698d417065c0d15a6d16209c", "name": "Siqi Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16209d", "name": "Siye Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16209e", "name": "Siyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16209f", "name": "Song Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a0", "name": "Tiancheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620a1", "name": "Tianchi Yue", "hidden": false}, {"_id": "698d417065c0d15a6d1620a2", "name": "Tianhao Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d1620a3", "name": "Tianning Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620a4", "name": "Tingdan Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620a5", "name": "Wang You", "hidden": false}, {"_id": "698d417065c0d15a6d1620a6", "name": "Wei Ji", "hidden": false}, {"_id": "698d417065c0d15a6d1620a7", "name": "Wei Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a8", "name": "Wei Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620a9", "name": "Weibo Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620aa", "user": {"_id": "6657620ea496f7fcb67c3871", "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg", "isPro": false, "fullname": "xieweihao", "user": "chalengr", "type": "user"}, "name": "Weihao Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:48.216Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ab", "name": "Wen Sun", "hidden": false}, {"_id": "698d417065c0d15a6d1620ac", "name": "Wenjin Deng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ad", "user": {"_id": "650c04795510464e85b47470", "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg", "isPro": false, "fullname": "wen", "user": "zhengwenzhen", "type": "user"}, "name": "Wenzhen Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:45.930Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ae", "name": "Wuxun Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620af", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b0", "name": "Xiangwen Kong", "hidden": false}, {"_id": "698d417065c0d15a6d1620b1", "name": "Xiangyu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b2", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b3", "name": "Xiaobo Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b4", "name": "Xiaojia Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b5", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620b6", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d1620b7", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620b8", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b9", "name": "Xin Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ba", "name": "Xin Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bb", "name": "Xin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bc", "name": "Xing Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620bd", "name": "Xingping Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620be", "name": "Xinran Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620bf", "name": "Xu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620c0", "user": {"_id": "64ec5b64bfb2aa06a46ff2d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg", "isPro": false, "fullname": "xuan he", "user": "tpa115k31", "type": "user"}, "name": "Xuan He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:36.240Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620c1", "name": "Xuanti Feng", "hidden": false}, {"_id": "698d417065c0d15a6d1620c2", "name": "Xuedan Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c3", "name": "Xuqiang Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620c4", "name": "Yanbo Yu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c5", "name": "Yang Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620c6", "name": "Yang Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c7", "name": "Yanlin Lai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c8", "name": "Yanming Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c9", "name": "Yaoyu Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ca", "name": "Yeqing Shen", "hidden": false}, {"_id": "698d417065c0d15a6d1620cb", "name": "Yibo Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d1620cc", "name": "Yichen Lv", "hidden": false}, {"_id": "698d417065c0d15a6d1620cd", "name": "Yicheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620ce", "name": "Yifeng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620cf", "name": "Yijing Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d0", "name": "Yikun Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d1", "name": "Yin Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d2", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d3", "name": "Yinmin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d4", "name": "Yitong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d5", "name": "Yixuan Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d6", "name": "Yiyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620d7", "name": "Yongchi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d8", "name": "Yongshen Long", "hidden": false}, {"_id": "698d417065c0d15a6d1620d9", "name": "Yongyao Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620da", "name": "Yousong Guan", "hidden": false}, {"_id": "698d417065c0d15a6d1620db", "name": "Yu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620dc", "name": "Yuang Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620dd", "name": "Yuanhao Ding", "hidden": false}, {"_id": "698d417065c0d15a6d1620de", "name": "Yuantao Fan", "hidden": false}, {"_id": "698d417065c0d15a6d1620df", "name": "Yuanzhen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e0", "name": "Yuchu Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620e1", "name": "Yudi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e2", "name": "Yue Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620e3", "name": "Yueqiang Lin", "hidden": false}, {"_id": "698d417065c0d15a6d1620e4", "name": "Yufan Lu", "hidden": false}, {"_id": "698d417065c0d15a6d1620e5", "name": "Yuling Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e6", "name": "Yunzhou Ju", "hidden": false}, {"_id": "698d417065c0d15a6d1620e7", "name": "Yurong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e8", "name": "Yusheng Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620e9", "name": "Yuxiang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ea", "name": "Yuyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620eb", "name": "Yuzhu Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620ec", "name": "Zejia Weng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ed", "name": "Zetao Hong", "hidden": false}, {"_id": "698d417065c0d15a6d1620ee", "name": "Zexi Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ef", "name": "Zhe Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620f0", "name": "Zheng Ge", "hidden": false}, {"_id": "698d417065c0d15a6d1620f1", "name": "Zheng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620f2", "name": "Zheng Zeng", "hidden": false}, {"_id": "698d417065c0d15a6d1620f3", "user": {"_id": "63607ace9ddc44e710e13f0f", "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg", "isPro": false, "fullname": "zy", "user": "lu-vae", "type": "user"}, "name": "Zhenyi Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:50.532Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620f4", "name": "Zhewei Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f5", "name": "Zhichao Chang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f6", "name": "Zhiguo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f7", "name": "Zhiheng Hu", "hidden": false}, {"_id": "698d417065c0d15a6d1620f8", "name": "Zidong Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f9", "name": "Zili Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fa", "name": "Ziqi Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620fb", "name": "Zixin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fc", "name": "Zixuan Wang", "hidden": false}], "publishedAt": "2026-02-11T07:53:51.000Z", "submittedOnDailyAt": "2026-02-12T00:26:49.880Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "upvotes": 150, "discussionId": "698d417165c0d15a6d1620fd", "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash", "githubRepoAddedBy": "user", "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.", "ai_keywords": ["Mixture-of-Experts", "sparse MoE", "foundation model", "active parameters", "interleaved attention", "sliding-window attention", "full attention", "Multi-Token Prediction", "reinforcement learning", "verifiable signals", "preference feedback", "off-policy training", "self-improvement", "IMO-AnswerBench", "LiveCodeBench", "tau2-Bench", "BrowseComp", "Terminal-Bench"], "githubStars": 1245, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Step 3.5 Flash\uff0c\u8fd9\u662f\u4e00\u4e2a\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u548c\u6267\u884c\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528196B\u53c2\u6570\u7684\u57fa\u7840\u548c11B\u6d3b\u8dc3\u53c2\u6570\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7\u4f18\u5316\u6ed1\u52a8\u7a97\u53e3\u548c\u591a\u6807\u8bb0\u9884\u6d4b\u6280\u672f\uff0c\u51cf\u5c11\u591a\u8f6e\u4ea4\u4e92\u7684\u5ef6\u8fdf\u548c\u6210\u672c\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u4fe1\u53f7\u4e0e\u504f\u597d\u53cd\u9988\uff0c\u652f\u6301\u81ea\u6211\u6539\u8fdb\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u4e0e\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\uff0c\u4e3a\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u90e8\u7f72\u63d0\u4f9b\u4e86\u5f3a\u5927\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Step 3.5 Flash is a new model that combines advanced intelligence with efficiency in computing.</li>\n  <li>It uses a large foundation of 196 billion parameters but activates only 11 billion for fast performance.</li>\n  <li>The model is designed to improve response time and reduce costs for complex interactions.</li>\n  <li>It features a learning framework that allows for consistent self-improvement in tasks like math and coding.</li>\n  <li>Step 3.5 Flash performs well in various benchmarks, matching the capabilities of other top models.</li>\n</ul>"}, "publishedAt": "2026-02-11T02:53:51.000Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 231, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Kimi K2.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u4e00\u822c\u667a\u80fd\u6c34\u5e73\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u4e24\u79cd\u6a21\u5f0f\u76f8\u4e92\u589e\u5f3a\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5305\u62ec\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\uff0c\u4e00\u4e2a\u81ea\u6211\u5bfc\u5411\u7684\u4ee3\u7406\u534f\u8c03\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u590d\u6742\u4efb\u52a1\u52a8\u6001\u5206\u89e3\u5e76\u5e76\u884c\u6267\u884c\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cKimi K2.5\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u7b49\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e76\u4e14Agent Swarm\u53ef\u4ee5\u5c06\u5ef6\u8fdf\u51cf\u5c11\u6700\u591a4.5\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kimi K2.5 is an open-source model that combines text and vision to improve general intelligence.</li>\n    <li>It uses techniques that allow text and images to work together better, such as joint training and reinforcement learning.</li>\n    <li>Kimi K2.5 features Agent Swarm, a system that breaks complex tasks into smaller parts and handles them at the same time.</li>\n    <li>The model has been tested and shows top performance in areas like coding, vision, reasoning, and other intelligent tasks.</li>\n    <li>The Kimi K2.5 model is available for researchers and developers to use for future projects.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.09082", "authors": [{"_id": "698bea506052d3bed96309cb", "name": "Veuns-Team", "hidden": false}, {"_id": "698bea506052d3bed96309cd", "name": "Changlong Gao", "hidden": false}, {"_id": "698bea506052d3bed96309ce", "user": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "name": "Zhangxuan Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:14.456Z", "hidden": false}, {"_id": "698bea506052d3bed96309cf", "name": "Yulin Liu", "hidden": false}, {"_id": "698bea506052d3bed96309d0", "name": "Xinyu Qiu", "hidden": false}, {"_id": "698bea506052d3bed96309d1", "name": "Shuheng Shen", "hidden": false}, {"_id": "698bea506052d3bed96309d2", "name": "Yue Wen", "hidden": false}, {"_id": "698bea506052d3bed96309d3", "name": "Tianyu Xia", "hidden": false}, {"_id": "698bea506052d3bed96309d4", "name": "Zhenyu Xu", "hidden": false}, {"_id": "698bea506052d3bed96309d5", "user": {"_id": "64cb238576200ec80fe988f8", "avatarUrl": "/avatars/42c48710c7881c9dfbcc075fec3cb600.svg", "isPro": false, "fullname": "zeus", "user": "zengw", "type": "user"}, "name": "Zhengwen Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:24:43.235Z", "hidden": false}, {"_id": "698bea506052d3bed96309d6", "user": {"_id": "654c9dac09dd7ef524a0be1e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png", "isPro": false, "fullname": "beitongzhou", "user": "syorami", "type": "user"}, "name": "Beitong Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:11.859Z", "hidden": false}, {"_id": "698bea506052d3bed96309d7", "name": "Xingran Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309d8", "name": "Weizhi Chen", "hidden": false}, {"_id": "698bea506052d3bed96309d9", "name": "Sunhao Dai", "hidden": false}, {"_id": "698bea506052d3bed96309da", "name": "Jingya Dou", "hidden": false}, {"_id": "698bea506052d3bed96309db", "name": "Yichen Gong", "hidden": false}, {"_id": "698bea506052d3bed96309dc", "name": "Yuan Guo", "hidden": false}, {"_id": "698bea506052d3bed96309dd", "name": "Zhenlin Guo", "hidden": false}, {"_id": "698bea506052d3bed96309de", "user": {"_id": "65e0763a9299e96ee674876e", "avatarUrl": "/avatars/0ea342c9f72fa3b8a8f634559d094907.svg", "isPro": false, "fullname": "fengdian", "user": "fengrudian", "type": "user"}, "name": "Feng Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:04.463Z", "hidden": false}, {"_id": "698bea506052d3bed96309df", "name": "Qian Li", "hidden": false}, {"_id": "698bea506052d3bed96309e0", "name": "Jinzhen Lin", "hidden": false}, {"_id": "698bea506052d3bed96309e1", "name": "Yuqi Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309e2", "name": "Linchao Zhu", "hidden": false}, {"_id": "698bea506052d3bed96309e3", "name": "Liang Chen", "hidden": false}, {"_id": "698bea506052d3bed96309e4", "name": "Zhenyu Guo", "hidden": false}, {"_id": "698bea506052d3bed96309e5", "name": "Changhua Meng", "hidden": false}, {"_id": "698bea506052d3bed96309e6", "name": "Weiqiang Wang", "hidden": false}], "publishedAt": "2026-02-09T18:43:40.000Z", "submittedOnDailyAt": "2026-02-11T00:10:55.649Z", "title": "UI-Venus-1.5 Technical Report", "submittedOnDailyBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "upvotes": 143, "discussionId": "698bea516052d3bed96309e7", "projectPage": "https://ui-venus.github.io/UI-Venus-1.5/", "githubRepo": "https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5", "githubRepoAddedBy": "user", "ai_summary": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.", "ai_keywords": ["GUI agents", "Mid-Training stage", "Online Reinforcement Learning", "full-trajectory rollouts", "Model Merging", "dense variants", "mixture-of-experts variant"], "githubStars": 708, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>UI-Venus-1.5\u662f\u4e00\u4e2a\u7edf\u4e00\u7684GUI\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u6570\u5b57\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u5316\u4ea4\u4e92\u3002</li>\n    <li>\u8be5\u6a21\u578b\u6709\u4e24\u79cd\u5bc6\u96c6\u53d8\u4f53\uff082B\u548c8B\uff09\u548c\u4e00\u79cd\u4e13\u5bb6\u6df7\u5408\u53d8\u4f53\uff0830B-A3B\uff09\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\u3002</li>\n    <li>UI-Venus-1.5\u5f15\u5165\u4e86\u4e09\u4e2a\u4e3b\u8981\u6280\u672f\u8fdb\u5c55\uff0c\u5305\u62ec\u5229\u752810\u4ebf\u4e2a\u6807\u8bb0\u7684\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\uff0c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u53ca\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6784\u5efa\u7684\u7edf\u4e00GUI\u4ee3\u7406\u3002</li>\n    <li>\u7ecf\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0cUI-Venus-1.5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u9ad8\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5f3a\u57fa\u7ebf\u3002</li>\n    <li>\u5b83\u5728\u5404\u79cd\u4e2d\u56fd\u79fb\u52a8\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u7528\u6237\u6307\u4ee4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UI-Venus-1.5 is a new GUI Agent designed to automate tasks in digital environments.</li>\n    <li>It includes two dense model variants and one mixture-of-experts variant to suit different applications.</li>\n    <li>The model introduces advances like a Mid-Training stage using 10 billion tokens, Online Reinforcement Learning for better navigation, and a unified model combining different domain-specific models.</li>\n    <li>UI-Venus-1.5 has achieved top performance on several benchmarks, outperforming previous models.</li>\n    <li>It shows strong navigation skills in various Chinese mobile apps, successfully following user commands in real-world situations.</li>\n</ul>"}, "publishedAt": "2026-02-09T13:43:40.000Z", "title": "UI-Venus-1.5 Technical Report", "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png", "numComments": 2, "submittedBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "fullname": "Zhangxuan Gu", "name": "zhangxgu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u91d1\u878d\u5e02\u573a\u566a\u97f3\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4alpha\u6316\u6398\u5bf9\u56de\u6d4b\u7ed3\u679c\u7684\u566a\u97f3\u548c\u5e02\u573a\u53d8\u5316\u975e\u5e38\u654f\u611f\u3002</li>\n    <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u7684alpha\u6316\u6398\u6846\u67b6\uff0c\u80fd\u901a\u8fc7\u8f68\u8ff9\u7ea7\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u6765\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u5b9a\u4f4d\u6bcf\u6761\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u6b65\u9aa4\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6539\uff0c\u5e76\u91cd\u7ec4\u9ad8\u6536\u76ca\u6bb5\u843d\u4ee5\u91cd\u7528\u6709\u6548\u6a21\u5f0f\u3002</li>\n    <li>\u5728\u4f7f\u7528GPT-5.2\u65f6\uff0cQuantaAlpha\u5728\u4e2d\u56fd\u8bc1\u5238\u6307\u6570300\uff08CSI 300\uff09\u4e0a\u53d6\u5f97\u4e8627.75%\u7684\u5e74\u5316\u6536\u76ca\u7387\u548c0.1501\u7684\u4fe1\u606f\u7cfb\u6570\u3002</li>\n    <li>QuantaAlpha\u5728\u4e0d\u540c\u5e02\u573a\u6307\u6570\uff08\u5982CSI 500\u548c\u6807\u51c6\u666e\u5c14500\uff09\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7a33\u5065\u6027\uff0c\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u8d85\u989d\u6536\u76ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets are unpredictable, making it hard to test new trading strategies effectively.</li>\n    <li>The QuantaAlpha framework improves the process of finding successful trading strategies by using a method that allows for better searching and learning from past experiences.</li>\n    <li>QuantaAlpha focuses on refining strategies by identifying and fixing weak points and combining successful elements from different strategies.</li>\n    <li>It maintains consistency in the strategies it develops, reducing unnecessary complexity and redundancy.</li>\n    <li>Tests show that QuantaAlpha outperforms previous models, achieving high returns and successfully transferring strategies across different market indices.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 19, 2026";
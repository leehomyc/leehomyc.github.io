window.trendingPapers = {
    "today": [{"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5206\u5f00\u5904\u7406\u7684\u95ee\u9898\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u80cc\u666f\uff0c\u5e76\u5c06\u5b83\u4eec\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u589e\u5f3a\u5176\u529f\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one complete system, unlike traditional separate methods.</li>\n    <li>The system can handle various inputs like text, images, and videos to produce unified content.</li>\n    <li>Kling-Omni uses a strong data foundation and advanced training methods for better performance.</li>\n    <li>It shows great skill in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15745", "authors": [{"_id": "6944bdd4fbf17e708e185fb8", "name": "Tiwei Bie", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fb9", "name": "Maosong Cao", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fba", "name": "Kun Chen", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fbb", "name": "Lun Du", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fbc", "user": {"_id": "6909e9153e33b19bf2f71b05", "avatarUrl": "/avatars/8fa060fa5c21ce57c2e5f87a0835af07.svg", "isPro": false, "fullname": "Mingliang Gong", "user": "bright-ai-infra", "type": "user"}, "name": "Mingliang Gong", "status": "admin_assigned", "statusLastChangedAt": "2025-12-19T09:49:22.650Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fbd", "user": {"_id": "64a68e21d09682887d9ed95a", "avatarUrl": "/avatars/2d7be7d9221b7a59ecbeb5383f70d83d.svg", "isPro": false, "fullname": "Zhuocheng Gong", "user": "gzhch", "type": "user"}, "name": "Zhuochen Gong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:22.280Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fbe", "name": "Yanmei Gu", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fbf", "name": "Jiaqi Hu", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc0", "user": {"_id": "625f8e6f673e5862a8c07f1a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677306231208-625f8e6f673e5862a8c07f1a.jpeg", "isPro": false, "fullname": "Bill H", "user": "lccurious", "type": "user"}, "name": "Zenan Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:17.788Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc1", "name": "Zhenzhong Lan", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc2", "name": "Chengxi Li", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc3", "user": {"_id": "64c07b488e2612254361153b", "avatarUrl": "/avatars/ade0f783cc4c2d3e73f402637f595471.svg", "isPro": false, "fullname": "chongxuan li", "user": "zhenxuan00", "type": "user"}, "name": "Chongxuan Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-19T09:56:49.796Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc4", "name": "Jianguo Li", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc5", "name": "Zehuan Li", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc6", "user": {"_id": "68ef4ecddfc956769ea3e909", "avatarUrl": "/avatars/2448a4805102351ef4a6ece7d7f88b02.svg", "isPro": false, "fullname": "Huabin liu", "user": "liuhuabin1229", "type": "user"}, "name": "Huabin Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-19T09:58:07.116Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc7", "user": {"_id": "646ed6708d316fde87b3eee3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6PjHGz3w2iPYa8eZCP2Qc.jpeg", "isPro": false, "fullname": "Liulin", "user": "Ulov888", "type": "user"}, "name": "Ling Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T10:49:51.184Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc8", "user": {"_id": "690077c498a1389fbb6e9a5d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690077c498a1389fbb6e9a5d/JPf4OH46m0uU7CH9aO4Yo.jpeg", "isPro": false, "fullname": "Guoshan Lu", "user": "luguoshan", "type": "user"}, "name": "Guoshan Lu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:23.767Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fc9", "name": "Xiaocheng Lu", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fca", "name": "Yuxin Ma", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fcb", "user": {"_id": "64d2eaee80ce1029be72bdcd", "avatarUrl": "/avatars/6917aa66459d6f9a4d6437381c496bdd.svg", "isPro": false, "fullname": "Jianfeng Tan", "user": "jianfengt", "type": "user"}, "name": "Jianfeng Tan", "status": "admin_assigned", "statusLastChangedAt": "2025-12-19T09:57:09.384Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fcc", "name": "Lanning Wei", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fcd", "user": {"_id": "64b8c89052b7353d8c6a1013", "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg", "isPro": false, "fullname": "Ji-Rong Wen", "user": "jrwen", "type": "user"}, "name": "Ji-Rong Wen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-19T09:57:23.843Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fce", "name": "Yipeng Xing", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fcf", "name": "Xiaolu Zhang", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fd0", "user": {"_id": "6725f5a7f05f62659e3615f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iiXdzhaSnExw2dwQQtLZ8.png", "isPro": false, "fullname": "Junbo Zhao", "user": "jakezhao2024", "type": "user"}, "name": "Junbo Zhao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:20.782Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fd1", "name": "Da Zheng", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fd2", "name": "Jun Zhou", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fd3", "user": {"_id": "63eb008e5c837d9968f1eb71", "avatarUrl": "/avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg", "isPro": false, "fullname": "Junlin Zhou", "user": "jlzhou", "type": "user"}, "name": "Junlin Zhou", "status": "admin_assigned", "statusLastChangedAt": "2025-12-19T12:13:31.053Z", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fd4", "name": "Zhanchao Zhou", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fd5", "name": "Liwang Zhu", "hidden": false}, {"_id": "6944bdd4fbf17e708e185fd6", "user": {"_id": "673b5f24e863f1d28b402efc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png", "isPro": false, "fullname": "yihongzhuang", "user": "utdawn", "type": "user"}, "name": "Yihong Zhuang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:19.300Z", "hidden": false}], "publishedAt": "2025-12-10T09:26:18.000Z", "submittedOnDailyAt": "2025-12-19T00:22:16.609Z", "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.", "upvotes": 52, "discussionId": "6944bdd5fbf17e708e185fd7", "githubRepo": "https://github.com/inclusionAI/LLaDA2.0", "githubRepoAddedBy": "user", "ai_summary": "LLaDA2.0 converts auto-regressive models into discrete diffusion large language models with a novel training scheme, achieving superior performance and efficiency at scale.", "ai_keywords": ["discrete diffusion large language models", "dLLM", "auto-regressive models", "knowledge inheritance", "progressive adaptation", "efficiency-aware design", "block-level WSD", "block diffusion", "full-sequence diffusion", "post-training alignment", "SFT", "DPO", "Mixture-of-Experts", "parallel decoding"], "githubStars": 159, "organization": {"_id": "67c1d682826160b28f778510", "name": "antgroup", "fullname": "Ant Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86LLaDA2.0\uff0c\u4e00\u79cd\u5177\u6709100\u4ebf\u53c2\u6570\u7684\u79bb\u6563\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u3002</li>\n    <li>LLaDA2.0\u901a\u8fc7\u7cfb\u7edf\u8f6c\u6362\u81ea\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u524d\u6cbf\u90e8\u7f72\u6a21\u5f0f\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e09\u9636\u6bb5\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u4ece\u9884\u8bad\u7ec3\u7684AR\u6a21\u578b\u8f6c\u6362\u4e3adLLM\uff0c\u5305\u542b\u70ed\u8eab\u3001\u7a33\u5b9a\u548c\u8870\u51cf\u4e09\u4e2a\u9636\u6bb5\u3002</li>\n    <li>\u901a\u8fc7\u540e\u671f\u8bad\u7ec3\u8c03\u6574\uff0cLLaDA2.0\u751f\u6210\u4e86\u4e24\u4e2a\u4f18\u5316\u6a21\u578b\uff1aLLaDA2.0-mini\uff0816\u4ebf\u53c2\u6570\uff09\u548cLLaDA2.0-flash\uff08100\u4ebf\u53c2\u6570\uff09\u3002</li>\n    <li>\u8fd9\u4e24\u4e2a\u5f00\u6e90\u6a21\u578b\u5728\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces LLaDA2.0, a new type of large language model with up to 100 billion parameters.</li>\n    <li>LLaDA2.0 converts existing auto-regressive models instead of training new ones from scratch, saving time and resources.</li>\n    <li>It uses a unique three-phase training method to improve efficiency and performance.</li>\n    <li>Two versions of the model, LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), are designed for practical use and optimized for tasks.</li>\n    <li>Both models are open-source and maintain effective performance while allowing for quick processing. </li>\n</ul>"}, "publishedAt": "2025-12-10T04:26:18.000Z", "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B", "summary": "This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15745.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "67c1d682826160b28f778510", "name": "antgroup", "fullname": "Ant Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16922", "authors": [{"_id": "6944c39ffbf17e708e18605d", "user": {"_id": "63f233820a16587ea967adc2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f233820a16587ea967adc2/1nSoZofPV7UseXzjI2qAH.png", "isPro": false, "fullname": "Sihan XU", "user": "sihanxu", "type": "user"}, "name": "Sihan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:57:39.350Z", "hidden": false}, {"_id": "6944c39ffbf17e708e18605e", "name": "Ziqiao Ma", "hidden": false}, {"_id": "6944c39ffbf17e708e18605f", "name": "Wenhao Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186060", "name": "Xuweiyi Chen", "hidden": false}, {"_id": "6944c39ffbf17e708e186061", "user": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "name": "Weiyang Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:57.800Z", "hidden": false}, {"_id": "6944c39ffbf17e708e186062", "name": "Joyce Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186063", "name": "Saining Xie", "hidden": false}, {"_id": "6944c39ffbf17e708e186064", "name": "Stella X. Yu", "hidden": false}], "publishedAt": "2025-12-18T18:59:58.000Z", "submittedOnDailyAt": "2025-12-19T01:03:18.428Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "submittedOnDailyBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "upvotes": 47, "discussionId": "6944c3a0fbf17e708e186065", "projectPage": "https://sihanxu.me/nepa", "githubRepo": "https://github.com/SihanXU/nepa", "githubRepoAddedBy": "user", "ai_summary": "Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.", "ai_keywords": ["generative pretraining", "predictive tasks", "Next-Embedding Predictive Autoregression (NEPA)", "causal masking", "stop gradient", "Transformer", "ImageNet-1k", "top-1 accuracy", "ViT-B", "ViT-L", "semantic segmentation", "ADE20K"], "githubStars": 43, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u9884\u8bad\u7ec3\u662f\u5426\u80fd\u7528\u4e8e\u81ea\u76d1\u7763\u89c6\u89c9\u5b66\u4e60\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u7684\u5d4c\u5165\uff0c\u800c\u4e0d\u662f\u8f93\u51fa\u7279\u5f81\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u4f7f\u7528\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u4e0b\u4e00\u5d4c\u5165\u9884\u6d4b\u81ea\u56de\u5f52\u201d\uff08NEPA\uff09\u7684\u65b9\u6cd5\uff0c\u6a21\u578b\u5b66\u4e60\u57fa\u4e8e\u8fc7\u53bb\u7684\u5d4c\u5165\u6765\u8fdb\u884c\u9884\u6d4b\u3002</li>\n    <li>\u7b80\u5355\u7684Transformer\u6a21\u578b\u5728ImageNet-1K\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u8fbe\u5230\u4e8683.8%\u548c85.3%\u7684\u51c6\u786e\u7387\u3002</li>\n    <li>NEPA\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u5f88\u597d\u7684\u6548\u679c\uff0c\u5e76\u4e14\u5177\u6709\u7b80\u5355\u53ef\u6269\u5c55\u7684\u7279\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>This research explores whether generative pretraining can improve self-supervised learning in visual tasks, similar to its success in natural language processing.</li>\n  <li>Instead of just learning features, the models are trained to predict future visual embeddings based on past ones, using a method called Next-Embedding Predictive Autoregression (NEPA).</li>\n  <li>A simple Transformer model trained on ImageNet-1k using this embedding prediction approach achieves good results without needing complex techniques like pixel reconstruction or contrastive loss.</li>\n  <li>NEPA shows strong performance, with top-1 accuracy of 83.8% and 85.3% for different model sizes, and it also transfers well to tasks like semantic segmentation.</li>\n  <li>The authors suggest that this approach offers a straightforward and scalable alternative to traditional visual self-supervised learning methods.</li>\n</ul>"}, "publishedAt": "2025-12-18T13:59:58.000Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16922.png", "numComments": 1, "submittedBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "fullname": "Weiyang Jin", "name": "Wayne-King", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16301", "authors": [{"_id": "6944be0ffbf17e708e185fde", "user": {"_id": "63724cfada3183d9d53f2009", "avatarUrl": "/avatars/17838fcf244ecf8d139343bb6c6d8562.svg", "isPro": false, "fullname": "Patrick Jiang", "user": "pat-jj", "type": "user"}, "name": "Pengcheng Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:04.988Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fdf", "user": {"_id": "650488e454b989666d042a49", "avatarUrl": "/avatars/3dc79c6f1a9dce872636dddd38a04670.svg", "isPro": false, "fullname": "Jiacheng Lin", "user": "linjc16", "type": "user"}, "name": "Jiacheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:02.806Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe0", "user": {"_id": "66d4af28033492801d82b890", "avatarUrl": "/avatars/5e8a2dc1b932a679341976d11b22f6c8.svg", "isPro": false, "fullname": "shi", "user": "Gabshi", "type": "user"}, "name": "Zhiyi Shi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T16:25:52.445Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe1", "name": "Zifeng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe2", "name": "Luxi He", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe3", "name": "Yichen Wu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe4", "name": "Ming Zhong", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe5", "user": {"_id": "649c5cf5c1ae48cf4d7dda34", "avatarUrl": "/avatars/a2264945f9f876b690017a93f225f937.svg", "isPro": false, "fullname": "Peiyang Song", "user": "p-song1", "type": "user"}, "name": "Peiyang Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:06.854Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe6", "name": "Qizheng Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe7", "name": "Heng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe8", "user": {"_id": "66a3f1c4c38ce500371fd8d4", "avatarUrl": "/avatars/381de938091f1a5c179eef72aa247bbf.svg", "isPro": false, "fullname": "Xueqiang Xu", "user": "XueqiangXu", "type": "user"}, "name": "Xueqiang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:00.653Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe9", "name": "Hanwen Xu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fea", "name": "Pengrui Han", "hidden": false}, {"_id": "6944be0ffbf17e708e185feb", "name": "Dylan Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fec", "name": "Jiashuo Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fed", "name": "Chaoqi Yang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fee", "name": "Kun Qian", "hidden": false}, {"_id": "6944be0ffbf17e708e185fef", "name": "Tian Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff0", "name": "Changran Hu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff1", "name": "Manling Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff2", "name": "Quanzheng Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff3", "name": "Hao Peng", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff4", "name": "Sheng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff5", "name": "Jingbo Shang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff6", "name": "Chao Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff7", "name": "Jiaxuan You", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff8", "name": "Liyuan Liu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff9", "name": "Pan Lu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffa", "name": "Yu Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffb", "name": "Heng Ji", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffc", "name": "Yejin Choi", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffd", "name": "Dawn Song", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffe", "name": "Jimeng Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fff", "name": "Jiawei Han", "hidden": false}], "publishedAt": "2025-12-18T08:38:51.000Z", "submittedOnDailyAt": "2025-12-19T00:23:16.990Z", "title": "Adaptation of Agentic AI", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "upvotes": 43, "discussionId": "6944be10fbf17e708e186000", "githubRepo": "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI", "githubRepoAddedBy": "user", "ai_summary": "This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.", "ai_keywords": ["agentic AI systems", "foundation models", "agent adaptations", "tool adaptations", "tool-execution-signaled", "agent-output-signaled", "agent-agnostic", "agent-supervised"], "githubStars": 262, "summary_zh": "<ul>\n    <li>\u524d\u6cbf\u7684\u81ea\u4e3b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u8fdb\u884c\u89c4\u5212\u3001\u63a8\u7406\u548c\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u4e92\u52a8\u3002</li>\n    <li>\u968f\u7740\u7cfb\u7edf\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u9002\u5e94\u6027\u6210\u4e3a\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u9760\u6027\u7684\u5173\u952e\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u6db5\u76d6\u4ee3\u7406\u9002\u5e94\u548c\u5de5\u5177\u9002\u5e94\u7684\u591a\u79cd\u5f62\u5f0f\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5e2e\u52a9\u6f84\u6e05\u9002\u5e94\u7b56\u7565\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u63d0\u4f9b\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u9009\u62e9\u6307\u5bfc\u3002</li>\n    <li>\u6587\u4e2d\u56de\u987e\u4e86\u5404\u7c7b\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7684\u6311\u6218\u548c\u673a\u9047\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic AI systems use foundation models that can adapt to perform complex tasks.</li>\n    <li>The paper presents a framework to organize research on how these systems adapt and use tools.</li>\n    <li>It breaks down adaptations into different types, helping clarify their trade-offs and uses.</li>\n    <li>The authors review various adaptation approaches, discussing their strengths and weaknesses.</li>\n    <li>The goal is to provide guidance for building better and more efficient AI systems.</li>\n</ul>"}, "publishedAt": "2025-12-18T03:38:51.000Z", "title": "Adaptation of Agentic AI", "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16301.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16915", "authors": [{"_id": "6944c28efbf17e708e18603e", "name": "Guibao Shen", "hidden": false}, {"_id": "6944c28efbf17e708e18603f", "name": "Yihua Du", "hidden": false}, {"_id": "6944c28efbf17e708e186040", "name": "Wenhang Ge", "hidden": false}, {"_id": "6944c28efbf17e708e186041", "name": "Jing He", "hidden": false}, {"_id": "6944c28efbf17e708e186042", "name": "Chirui Chang", "hidden": false}, {"_id": "6944c28efbf17e708e186043", "name": "Donghao Zhou", "hidden": false}, {"_id": "6944c28efbf17e708e186044", "name": "Zhen Yang", "hidden": false}, {"_id": "6944c28efbf17e708e186045", "name": "Luozhou Wang", "hidden": false}, {"_id": "6944c28efbf17e708e186046", "name": "Xin Tao", "hidden": false}, {"_id": "6944c28efbf17e708e186047", "name": "Ying-Cong Chen", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65d5aa45dca2a85f0fe895f3/0qAjEuYcHrF_fPEhemUU6.mp4"], "publishedAt": "2025-12-18T18:59:50.000Z", "submittedOnDailyAt": "2025-12-19T00:51:00.837Z", "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors", "submittedOnDailyBy": {"_id": "65d5aa45dca2a85f0fe895f3", "avatarUrl": "/avatars/a3cbcade6ea101e99f58641aa409fdfe.svg", "isPro": false, "fullname": "Guibao SHEN", "user": "PaulSHEN1", "type": "user"}, "summary": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.", "upvotes": 32, "discussionId": "6944c28efbf17e708e186048", "ai_summary": "StereoPilot, a feed-forward model leveraging a learnable domain switcher and cycle consistency loss, synthesizes high-quality stereo video directly without depth maps, outperforming existing methods in visual fidelity and computational efficiency.", "ai_keywords": ["StereoPilot", "learnable domain switcher", "cycle consistency loss", "feed-forward model", "stereo video conversion", "UniStereo", "stereo formats", "visual fidelity", "computational efficiency"], "organization": {"_id": "65ad19cac14c3cf579ad9b68", "name": "HKUSTGZ", "fullname": "HKUSTGZ"}, "summary_zh": "<ul>\n    <li>\u7acb\u4f53\u663e\u793a\u6280\u672f\uff08\u5982VR\u5934\u76d4\u548c3D\u5f71\u9662\uff09\u8fc5\u901f\u53d1\u5c55\uff0c\u5bfc\u81f4\u5bf9\u9ad8\u8d28\u91cf\u7acb\u4f53\u89c6\u9891\u5185\u5bb9\u7684\u9700\u6c42\u589e\u52a0\u3002</li>\n    <li>\u751f\u4ea73D\u89c6\u9891\u4ecd\u7136\u6602\u8d35\u4e14\u590d\u6742\uff0c\u81ea\u52a8\u5355\u76ee\u8f6c\u7acb\u4f53\u7684\u8fc7\u7a0b\u53d7\u9650\u4e8e\u591a\u9636\u6bb5\u7684\u201c\u6df1\u5ea6\u626d\u66f2\u586b\u5145\u201d\u65b9\u6cd5\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86UniStereo\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u7acb\u4f53\u89c6\u9891\u8f6c\u6362\u7684\u5927\u89c4\u6a21\u7edf\u4e00\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e24\u79cd\u7acb\u4f53\u683c\u5f0f\u3002</li>\n    <li>\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86StereoPilot\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u524d\u9988\u6a21\u578b\uff0c\u53ef\u4ee5\u76f4\u63a5\u5408\u6210\u76ee\u6807\u89c6\u56fe\uff0c\u65e0\u9700\u663e\u5f0f\u6df1\u5ea6\u56fe\u6216\u8fed\u4ee3\u91c7\u6837\u3002</li>\n    <li>StereoPilot\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The demand for high-quality 3D video content is increasing due to the popularity of VR headsets and 3D cinemas.</li>\n    <li>Creating 3D videos is expensive and complicated, and current automatic conversion methods have several limitations.</li>\n    <li>UniStereo is a new large dataset designed for stereo video conversion, supporting different stereo formats for better testing and training.</li>\n    <li>StereoPilot is a new model that efficiently creates stereo views without needing depth maps or complex processes.</li>\n    <li>Tests show that StereoPilot works better than existing methods in terms of image quality and speed.</li>\n</ul>"}, "publishedAt": "2025-12-18T13:59:50.000Z", "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors", "summary": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65d5aa45dca2a85f0fe895f3/0qAjEuYcHrF_fPEhemUU6.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16915.png", "numComments": 1, "submittedBy": {"_id": "65d5aa45dca2a85f0fe895f3", "avatarUrl": "/avatars/a3cbcade6ea101e99f58641aa409fdfe.svg", "fullname": "Guibao SHEN", "name": "PaulSHEN1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "65ad19cac14c3cf579ad9b68", "name": "HKUSTGZ", "fullname": "HKUSTGZ"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13507", "authors": [{"_id": "6942c6c1fb33037a39577c81", "name": "Heyi Chen", "hidden": false}, {"_id": "6942c6c1fb33037a39577c82", "name": "Siyan Chen", "hidden": false}, {"_id": "6942c6c1fb33037a39577c83", "name": "Xin Chen", "hidden": false}, {"_id": "6942c6c1fb33037a39577c84", "name": "Yanfei Chen", "hidden": false}, {"_id": "6942c6c1fb33037a39577c85", "name": "Ying Chen", "hidden": false}, {"_id": "6942c6c1fb33037a39577c86", "name": "Zhuo Chen", "hidden": false}, {"_id": "6942c6c1fb33037a39577c87", "name": "Feng Cheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577c88", "name": "Tianheng Cheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577c89", "name": "Xinqi Cheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577c8a", "name": "Xuyan Chi", "hidden": false}, {"_id": "6942c6c1fb33037a39577c8b", "name": "Jian Cong", "hidden": false}, {"_id": "6942c6c1fb33037a39577c8c", "name": "Jing Cui", "hidden": false}, {"_id": "6942c6c1fb33037a39577c8d", "name": "Qinpeng Cui", "hidden": false}, {"_id": "6942c6c1fb33037a39577c8e", "name": "Qide Dong", "hidden": false}, {"_id": "6942c6c1fb33037a39577c8f", "name": "Junliang Fan", "hidden": false}, {"_id": "6942c6c1fb33037a39577c90", "name": "Jing Fang", "hidden": false}, {"_id": "6942c6c1fb33037a39577c91", "name": "Zetao Fang", "hidden": false}, {"_id": "6942c6c1fb33037a39577c92", "name": "Chengjian Feng", "hidden": false}, {"_id": "6942c6c1fb33037a39577c93", "name": "Han Feng", "hidden": false}, {"_id": "6942c6c1fb33037a39577c94", "name": "Mingyuan Gao", "hidden": false}, {"_id": "6942c6c1fb33037a39577c95", "name": "Yu Gao", "hidden": false}, {"_id": "6942c6c1fb33037a39577c96", "name": "Dong Guo", "hidden": false}, {"_id": "6942c6c1fb33037a39577c97", "name": "Qiushan Guo", "hidden": false}, {"_id": "6942c6c1fb33037a39577c98", "name": "Boyang Hao", "hidden": false}, {"_id": "6942c6c1fb33037a39577c99", "name": "Qingkai Hao", "hidden": false}, {"_id": "6942c6c1fb33037a39577c9a", "name": "Bibo He", "hidden": false}, {"_id": "6942c6c1fb33037a39577c9b", "name": "Qian He", "hidden": false}, {"_id": "6942c6c1fb33037a39577c9c", "name": "Tuyen Hoang", "hidden": false}, {"_id": "6942c6c1fb33037a39577c9d", "name": "Ruoqing Hu", "hidden": false}, {"_id": "6942c6c1fb33037a39577c9e", "name": "Xi Hu", "hidden": false}, {"_id": "6942c6c1fb33037a39577c9f", "name": "Weilin Huang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca0", "name": "Zhaoyang Huang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca1", "name": "Zhongyi Huang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca2", "name": "Donglei Ji", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca3", "name": "Siqi Jiang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca4", "name": "Wei Jiang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca5", "name": "Yunpu Jiang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca6", "name": "Zhuo Jiang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca7", "name": "Ashley Kim", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca8", "name": "Jianan Kong", "hidden": false}, {"_id": "6942c6c1fb33037a39577ca9", "name": "Zhichao Lai", "hidden": false}, {"_id": "6942c6c1fb33037a39577caa", "name": "Shanshan Lao", "hidden": false}, {"_id": "6942c6c1fb33037a39577cab", "name": "Yichong Leng", "hidden": false}, {"_id": "6942c6c1fb33037a39577cac", "name": "Ai Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cad", "name": "Feiya Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cae", "name": "Gen Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577caf", "name": "Huixia Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb0", "name": "JiaShi Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb1", "name": "Liang Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb2", "name": "Ming Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb3", "name": "Shanshan Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb4", "name": "Tao Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb5", "name": "Xian Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb6", "name": "Xiaojie Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb7", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb8", "name": "Xingxing Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cb9", "name": "Yameng Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cba", "name": "Yifu Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cbb", "name": "Yiying Li", "hidden": false}, {"_id": "6942c6c1fb33037a39577cbc", "name": "Chao Liang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cbd", "name": "Han Liang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cbe", "name": "Jianzhong Liang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cbf", "name": "Ying Liang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc0", "name": "Zhiqiang Liang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc1", "name": "Wang Liao", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc2", "name": "Yalin Liao", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc3", "name": "Heng Lin", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc4", "name": "Kengyu Lin", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc5", "name": "Shanchuan Lin", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc6", "name": "Xi Lin", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc7", "name": "Zhijie Lin", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc8", "name": "Feng Ling", "hidden": false}, {"_id": "6942c6c1fb33037a39577cc9", "name": "Fangfang Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cca", "name": "Gaohong Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577ccb", "name": "Jiawei Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577ccc", "name": "Jie Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577ccd", "name": "Jihao Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cce", "name": "Shouda Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577ccf", "name": "Shu Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd0", "name": "Sichao Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd1", "name": "Songwei Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd2", "name": "Xin Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd3", "name": "Xue Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd4", "name": "Yibo Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd5", "name": "Zikun Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd6", "name": "Zuxi Liu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd7", "name": "Junlin Lyu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd8", "name": "Lecheng Lyu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cd9", "name": "Qian Lyu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cda", "name": "Han Mu", "hidden": false}, {"_id": "6942c6c1fb33037a39577cdb", "name": "Xiaonan Nie", "hidden": false}, {"_id": "6942c6c1fb33037a39577cdc", "name": "Jingzhe Ning", "hidden": false}, {"_id": "6942c6c1fb33037a39577cdd", "name": "Xitong Pan", "hidden": false}, {"_id": "6942c6c1fb33037a39577cde", "name": "Yanghua Peng", "hidden": false}, {"_id": "6942c6c1fb33037a39577cdf", "name": "Lianke Qin", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce0", "name": "Xueqiong Qu", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce1", "name": "Yuxi Ren", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce2", "name": "Kai Shen", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce3", "name": "Guang Shi", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce4", "name": "Lei Shi", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce5", "name": "Yan Song", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce6", "name": "Yinglong Song", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce7", "name": "Fan Sun", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce8", "name": "Li Sun", "hidden": false}, {"_id": "6942c6c1fb33037a39577ce9", "name": "Renfei Sun", "hidden": false}, {"_id": "6942c6c1fb33037a39577cea", "name": "Yan Sun", "hidden": false}, {"_id": "6942c6c1fb33037a39577ceb", "name": "Zeyu Sun", "hidden": false}, {"_id": "6942c6c1fb33037a39577cec", "name": "Wenjing Tang", "hidden": false}, {"_id": "6942c6c1fb33037a39577ced", "name": "Yaxue Tang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cee", "name": "Zirui Tao", "hidden": false}, {"_id": "6942c6c1fb33037a39577cef", "name": "Feng Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf0", "name": "Furui Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf1", "name": "Jinran Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf2", "name": "Junkai Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf3", "name": "Ke Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf4", "name": "Kexin Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf5", "name": "Qingyi Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf6", "name": "Rui Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf7", "name": "Sen Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf8", "name": "Shuai Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cf9", "name": "Tingru Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cfa", "name": "Weichen Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cfb", "name": "Xin Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cfc", "name": "Yanhui Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cfd", "name": "Yue Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cfe", "name": "Yuping Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577cff", "name": "Yuxuan Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d00", "name": "Ziyu Wang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d01", "name": "Guoqiang Wei", "hidden": false}, {"_id": "6942c6c1fb33037a39577d02", "name": "Wanru Wei", "hidden": false}, {"_id": "6942c6c1fb33037a39577d03", "name": "Di Wu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d04", "name": "Guohong Wu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d05", "name": "Hanjie Wu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d06", "name": "Jian Wu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d07", "user": {"_id": "6381c5d63680a7cf34e08ca9", "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg", "isPro": false, "fullname": "wujie10558@gmail.com", "user": "wujie10", "type": "user"}, "name": "Jie Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:07:28.238Z", "hidden": false}, {"_id": "6942c6c1fb33037a39577d08", "name": "Ruolan Wu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d09", "name": "Xinglong Wu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d0a", "name": "Yonghui Wu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d0b", "name": "Ruiqi Xia", "hidden": false}, {"_id": "6942c6c1fb33037a39577d0c", "name": "Liang Xiang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d0d", "name": "Fei Xiao", "hidden": false}, {"_id": "6942c6c1fb33037a39577d0e", "name": "XueFeng Xiao", "hidden": false}, {"_id": "6942c6c1fb33037a39577d0f", "name": "Pan Xie", "hidden": false}, {"_id": "6942c6c1fb33037a39577d10", "name": "Shuangyi Xie", "hidden": false}, {"_id": "6942c6c1fb33037a39577d11", "name": "Shuang Xu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d12", "name": "Jinlan Xue", "hidden": false}, {"_id": "6942c6c1fb33037a39577d13", "name": "Shen Yan", "hidden": false}, {"_id": "6942c6c1fb33037a39577d14", "name": "Bangbang Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d15", "name": "Ceyuan Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d16", "name": "Jiaqi Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d17", "name": "Runkai Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d18", "name": "Tao Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d19", "name": "Yang Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d1a", "name": "Yihang Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d1b", "name": "ZhiXian Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d1c", "name": "Ziyan Yang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d1d", "name": "Songting Yao", "hidden": false}, {"_id": "6942c6c1fb33037a39577d1e", "name": "Yifan Yao", "hidden": false}, {"_id": "6942c6c1fb33037a39577d1f", "name": "Zilyu Ye", "hidden": false}, {"_id": "6942c6c1fb33037a39577d20", "name": "Bowen Yu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d21", "name": "Jian Yu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d22", "name": "Chujie Yuan", "hidden": false}, {"_id": "6942c6c1fb33037a39577d23", "name": "Linxiao Yuan", "hidden": false}, {"_id": "6942c6c1fb33037a39577d24", "name": "Sichun Zeng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d25", "name": "Weihong Zeng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d26", "name": "Xuejiao Zeng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d27", "name": "Yan Zeng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d28", "name": "Chuntao Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d29", "name": "Heng Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d2a", "name": "Jingjie Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d2b", "name": "Kuo Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d2c", "name": "Liang Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d2d", "name": "Liying Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d2e", "name": "Manlin Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d2f", "name": "Ting Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d30", "name": "Weida Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d31", "name": "Xiaohe Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d32", "name": "Xinyan Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d33", "name": "Yan Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d34", "name": "Yuan Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d35", "name": "Zixiang Zhang", "hidden": false}, {"_id": "6942c6c1fb33037a39577d36", "name": "Fengxuan Zhao", "hidden": false}, {"_id": "6942c6c1fb33037a39577d37", "name": "Huating Zhao", "hidden": false}, {"_id": "6942c6c1fb33037a39577d38", "name": "Yang Zhao", "hidden": false}, {"_id": "6942c6c1fb33037a39577d39", "name": "Hao Zheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d3a", "name": "Jianbin Zheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d3b", "name": "Xiaozheng Zheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d3c", "name": "Yangyang Zheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d3d", "name": "Yijie Zheng", "hidden": false}, {"_id": "6942c6c1fb33037a39577d3e", "name": "Jiexin Zhou", "hidden": false}, {"_id": "6942c6c1fb33037a39577d3f", "name": "Jiahui Zhu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d40", "name": "Kuan Zhu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d41", "name": "Shenhan Zhu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d42", "name": "Wenjia Zhu", "hidden": false}, {"_id": "6942c6c1fb33037a39577d43", "name": "Benhui Zou", "hidden": false}, {"_id": "6942c6c1fb33037a39577d44", "name": "Feilong Zuo", "hidden": false}], "publishedAt": "2025-12-15T16:36:52.000Z", "submittedOnDailyAt": "2025-12-19T01:03:13.713Z", "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "submittedOnDailyBy": {"_id": "6381c5d63680a7cf34e08ca9", "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg", "isPro": false, "fullname": "wujie10558@gmail.com", "user": "wujie10", "type": "user"}, "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "upvotes": 30, "discussionId": "6942c6c1fb33037a39577d45", "projectPage": "https://seed.bytedance.com/seedance1_5_pro", "ai_summary": "Seedance 1.5 pro, a dual-branch Diffusion Transformer model, achieves high-quality audio-visual synchronization and generation through cross-modal integration, post-training optimizations, and an acceleration framework.", "ai_keywords": ["Diffusion Transformer", "cross-modal joint module", "Supervised Fine-Tuning", "Reinforcement Learning from Human Feedback", "multi-dimensional reward models", "multilingual and dialect lip-syncing", "dynamic cinematic camera control", "narrative coherence"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Seedance 1.5 pro \u662f\u4e00\u4e2a\u4e13\u4e3a\u97f3\u89c6\u9891\u751f\u6210\u8bbe\u8ba1\u7684\u57fa\u7840\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u53cc\u5206\u652f\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5b9e\u73b0\u97f3\u89c6\u9891\u540c\u6b65\u548c\u9ad8\u8d28\u91cf\u751f\u6210\u3002</li>\n    <li>\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0c\u8fdb\u884c\u4e86\u540e\u671f\u4f18\u5316\u3002</li>\n    <li>\u5f15\u5165\u52a0\u901f\u6846\u67b6\uff0c\u4f7f\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u8d85\u8fc710\u500d\u3002</li>\n    <li>\u652f\u6301\u591a\u8bed\u8a00\u548c\u65b9\u8a00\u7684\u53e3\u578b\u540c\u6b65\uff0c\u9002\u5408\u4e13\u4e1a\u5185\u5bb9\u521b\u4f5c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Seedance 1.5 pro is a new model designed for creating audio and video together.</li>\n    <li>It uses advanced technology to achieve high-quality audio and video synchronization.</li>\n    <li>Post-training improvements include fine-tuning with quality data and feedback from humans.</li>\n    <li>The model can generate content faster, with a speed increase of more than 10 times.</li>\n    <li>It features accurate lip-syncing in different languages, camera control, and strong storytelling.</li>\n</ul>"}, "publishedAt": "2025-12-15T11:36:52.000Z", "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13507.png", "numComments": 1, "submittedBy": {"_id": "6381c5d63680a7cf34e08ca9", "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg", "fullname": "wujie10558@gmail.com", "name": "wujie10", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16913", "authors": [{"_id": "6944c315fbf17e708e18604a", "name": "Xin Lin", "hidden": false}, {"_id": "6944c315fbf17e708e18604b", "name": "Meixi Song", "hidden": false}, {"_id": "6944c315fbf17e708e18604c", "name": "Dizhe Zhang", "hidden": false}, {"_id": "6944c315fbf17e708e18604d", "name": "Wenxuan Lu", "hidden": false}, {"_id": "6944c315fbf17e708e18604e", "user": {"_id": "641d211e353524fe41f16387", "avatarUrl": "/avatars/c6e72c82c029b415a035beebee50b52c.svg", "isPro": true, "fullname": "Haodong Li", "user": "haodongli", "type": "user"}, "name": "Haodong Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:57:41.262Z", "hidden": false}, {"_id": "6944c315fbf17e708e18604f", "name": "Bo Du", "hidden": false}, {"_id": "6944c315fbf17e708e186050", "name": "Ming-Hsuan Yang", "hidden": false}, {"_id": "6944c315fbf17e708e186051", "name": "Truong Nguyen", "hidden": false}, {"_id": "6944c315fbf17e708e186052", "name": "Lu Qi", "hidden": false}], "publishedAt": "2025-12-18T18:59:29.000Z", "submittedOnDailyAt": "2025-12-19T00:45:20.122Z", "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation", "submittedOnDailyBy": {"_id": "68e786cbfa2b7fd74a46eb23", "avatarUrl": "/avatars/8292af16a47ae15b389f17adb67f3e3a.svg", "isPro": true, "fullname": "Insta360-Research", "user": "Insta360-Research", "type": "user"}, "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP_website/ {https://insta360-research-team.github.io/DAP\\_website/}", "upvotes": 27, "discussionId": "6944c316fbf17e708e186053", "ai_summary": "A panoramic metric depth foundation model using DINOv3-Large and a three-stage pseudo-label pipeline achieves robust performance across diverse real-world scenes.", "ai_keywords": ["DINOv3-Large", "pseudo-label curation pipeline", "range mask head", "sharpness-centric optimization", "geometry-centric optimization", "Stanford2D3D", "Matterport3D", "Deep360"], "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u666f\u5ea6\u91cf\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u573a\u666f\u8ddd\u79bb\u4e0b\u8fdb\u884c\u6cdb\u5316\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u5408\u516c\u5171\u6570\u636e\u96c6\u3001\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u5168\u666f\u56fe\u50cf\uff0c\u6536\u96c6\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002</li>\n    <li>\u5f15\u5165\u4e09\u9636\u6bb5\u4f2a\u6807\u7b7e\u7b56\u5212\u6d41\u7a0b\uff0c\u4ee5\u51cf\u5c11\u5ba4\u5185/\u5ba4\u5916\u548c\u5408\u6210/\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002</li>\n    <li>\u91c7\u7528DINOv3-Large\u4f5c\u4e3a\u6a21\u578b\u4e3b\u5e72\uff0c\u5e76\u5f15\u5165\u6539\u8fdb\u63aa\u65bd\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u591a\u6837\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u96f6-shot\u6cdb\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study presents a new model for measuring depth in various scenes from different distances.</li>\n    <li>A large dataset was created by combining public datasets, synthetic data, and real images from the internet.</li>\n    <li>A special pipeline was developed to ensure accurate labels for images that didn't have them.</li>\n    <li>The model uses advanced techniques to improve its accuracy and consistency when predicting depth.</li>\n    <li>Tests showed the model performs well across different benchmarks and can generalize to new scenes effectively.</li>\n</ul>"}, "publishedAt": "2025-12-18T13:59:29.000Z", "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation", "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: https://insta360-research-team.github.io/DAP_website/ {https://insta360-research-team.github.io/DAP\\_website/}", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16913.png", "numComments": 1, "submittedBy": {"_id": "68e786cbfa2b7fd74a46eb23", "avatarUrl": "/avatars/8292af16a47ae15b389f17adb67f3e3a.svg", "fullname": "Insta360-Research", "name": "Insta360-Research", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16923", "authors": [{"_id": "6944d824fbf17e708e1860f3", "user": {"_id": "6391eaf6a22277aa7d6ece6f", "avatarUrl": "/avatars/7a709e7039dac07c7ca24d5e23f7785e.svg", "isPro": false, "fullname": "Chun-Wei Tuan Mu", "user": "rayray9999", "type": "user"}, "name": "Chun-Wei Tuan Mu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:57:31.371Z", "hidden": false}, {"_id": "6944d824fbf17e708e1860f4", "name": "Jia-Bin Huang", "hidden": false}, {"_id": "6944d824fbf17e708e1860f5", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/9Vljy4RtRgMfLQ2TrP_x5.mp4"], "publishedAt": "2025-12-18T18:59:59.000Z", "submittedOnDailyAt": "2025-12-19T02:15:37.076Z", "title": "Generative Refocusing: Flexible Defocus Control from a Single Image", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.", "upvotes": 25, "discussionId": "6944d825fbf17e708e1860f6", "projectPage": "https://generative-refocusing.github.io/", "githubRepo": "https://github.com/rayray9999/Genfocus", "githubRepoAddedBy": "user", "ai_summary": "Generative Refocusing uses semi-supervised learning with DeblurNet and BokehNet to achieve high-quality single-image refocusing with controllable bokeh and text-guided adjustments.", "ai_keywords": ["DeblurNet", "BokehNet", "semi-supervised training", "defocus deblurring", "bokeh synthesis", "EXIF metadata", "text-guided adjustments"], "githubStars": 27, "summary_zh": "<ul>\n    <li>\u666f\u6df1\u63a7\u5236\u5728\u6444\u5f71\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u83b7\u5f97\u5b8c\u7f8e\u7684\u7126\u70b9\u5f80\u5f80\u9700\u8981\u591a\u6b21\u5c1d\u8bd5\u6216\u7279\u6b8a\u8bbe\u5907\u3002</li>\n    <li>\u5355\u56fe\u50cf\u91cd\u805a\u7126\u4ecd\u7136\u5f88\u96be\uff0c\u9700\u8981\u6062\u590d\u6e05\u6670\u5185\u5bb9\u548c\u521b\u5efa\u771f\u5b9e\u7684\u865a\u5316\u6548\u679c\u3002</li>\n    <li>\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u7f3a\u70b9\uff0c\u5982\u9700\u8981\u5168\u7126\u70b9\u8f93\u5165\u3001\u4f9d\u8d56\u6a21\u62df\u5668\u7684\u5408\u6210\u6570\u636e\uff0c\u4ee5\u53ca\u5bf9\u5149\u5708\u7684\u63a7\u5236\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u751f\u6210\u91cd\u805a\u7126\u7684\u65b9\u6cd5\uff0c\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\u4f7f\u7528DeblurNet\u6062\u590d\u5168\u7126\u70b9\u56fe\u50cf\uff0c\u4f7f\u7528BokehNet\u521b\u5efa\u53ef\u63a7\u7684\u865a\u5316\u6548\u679c\u3002</li>\n    <li>\u6211\u4eec\u7684\u521b\u65b0\u5728\u4e8e\u534a\u76d1\u7763\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e86\u5408\u6210\u914d\u5bf9\u6570\u636e\u548c\u672a\u914d\u5bf9\u7684\u771f\u5b9e\u865a\u5316\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u5141\u8bb8\u6587\u672c\u5f15\u5bfc\u8c03\u6574\u548c\u81ea\u5b9a\u4e49\u5149\u5708\u5f62\u72b6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Depth-of-field control is important in photography, but achieving perfect focus can be challenging.</li>\n    <li>Current single-image refocusing methods have major limitations, such as needing all-in-focus inputs and relying on synthetic data.</li>\n    <li>Generative Refocusing is a new two-step process that improves image focus and creates realistic bokeh.</li>\n    <li>This method uses semi-supervised training, combining synthetic and real data to better capture optical characteristics.</li>\n    <li>Our approach outperforms existing methods and allows for text-guided adjustments and custom aperture shapes.</li>\n</ul>"}, "publishedAt": "2025-12-18T13:59:59.000Z", "title": "Generative Refocusing: Flexible Defocus Control from a Single Image", "summary": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/9Vljy4RtRgMfLQ2TrP_x5.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16923.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16625", "authors": [{"_id": "6944bdf2fbf17e708e185fd9", "name": "Linghui Shen", "hidden": false}, {"_id": "6944bdf2fbf17e708e185fda", "name": "Mingyue Cui", "hidden": false}, {"_id": "6944bdf2fbf17e708e185fdb", "user": {"_id": "634cfebc350bcee9bed20a4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png", "isPro": false, "fullname": "Xingyi Yang", "user": "adamdad", "type": "user"}, "name": "Xingyi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:09.375Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/e0DL12ili-5bxUdYoakWm.png"], "publishedAt": "2025-12-18T15:01:44.000Z", "submittedOnDailyAt": "2025-12-19T00:30:00.085Z", "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers", "submittedOnDailyBy": {"_id": "634cfebc350bcee9bed20a4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png", "isPro": false, "fullname": "Xingyi Yang", "user": "adamdad", "type": "user"}, "summary": "In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.", "upvotes": 21, "discussionId": "6944bdf3fbf17e708e185fdc", "projectPage": "https://linghuiishen.github.io/decontext_project_page/", "githubRepo": "https://github.com/LinghuiiShen/DeContext", "githubRepoAddedBy": "user", "ai_summary": "DeContext defends against unauthorized in-context image editing by weakening cross-attention pathways in multimodal attention layers, preserving visual quality while blocking unwanted modifications.", "ai_keywords": ["in-context diffusion models", "multimodal attention layers", "cross-attention pathways", "DeContext", "denoising steps", "transformer blocks", "Flux Kontext", "Step1X-Edit"], "githubStars": 7, "organization": {"_id": "646ecc368d316fde87b3b6e3", "name": "PolyUHK", "fullname": "The Hong Kong Polytechnic University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"}, "summary_zh": "<ul>\n    <li>\u4e0a\u4e0b\u6587\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u8ba9\u7528\u6237\u8f7b\u677e\u771f\u5b9e\u5730\u4fee\u6539\u56fe\u50cf\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u9690\u79c1\u95ee\u9898\u3002</li>\n    <li>\u4e2a\u4eba\u56fe\u50cf\u53ef\u80fd\u4f1a\u88ab\u6076\u610f\u4f7f\u7528\uff0c\u4f8b\u5982\u8eab\u4efd\u5192\u5145\u6216\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5DeContext\uff0c\u65e8\u5728\u4fdd\u62a4\u8f93\u5165\u56fe\u50cf\u4e0d\u88ab\u672a\u7ecf\u6388\u6743\u7684\u7f16\u8f91\u3002</li>\n    <li>DeContext\u901a\u8fc7\u6ce8\u5165\u5c0f\u7684\u3001\u6709\u9488\u5bf9\u6027\u7684\u6270\u52a8\u6765\u524a\u5f31\u8de8\u6ce8\u610f\u529b\u5c42\uff0c\u4ece\u800c\u65ad\u5f00\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8054\u7cfb\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cDeContext\u80fd\u6709\u6548\u963b\u6b62\u4e0d\u5fc5\u8981\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>In-context diffusion models can easily and realistically change images, but this raises privacy issues like identity theft and misinformation.</li>\n    <li>Previous research focused on protecting personalized text-to-image generation, but the safety of large-scale image models has not been thoroughly studied.</li>\n    <li>The new method, DeContext, aims to protect images from unauthorized edits by weakening key connections in the model that link input and output.</li>\n    <li>DeContext uses small adjustments to disrupt how information flows through the model, making it a simple but effective defense.</li>\n    <li>Tests show that DeContext successfully prevents unwanted edits while keeping the image quality intact.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:01:44.000Z", "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers", "summary": "In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/e0DL12ili-5bxUdYoakWm.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16625.png", "numComments": 1, "submittedBy": {"_id": "634cfebc350bcee9bed20a4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png", "fullname": "Xingyi Yang", "name": "adamdad", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20}, "organization": {"_id": "646ecc368d316fde87b3b6e3", "name": "PolyUHK", "fullname": "The Hong Kong Polytechnic University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/646ecbc0cbb7bb996513e298/Akb4zKqIP9kb9PQoUPUmj.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16636", "authors": [{"_id": "6944eb49fbf17e708e186136", "name": "Giorgos Petsangourakis", "hidden": false}, {"_id": "6944eb49fbf17e708e186137", "name": "Christos Sgouropoulos", "hidden": false}, {"_id": "6944eb49fbf17e708e186138", "name": "Bill Psomas", "hidden": false}, {"_id": "6944eb49fbf17e708e186139", "name": "Theodoros Giannakopoulos", "hidden": false}, {"_id": "6944eb49fbf17e708e18613a", "name": "Giorgos Sfikas", "hidden": false}, {"_id": "6944eb49fbf17e708e18613b", "name": "Ioannis Kakogeorgiou", "hidden": false}], "publishedAt": "2025-12-18T15:10:42.000Z", "submittedOnDailyAt": "2025-12-19T03:38:51.121Z", "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion", "submittedOnDailyBy": {"_id": "661ba524bd9243bf7e598355", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661ba524bd9243bf7e598355/i77yD4XgJn2vUbn_mIsT8.jpeg", "isPro": false, "fullname": "Ioannis Kakogeorgiou", "user": "gkakogeorgiou", "type": "user"}, "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .", "upvotes": 19, "discussionId": "6944eb4afbf17e708e18613c", "githubRepo": "https://github.com/giorgospets/reglue", "githubRepoAddedBy": "user", "ai_summary": "REGLUE, a unified latent diffusion framework, enhances image synthesis by jointly modeling VAE latents, patch-level VFM semantics, and global tokens, improving semantic supervision and convergence.", "ai_keywords": ["Latent diffusion models", "VAE", "Vision Foundation Models", "representation alignment", "convolutional semantic compressor", "nonlinear aggregation", "multi-layer VFM features", "FID", "SiT backbone", "external alignment loss"], "githubStars": 0, "summary_zh": "<ul>\n    <li>\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u5728\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8f83\u6162\uff0c\u6837\u672c\u8d28\u91cf\u53d7\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86REGLUE\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86VAE\u56fe\u50cf\u6f5c\u53d8\u91cf\u3001\u5c40\u90e8VFM\u8bed\u4e49\u548c\u5168\u5c40[CLS]\u6807\u8bb0\u3002</li>\n    <li>\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u8bed\u4e49\u538b\u7f29\u5668\uff0c\u5c06\u591a\u5c42VFM\u7279\u5f81\u975e\u7ebf\u6027\u805a\u5408\u4e3a\u4f4e\u7ef4\u7ed3\u6784\u5316\u8868\u793a\u3002</li>\n    <li>\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0cREGLUE\u5728FID\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u52a0\u5feb\u4e86\u6536\u655b\u901f\u5ea6\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u7a7a\u95f4VFM\u8bed\u4e49\u7684\u5145\u5206\u5229\u7528\u548c\u975e\u7ebf\u6027\u538b\u7f29\u662f\u5173\u952e\uff0c\u540c\u65f6\u5168\u5c40\u6807\u8bb0\u548c\u5916\u90e8\u5bf9\u9f50\u53ef\u4f5c\u4e3a\u6709\u6548\u8865\u5145\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Latent diffusion models (LDMs) are great for creating images, but their training takes time and can limit image quality.</li>\n    <li>REGLUE is a new method that combines different types of image features to improve image generation.</li>\n    <li>It uses a special technique to merge detailed local features and overall image information, making the process more efficient.</li>\n    <li>Tests show that REGLUE performs better and trains faster than existing models on ImageNet.</li>\n    <li>Key findings include the importance of using spatial features, the benefits of nonlinear compression, and the effectiveness of combining global and local information.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:10:42.000Z", "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion", "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16636.png", "numComments": 1, "submittedBy": {"_id": "661ba524bd9243bf7e598355", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661ba524bd9243bf7e598355/i77yD4XgJn2vUbn_mIsT8.jpeg", "fullname": "Ioannis Kakogeorgiou", "name": "gkakogeorgiou", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5206\u5f00\u5904\u7406\u7684\u95ee\u9898\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u80cc\u666f\uff0c\u5e76\u5c06\u5b83\u4eec\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u589e\u5f3a\u5176\u529f\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one complete system, unlike traditional separate methods.</li>\n    <li>The system can handle various inputs like text, images, and videos to produce unified content.</li>\n    <li>Kling-Omni uses a strong data foundation and advanced training methods for better performance.</li>\n    <li>It shows great skill in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u8003\u8651\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u9650\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5982FVD\u53ea\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u63a8\u7406\u5931\u8d25\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86MMGR\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u3002</li>\n    <li>MMGR\u5bf9\u751f\u6210\u63a8\u7406\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u7b49\u4e09\u4e2a\u9886\u57df\u3002</li>\n    <li>\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e00\u822c\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u5c24\u5176\u5728\u957f\u671f\u7a7a\u95f4\u89c4\u5212\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic videos, but they may not always follow physical and logical rules.</li>\n    <li>Current evaluation methods focus on visual quality and miss important reasoning problems like causality and consistency.</li>\n    <li>MMGR is a new evaluation framework that tests five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>It assesses models in areas like Abstract Reasoning, real-world navigation, and Physical Commonsense.</li>\n    <li>Benchmarking shows that while some models perform well in Physical Commonsense, they struggle significantly in Abstract Reasoning and long-term spatial planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u53d8\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5229\u7528\u6821\u51c6\u6b65\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002</li>\n    <li>\u5f00\u53d1\u4e86Step-GUI\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u8bbe\u5907\u548c\u7528\u6237\u9690\u79c1\u3002</li>\n    <li>\u63d0\u51fa\u4e86GUI-MCP\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u7ed3\u5408\u4f4e\u7ea7\u64cd\u4f5c\u548c\u9ad8\u7ea7\u4efb\u52a1\u59d4\u6d3e\uff0c\u786e\u4fdd\u654f\u611f\u6570\u636e\u5b89\u5168\u3002</li>\n    <li>\u5f15\u5165AndroidDaily\u57fa\u51c6\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u771f\u5b9e\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b3146\u4e2a\u9759\u6001\u52a8\u4f5c\u548c235\u4e2a\u7aef\u5230\u7aef\u4efb\u52a1\u3002</li>\n    <li>\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4e92\u52a8\u4e2d\u7684\u5f3a\u5927\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models improve GUI automation by using a cost-effective training method that ensures high-quality data and reliable results.</li>\n    <li>Step-GUI models perform very well in GUI tasks, achieving top scores in various benchmarks while still being generally capable.</li>\n    <li>A new protocol called GUI-MCP helps standardize how GUI agents operate across different devices, ensuring user privacy by keeping sensitive information on the device.</li>\n    <li>AndroidDaily is a new benchmark that tests how well these agents perform in real-life mobile situations, with a focus on common tasks.</li>\n    <li>This research shows significant progress in developing practical GUI agents that can be used effectively in everyday digital interactions.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u63a8\u65ad\u901f\u5ea6\u6162\uff0c\u800c\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u867d\u7136\u53ef\u4ee5\u5e76\u884c\u5904\u7406\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u751f\u6210\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ReFusion\uff0c\u4e00\u79cd\u65b0\u7684\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5e76\u884c\u89e3\u7801\u63d0\u5347\u5230\u66f4\u9ad8\u7684\u69fd\u4f4d\u6c34\u5e73\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n    <li>ReFusion\u4f7f\u7528\u201c\u8ba1\u5212\u548c\u586b\u5145\u201d\u7684\u8fed\u4ee3\u89e3\u7801\u8fc7\u7a0b\uff0c\u9996\u5148\u8bc6\u522b\u5f31\u4f9d\u8d56\u7684\u69fd\u4f4d\uff0c\u7136\u540e\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u69fd\u4f4d\u3002</li>\n    <li>\u8fd9\u79cd\u69fd\u4f4d\u8bbe\u8ba1\u5141\u8bb8\u5b8c\u5168\u91cd\u7528\u952e\u503c\u7f13\u5b58\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5b66\u4e60\u590d\u6742\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cReFusion\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6027\u80fd\u63d0\u534734%\uff0c\u901f\u5ea6\u63d0\u9ad818\u500d\uff0c\u540c\u65f6\u7f29\u5c0f\u4e86\u4e0e\u5f3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive models (ARMs) are slow at making predictions one at a time.</li>\n    <li>Masked diffusion models (MDMs) are faster but have problems with high resource use and generating unclear results.</li>\n    <li>ReFusion is a new type of MDM that improves efficiency by working with groups of tokens, called slots, instead of individual tokens.</li>\n    <li>This model uses a two-step process: it plans which slots to focus on and then fills them in all at once.</li>\n    <li>ReFusion shows better performance and speed compared to previous models, with significant gains in both areas across various tests.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13687", "authors": [{"_id": "6940ee0665f1e24a1178066d", "user": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "name": "Jingfeng Yao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:03.365Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066e", "user": {"_id": "6264bf5a1ed8d81e47ae3a62", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650769741842-noauth.jpeg", "isPro": false, "fullname": "Yuda Song", "user": "IDKiro", "type": "user"}, "name": "Yuda Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:38.071Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066f", "user": {"_id": "64192280d459c9e7fbb03aa1", "avatarUrl": "/avatars/89935de92f3f9107d7b768b82fb27e70.svg", "isPro": false, "fullname": "Zhou", "user": "Yucong", "type": "user"}, "name": "Yucong Zhou", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:50.881Z", "hidden": false}, {"_id": "6940ee0665f1e24a11780670", "user": {"_id": "62600de6d47e3dbae32ce1ce", "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg", "isPro": false, "fullname": "Xinggang Wang", "user": "xinggangw", "type": "user"}, "name": "Xinggang Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:40.199Z", "hidden": false}], "publishedAt": "2025-12-15T18:59:54.000Z", "submittedOnDailyAt": "2025-12-16T07:11:50.997Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "submittedOnDailyBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "upvotes": 66, "discussionId": "6940ee0665f1e24a11780671", "githubRepo": "https://github.com/MiniMax-AI/VTP", "githubRepoAddedBy": "user", "ai_summary": "A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.", "ai_keywords": ["latent space", "visual tokenizers", "VAEs", "reconstruction-based training", "low-level information", "pre-training scaling problem", "high-level semantics", "VTP", "image-text contrastive", "self-supervised", "reconstruction losses", "generative performance", "ImageNet", "zero-shot accuracy", "rFID", "FLOPS", "DiT", "advanced distillation methods"], "githubStars": 92, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "summary_zh": "<ul>\n  <li>\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u5bf9\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002</li>\n  <li>\u4f20\u7edf\u7684\u91cd\u5efa\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u503e\u5411\u4e8e\u4f4e\u7ea7\u4fe1\u606f\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002</li>\n  <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u7f16\u7801\u5668\u9884\u8bad\u7ec3\u6846\u67b6VTP\uff0c\u7ed3\u5408\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u3002</li>\n  <li>\u7814\u7a76\u53d1\u73b0\u7406\u89e3\u80fd\u529b\u662f\u751f\u6210\u8d28\u91cf\u7684\u5173\u952e\uff0c\u4e14VTP\u5728\u751f\u6210\u6027\u80fd\u4e0a\u5177\u6709\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002</li>\n  <li>VTP\u5728\u9884\u8bad\u7ec3\u540e\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u901f\u5ea6\u548c\u8d28\u91cf\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The quality of visual tokenizers is important for generative models, but current training methods focus too much on low-level details.</li>\n    <li>This focus leads to a problem where more computing power does not improve generation quality, known as the \"pre-training scaling problem.\"</li>\n    <li>A new framework called VTP combines various training methods to better capture high-level meanings in images.</li>\n    <li>VTP shows that understanding high-level concepts improves generation results, and it scales better with compute and data.</li>\n    <li>After pre-training, VTP outperforms other methods, achieving faster generation and better accuracy without changing training setups.</li>\n</ul>"}, "publishedAt": "2025-12-15T13:59:54.000Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13687.png", "numComments": 1, "submittedBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "fullname": "Jingfeng Yao", "name": "MapleF9", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13564", "authors": [{"_id": "6940d68565f1e24a1178056c", "user": {"_id": "6544b9b646dbdeca34ee5f52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png", "isPro": false, "fullname": "Yuyang Hu", "user": "namespace-ERI", "type": "user"}, "name": "Yuyang Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:40.040Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056d", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:40.844Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056e", "name": "Yanwei Yue", "hidden": false}, {"_id": "6940d68565f1e24a1178056f", "name": "Guibin Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780570", "name": "Boyang Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780571", "name": "Fangyi Zhu", "hidden": false}, {"_id": "6940d68565f1e24a11780572", "name": "Jiahang Lin", "hidden": false}, {"_id": "6940d68565f1e24a11780573", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:38.698Z", "hidden": false}, {"_id": "6940d68565f1e24a11780574", "name": "Shihan Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780575", "name": "Zhiheng Xi", "hidden": false}, {"_id": "6940d68565f1e24a11780576", "name": "Senjie Jin", "hidden": false}, {"_id": "6940d68565f1e24a11780577", "user": {"_id": "62e52483a944e2a56cd2c6ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg", "isPro": false, "fullname": "Jiejun Tan", "user": "zstanjj", "type": "user"}, "name": "Jiejun Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:42.726Z", "hidden": false}, {"_id": "6940d68565f1e24a11780578", "name": "Yanbin Yin", "hidden": false}, {"_id": "6940d68565f1e24a11780579", "name": "Jiongnan Liu", "hidden": false}, {"_id": "6940d68565f1e24a1178057a", "name": "Zeyu Zhang", "hidden": false}, {"_id": "6940d68565f1e24a1178057b", "user": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "isPro": false, "fullname": "SunZX", "user": "Jeryi", "type": "user"}, "name": "Zhongxiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T22:32:57.018Z", "hidden": false}, {"_id": "6940d68565f1e24a1178057c", "name": "Yutao Zhu", "hidden": false}, {"_id": "6940d68565f1e24a1178057d", "name": "Hao Sun", "hidden": false}, {"_id": "6940d68565f1e24a1178057e", "name": "Boci Peng", "hidden": false}, {"_id": "6940d68565f1e24a1178057f", "name": "Zhenrong Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780580", "name": "Xuanbo Fan", "hidden": false}, {"_id": "6940d68565f1e24a11780581", "name": "Jiaxin Guo", "hidden": false}, {"_id": "6940d68565f1e24a11780582", "name": "Xinlei Yu", "hidden": false}, {"_id": "6940d68565f1e24a11780583", "name": "Zhenhong Zhou", "hidden": false}, {"_id": "6940d68565f1e24a11780584", "name": "Zewen Hu", "hidden": false}, {"_id": "6940d68565f1e24a11780585", "name": "Jiahao Huo", "hidden": false}, {"_id": "6940d68565f1e24a11780586", "name": "Junhao Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780587", "name": "Yuwei Niu", "hidden": false}, {"_id": "6940d68565f1e24a11780588", "name": "Yu Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780589", "name": "Zhenfei Yin", "hidden": false}, {"_id": "6940d68565f1e24a1178058a", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6940d68565f1e24a1178058b", "name": "Yue Liao", "hidden": false}, {"_id": "6940d68565f1e24a1178058c", "name": "Qiankun Li", "hidden": false}, {"_id": "6940d68565f1e24a1178058d", "name": "Kun Wang", "hidden": false}, {"_id": "6940d68565f1e24a1178058e", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "6940d68565f1e24a1178058f", "name": "Yixin Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780590", "name": "Dawei Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780591", "name": "Qi Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780592", "name": "Tao Gui", "hidden": false}, {"_id": "6940d68565f1e24a11780593", "name": "Shirui Pan", "hidden": false}, {"_id": "6940d68565f1e24a11780594", "name": "Yan Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780595", "name": "Philip Torr", "hidden": false}, {"_id": "6940d68565f1e24a11780596", "name": "Zhicheng Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780597", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6940d68565f1e24a11780598", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6940d68565f1e24a11780599", "name": "Yu-Gang Jiang", "hidden": false}, {"_id": "6940d68565f1e24a1178059a", "name": "Shuicheng Yan", "hidden": false}], "publishedAt": "2025-12-15T17:22:34.000Z", "submittedOnDailyAt": "2025-12-16T01:18:34.363Z", "title": "Memory in the Age of AI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "upvotes": 62, "discussionId": "6940d68565f1e24a1178059b", "githubRepo": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List", "githubRepoAddedBy": "user", "ai_summary": "This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.", "ai_keywords": ["agent memory", "LLM memory", "retrieval augmented generation (RAG)", "context engineering", "token-level memory", "parametric memory", "latent memory", "factual memory", "experiential memory", "working memory", "memory benchmarks", "open-source frameworks", "memory automation", "reinforcement learning integration", "multimodal memory", "multi-agent memory", "trustworthiness issues"], "githubStars": 115, "summary_zh": "<ul>\n    <li>\u8bb0\u5fc6\u662f\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7684\u91cd\u8981\u80fd\u529b\uff0c\u7814\u7a76\u6b63\u5728\u5feb\u901f\u53d1\u5c55\u3002</li>\n    <li>\u5f53\u524d\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u5b58\u5728\u8bb8\u591a\u4e0d\u540c\u7684\u52a8\u673a\u3001\u5b9e\u73b0\u65b9\u5f0f\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u5bfc\u81f4\u9886\u57df\u5206\u6563\u3002</li>\n    <li>\u4f20\u7edf\u7684\u8bb0\u5fc6\u5206\u7c7b\uff08\u5982\u957f\u671f/\u77ed\u671f\u8bb0\u5fc6\uff09\u4e0d\u8db3\u4ee5\u6db5\u76d6\u73b0\u4ee3\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u7684\u591a\u6837\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u5305\u62ec\u8bb0\u5fc6\u7684\u5f62\u5f0f\u3001\u529f\u80fd\u548c\u52a8\u6001\u5206\u6790\u3002</li>\n    <li>\u603b\u7ed3\u4e86\u5185\u5b58\u57fa\u51c6\u548c\u5f00\u6e90\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5982\u8bb0\u5fc6\u81ea\u52a8\u5316\u548c\u591a\u6a21\u6001\u8bb0\u5fc6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agent memory is a key feature of AI systems and is gaining more attention in research.</li>\n    <li>Current studies on agent memory are varied and lack clear definitions, making the field fragmented.</li>\n    <li>This work aims to clarify what agent memory is and how it differs from related concepts.</li>\n    <li>It categorizes agent memory into different types based on forms (token-level, parametric, latent), functions (factual, experiential, working), and dynamics (how memory is formed and used over time).</li>\n    <li>The paper also highlights future research areas, including memory automation and multi-agent systems, and provides resources for practical development.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:22:34.000Z", "title": "Memory in the Age of AI Agents", "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13564.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "isAuthorParticipating": false}, {"paper": {"id": "2512.12967", "authors": [{"_id": "6940d25d65f1e24a11780417", "user": {"_id": "64777a346e6c7ac608c1e9bf", "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg", "isPro": false, "fullname": "Weizhou Shen", "user": "shenwzh3", "type": "user"}, "name": "Weizhou Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:42.079Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780418", "user": {"_id": "64c9b0f28d2d187c24d1e6c1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png", "isPro": false, "fullname": "ZiYi Yang", "user": "AALF", "type": "user"}, "name": "Ziyi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:56.062Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780419", "name": "Chenliang Li", "hidden": false}, {"_id": "6940d25d65f1e24a1178041a", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "6940d25d65f1e24a1178041b", "name": "Miao Peng", "hidden": false}, {"_id": "6940d25d65f1e24a1178041c", "name": "Huashan Sun", "hidden": false}, {"_id": "6940d25d65f1e24a1178041d", "name": "Yingcheng Shi", "hidden": false}, {"_id": "6940d25d65f1e24a1178041e", "name": "Shengyi Liao", "hidden": false}, {"_id": "6940d25d65f1e24a1178041f", "name": "Shaopeng Lai", "hidden": false}, {"_id": "6940d25d65f1e24a11780420", "name": "Bo Zhang", "hidden": false}, {"_id": "6940d25d65f1e24a11780421", "name": "Dayiheng Liu", "hidden": false}, {"_id": "6940d25d65f1e24a11780422", "name": "Fei Huang", "hidden": false}, {"_id": "6940d25d65f1e24a11780423", "name": "Jingren Zhou", "hidden": false}, {"_id": "6940d25d65f1e24a11780424", "name": "Ming Yan", "hidden": false}], "publishedAt": "2025-12-15T04:11:11.000Z", "submittedOnDailyAt": "2025-12-16T01:29:41.007Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "upvotes": 54, "discussionId": "6940d25d65f1e24a11780425", "githubRepo": "https://github.com/Tongyi-Zhiwen/Qwen-Doc", "githubRepoAddedBy": "user", "ai_summary": "QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.", "ai_keywords": ["Long-Context Data Synthesis Pipeline", "atomic facts", "verifiable reasoning questions", "Stabilized Reinforcement Learning", "task-balanced sampling", "task-specific advantage estimation", "Adaptive Entropy-Controlled Policy Optimization", "Memory-Augmented Architecture", "multi-stage fusion RL training", "single-pass reasoning", "iterative memory-based processing", "memory-agent framework"], "githubStars": 311, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86QwenLong-L1.5\u6a21\u578b\uff0c\u5177\u5907\u4f18\u79c0\u7684\u957f\u6587\u672c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u901a\u8fc7\u7cfb\u7edf\u7684\u540e\u7eed\u8bad\u7ec3\u521b\u65b0\uff0c\u5f00\u53d1\u4e86\u957f\u6587\u672c\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u751f\u6210\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u3002</li>\n    <li>\u91c7\u7528\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u957f\u6587\u672c\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002</li>\n    <li>\u5efa\u7acb\u4e86\u8bb0\u5fc6\u589e\u5f3a\u67b6\u6784\uff0c\u652f\u6301\u8d85\u8fc74M\u6807\u8bb0\u7684\u8d85\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u3002</li>\n    <li>\u5728\u957f\u6587\u672c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwenLong-L1.5\u7684\u8868\u73b0\u4e0eGPT-5\u548cGemini-2.5-Pro\u76f8\u5f53\uff0c\u5e73\u5747\u63d0\u53479.90\u5206\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QwenLong-L1.5 is a new model designed for better reasoning with long texts through innovative training methods.</li>\n    <li>It uses a special data generation process to create challenging questions that help improve reasoning across large amounts of information.</li>\n    <li>The model includes advanced techniques to ensure stable learning when handling long contexts, improving how it learns from rewards.</li>\n    <li>QwenLong-L1.5 has a memory system that helps it manage very long text inputs, even those over 4 million tokens.</li>\n    <li>It performs as well as or better than other leading models on long-context reasoning tasks and shows improved skills in areas like scientific reasoning and extended conversations.</li>\n</ul>"}, "publishedAt": "2025-12-14T23:11:11.000Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12967.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13604", "authors": [{"_id": "6940d44165f1e24a11780535", "name": "Jianxiong Gao", "hidden": false}, {"_id": "6940d44165f1e24a11780536", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "6940d44165f1e24a11780537", "name": "Xian Liu", "hidden": false}, {"_id": "6940d44165f1e24a11780538", "user": {"_id": "64970d3d9c3b29dca8633f87", "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg", "isPro": false, "fullname": "JunhaoZhuang", "user": "JunhaoZhuang", "type": "user"}, "name": "Junhao Zhuang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:51.620Z", "hidden": false}, {"_id": "6940d44165f1e24a11780539", "name": "Chengming Xu", "hidden": false}, {"_id": "6940d44165f1e24a1178053a", "name": "Jianfeng Feng", "hidden": false}, {"_id": "6940d44165f1e24a1178053b", "name": "Yu Qiao", "hidden": false}, {"_id": "6940d44165f1e24a1178053c", "name": "Yanwei Fu", "hidden": false}, {"_id": "6940d44165f1e24a1178053d", "user": {"_id": "635f8ed47c05eb9f59963d3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg", "isPro": false, "fullname": "ChenyangSi", "user": "ChenyangSi", "type": "user"}, "name": "Chenyang Si", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:49.025Z", "hidden": false}, {"_id": "6940d44165f1e24a1178053e", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-15T17:59:58.000Z", "submittedOnDailyAt": "2025-12-16T01:12:24.486Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "submittedOnDailyBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "isPro": false, "fullname": "Jianxiong Gao", "user": "Jianxiong", "type": "user"}, "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "upvotes": 52, "discussionId": "6940d44165f1e24a1178053f", "projectPage": "https://vchitect.github.io/LongVie2-project/", "ai_summary": "LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.", "ai_keywords": ["autoregressive framework", "multi-modal guidance", "degradation-aware training", "history-context guidance", "video world models", "controllability", "visual quality", "temporal consistency", "LongVGenBench", "state-of-the-art performance", "temporal coherence", "visual fidelity"], "summary_zh": "<ul>\n    <li>\u6784\u5efa\u89c6\u9891\u4e16\u754c\u6a21\u578b\u9700\u8981\u5177\u5907\u53ef\u63a7\u6027\u3001\u957f\u671f\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e09\u4e2a\u91cd\u8981\u7279\u6027\u3002</li>\n    <li>\u63d0\u51fa\u4e86LongVie 2\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u6765\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>\u7b2c\u4e00\u9636\u6bb5\u662f\u591a\u6a21\u6001\u6307\u5bfc\uff0c\u901a\u8fc7\u63a7\u5236\u4fe1\u53f7\u63d0\u9ad8\u53ef\u63a7\u6027\u3002</li>\n    <li>\u7b2c\u4e8c\u9636\u6bb5\u5173\u6ce8\u4e8e\u8f93\u5165\u5e27\u7684\u964d\u89e3\u611f\u77e5\u8bad\u7ec3\uff0c\u4ee5\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002</li>\n    <li>\u7b2c\u4e09\u9636\u6bb5\u662f\u5386\u53f2\u4e0a\u4e0b\u6587\u6307\u5bfc\uff0c\u4ee5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u652f\u6301\u6700\u957f\u4e94\u5206\u949f\u7684\u89c6\u9891\u8fde\u7eed\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>LongVie 2 is a new system for creating video world models that aims to improve how we understand and generate videos.</li>\n  <li>It focuses on three key features: being controllable, maintaining high visual quality over time, and ensuring consistency in how videos look and feel.</li>\n  <li>The system is developed in three stages: enhancing control using different signals, training to keep video quality high, and ensuring smooth transitions between video clips.</li>\n  <li>LongVGenBench is introduced as a benchmark with 100 high-quality one-minute videos from various real and synthetic environments for testing performance.</li>\n  <li>LongVie 2 shows excellent results in controllability, visual quality, and can generate videos continuously for up to five minutes.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:59:58.000Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13604.png", "numComments": 1, "submittedBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "fullname": "Jianxiong Gao", "name": "Jianxiong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13281", "authors": [{"_id": "6940d82465f1e24a117805c2", "name": "Jiaqi Wang", "hidden": false}, {"_id": "6940d82465f1e24a117805c3", "name": "Weijia Wu", "hidden": false}, {"_id": "6940d82465f1e24a117805c4", "name": "Yi Zhan", "hidden": false}, {"_id": "6940d82465f1e24a117805c5", "name": "Rui Zhao", "hidden": false}, {"_id": "6940d82465f1e24a117805c6", "name": "Ming Hu", "hidden": false}, {"_id": "6940d82465f1e24a117805c7", "name": "James Cheng", "hidden": false}, {"_id": "6940d82465f1e24a117805c8", "name": "Wei Liu", "hidden": false}, {"_id": "6940d82465f1e24a117805c9", "name": "Philip Torr", "hidden": false}, {"_id": "6940d82465f1e24a117805ca", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T14:04:37.407Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "publishedAt": "2025-12-15T12:41:23.000Z", "submittedOnDailyAt": "2025-12-17T08:26:01.077Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "submittedOnDailyBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "upvotes": 50, "discussionId": "6940d82565f1e24a117805cb", "projectPage": "https://video-reality-test.github.io/", "githubRepo": "https://github.com/video-reality-test/video-reality-test", "githubRepoAddedBy": "user", "ai_summary": "The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.", "ai_keywords": ["ASMR", "audio-visual coupling", "Veo3.1-Fast", "Gemini 2.5-Pro", "perceptual realism", "real-fake discrimination", "audio-visual consistency"], "githubStars": 13, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97\u751f\u6210\u7684\u89c6\u9891\u5185\u5bb9\u4e0e\u771f\u5b9e\u89c6\u9891\u96be\u4ee5\u533a\u5206\uff0c\u5bfc\u81f4\u68c0\u6d4bAI\u751f\u6210\u89c6\u9891\u6210\u4e3a\u793e\u4f1a\u65b0\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u68c0\u6d4b\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u65e0\u97f3\u9891\u89c6\u9891\uff0c\u5173\u6ce8\u5e7f\u6cdb\u53d9\u4e8b\u9886\u57df\uff0c\u5e76\u4e14\u4ec5\u9650\u4e8e\u5206\u7c7b\uff0c\u5c1a\u672a\u63a2\u8ba8\u97f3\u9891\u4e0e\u89c6\u9891\u7684\u7ed3\u5408\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86\u201c\u89c6\u9891\u73b0\u5b9e\u6d4b\u8bd5\u201d\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9ASMR\u89c6\u9891\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u97f3\u89c6\u9891\u7d27\u5bc6\u7ed3\u5408\u4e0b\u7684\u611f\u77e5\u73b0\u5b9e\u6027\u3002</li>\n    <li>\u6d4b\u8bd5\u5305\u542b\u771f\u5b9e\u7684ASMR\u89c6\u9891\uff0c\u5173\u6ce8\u7ec6\u81f4\u7684\u52a8\u4f5c-\u7269\u4f53\u4e92\u52a8\uff0c\u5e76\u901a\u8fc7\u521b\u4f5c\u8005-\u8bc4\u5ba1\u8005\u534f\u8bae\u8fdb\u884c\u8bc4\u4f30\u3002</li>\n    <li>\u5b9e\u9a8c\u53d1\u73b0\u6700\u597d\u7684\u751f\u6210\u6a21\u578bVeo3.1-Fast\u80fd\u591f\u6b3a\u9a97\u5927\u591a\u6570VLMs\uff0c\u4e14\u52a0\u5165\u97f3\u9891\u53ef\u4ee5\u63d0\u9ad8\u771f\u5b9e\u4e0e\u865a\u5047\u4e4b\u95f4\u7684\u533a\u5206\uff0c\u4f46\u67d0\u4e9b\u8868\u9762\u7ebf\u7d22\u4ecd\u53ef\u80fd\u8bef\u5bfc\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video generation technology has improved, making it hard to tell AI-made videos apart from real ones.</li>\n    <li>Most previous tests for detecting AI-generated content focused only on videos without sound and did not assess the combination of audio and video.</li>\n    <li>The new Video Reality Test benchmark uses ASMR videos to evaluate how realistic AI-generated videos are when paired with sound.</li>\n    <li>In experiments, even the best AI video generator fooled many detection models, highlighting challenges in distinguishing real from fake.</li>\n    <li>Adding sound helps in identifying fake videos, but some simple clues like watermarks can still trick detection models.</li>\n</ul>"}, "publishedAt": "2025-12-15T07:41:23.000Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13281.png", "numComments": 2, "submittedBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "fullname": "Qinghong (Kevin) Lin", "name": "KevinQHLin", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 41}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14614", "authors": [{"_id": "694219f25d5b2dc1052747ff", "user": {"_id": "64897b1f0ec897cfe579a399", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg", "isPro": false, "fullname": "wenq", "user": "wenqsun", "type": "user"}, "name": "Wenqiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:38.348Z", "hidden": false}, {"_id": "694219f25d5b2dc105274800", "name": "Haiyu Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274801", "name": "Haoyuan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274802", "name": "Junta Wu", "hidden": false}, {"_id": "694219f25d5b2dc105274803", "name": "Zehan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274804", "name": "Zhenwei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274805", "name": "Yunhong Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274806", "name": "Jun Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274807", "name": "Tengfei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274808", "name": "Chunchao Guo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "publishedAt": "2025-12-16T17:22:46.000Z", "submittedOnDailyAt": "2025-12-17T00:24:30.301Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "upvotes": 48, "discussionId": "694219f35d5b2dc105274809", "projectPage": "https://3d-models.hunyuan.tencent.com/world/", "githubRepo": "https://github.com/Tencent-Hunyuan/HY-WorldPlay", "githubRepoAddedBy": "user", "ai_summary": "WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.", "ai_keywords": ["Dual Action Representation", "Reconstituted Context Memory", "temporal reframing", "Context Forcing", "memory-aware model", "long-horizon streaming video"], "githubStars": 302, "summary_zh": "<ul>\n    <li>WorldPlay \u662f\u4e00\u79cd\u6d41\u5a92\u4f53\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u3001\u4e92\u52a8\u7684\u4e16\u754c\u5efa\u6a21\uff0c\u5e76\u4fdd\u6301\u957f\u671f\u51e0\u4f55\u4e00\u81f4\u6027\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u4e09\u9879\u5173\u952e\u521b\u65b0\u6765\u89e3\u51b3\u901f\u5ea6\u548c\u5185\u5b58\u4e4b\u95f4\u7684\u6743\u8861\u3002</li>\n    <li>\u4f7f\u7528\u53cc\u91cd\u52a8\u4f5c\u8868\u793a\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u952e\u76d8\u548c\u9f20\u6807\u8f93\u5165\u8fdb\u884c\u7a33\u5065\u7684\u52a8\u4f5c\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u91cd\u6784\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u52a8\u6001\u91cd\u5efa\u4e0a\u4e0b\u6587\uff0c\u786e\u4fdd\u957f\u671f\u4e00\u81f4\u6027\uff0c\u5e76\u6709\u6548\u51cf\u8f7b\u5185\u5b58\u8870\u51cf\u3002</li>\n    <li>WorldPlay \u80fd\u4ee5 720p 24 FPS \u7684\u901f\u5ea6\u751f\u6210\u9ad8\u4e00\u81f4\u6027\u7684\u957f\u65f6\u95f4\u6d41\u5a92\u4f53\u89c6\u9891\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>WorldPlay is a new video model that allows real-time, interactive 3D world creation with consistent geometry.</li>\n    <li>It features a Dual Action Representation for better control based on user inputs like keyboard and mouse.</li>\n    <li>The Reconstituted Context Memory system helps maintain important information from past frames to ensure long-term consistency.</li>\n    <li>Context Forcing is a new method that helps the model remember long-range information while keeping processing fast and accurate.</li>\n    <li>WorldPlay can produce high-quality 720p video at 24 frames per second and performs well across various scenes.</li>\n</ul>"}, "publishedAt": "2025-12-16T12:22:46.000Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14614.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u8f6f\u4ef6\u5f00\u53d1\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u7ffb\u8bd1\u6210\u529f\u80fd\u4ee3\u7801\u3002</li>\n    <li>\u901a\u8fc7\u5de5\u5177\u5982Github Copilot\u548cClaude Code\u7b49\uff0cLLMs\u5728\u5546\u4e1a\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u523095%\u4ee5\u4e0a\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u5168\u9762\u6307\u5357\uff0c\u5206\u6790\u4e86\u6a21\u578b\u751f\u547d\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u5305\u62ec\u6570\u636e\u6574\u7406\u548c\u540e\u671f\u8bad\u7ec3\u3002</li>\n    <li>\u5bf9\u901a\u7528\u548c\u4e13\u7528\u4ee3\u7801LLMs\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u6280\u672f\u3001\u8bbe\u8ba1\u51b3\u7b56\u548c\u6743\u8861\u3002</li>\n    <li>\u7814\u7a76\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\u4ee5\u5206\u6790\u4ee3\u7801\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are changing software development by turning natural language into code, with tools like GitHub Copilot leading the way.</li>\n    <li>The technology has improved from simple rule-based systems to advanced models, achieving over 95% success rates on coding tasks.</li>\n    <li>This work provides a detailed guide on code LLMs, examining their entire life cycle from data setup to post-training techniques.</li>\n    <li>It compares general LLMs and specialized code LLMs, discussing their strengths and weaknesses.</li>\n    <li>The study highlights the gap between academic research and real-world coding needs, focusing on issues like code accuracy, security, and workflow integration.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u517c\u987e\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u5408\u957f\u6587\u672c\u573a\u666f\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6027\u80fd\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u672c\u8d85\u8d8aGPT-5\uff0c\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u63a5\u8fd1Gemini-3.0-Pro\u3002</li>\n    <li>\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u83b7\u5f97\u91d1\u724c\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u578b\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u590d\u6742\u3001\u4e92\u52a8\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6307\u4ee4\u6267\u884c\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that is both efficient and effective at reasoning and performance.</li>\n    <li>It features a new attention mechanism called DeepSeek Sparse Attention (DSA) that reduces computing needs while maintaining performance in longer contexts.</li>\n    <li>The model uses a strong reinforcement learning approach, allowing it to compete with and even outperform GPT-5 in some areas.</li>\n    <li>DeepSeek-V3.2-Speciale demonstrates excellent reasoning skills, winning top awards in major international math and informatics competitions.</li>\n    <li>It includes a new system for creating training data that helps improve its ability to follow instructions and generalize in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5148\u8fdb\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\uff08\u5982Qwen-Image\u7b49\uff09\u53c2\u6570\u91cf\u8fc7\u5927\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u57fa\u7840\u751f\u6210\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u4e3a6B\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u538b\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u901a\u8fc7\u4f18\u5316\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u6210\u672c\uff0c\u5e76\u4e14\u80fd\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u8fd0\u884c\u3002</li>\n    <li>\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6f14\u793a\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u83b7\u5f97\u7684\u9ad8\u6027\u80fd\u751f\u6210\u6a21\u578b\u7684\u5f00\u53d1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The field of high-performance image generation is mostly led by proprietary models, while open-source options have large parameter counts that make them hard to use on regular hardware.</li>\n    <li>We introduce Z-Image, a more efficient image generation model with only 6 billion parameters, challenging the idea that bigger is always better.</li>\n    <li>Z-Image was trained using optimized processes, completing the training in 314,000 GPU hours at a cost of about $630,000.</li>\n    <li>It offers fast performance, working well on both high-end and more affordable hardware, and includes an editing model called Z-Image-Edit.</li>\n    <li>Our model achieves impressive results in photorealistic image generation and bilingual text rendering, showing that high-quality outputs can be produced with less computing power.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u7406\u89e3\u957f\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u6d4f\u89c8\u548c\u7ec6\u8282\u5ba1\u67e5\u6765\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u805a\u7126\u4e8e\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\uff0c\u4ee5\u83b7\u5f97\u66f4\u7ec6\u81f4\u7684\u89c6\u9891\u5e27\u3002</li>\n    <li>\u4e3a\u4e86\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\uff0c\u6211\u4eec\u6574\u7406\u4e86\u4e00\u4e2a\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u5305\u62ec\u8bad\u7ec3\u548c\u8bc4\u4f30\u6240\u9700\u7684\u6837\u672c\u3002</li>\n    <li>\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0cLongVT\u5728\u56db\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) can analyze videos but struggle with long videos due to sparse information.</li>\n    <li>We created LongVT, a framework that helps these models reason about long videos by first getting an overview and then focusing on specific clips.</li>\n    <li>LongVT uses the model's ability to identify important clips and gather detailed frames for better understanding.</li>\n    <li>We are releasing a new dataset called VideoSIAH to help train and evaluate LongVT, which includes over 240,000 samples for training and 1,280 QA pairs for testing.</li>\n    <li>LongVT has been tested and shows better results than existing methods in understanding and reasoning about long videos.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u6e05\u6670\u5ea6\u4e14\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u65f6\u95f4\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\u4e3b\u4e49\uff08TPP\uff09\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u5206\u5e03\u5f0f\u5904\u7406\uff0c\u6253\u7834\u4e86\u81ea\u56de\u5f52\u74f6\u9888\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u6d41\u5a92\u4f53\u751f\u6210\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u6821\u51c6\u5916\u89c2\u6765\u63d0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u6807\u5fd7\u7740\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u65b0\u91cc\u7a0b\u7891\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating high-quality avatars in real-time using a powerful 14-billion-parameter diffusion model.</li>\n    <li>The system uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by using multiple GPUs, allowing for low-latency streaming.</li>\n    <li>To improve video quality and keep the avatar consistent, a method called Rolling Sink Frame Mechanism (RSFM) is used to adjust the avatar's appearance over time.</li>\n    <li>Live Avatar can generate videos at 20 frames per second using five GPUs, making it one of the fastest and most effective systems for real-time avatar generation.</li>\n    <li>This work sets a new standard for using advanced video generation models in applications that require long videos.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4e86\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u7684\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u6a21\u5f0f\u4e0a\u8fdb\u884c\u6df1\u5c42\u6b21\u7684\u5de5\u7a0b\u8bbe\u8ba1\u548cSQL\u7ba1\u9053\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u6027\u5546\u4e1a\u95ee\u9898\uff0c\u8981\u6c42\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728DAComp\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>\u901a\u8fc7\u8bc6\u522b\u8fd9\u4e9b\u5c40\u9650\u6027\uff0cDAComp\u4e3a\u5f00\u53d1\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks that reflect real-world data workflows in businesses.</li>\n    <li>It includes data engineering tasks that involve creating and updating SQL pipelines and data analysis tasks that focus on solving business problems.</li>\n    <li>Performance on data engineering tasks is very low, with success rates below 20%, indicating difficulties in managing complex data processes.</li>\n    <li>Data analysis tasks also show poor results, averaging below 40%, revealing challenges in reasoning and problem-solving.</li>\n    <li>DAComp helps identify these weaknesses, aiming to improve the development of autonomous data tools for companies.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5206\u5f00\u5904\u7406\u7684\u95ee\u9898\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u80cc\u666f\uff0c\u5e76\u5c06\u5b83\u4eec\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u589e\u5f3a\u5176\u529f\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one complete system, unlike traditional separate methods.</li>\n    <li>The system can handle various inputs like text, images, and videos to produce unified content.</li>\n    <li>Kling-Omni uses a strong data foundation and advanced training methods for better performance.</li>\n    <li>It shows great skill in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21631", "authors": [{"_id": "692ffb1a26742347f61daf38", "user": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "name": "Shuai Bai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:34:29.118Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf39", "name": "Yuxuan Cai", "hidden": false}, {"_id": "692ffb1a26742347f61daf3a", "name": "Ruizhe Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3b", "name": "Keqin Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3c", "user": {"_id": "63f30b870a16587ea970edfe", "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg", "isPro": false, "fullname": "Xiong-Hui Chen", "user": "xionghuichen", "type": "user"}, "name": "Xionghui Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:42.689Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3d", "user": {"_id": "65b2529285b6c21448a10d65", "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg", "isPro": false, "fullname": "Zesen Cheng", "user": "ClownRat", "type": "user"}, "name": "Zesen Cheng", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:51.365Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3e", "name": "Lianghao Deng", "hidden": false}, {"_id": "692ffb1a26742347f61daf3f", "name": "Wei Ding", "hidden": false}, {"_id": "692ffb1a26742347f61daf40", "name": "Chang Gao", "hidden": false}, {"_id": "692ffb1a26742347f61daf41", "name": "Chunjiang Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf42", "name": "Wenbin Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf43", "name": "Zhifang Guo", "hidden": false}, {"_id": "692ffb1a26742347f61daf44", "user": {"_id": "656f1b21b075b63c90ba02ee", "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg", "isPro": false, "fullname": "Huang Qidong", "user": "shikiw", "type": "user"}, "name": "Qidong Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:49.065Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf45", "name": "Jie Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf46", "name": "Fei Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf47", "name": "Binyuan Hui", "hidden": false}, {"_id": "692ffb1a26742347f61daf48", "name": "Shutong Jiang", "hidden": false}, {"_id": "692ffb1a26742347f61daf49", "name": "Zhaohai Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4a", "name": "Mingsheng Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4b", "name": "Mei Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4c", "user": {"_id": "6346be8f7fb9f11870c63998", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png", "isPro": false, "fullname": "Kaixin Li", "user": "likaixin", "type": "user"}, "name": "Kaixin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:53.648Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4d", "user": {"_id": "67a31313cf9d856beb7f9afb", "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg", "isPro": false, "fullname": "Zicheng Lin", "user": "etonlin", "type": "user"}, "name": "Zicheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:50.803Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4e", "name": "Junyang Lin", "hidden": false}, {"_id": "692ffb1a26742347f61daf4f", "name": "Xuejing Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf50", "name": "Jiawei Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf51", "name": "Chenglong Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf52", "name": "Yang Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf53", "name": "Dayiheng Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf54", "user": {"_id": "64e72776e9fc9d0475ef5188", "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg", "isPro": false, "fullname": "Shixuan Liu", "user": "liusx", "type": "user"}, "name": "Shixuan Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:58.470Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf55", "name": "Dunjie Lu", "hidden": false}, {"_id": "692ffb1a26742347f61daf56", "name": "Ruilin Luo", "hidden": false}, {"_id": "692ffb1a26742347f61daf57", "name": "Chenxu Lv", "hidden": false}, {"_id": "692ffb1a26742347f61daf58", "name": "Rui Men", "hidden": false}, {"_id": "692ffb1a26742347f61daf59", "name": "Lingchen Meng", "hidden": false}, {"_id": "692ffb1a26742347f61daf5a", "name": "Xuancheng Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5b", "name": "Xingzhang Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5c", "name": "Sibo Song", "hidden": false}, {"_id": "692ffb1a26742347f61daf5d", "name": "Yuchong Sun", "hidden": false}, {"_id": "692ffb1a26742347f61daf5e", "name": "Jun Tang", "hidden": false}, {"_id": "692ffb1a26742347f61daf5f", "name": "Jianhong Tu", "hidden": false}, {"_id": "692ffb1a26742347f61daf60", "name": "Jianqiang Wan", "hidden": false}, {"_id": "692ffb1a26742347f61daf61", "name": "Peng Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf62", "name": "Pengfei Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf63", "name": "Qiuyue Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf64", "name": "Yuxuan Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf65", "name": "Tianbao Xie", "hidden": false}, {"_id": "692ffb1a26742347f61daf66", "name": "Yiheng Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf67", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:51.583Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf68", "name": "Jin Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf69", "name": "Zhibo Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6a", "name": "Mingkun Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6b", "name": "Jianxin Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6c", "name": "An Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6d", "name": "Bowen Yu", "hidden": false}, {"_id": "692ffb1a26742347f61daf6e", "name": "Fei Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6f", "name": "Hang Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf70", "name": "Xi Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf71", "name": "Bo Zheng", "hidden": false}, {"_id": "692ffb1a26742347f61daf72", "name": "Humen Zhong", "hidden": false}, {"_id": "692ffb1a26742347f61daf73", "name": "Jingren Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf74", "name": "Fan Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf75", "name": "Jing Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf76", "user": {"_id": "627d2723401f42c57b6b7c0c", "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg", "isPro": false, "fullname": "Yuanzhi Zhu", "user": "Yuanzhi", "type": "user"}, "name": "Yuanzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:59.879Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf77", "name": "Ke Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "publishedAt": "2025-11-26T17:59:08.000Z", "submittedOnDailyAt": "2025-12-04T01:02:46.772Z", "title": "Qwen3-VL Technical Report", "submittedOnDailyBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "upvotes": 110, "discussionId": "692ffb1b26742347f61daf78", "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.", "ai_keywords": ["vision-language model", "interleaved contexts", "multimodal benchmarks", "dense variants", "mixture-of-experts", "pure-text understanding", "long-context comprehension", "multimodal reasoning", "MMMU", "visual-math benchmarks", "interleaved-MRoPE", "DeepStack", "text-based time alignment", "T-RoPE", "explicit textual timestamp alignment", "vision-language alignment", "image-grounded reasoning", "agentic decision-making", "multimodal code intelligence"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>Qwen3-VL\u662fQwen\u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u6700\u9ad8256K\u4e2atoken\u7684\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u6df7\u5408\u8f93\u5165\uff0c\u9002\u5e94\u4e0d\u540c\u7684\u5ef6\u8fdf\u548c\u8d28\u91cf\u9700\u6c42\u3002</li>\n    <li>Qwen3-VL\u5728\u7eaf\u6587\u672c\u7406\u89e3\u3001\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u5904\u7406\u590d\u6742\u7684\u6587\u6863\u548c\u89c6\u9891\u3002</li>\n    <li>\u6a21\u578b\u67b6\u6784\u5347\u7ea7\u5305\u62ec\u66f4\u5f3a\u7684\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\u3001\u6df1\u5ea6\u5806\u53e0\u96c6\u6210\u548c\u89c6\u9891\u65f6\u95f4\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u7684\u5bf9\u9f50\u7cbe\u5ea6\u3002</li>\n    <li>Qwen3-VL\u53ef\u4f5c\u4e3a\u56fe\u50cf\u57fa\u7840\u63a8\u7406\u3001\u51b3\u7b56\u548c\u591a\u6a21\u6001\u667a\u80fd\u7684\u57fa\u7840\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-VL is the latest and most powerful vision-language model in the Qwen series, excelling in various multimodal benchmarks.</li>\n    <li>The model can handle up to 256,000 tokens, integrating text, images, and videos effectively.</li>\n    <li>It comes in different sizes to balance speed and quality, including both dense and mixture-of-experts versions.</li>\n    <li>Qwen3-VL offers improved text understanding, long-context comprehension, and advanced reasoning for images and videos.</li>\n    <li>It includes architectural upgrades for better modeling of spatial and temporal data, enhancing its overall performance.</li>\n</ul>"}, "publishedAt": "2025-11-26T12:59:08.000Z", "title": "Qwen3-VL Technical Report", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png", "numComments": 3, "submittedBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "fullname": "shuai bai", "name": "ShuaiBai623", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 21, 2025";
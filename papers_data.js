window.trendingPapers = {
    "today": [{"paper": {"id": "2601.06789", "authors": [{"_id": "69671036c5e371f6b235d143", "user": {"_id": "692881094c3f4293dfe29e3d", "avatarUrl": "/avatars/bddfaae8041a45498d46ef65ba17c920.svg", "isPro": false, "fullname": "qihao wang", "user": "jimson991", "type": "user"}, "name": "Qihao Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:07.079Z", "hidden": false}, {"_id": "69671036c5e371f6b235d144", "user": {"_id": "64b74fca17570fdff9b2aded", "avatarUrl": "/avatars/8b3519a7011af52dadc87ffef700c77c.svg", "isPro": false, "fullname": "Ziming Cheng", "user": "cadche", "type": "user"}, "name": "Ziming Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:13.370Z", "hidden": false}, {"_id": "69671036c5e371f6b235d145", "user": {"_id": "6513ee3c9af40a65586b43f5", "avatarUrl": "/avatars/815ed3876cefa12b25bf955edcbf71a3.svg", "isPro": false, "fullname": "shuo zhang", "user": "shuozhang", "type": "user"}, "name": "Shuo Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:18.640Z", "hidden": false}, {"_id": "69671036c5e371f6b235d146", "name": "Fan Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d147", "name": "Rui Xu", "hidden": false}, {"_id": "69671036c5e371f6b235d148", "name": "Heng Lian", "hidden": false}, {"_id": "69671036c5e371f6b235d149", "user": {"_id": "65bb3c545a5dbabc818e9044", "avatarUrl": "/avatars/4c239557bd5e33179cbf4f3a440bbf33.svg", "isPro": false, "fullname": "Kunyi Wang", "user": "KunyiWang", "type": "user"}, "name": "Kunyi Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:25.411Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14a", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaoming Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:47.145Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14b", "name": "Jianghao Yin", "hidden": false}, {"_id": "69671036c5e371f6b235d14c", "name": "Sen Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14d", "name": "Yue Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14e", "user": {"_id": "64803e5dc57f629056c601f1", "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg", "isPro": false, "fullname": "Shaolei Zhang", "user": "zhangshaolei", "type": "user"}, "name": "Shaolei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:13.491Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14f", "name": "Yanbing Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d150", "user": {"_id": "6874f7f0f8e67e9b5714adf2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png", "isPro": false, "fullname": "RongHao Chen", "user": "SuPA4ki", "type": "user"}, "name": "Ronghao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:02.800Z", "hidden": false}, {"_id": "69671036c5e371f6b235d151", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:57.426Z", "hidden": false}], "publishedAt": "2026-01-11T06:41:26.000Z", "submittedOnDailyAt": "2026-01-14T01:40:39.607Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "upvotes": 59, "discussionId": "69671036c5e371f6b235d152", "githubRepo": "https://github.com/QuantaAlpha/MemGovern", "githubRepoAddedBy": "user", "ai_summary": "MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.", "ai_keywords": ["autonomous software engineering", "SWE agents", "closed-world limitation", "open-world experience", "GitHub", "experience governance", "experience cards", "agentic experience search", "SWE-bench Verified"], "githubStars": 19, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4ee3\u7406\u5728\u7f16\u7a0b\u4e0a\u6709\u5f88\u5927\u8fdb\u6b65\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u201c\u5c01\u95ed\u4e16\u754c\u201d\u7684\u9650\u5236\u3002</li>\n    <li>\u8fd9\u4e9b\u4ee3\u7406\u5728\u4fee\u590d\u9519\u8bef\u65f6\u5ffd\u89c6\u4e86GitHub\u7b49\u5e73\u53f0\u4e0a\u4e30\u5bcc\u7684\u4eba\u7c7b\u7ecf\u9a8c\u3002</li>\n    <li>\u73b0\u6709\u7684\u95ee\u9898\u8ddf\u8e2a\u6570\u636e\u7ed3\u6784\u4e0d\u5b8c\u5584\uff0c\u9650\u5236\u4e86\u8bbf\u95ee\u8fd9\u4e9b\u7ecf\u9a8c\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86MemGovern\u6846\u67b6\uff0c\u5c06\u539f\u59cbGitHub\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u4f9b\u4ee3\u7406\u4f7f\u7528\u7684\u7ecf\u9a8c\u5361\u7247\u3002</li>\n    <li>MemGovern\u751f\u6210\u4e86135,000\u4e2a\u7ecf\u9a8c\u5361\u7247\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u7387\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autonomous software engineering agents currently struggle with a \"closed-world\" limitation, relying only on local context to fix bugs.</li>\n    <li>They often overlook valuable historical experience found on platforms like GitHub due to unstructured and fragmented issue-tracking data.</li>\n    <li>The paper presents MemGovern, a framework that transforms raw GitHub data into useful knowledge for these agents.</li>\n    <li>MemGovern creates \"experience cards\" from human experience and enables agents to search for this expertise effectively.</li>\n    <li>With 135,000 experience cards, MemGovern improves bug resolution rates by 4.65% and acts as a plug-in for enhanced agent memory systems.</li>\n</ul>"}, "publishedAt": "2026-01-11T01:41:26.000Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06789.png", "numComments": 1, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07022", "authors": [{"_id": "6966474587c71000b5a910d2", "user": {"_id": "65446c938737c799e9ad6f83", "avatarUrl": "/avatars/6ade251e01442b14cbf8cd7888358fd1.svg", "isPro": false, "fullname": "Sungrae Park", "user": "sungrae-park", "type": "user"}, "name": "Sungrae Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:41:08.385Z", "hidden": false}, {"_id": "6966474587c71000b5a910d3", "name": "Sanghoon Kim", "hidden": false}, {"_id": "6966474587c71000b5a910d4", "name": "Jungho Cho", "hidden": false}, {"_id": "6966474587c71000b5a910d5", "name": "Gyoungjin Gim", "hidden": false}, {"_id": "6966474587c71000b5a910d6", "name": "Dawoon Jung", "hidden": false}, {"_id": "6966474587c71000b5a910d7", "name": "Mikyoung Cha", "hidden": false}, {"_id": "6966474587c71000b5a910d8", "name": "Eunhae Choo", "hidden": false}, {"_id": "6966474587c71000b5a910d9", "name": "Taekgyu Hong", "hidden": false}, {"_id": "6966474587c71000b5a910da", "name": "Minbyul Jeong", "hidden": false}, {"_id": "6966474587c71000b5a910db", "name": "SeHwan Joo", "hidden": false}, {"_id": "6966474587c71000b5a910dc", "name": "Minsoo Khang", "hidden": false}, {"_id": "6966474587c71000b5a910dd", "name": "Eunwon Kim", "hidden": false}, {"_id": "6966474587c71000b5a910de", "name": "Minjeong Kim", "hidden": false}, {"_id": "6966474587c71000b5a910df", "name": "Sujeong Kim", "hidden": false}, {"_id": "6966474587c71000b5a910e0", "name": "Yunsu Kim", "hidden": false}, {"_id": "6966474587c71000b5a910e1", "name": "Hyeonju Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e2", "name": "Seunghyun Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e3", "name": "Sukyung Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e4", "name": "Siyoung Park", "hidden": false}, {"_id": "6966474587c71000b5a910e5", "name": "Gyungin Shin", "hidden": false}, {"_id": "6966474587c71000b5a910e6", "user": {"_id": "64f04fa29a957782e2224dea", "avatarUrl": "/avatars/db853c30ceb59ddabc9a83dc25845690.svg", "isPro": false, "fullname": "Inseo Song", "user": "SSON9", "type": "user"}, "name": "Inseo Song", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:52:53.501Z", "hidden": false}, {"_id": "6966474587c71000b5a910e7", "name": "Wonho Song", "hidden": false}, {"_id": "6966474587c71000b5a910e8", "name": "Seonghoon Yang", "hidden": false}, {"_id": "6966474587c71000b5a910e9", "user": {"_id": "66e0d4bf290df82f137de44c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e0d4bf290df82f137de44c/RJ43RxY56_OvZmauF88Tw.jpeg", "isPro": false, "fullname": "Kyle Yi", "user": "younatics", "type": "user"}, "name": "Seungyoun Yi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T12:42:43.424Z", "hidden": false}, {"_id": "6966474587c71000b5a910ea", "name": "Sanghoon Yoon", "hidden": false}, {"_id": "6966474587c71000b5a910eb", "name": "Jeonghyun Ko", "hidden": false}, {"_id": "6966474587c71000b5a910ec", "name": "Seyoung Song", "hidden": false}, {"_id": "6966474587c71000b5a910ed", "name": "Keunwoo Choi", "hidden": false}, {"_id": "6966474587c71000b5a910ee", "name": "Hwalsuk Lee", "hidden": false}, {"_id": "6966474587c71000b5a910ef", "name": "Sunghun Kim", "hidden": false}, {"_id": "6966474587c71000b5a910f0", "name": "Du-Seong Chang", "hidden": false}, {"_id": "6966474587c71000b5a910f1", "name": "Kyunghyun Cho", "hidden": false}, {"_id": "6966474587c71000b5a910f2", "name": "Junsuk Choe", "hidden": false}, {"_id": "6966474587c71000b5a910f3", "name": "Hwaran Lee", "hidden": false}, {"_id": "6966474587c71000b5a910f4", "name": "Jae-Gil Lee", "hidden": false}, {"_id": "6966474587c71000b5a910f5", "name": "KyungTae Lim", "hidden": false}, {"_id": "6966474587c71000b5a910f6", "name": "Alice Oh", "hidden": false}], "publishedAt": "2026-01-11T18:33:09.000Z", "submittedOnDailyAt": "2026-01-14T02:22:03.363Z", "title": "Solar Open Technical Report", "submittedOnDailyBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "isPro": false, "fullname": "Minbyul Jeong", "user": "Minbyul", "type": "user"}, "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.", "upvotes": 50, "discussionId": "6966474587c71000b5a910f7", "ai_summary": "Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.", "ai_keywords": ["Mixture-of-Experts", "language model", "underserved languages", "data synthesis", "progressive curriculum", "reinforcement learning", "SnapPO", "domain-specific data", "quality thresholds", "composition optimization"], "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "summary_zh": "<ul>\n    <li>Solar Open\u662f\u4e00\u4e2a\u9488\u5bf9\u6b20\u53d1\u8fbe\u8bed\u8a00\u76841020\u4ebf\u53c2\u6570\u7684\u53cc\u8bed\u6df7\u5408\u4e13\u5bb6\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u5408\u62104.5\u4e07\u4ebf\u4e2a\u9ad8\u8d28\u91cf\u3001\u7279\u5b9a\u9886\u57df\u548c\u5f3a\u5316\u5b66\u4e60\u5bfc\u5411\u7684\u6570\u636e\u6765\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>Solar Open\u4f18\u5316\u4e86\u6570\u636e\u7684\u7ec4\u5408\u3001\u8d28\u91cf\u6807\u51c6\u548c\u9886\u57df\u8986\u76d6\uff0c\u5904\u7406\u4e8620\u4e07\u4ebf\u4e2a\u6570\u636e\u6807\u8bb0\u3002</li>\n    <li>\u4f7f\u7528SnapPO\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\uff0c\u4ee5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSolar Open\u5c55\u73b0\u4e86\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6b20\u53d1\u8fbe\u8bed\u8a00AI\u5f00\u53d1\u4e2d\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Solar Open is a large language model designed for languages that lack resources, with 102 billion parameters.</li>\n    <li>It tackles challenges like limited data for these languages by creating 4.5 trillion tokens of specialized training data.</li>\n    <li>The model organizes this data using a structured approach to ensure quality and coverage across various topics.</li>\n    <li>It uses a new optimization method called SnapPO to improve reasoning abilities through reinforcement learning.</li>\n    <li>Tests in English and Korean show that Solar Open performs well, proving this method works for developing AI in underserved languages.</li>\n</ul>"}, "publishedAt": "2026-01-11T13:33:09.000Z", "title": "Solar Open Technical Report", "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07022.png", "numComments": 1, "submittedBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "fullname": "Minbyul Jeong", "name": "Minbyul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.04745", "authors": [{"_id": "6964724e138cc47cbd765325", "user": {"_id": "68e4ba9bb3738c567535654e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e4ba9bb3738c567535654e/DmkMgEaKb3N3bnbJYk1cC.png", "isPro": false, "fullname": "wu", "user": "realty2333", "type": "user"}, "name": "Tingyu Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:53:39.763Z", "hidden": false}, {"_id": "6964724e138cc47cbd765326", "user": {"_id": "692d850486aa9dfeebcf10b5", "avatarUrl": "/avatars/6f7782844275f3eec7d8466fab787923.svg", "isPro": false, "fullname": "Zhisheng Chen", "user": "Zhisheng888", "type": "user"}, "name": "Zhisheng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:43.502Z", "hidden": false}, {"_id": "6964724e138cc47cbd765327", "name": "Ziyan Weng", "hidden": false}, {"_id": "6964724e138cc47cbd765328", "user": {"_id": "6776ce2f10eb0715dbb89df6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GH7VYlzgdrUEDlQfW60Ez.png", "isPro": false, "fullname": "Shuhe Wangv2", "user": "Super-shuhe-v2", "type": "user"}, "name": "Shuhe Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:52.050Z", "hidden": false}, {"_id": "6964724e138cc47cbd765329", "user": {"_id": "6411c9c71d87842eedc5ad23", "avatarUrl": "/avatars/b8a06aeafbbf7272a831534c2307d65e.svg", "isPro": false, "fullname": "Chenglong Li", "user": "ChenglongLi", "type": "user"}, "name": "Chenglong Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:57.344Z", "hidden": false}, {"_id": "6964724e138cc47cbd76532a", "name": "Shuo Zhang", "hidden": false}, {"_id": "6964724e138cc47cbd76532b", "name": "Sen Hu", "hidden": false}, {"_id": "6964724e138cc47cbd76532c", "name": "Silin Wu", "hidden": false}, {"_id": "6964724e138cc47cbd76532d", "user": {"_id": "68f287f2faba6f123f8a3b3c", "avatarUrl": "/avatars/58a34b0f45bb34d74f86a638eff7dc94.svg", "isPro": false, "fullname": "Qizhen Lan", "user": "lanqz7766", "type": "user"}, "name": "Qizhen Lan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:03.306Z", "hidden": false}, {"_id": "6964724e138cc47cbd76532e", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:08.549Z", "hidden": false}, {"_id": "6964724e138cc47cbd76532f", "user": {"_id": "6874f7f0f8e67e9b5714adf2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png", "isPro": false, "fullname": "RongHao Chen", "user": "SuPA4ki", "type": "user"}, "name": "Ronghao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:13.928Z", "hidden": false}], "publishedAt": "2026-01-08T09:11:33.000Z", "submittedOnDailyAt": "2026-01-14T05:31:09.992Z", "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.", "upvotes": 46, "discussionId": "6964724f138cc47cbd765330", "githubRepo": "https://github.com/QuantaAlpha/KnowMeBench", "githubRepoAddedBy": "auto", "ai_summary": "Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.", "ai_keywords": ["memory benchmarks", "autobiographical narratives", "retrieval-augmented systems", "factual recall", "subjective state attribution", "principle-level reasoning"], "githubStars": 83, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u7684\u957f\u671f\u8bb0\u5fc6\u57fa\u51c6\u4e3b\u8981\u4f7f\u7528\u591a\u8f6e\u5bf9\u8bdd\u6216\u5408\u6210\u7528\u6237\u5386\u53f2\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4e2a\u4eba\u7406\u89e3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\\BenchName\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u957f\u7bc7\u81ea\u4f20\u53d9\u8ff0\u7684\u516c\u5f00\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u63a8\u65ad\u7a33\u5b9a\u52a8\u673a\u548c\u51b3\u7b56\u539f\u5219\u7684\u4e30\u5bcc\u8bc1\u636e\u3002</li>\n    <li>\\BenchName\u5c06\u6bcf\u4e2a\u53d9\u8ff0\u91cd\u6784\u4e3a\u6709\u65f6\u95f4\u951a\u70b9\u7684\u95ea\u56de\u6d41\uff0c\u5e76\u901a\u8fc7\u4e0e\u8bc1\u636e\u76f8\u5173\u7684\u95ee\u9898\u8bc4\u4f30\u6a21\u578b\u3002</li>\n    <li>\u591a\u6837\u5316\u53d9\u8ff0\u6765\u6e90\u7684\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u4e3b\u8981\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u5728\u65f6\u95f4\u57fa\u7840\u7684\u89e3\u91ca\u548c\u66f4\u9ad8\u5c42\u6b21\u7684\u63a8\u7406\u4e0a\u4ecd\u5b58\u5728\u9519\u8bef\u3002</li>\n    <li>\u8fd9\u8868\u660e\u9700\u8981\u8d85\u8d8a\u68c0\u7d22\u7684\u8bb0\u5fc6\u673a\u5236\u6765\u6539\u8fdb\u6a21\u578b\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current memory benchmarks mostly use dialogues or fake histories, which don't fully test understanding of a person.</li>\n    <li>We introduce \\BenchName, a new benchmark using real-life autobiographical stories to better evaluate understanding.</li>\n    <li>This benchmark focuses on actions, context, and thoughts to reveal motivations and decision-making processes.</li>\n    <li>It includes questions about facts, personal feelings, and reasoning to assess model performance.</li>\n    <li>Our findings show that while some systems improve on factual answers, they struggle with explanations and deeper reasoning, indicating a need for better memory methods.</li>\n</ul>"}, "publishedAt": "2026-01-08T04:11:33.000Z", "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions", "summary": "Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04745.png", "numComments": 1, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08225", "authors": [{"_id": "6967202cc5e371f6b235d1cc", "name": "Jungho Cho", "hidden": false}, {"_id": "6967202cc5e371f6b235d1cd", "name": "Minbyul Jeong", "hidden": false}, {"_id": "6967202cc5e371f6b235d1ce", "user": {"_id": "65446c938737c799e9ad6f83", "avatarUrl": "/avatars/6ade251e01442b14cbf8cd7888358fd1.svg", "isPro": false, "fullname": "Sungrae Park", "user": "sungrae-park", "type": "user"}, "name": "Sungrae Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:29.339Z", "hidden": false}], "publishedAt": "2026-01-13T05:14:09.000Z", "submittedOnDailyAt": "2026-01-14T02:19:55.431Z", "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale", "submittedOnDailyBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "isPro": false, "fullname": "Minbyul Jeong", "user": "Minbyul", "type": "user"}, "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.", "upvotes": 40, "discussionId": "6967202dc5e371f6b235d1cf", "ai_summary": "Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.", "ai_keywords": ["large reasoning models", "multi-turn dialogue generation", "task-oriented simulation", "user simulator", "behavioral rules", "turn-by-turn feedback", "automated task generation", "high-density dataset", "human-agent interaction"], "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u9700\u6c42\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u3002</li>\n    <li>\u73b0\u6709\u7684\u6570\u636e\u96c6\u548c\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u5de5\u5177\u96c6\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u7684\u5f00\u653e\u5f0f\u4eba\u673a\u534f\u4f5c\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528LRM\u6a21\u62df\u5668\u52a8\u6001\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684\u5de5\u5177\u3002</li>\n    <li>\u4f20\u7edf\u7684\u4efb\u52a1\u5bfc\u5411\u8bbe\u8ba1\u5f80\u5f80\u5bfc\u81f4\u4e0e\u7528\u6237\u7684\u4e92\u52a8\u8f83\u5c11\uff0c\u65e0\u6cd5\u751f\u6210\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u5e38\u89c1\u7684\u9ad8\u8f6e\u6b21\u4ea4\u6d41\u3002</li>\n    <li>\u6211\u4eec\u8f6c\u5411\u7528\u6237\u5bfc\u5411\u7684\u6a21\u62df\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u4fc3\u8fdb\u66f4\u771f\u5b9e\u7684\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\uff0c\u9002\u5e94\u5b9e\u9645\u95ee\u9898\u89e3\u51b3\u7684\u9700\u6c42\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The shift to using large reasoning models (LRMs) as independent agents has increased the need for advanced multi-turn tool-use skills.</li>\n    <li>Current datasets are limited because they use fixed tools that can't adapt to complex human-agent collaborations.</li>\n    <li>We created a system to automatically generate realistic dialogues that include specific tools for completing tasks.</li>\n    <li>Our approach focuses on mimicking human interactions, allowing for longer and more meaningful conversations rather than just quick task completion.</li>\n    <li>This system can generate a rich dataset that mirrors real-world interactions by allowing multiple tasks to be completed in one conversation.</li>\n</ul>"}, "publishedAt": "2026-01-13T00:14:09.000Z", "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale", "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08225.png", "numComments": 2, "submittedBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "fullname": "Minbyul Jeong", "name": "Minbyul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24965", "authors": [{"_id": "6958946b832867f253525a3a", "name": "Siyuan Hu", "hidden": false}, {"_id": "6958946b832867f253525a3b", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:53:47.889Z", "hidden": false}, {"_id": "6958946b832867f253525a3c", "user": {"_id": "661ab3da2b14565c7acccf5c", "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg", "isPro": false, "fullname": "Mike Zheng Shou", "user": "AnalMom", "type": "user"}, "name": "Mike Zheng Shou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:51:00.387Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/_OdaniV8mAw_Y2nQ85G_I.mp4"], "publishedAt": "2025-12-31T16:51:14.000Z", "submittedOnDailyAt": "2026-01-14T02:46:22.550Z", "title": "ShowUI-\u03c0: Flow-based Generative Models as GUI Dexterous Hands", "submittedOnDailyBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-\u03c0, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-\u03c0 achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.", "upvotes": 34, "discussionId": "6958946b832867f253525a3f", "projectPage": "https://showlab.github.io/showui-pi", "githubRepo": "https://github.com/showlab/showui-pi", "githubRepoAddedBy": "auto", "githubStars": 18, "organization": {"_id": "63a553c4ce5763e06f78669c", "name": "showlab", "fullname": "Show Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"}, "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aShowUI-\u03c0\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u7075\u6d3b\u5730\u64cd\u4f5c\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u3002</li>\n    <li>\u91c7\u7528\u7edf\u4e00\u7684\u79bb\u6563-\u8fde\u7eed\u52a8\u4f5c\u6a21\u578b\uff0c\u7ed3\u5408\u70b9\u51fb\u548c\u62d6\u52a8\u64cd\u4f5c\uff0c\u9002\u5e94\u4e0d\u540c\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002</li>\n    <li>\u901a\u8fc7\u6d41\u5f0f\u52a8\u4f5c\u751f\u6210\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u52a8\u4f5c\u4e13\u5bb6\uff0c\u4ece\u89c6\u89c9\u89c2\u5bdf\u4e2d\u9884\u6d4b\u5149\u6807\u7684\u5fae\u8c03\uff0c\u786e\u4fdd\u5e73\u6ed1\u7a33\u5b9a\u7684\u8f68\u8ff9\u3002</li>\n    <li>\u6536\u96c6\u548c\u5408\u6210\u4e865\u4e2a\u9886\u57df\uff08\u5982PowerPoint\u548cAdobe Premiere Pro\uff09\u76842\u4e07\u6761\u62d6\u52a8\u8f68\u8ff9\uff0c\u5e76\u63a8\u51fa\u4e86ScreenDrag\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30GUI\u4ee3\u7406\u7684\u62d6\u52a8\u80fd\u529b\u3002</li>\n    <li>ShowUI-\u03c0\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5f97\u520626.98\uff0c\u663e\u793a\u51fa\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\u548c\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ShowUI-\u03c0 is a new model designed to help computers better manipulate graphical user interfaces (GUIs) like humans do.</li>\n    <li>It combines discrete actions (like clicking) with continuous actions (like dragging) in one model, allowing for more flexible interactions.</li>\n    <li>The model uses a special method to predict small cursor movements based on what it sees, making dragging smoother and more stable.</li>\n    <li>Researchers collected and created a large dataset of drag movements from various software tools to train and evaluate the model.</li>\n    <li>ShowUI-\u03c0 outperforms existing GUI agents in drag tasks, showing it can effectively mimic human-like control in digital environments.</li>\n</ul>"}, "publishedAt": "2025-12-31T11:51:14.000Z", "title": "ShowUI-\u03c0: Flow-based Generative Models as GUI Dexterous Hands", "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-\u03c0, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-\u03c0 achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/_OdaniV8mAw_Y2nQ85G_I.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24965.png", "numComments": 1, "submittedBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "fullname": "Qinghong (Kevin) Lin", "name": "KevinQHLin", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 41, "isUserFollowing": false}, "organization": {"_id": "63a553c4ce5763e06f78669c", "name": "showlab", "fullname": "Show Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08079", "authors": [{"_id": "6966fc3cc5e371f6b235d02e", "user": {"_id": "64a627232944e255ef574dda", "avatarUrl": "/avatars/4c2fd5bf922013fe691c6a3e3fa138a2.svg", "isPro": false, "fullname": "Hongjin Qian", "user": "TommyChien", "type": "user"}, "name": "Hongjin Qian", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:24.206Z", "hidden": false}, {"_id": "6966fc3cc5e371f6b235d02f", "name": "Zhao Cao", "hidden": false}, {"_id": "6966fc3cc5e371f6b235d030", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:47.540Z", "hidden": false}], "publishedAt": "2026-01-12T23:44:59.000Z", "submittedOnDailyAt": "2026-01-14T08:49:23.887Z", "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.\n  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.\n  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.", "upvotes": 31, "discussionId": "6966fc3cc5e371f6b235d031", "githubRepo": "https://github.com/qhjqhj00/MemoBrain", "githubRepoAddedBy": "user", "ai_summary": "Memory management in tool-augmented agents is crucial for maintaining coherent, goal-directed reasoning over extended tasks, requiring explicit mechanisms to track and organize reasoning steps within constrained contexts.", "ai_keywords": ["tool-augmented agents", "long-horizon reasoning", "working context", "executive memory model", "dependency-aware memory", "reasoning steps", "cognitive control", "reasoning trajectories", "context budget", "memory pruning", "sub-trajectory folding"], "githubStars": 39, "summary_zh": "<ul>\n    <li>\u590d\u6742\u63a8\u7406\u5728\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\u4e2d\u901a\u5e38\u9700\u8981\u8f83\u957f\u7684\u65f6\u95f4\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u548c\u5de5\u5177\u4e34\u65f6\u6570\u636e\u7684\u79ef\u7d2f\u3002</li>\n    <li>\u6ca1\u6709\u660e\u786e\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u8fd9\u79cd\u79ef\u7d2f\u4f1a\u5f71\u54cd\u903b\u8f91\u8fde\u8d2f\u6027\uff0c\u59a8\u788d\u4efb\u52a1\u7684\u5bf9\u9f50\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MemoBrain\uff0c\u4e00\u4e2a\u7528\u4e8e\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u7684\u6267\u884c\u8bb0\u5fc6\u6a21\u578b\uff0c\u80fd\u591f\u6784\u5efa\u4f9d\u8d56\u5173\u7cfb\u8bb0\u5fc6\uff0c\u6355\u6349\u91cd\u8981\u7684\u4e2d\u95f4\u72b6\u6001\u3002</li>\n    <li>MemoBrain \u5728\u63a8\u7406\u4ee3\u7406\u65c1\u8fb9\u5de5\u4f5c\uff0c\u7ba1\u7406\u63a8\u7406\u8fdb\u7a0b\uff0c\u53bb\u9664\u65e0\u6548\u6b65\u9aa4\uff0c\u6298\u53e0\u5df2\u5b8c\u6210\u7684\u5b50\u8f68\u8ff9\u3002</li>\n    <li>\u6211\u4eec\u5728GAIA\u3001WebWalker\u548cBrowseComp-Plus\u7b49\u957f\u671f\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u4e86MemoBrain\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Complex reasoning with tool-augmented agents can create too much information, making it hard for language models to keep track.</li>\n    <li>Without a proper memory system, this information overload can confuse logical thinking and task focus.</li>\n    <li>MemoBrain is a new memory model designed to help agents manage their reasoning by keeping important information organized.</li>\n    <li>It helps by removing unnecessary steps, summarizing completed tasks, and maintaining a clear focus within a limited memory space.</li>\n    <li>Tests show that MemoBrain improves performance on difficult long-term tasks compared to existing methods.</li>\n</ul>"}, "publishedAt": "2026-01-12T18:44:59.000Z", "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning", "summary": "Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.\n  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.\n  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08079.png", "numComments": 1, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06487", "authors": [{"_id": "69660763fc8c4ecc02c7fab6", "user": {"_id": "66259e3f9277b825c8e11168", "avatarUrl": "/avatars/078ab932f6837f502851b75bc719ab2c.svg", "isPro": false, "fullname": "Zhang", "user": "QiangZhang", "type": "user"}, "name": "Qiang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:53:23.892Z", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fab7", "user": {"_id": "61454d930aac1efe3e8d842e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61454d930aac1efe3e8d842e/sQELHgJP6c_547OrT8JZJ.jpeg", "isPro": false, "fullname": "boli", "user": "bcol", "type": "user"}, "name": "Boli Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:22.457Z", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fab8", "name": "Fanrui Zhang", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fab9", "name": "Ruixue Ding", "hidden": false}, {"_id": "69660763fc8c4ecc02c7faba", "name": "Shihang Wang", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fabb", "user": {"_id": "657429d833e5a4bf5b278615", "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg", "isPro": false, "fullname": "QiuchenWang", "user": "Qiuchen-Wang", "type": "user"}, "name": "Qiuchen Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:52:59.514Z", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fabc", "name": "Yinfeng Huang", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fabd", "user": {"_id": "623437bd4f78e3acb8bd14bd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623437bd4f78e3acb8bd14bd/6ZIRpE9We4TiMtzDBtPFS.jpeg", "isPro": false, "fullname": "Haonan Zhang", "user": "haonanzhang", "type": "user"}, "name": "Haonan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:53:47.932Z", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fabe", "name": "Rongxiang Zhu", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fabf", "name": "Pengyong Wang", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fac0", "name": "Ailin Ren", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fac1", "name": "Xin Li", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fac2", "name": "Pengjun Xie", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fac3", "name": "Jiawei Liu", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fac4", "name": "Ning Guo", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fac5", "name": "Jingren Zhou", "hidden": false}, {"_id": "69660763fc8c4ecc02c7fac6", "name": "Zheng-Jun Zha", "hidden": false}], "publishedAt": "2026-01-10T08:43:07.000Z", "submittedOnDailyAt": "2026-01-14T01:12:42.456Z", "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking", "submittedOnDailyBy": {"_id": "657429d833e5a4bf5b278615", "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg", "isPro": false, "fullname": "QiuchenWang", "user": "Qiuchen-Wang", "type": "user"}, "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.", "upvotes": 30, "discussionId": "69660764fc8c4ecc02c7fac7", "projectPage": "https://tongyi-agent.github.io/blog/arenarl/", "githubRepo": "https://github.com/Alibaba-NLP/qqr", "githubRepoAddedBy": "user", "ai_summary": "Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.", "ai_keywords": ["reinforcement learning", "large language model agents", "reward models", "scalar scores", "discrimination collapse", "intra-group relative ranking", "pairwise evaluation", "multi-level rubrics", "adversarial arena", "tournament-based ranking", "advantage estimation", "efficiency", "precision", "benchmarks", "SFT", "RL training", "multi-dimensional evaluation"], "githubStars": 49, "organization": {"_id": "661f98de142a51d630dbbcc4", "name": "Alibaba-NLP", "fullname": "Alibaba-NLP", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u9a8c\u8bc1\u7ed3\u679c\u7684\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u4f46\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\uff08\u5982\u590d\u6742\u65c5\u884c\u89c4\u5212\uff09\u4e2d\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u5956\u52b1\u6a21\u578b\u6765\u4e3a\u5355\u4e2a\u54cd\u5e94\u6253\u5206\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u8bc4\u5206\u5931\u53bb\u8fa8\u522b\u529b\uff0c\u5f71\u54cd\u4f18\u5316\u6548\u679c\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86ArenaRL\uff0c\u5b83\u901a\u8fc7\u7ec4\u5185\u76f8\u5bf9\u6392\u540d\u53d6\u4ee3\u70b9\u6570\u8bc4\u5206\uff0c\u4f7f\u7528\u591a\u7ea7\u8bc4\u5206\u6807\u51c6\u6765\u5bf9\u8f68\u8ff9\u8fdb\u884c\u7ec6\u81f4\u8bc4\u5206\u3002</li>\n    <li>ArenaRL\u6784\u5efa\u4e86\u4e00\u4e2a\u5bf9\u6297\u6027\u7ade\u6280\u573a\u548c\u57fa\u4e8e\u6bd4\u8d5b\u7684\u6392\u540d\u65b9\u6848\uff0c\u4ee5\u83b7\u53d6\u66f4\u7a33\u5b9a\u7684\u4f18\u52bf\u4fe1\u53f7\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc1\u660e\uff0cArenaRL\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u660e\u663e\u4f18\u4e8e\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) has improved large language model (LLM) agents in tasks with clear outcomes, but struggles with complex, open-ended tasks like travel planning.</li>\n    <li>Current RL methods often use pointwise scoring, which can lead to difficulties in distinguishing between different solutions, causing scores to cluster too closely together.</li>\n    <li>ArenaRL is a new RL approach that focuses on comparing solutions within groups instead of using single scores, which helps improve the evaluation of different paths.</li>\n    <li>This method uses a tournament-style ranking system to provide more accurate advantage signals without the high computational costs of traditional pairwise comparisons.</li>\n    <li>Two new benchmarks, Open-Travel and Open-DeepResearch, were created to test RL in open-ended tasks and show that ArenaRL significantly outperforms standard RL methods.</li>\n</ul>"}, "publishedAt": "2026-01-10T03:43:07.000Z", "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking", "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06487.png", "numComments": 1, "submittedBy": {"_id": "657429d833e5a4bf5b278615", "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg", "fullname": "QiuchenWang", "name": "Qiuchen-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "661f98de142a51d630dbbcc4", "name": "Alibaba-NLP", "fullname": "Alibaba-NLP", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08584", "authors": [{"_id": "6967031ec5e371f6b235d068", "name": "Alexander H. Liu", "hidden": false}, {"_id": "6967031ec5e371f6b235d069", "name": "Kartik Khandelwal", "hidden": false}, {"_id": "6967031ec5e371f6b235d06a", "user": {"_id": "627bf27cf19c5eb46d54cea8", "avatarUrl": "/avatars/b8ca0b4e841858c1d234671187234f56.svg", "isPro": false, "fullname": "Sandeep Subramanian", "user": "MaximumEntropy", "type": "user"}, "name": "Sandeep Subramanian", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:51:19.735Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d06b", "user": {"_id": "686e4868213b9b0627a4d4a0", "avatarUrl": "/avatars/e3ec63c0944dacafca8a3eaff84da061.svg", "isPro": false, "fullname": "Victor Jouault", "user": "victorjouault", "type": "user"}, "name": "Victor Jouault", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:51:25.262Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d06c", "name": "Abhinav Rastogi", "hidden": false}, {"_id": "6967031ec5e371f6b235d06d", "user": {"_id": "64d5010b050438e3b92247ad", "avatarUrl": "/avatars/c527bba4869617b468bd8b90a5d77d7f.svg", "isPro": false, "fullname": "adrien sade", "user": "sade-adrien", "type": "user"}, "name": "Adrien Sad\u00e9", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:51:34.426Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d06e", "user": {"_id": "6835a7b06cf009790a12a724", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8VcjSbqPctm8NU8bTExf4.png", "isPro": false, "fullname": "Alan Jeffares", "user": "alanjeffares", "type": "user"}, "name": "Alan Jeffares", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:51:42.957Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d06f", "name": "Albert Jiang", "hidden": false}, {"_id": "6967031ec5e371f6b235d070", "name": "Alexandre Cahill", "hidden": false}, {"_id": "6967031ec5e371f6b235d071", "name": "Alexandre Gavaudan", "hidden": false}, {"_id": "6967031ec5e371f6b235d072", "user": {"_id": "6391c7456176fbc67b9ecd2c", "avatarUrl": "/avatars/4aa9be94d8284118b8018362abf11f01.svg", "isPro": false, "fullname": "Alexandre Sablayrolles", "user": "alexsablay", "type": "user"}, "name": "Alexandre Sablayrolles", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:52:00.082Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d073", "name": "Am\u00e9lie H\u00e9liou", "hidden": false}, {"_id": "6967031ec5e371f6b235d074", "user": {"_id": "63c0eeeb3bdc86f8108e22e7", "avatarUrl": "/avatars/0aab18e9d3c1f7221434ad1f4de530b1.svg", "isPro": false, "fullname": "Amos You", "user": "amosyou", "type": "user"}, "name": "Amos You", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:52:10.792Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d075", "user": {"_id": "62cdbd1ba9be5c19555f7ffc", "avatarUrl": "/avatars/d7d60a8628127f62b276bc1815938ea9.svg", "isPro": false, "fullname": "Andy Ehrenberg", "user": "andyehrenberg", "type": "user"}, "name": "Andy Ehrenberg", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:52:17.987Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d076", "name": "Andy Lo", "hidden": false}, {"_id": "6967031ec5e371f6b235d077", "name": "Anton Eliseev", "hidden": false}, {"_id": "6967031ec5e371f6b235d078", "user": {"_id": "66e184e86048d62cd8fb4e52", "avatarUrl": "/avatars/bc633262648dd5335171e5372552d67d.svg", "isPro": false, "fullname": "Antonia Calvi", "user": "NinaCalvi", "type": "user"}, "name": "Antonia Calvi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:52:31.745Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d079", "user": {"_id": "630df38664f1f8d0c76c01fd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661858687827-noauth.jpeg", "isPro": false, "fullname": "Avinash Sooriyarachchi", "user": "AviSoori1x", "type": "user"}, "name": "Avinash Sooriyarachchi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:52:37.953Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d07a", "user": {"_id": "64c1b9ab203c836866dcb1dc", "avatarUrl": "/avatars/3f400cbabcf0b5ed71761f98a4ce858a.svg", "isPro": false, "fullname": "Baptiste Bout", "user": "baptiste-bt", "type": "user"}, "name": "Baptiste Bout", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:52:44.142Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d07b", "name": "Baptiste Rozi\u00e8re", "hidden": false}, {"_id": "6967031ec5e371f6b235d07c", "name": "Baudouin De Monicault", "hidden": false}, {"_id": "6967031ec5e371f6b235d07d", "user": {"_id": "6818899e66f9244540972090", "avatarUrl": "/avatars/95a79dceedbf91760ef5ec5be92bc7b3.svg", "isPro": false, "fullname": "Clemence Lanfranchi", "user": "ClemenceMistral", "type": "user"}, "name": "Cl\u00e9mence Lanfranchi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:53:01.383Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d07e", "user": {"_id": "6368fd9c5774ddb7e8effa3e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6368fd9c5774ddb7e8effa3e/d2BcekRfTUX-X5VDC_wbi.jpeg", "isPro": false, "fullname": "Corentin Barreau", "user": "CorentinBarreau", "type": "user"}, "name": "Corentin Barreau", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:53:08.522Z", "hidden": false}, {"_id": "6967031ec5e371f6b235d07f", "name": "Cyprien Courtot", "hidden": false}, {"_id": "6967031ec5e371f6b235d080", "name": "Daniele Grattarola", "hidden": false}, {"_id": "6967031ec5e371f6b235d081", "name": "Darius Dabert", "hidden": false}, {"_id": "6967031ec5e371f6b235d082", "name": "Diego de las Casas", "hidden": false}, {"_id": "6967031ec5e371f6b235d083", "name": "Elliot Chane-Sane", "hidden": false}, {"_id": "6967031ec5e371f6b235d084", "name": "Faruk Ahmed", "hidden": false}, {"_id": "6967031ec5e371f6b235d085", "name": "Gabrielle Berrada", "hidden": false}, {"_id": "6967031ec5e371f6b235d086", "name": "Ga\u00ebtan Ecrepont", "hidden": false}, {"_id": "6967031ec5e371f6b235d087", "name": "Gauthier Guinet", "hidden": false}, {"_id": "6967031ec5e371f6b235d088", "name": "Georgii Novikov", "hidden": false}, {"_id": "6967031ec5e371f6b235d089", "name": "Guillaume Kunsch", "hidden": false}, {"_id": "6967031ec5e371f6b235d08a", "name": "Guillaume Lample", "hidden": false}, {"_id": "6967031ec5e371f6b235d08b", "name": "Guillaume Martin", "hidden": false}, {"_id": "6967031ec5e371f6b235d08c", "name": "Gunshi Gupta", "hidden": false}, {"_id": "6967031ec5e371f6b235d08d", "name": "Jan Ludziejewski", "hidden": false}, {"_id": "6967031ec5e371f6b235d08e", "name": "Jason Rute", "hidden": false}, {"_id": "6967031ec5e371f6b235d08f", "name": "Joachim Studnia", "hidden": false}, {"_id": "6967031ec5e371f6b235d090", "name": "Jonas Amar", "hidden": false}, {"_id": "6967031ec5e371f6b235d091", "name": "Jos\u00e9phine Delas", "hidden": false}, {"_id": "6967031ec5e371f6b235d092", "name": "Josselin Somerville Roberts", "hidden": false}, {"_id": "6967031ec5e371f6b235d093", "name": "Karmesh Yadav", "hidden": false}, {"_id": "6967031ec5e371f6b235d094", "name": "Khyathi Chandu", "hidden": false}, {"_id": "6967031ec5e371f6b235d095", "name": "Kush Jain", "hidden": false}, {"_id": "6967031ec5e371f6b235d096", "name": "Laurence Aitchison", "hidden": false}, {"_id": "6967031ec5e371f6b235d097", "name": "Laurent Fainsin", "hidden": false}, {"_id": "6967031ec5e371f6b235d098", "name": "L\u00e9onard Blier", "hidden": false}, {"_id": "6967031ec5e371f6b235d099", "name": "Lingxiao Zhao", "hidden": false}, {"_id": "6967031ec5e371f6b235d09a", "name": "Louis Martin", "hidden": false}, {"_id": "6967031ec5e371f6b235d09b", "name": "Lucile Saulnier", "hidden": false}, {"_id": "6967031ec5e371f6b235d09c", "name": "Luyu Gao", "hidden": false}, {"_id": "6967031ec5e371f6b235d09d", "name": "Maarten Buyl", "hidden": false}, {"_id": "6967031ec5e371f6b235d09e", "name": "Margaret Jennings", "hidden": false}, {"_id": "6967031ec5e371f6b235d09f", "name": "Marie Pellat", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a0", "name": "Mark Prins", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a1", "name": "Mathieu Poir\u00e9e", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a2", "name": "Mathilde Guillaumin", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a3", "name": "Matthieu Dinot", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a4", "name": "Matthieu Futeral", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a5", "name": "Maxime Darrin", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a6", "name": "Maximilian Augustin", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a7", "name": "Mia Chiquier", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a8", "name": "Michel Schimpf", "hidden": false}, {"_id": "6967031ec5e371f6b235d0a9", "name": "Nathan Grinsztajn", "hidden": false}, {"_id": "6967031ec5e371f6b235d0aa", "name": "Neha Gupta", "hidden": false}, {"_id": "6967031ec5e371f6b235d0ab", "name": "Nikhil Raghuraman", "hidden": false}, {"_id": "6967031ec5e371f6b235d0ac", "name": "Olivier Bousquet", "hidden": false}, {"_id": "6967031ec5e371f6b235d0ad", "name": "Olivier Duchenne", "hidden": false}, {"_id": "6967031ec5e371f6b235d0ae", "name": "Patricia Wang", "hidden": false}, {"_id": "6967031ec5e371f6b235d0af", "name": "Patrick von Platen", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b0", "name": "Paul Jacob", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b1", "name": "Paul Wambergue", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b2", "name": "Paula Kurylowicz", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b3", "name": "Pavankumar Reddy Muddireddy", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b4", "name": "Philom\u00e8ne Chagniot", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b5", "name": "Pierre Stock", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b6", "name": "Pravesh Agrawal", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b7", "name": "Quentin Torroba", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b8", "name": "Romain Sauvestre", "hidden": false}, {"_id": "6967031ec5e371f6b235d0b9", "name": "Roman Soletskyi", "hidden": false}, {"_id": "6967031ec5e371f6b235d0ba", "name": "Rupert Menneer", "hidden": false}, {"_id": "6967031ec5e371f6b235d0bb", "name": "Sagar Vaze", "hidden": false}, {"_id": "6967031ec5e371f6b235d0bc", "name": "Samuel Barry", "hidden": false}, {"_id": "6967031ec5e371f6b235d0bd", "name": "Sanchit Gandhi", "hidden": false}, {"_id": "6967031ec5e371f6b235d0be", "name": "Siddhant Waghjale", "hidden": false}, {"_id": "6967031ec5e371f6b235d0bf", "name": "Siddharth Gandhi", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c0", "name": "Soham Ghosh", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c1", "name": "Srijan Mishra", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c2", "name": "Sumukh Aithal", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c3", "name": "Szymon Antoniak", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c4", "name": "Teven Le Scao", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c5", "name": "Th\u00e9o Cachet", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c6", "name": "Theo Simon Sorg", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c7", "name": "Thibaut Lavril", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c8", "name": "Thiziri Nait Saada", "hidden": false}, {"_id": "6967031ec5e371f6b235d0c9", "name": "Thomas Chabal", "hidden": false}, {"_id": "6967031ec5e371f6b235d0ca", "name": "Thomas Foubert", "hidden": false}, {"_id": "6967031ec5e371f6b235d0cb", "name": "Thomas Robert", "hidden": false}, {"_id": "6967031ec5e371f6b235d0cc", "name": "Thomas Wang", "hidden": false}, {"_id": "6967031ec5e371f6b235d0cd", "name": "Tim Lawson", "hidden": false}, {"_id": "6967031ec5e371f6b235d0ce", "name": "Tom Bewley", "hidden": false}, {"_id": "6967031ec5e371f6b235d0cf", "name": "Tom Bewley", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d0", "name": "Tom Edwards", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d1", "name": "Umar Jamil", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d2", "name": "Umberto Tomasini", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d3", "name": "Valeriia Nemychnikova", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d4", "name": "Van Phung", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d5", "name": "Vincent Maladi\u00e8re", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d6", "name": "Virgile Richard", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d7", "name": "Wassim Bouaziz", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d8", "name": "Wen-Ding Li", "hidden": false}, {"_id": "6967031ec5e371f6b235d0d9", "name": "William Marshall", "hidden": false}, {"_id": "6967031ec5e371f6b235d0da", "name": "Xinghui Li", "hidden": false}, {"_id": "6967031ec5e371f6b235d0db", "name": "Xinyu Yang", "hidden": false}, {"_id": "6967031ec5e371f6b235d0dc", "name": "Yassine El Ouahidi", "hidden": false}, {"_id": "6967031ec5e371f6b235d0dd", "name": "Yihan Wang", "hidden": false}, {"_id": "6967031ec5e371f6b235d0de", "name": "Yunhao Tang", "hidden": false}, {"_id": "6967031ec5e371f6b235d0df", "name": "Zaccharie Ramzi", "hidden": false}], "publishedAt": "2026-01-13T14:06:03.000Z", "submittedOnDailyAt": "2026-01-14T00:14:55.855Z", "title": "Ministral 3", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.", "upvotes": 28, "discussionId": "6967031ec5e371f6b235d0e0", "ai_summary": "The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.", "ai_keywords": ["parameter-efficient dense language models", "cascade distillation", "iterative pruning", "continued training", "distillation technique", "instruction finetuned", "reasoning model", "image understanding capabilities"], "organization": {"_id": "64edf4004f42c35eea1b1632", "name": "mistralai", "fullname": "Mistral AI_", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faMinistral 3\u7cfb\u5217\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u548c\u5185\u5b58\u53d7\u9650\u7684\u5e94\u7528\u3002</li>\n    <li>\u63d0\u4f9b\u4e09\u79cd\u6a21\u578b\u5927\u5c0f\uff1a3B\u30018B\u548c14B\u53c2\u6570\u3002</li>\n    <li>\u6bcf\u79cd\u6a21\u578b\u90fd\u6709\u4e09\u79cd\u53d8\u4f53\uff1a\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u548c\u590d\u6742\u95ee\u9898\u89e3\u51b3\u6a21\u578b\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u901a\u8fc7\u7ea7\u8054\u84b8\u998f\u6280\u672f\u63a8\u51faMinistral 3\u6a21\u578b\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u6bcf\u4e2a\u6a21\u578b\u5177\u5907\u56fe\u50cf\u7406\u89e3\u80fd\u529b\uff0c\u6240\u6709\u6a21\u578b\u90fd\u5728Apache 2.0\u8bb8\u53ef\u8bc1\u4e0b\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The Ministral 3 series includes efficient language models designed for devices with limited computing power and memory.</li>\n    <li>There are three model sizes available: 3 billion, 8 billion, and 14 billion parameters.</li>\n    <li>Each model size has three versions: a general-purpose base model, an instruction-tuned model, and a reasoning model for solving complex problems.</li>\n    <li>The models are created using a method called Cascade Distillation, which involves iterative training and pruning.</li>\n    <li>All models have image understanding features and are released under the Apache 2.0 license.</li>\n</ul>"}, "publishedAt": "2026-01-13T09:06:03.000Z", "title": "Ministral 3", "summary": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08584.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 208, "isUserFollowing": false}, "organization": {"_id": "64edf4004f42c35eea1b1632", "name": "mistralai", "fullname": "Mistral AI_", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08831", "authors": [{"_id": "6967aa5cc5e371f6b235d2d8", "name": "Yang-Che Sun", "hidden": false}, {"_id": "6967aa5cc5e371f6b235d2d9", "name": "Cheng Sun", "hidden": false}, {"_id": "6967aa5cc5e371f6b235d2da", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "6967aa5cc5e371f6b235d2db", "name": "Fu-En Yang", "hidden": false}, {"_id": "6967aa5cc5e371f6b235d2dc", "name": "Min-Hung Chen", "hidden": false}, {"_id": "6967aa5cc5e371f6b235d2dd", "name": "Yen-Yu Lin", "hidden": false}, {"_id": "6967aa5cc5e371f6b235d2de", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/qT8bYUbrv5gN2DdSwhj8x.mp4"], "publishedAt": "2026-01-13T18:59:54.000Z", "submittedOnDailyAt": "2026-01-14T12:09:27.723Z", "title": "3AM: Segment Anything with Geometric Consistency in Videos", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/", "upvotes": 21, "discussionId": "6967aa5dc5e371f6b235d2df", "projectPage": "https://jayisaking.github.io/3AM-Page/", "ai_summary": "3AM enhances video object segmentation by integrating 3D-aware features from MUSt3R into SAM2, achieving improved viewpoint consistency with only RGB input at inference.", "ai_keywords": ["SAM2", "MUSt3R", "3D-aware features", "Feature Merger", "multi-level features", "geometric correspondence", "spatial position", "visual similarity", "field-of-view aware sampling", "wide-baseline motion", "IoU", "Positive IoU"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u7269\u4f53\u5206\u5272\u65b9\u6cd5\u5728\u5927\u89c6\u89d2\u53d8\u5316\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf3D\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u9700\u8981\u76f8\u673a\u4f4d\u59ff\u548c\u6df1\u5ea6\u56fe\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e863AM\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c06MUSt3R\u76843D\u7279\u5f81\u4e0eSAM2\u7ed3\u5408\u7684\u8bad\u7ec3\u65f6\u589e\u5f3a\u65b9\u6cd5\u3002</li>\n    <li>3AM\u901a\u8fc7\u878d\u5408\u591a\u5c42MUSt3R\u7279\u5f81\u548cSAM2\u7684\u5916\u89c2\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u4e00\u81f4\u7684\u8bc6\u522b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u53ea\u9700RGB\u8f93\u5165\uff0c\u4e0d\u9700\u8981\u76f8\u673a\u4f4d\u59ff\u6216\u9884\u5904\u7406\u3002</li>\n    <li>\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0c3AM\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8eSAM2\uff0c\u63d0\u5347\u4e8690.6%\u7684IoU\u548c71.7%\u7684\u6b63IoU\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video object segmentation methods like SAM2 work well but struggle with big changes in viewpoint.</li>\n    <li>3AM is a new approach that combines 3D features from MUSt3R with SAM2 to improve performance.</li>\n    <li>This method uses a feature merger to blend different levels of features, improving recognition based on geometry and appearance.</li>\n    <li>It includes a smart sampling strategy to ensure consistent object regions are observed, aiding 3D learning.</li>\n    <li>3AM only needs RGB images during testing, without requiring extra data like camera positions, and shows significant improvement over previous methods in challenging datasets.</li>\n</ul>"}, "publishedAt": "2026-01-13T13:59:54.000Z", "title": "3AM: Segment Anything with Geometric Consistency in Videos", "summary": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/qT8bYUbrv5gN2DdSwhj8x.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08831.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.07264", "authors": [{"_id": "69671055c5e371f6b235d154", "user": {"_id": "65b8909c89eb3dfbe8d26780", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg", "isPro": false, "fullname": "Weihao XUAN", "user": "weihao1115", "type": "user"}, "name": "Weihao Xuan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:51:23.840Z", "hidden": false}, {"_id": "69671055c5e371f6b235d155", "name": "Qingcheng Zeng", "hidden": false}, {"_id": "69671055c5e371f6b235d156", "name": "Heli Qi", "hidden": false}, {"_id": "69671055c5e371f6b235d157", "user": {"_id": "67c8af4885b7a86d7515b77c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4bRu0MKg6voGX5fyqninn.png", "isPro": false, "fullname": "Lorenzo Xiao", "user": "lrzneedresearch", "type": "user"}, "name": "Yunze Xiao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T12:47:43.795Z", "hidden": false}, {"_id": "69671055c5e371f6b235d158", "user": {"_id": "65aaf7bb494b2faa870f04d2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5twYdL2f-2h3Dx0BhC7HW.png", "isPro": false, "fullname": "JJ Wang", "user": "junjuewang", "type": "user"}, "name": "Junjue Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:54:17.992Z", "hidden": false}, {"_id": "69671055c5e371f6b235d159", "user": {"_id": "635c8bafe3737b9e4e29406a", "avatarUrl": "/avatars/884e8e086fe374512ed6956ab59038af.svg", "isPro": false, "fullname": "naoto yoko", "user": "Naotoyokoyama", "type": "user"}, "name": "Naoto Yokoya", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:54:24.392Z", "hidden": false}], "publishedAt": "2026-01-12T07:10:35.000Z", "submittedOnDailyAt": "2026-01-14T01:18:38.163Z", "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents", "submittedOnDailyBy": {"_id": "63bc77661374e3ef9135735f", "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg", "isPro": false, "fullname": "Qingcheng Zeng", "user": "qcz", "type": "user"}, "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", "upvotes": 21, "discussionId": "69671055c5e371f6b235d15a", "ai_summary": "Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.", "ai_keywords": ["large language models", "tool-integrated agents", "calibration", "reinforcement learning", "reward design", "verbalized calibration", "evidence tools", "verification tools"], "summary_zh": "<ul>\n    <li>\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u4ee3\u7406\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u786e\u4fdd\u5176\u53ef\u4fe1\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002</li>\n    <li>\u53ef\u4fe1\u6027\u7684\u4e00\u4e2a\u91cd\u8981\u56e0\u7d20\u662f\u6821\u51c6\uff0c\u5373\u4ee3\u7406\u8868\u8fbe\u7684\u81ea\u4fe1\u7a0b\u5ea6\u5e94\u4e0e\u5176\u5b9e\u9645\u8868\u73b0\u76f8\u7b26\u3002</li>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u53e3\u5934\u6821\u51c6\uff0c\u53d1\u73b0\u4e0d\u540c\u5de5\u5177\u7c7b\u578b\u5bfc\u81f4\u4e86\u81ea\u4fe1\u5fc3\u7684\u663e\u8457\u5dee\u5f02\u3002</li>\n    <li>\u8bc1\u636e\u5de5\u5177\uff08\u5982\u7f51\u7edc\u641c\u7d22\uff09\u4f1a\u5f15\u53d1\u4e25\u91cd\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u800c\u9a8c\u8bc1\u5de5\u5177\uff08\u5982\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u5219\u80fd\u901a\u8fc7\u786e\u5b9a\u6027\u53cd\u9988\u6765\u51cf\u5c11\u6821\u51c6\u5931\u8bef\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u4efb\u52a1\u51c6\u786e\u6027\u548c\u6821\u51c6\uff0c\u5f3a\u8c03\u4e86\u7279\u5b9a\u9886\u57df\u6821\u51c6\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autonomous agents using large language models (LLMs) are improving in handling complex tasks, but trustworthiness is still a big concern.</li>\n    <li>Calibration is important; it means that an agent should express confidence that matches its actual performance.</li>\n    <li>Research shows that different tools affect how well agents are calibrated, with evidence tools causing overconfidence and verification tools helping improve accuracy.</li>\n    <li>A new reinforcement learning framework is proposed to enhance both task accuracy and calibration for these agents.</li>\n    <li>The study suggests that specialized calibration strategies are needed for different tools, helping agents communicate uncertainty better in real-world situations.</li>\n</ul>"}, "publishedAt": "2026-01-12T02:10:35.000Z", "title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents", "summary": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07264.png", "numComments": 1, "submittedBy": {"_id": "63bc77661374e3ef9135735f", "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg", "fullname": "Qingcheng Zeng", "name": "qcz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u73b0\u5b9e\u4e2d\u7684\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u5206\u6563\u5728\u4e92\u8054\u7f51\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u7684\u5f00\u653e\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u8981\u6c42\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u4e92\u52a8\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u201cAgentic\u201d\u65b9\u6cd5\u7684\u8868\u73b0\u4e0d\u603b\u662f\u4f18\u4e8e\u201cWorkflow\u201d\uff0c\u5176\u4f18\u52bf\u4f9d\u8d56\u4e8e\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u4e3a\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7814\u7a76\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u4e0b\u4e00\u4ee3\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Real-world video question answering needs to extract visual clues from videos and find answers from the web.</li>\n    <li>VideoDR is a new benchmark designed to help with video-based question answering, focusing on video and web interactions.</li>\n    <li>The benchmark includes high-quality samples across six different topics, created with careful human review.</li>\n    <li>Tests showed that two approaches, Workflow and Agentic, perform differently, with Agentic's success depending on how well it manages video clues during searches.</li>\n    <li>The study identifies challenges like maintaining focus and consistency over long searches as key issues for future video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u4eba\u7c7b\u5728\u638c\u63e1\u8bed\u8a00\u4e4b\u524d\u5c31\u5177\u5907\u57fa\u672c\u7684\u89c6\u89c9\u6280\u80fd\uff0c\u4f46\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u5176\u8106\u5f31\u7684\u89c6\u89c9\u7406\u89e3\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u4e00\u4e9b\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4e0d\u53ca\u4eba\u7c7b\uff0c\u751a\u81f3\u4e09\u5c81\u7684\u5c0f\u5b69\u90fd\u80fd\u8f7b\u677e\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u4e86\u7814\u7a76\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u72ec\u7acb\u4e8e\u8bed\u8a00\u77e5\u8bc6\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\uff0c\u6db5\u76d6\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\uff0c\u7ed3\u679c\u663e\u793a\u9886\u5148\u7684MLLMs\u7684\u8868\u73b0\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\u3002</li>\n    <li>\u5c3d\u7ba1\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5f53\u524d\u7684MLLMs\u4ecd\u7f3a\u4e4f\u57fa\u672c\u7684\u89c6\u89c9\u80fd\u529b\uff0cBabyVision\u7684\u8fdb\u5c55\u662f\u671d\u5411\u4eba\u7c7b\u6c34\u5e73\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u4e00\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills early, but modern Multimodal LLMs struggle with basic visual tasks.</li>\n    <li>We created BabyVision, a benchmark to test visual abilities of MLLMs without relying on language.</li>\n    <li>BabyVision includes 388 tasks across 22 subclasses and four categories.</li>\n    <li>Top MLLMs, like Gemini3-Pro-Preview, score much lower than human averages, indicating poor visual understanding.</li>\n    <li>BabyVision aims to improve visual perception and reasoning in AI and includes a new tool for visual reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5b9a\u4f4d\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5ffd\u7565\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\uff0c\u8d4b\u4e88\u5176\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4ee3\u7406-\u5730\u56fe\u5faa\u73af\u3002</li>\n    <li>\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\uff0c\u4ee5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u548c\u63a2\u7d22\u591a\u4e2a\u5019\u9009\u8def\u5f84\u3002</li>\n    <li>\u5728\u5b9e\u9645\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The task of image geolocalization is to find out where a photo was taken on Earth using visual clues.</li>\n    <li>This study introduces a new way to help models use maps, which is a common human strategy, by creating a \"Thinking with Map\" ability.</li>\n    <li>The approach includes two steps: using reinforcement learning to improve decision-making and a method to test multiple options before making a final guess.</li>\n    <li>To test the new method, the researchers created MAPBench, a benchmark using real-world images for training and evaluation.</li>\n    <li>Results show that this new method performs better than existing models, especially increasing accuracy significantly in predictions.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u7528\u6237\u5e0c\u671b\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u5e76\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u5f15\u5165\u591a\u4e2a\u5956\u52b1\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u671d\u7740\u671f\u671b\u7684\u884c\u4e3a\u53d1\u5c55\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u76f4\u63a5\u5e94\u7528\u201c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u201d\uff08GRPO\uff09\u53ef\u80fd\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u4ef7\u503c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u201c\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316\u201d\uff08GDPO\uff09\uff0c\u5b83\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u5956\u52b1\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Language models need to provide accurate responses and behave according to different human preferences in various situations.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to guide models, but a method called Group Relative Policy Optimization (GRPO) may not work well for this.</li>\n    <li>Applying GRPO can lead to problems like reduced training effectiveness and failures during training.</li>\n    <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that avoids these issues by treating rewards separately.</li>\n    <li>GDPO is shown to perform better than GRPO in tasks like tool calling, math reasoning, and coding reasoning, improving both accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05593", "authors": [{"_id": "6965b990fc8c4ecc02c7f8df", "name": "Jingcheng Hu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e0", "name": "Yinmin Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e1", "name": "Shijie Shang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e2", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e3", "name": "Yue Peng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e4", "name": "Zhewei Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e5", "name": "Hebin Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e6", "name": "Xin Wu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e7", "name": "Jie Cheng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e8", "name": "Fanqi Wan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e9", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ea", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8eb", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ec", "name": "Ailin Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ed", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ee", "name": "Qi Han", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ef", "name": "Zheng Ge", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f0", "name": "Daxin Jiang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f1", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f2", "name": "Heung-Yeung Shum", "hidden": false}], "publishedAt": "2026-01-09T07:24:43.000Z", "submittedOnDailyAt": "2026-01-13T00:51:45.124Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "upvotes": 62, "discussionId": "6965b990fc8c4ecc02c7f8f3", "githubRepo": "https://github.com/stepfun-ai/PaCoRe", "githubRepoAddedBy": "user", "ai_summary": "Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.", "ai_keywords": ["test-time compute", "sequential reasoning", "parallel exploration", "message-passing architecture", "reinforcement learning", "multi-million-token", "HMMT 2025", "GPT-5"], "githubStars": 261, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPaCoRe\u7684\u5e76\u884c\u534f\u8c03\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>PaCoRe\u91c7\u7528\u591a\u8f6e\u6d88\u606f\u4f20\u9012\u67b6\u6784\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5e76\u884c\u63a2\u7d22\u6765\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u95f4\u8ba1\u7b97\uff08TTC\uff09\u3002</li>\n    <li>\u6bcf\u4e00\u8f6e\u4f1a\u542f\u52a8\u591a\u4e2a\u5e76\u884c\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u5c06\u7ed3\u679c\u538b\u7f29\u6210\u6d88\u606f\uff0c\u4ee5\u6307\u5bfc\u4e0b\u4e00\u8f6e\u63a8\u7406\uff0c\u6700\u7ec8\u4ea7\u751f\u7b54\u6848\u3002</li>\n    <li>\u7ecf\u8fc7\u57fa\u4e8e\u7ed3\u679c\u7684\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5904\u7406\u591a\u767e\u4e07\u4e2a\u6709\u6548TTC\u800c\u4e0d\u8d85\u51fa\u4e0a\u4e0b\u6587\u9650\u5236\u3002</li>\n    <li>\u6b64\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7cfb\u7edf\uff0c\u5f00\u6e90\u4e86\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u8bad\u7ec3\u6570\u636e\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>PaCoRe is a new framework for training and using language models that improves their reasoning capabilities.</li>\n  <li>Unlike traditional models that reason sequentially, PaCoRe uses parallel reasoning and message-passing to enhance performance.</li>\n  <li>This approach allows the model to handle much larger amounts of information without being limited by context size.</li>\n  <li>It has shown significant improvements in various fields, especially in mathematics, outperforming existing models like GPT-5.</li>\n  <li>The creators are sharing their model and training resources to support further research and development.</li>\n</ul>"}, "publishedAt": "2026-01-09T02:24:43.000Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05593.png", "numComments": 1, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06789", "authors": [{"_id": "69671036c5e371f6b235d143", "user": {"_id": "692881094c3f4293dfe29e3d", "avatarUrl": "/avatars/bddfaae8041a45498d46ef65ba17c920.svg", "isPro": false, "fullname": "qihao wang", "user": "jimson991", "type": "user"}, "name": "Qihao Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:07.079Z", "hidden": false}, {"_id": "69671036c5e371f6b235d144", "user": {"_id": "64b74fca17570fdff9b2aded", "avatarUrl": "/avatars/8b3519a7011af52dadc87ffef700c77c.svg", "isPro": false, "fullname": "Ziming Cheng", "user": "cadche", "type": "user"}, "name": "Ziming Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:13.370Z", "hidden": false}, {"_id": "69671036c5e371f6b235d145", "user": {"_id": "6513ee3c9af40a65586b43f5", "avatarUrl": "/avatars/815ed3876cefa12b25bf955edcbf71a3.svg", "isPro": false, "fullname": "shuo zhang", "user": "shuozhang", "type": "user"}, "name": "Shuo Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:18.640Z", "hidden": false}, {"_id": "69671036c5e371f6b235d146", "name": "Fan Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d147", "name": "Rui Xu", "hidden": false}, {"_id": "69671036c5e371f6b235d148", "name": "Heng Lian", "hidden": false}, {"_id": "69671036c5e371f6b235d149", "user": {"_id": "65bb3c545a5dbabc818e9044", "avatarUrl": "/avatars/4c239557bd5e33179cbf4f3a440bbf33.svg", "isPro": false, "fullname": "Kunyi Wang", "user": "KunyiWang", "type": "user"}, "name": "Kunyi Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:25.411Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14a", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaoming Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:47.145Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14b", "name": "Jianghao Yin", "hidden": false}, {"_id": "69671036c5e371f6b235d14c", "name": "Sen Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14d", "name": "Yue Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14e", "user": {"_id": "64803e5dc57f629056c601f1", "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg", "isPro": false, "fullname": "Shaolei Zhang", "user": "zhangshaolei", "type": "user"}, "name": "Shaolei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:13.491Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14f", "name": "Yanbing Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d150", "user": {"_id": "6874f7f0f8e67e9b5714adf2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png", "isPro": false, "fullname": "RongHao Chen", "user": "SuPA4ki", "type": "user"}, "name": "Ronghao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:02.800Z", "hidden": false}, {"_id": "69671036c5e371f6b235d151", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:57.426Z", "hidden": false}], "publishedAt": "2026-01-11T06:41:26.000Z", "submittedOnDailyAt": "2026-01-14T01:40:39.607Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "upvotes": 59, "discussionId": "69671036c5e371f6b235d152", "githubRepo": "https://github.com/QuantaAlpha/MemGovern", "githubRepoAddedBy": "user", "ai_summary": "MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.", "ai_keywords": ["autonomous software engineering", "SWE agents", "closed-world limitation", "open-world experience", "GitHub", "experience governance", "experience cards", "agentic experience search", "SWE-bench Verified"], "githubStars": 19, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4ee3\u7406\u5728\u7f16\u7a0b\u4e0a\u6709\u5f88\u5927\u8fdb\u6b65\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u201c\u5c01\u95ed\u4e16\u754c\u201d\u7684\u9650\u5236\u3002</li>\n    <li>\u8fd9\u4e9b\u4ee3\u7406\u5728\u4fee\u590d\u9519\u8bef\u65f6\u5ffd\u89c6\u4e86GitHub\u7b49\u5e73\u53f0\u4e0a\u4e30\u5bcc\u7684\u4eba\u7c7b\u7ecf\u9a8c\u3002</li>\n    <li>\u73b0\u6709\u7684\u95ee\u9898\u8ddf\u8e2a\u6570\u636e\u7ed3\u6784\u4e0d\u5b8c\u5584\uff0c\u9650\u5236\u4e86\u8bbf\u95ee\u8fd9\u4e9b\u7ecf\u9a8c\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86MemGovern\u6846\u67b6\uff0c\u5c06\u539f\u59cbGitHub\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u4f9b\u4ee3\u7406\u4f7f\u7528\u7684\u7ecf\u9a8c\u5361\u7247\u3002</li>\n    <li>MemGovern\u751f\u6210\u4e86135,000\u4e2a\u7ecf\u9a8c\u5361\u7247\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u7387\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autonomous software engineering agents currently struggle with a \"closed-world\" limitation, relying only on local context to fix bugs.</li>\n    <li>They often overlook valuable historical experience found on platforms like GitHub due to unstructured and fragmented issue-tracking data.</li>\n    <li>The paper presents MemGovern, a framework that transforms raw GitHub data into useful knowledge for these agents.</li>\n    <li>MemGovern creates \"experience cards\" from human experience and enables agents to search for this expertise effectively.</li>\n    <li>With 135,000 experience cards, MemGovern improves bug resolution rates by 4.65% and acts as a plug-in for enhanced agent memory systems.</li>\n</ul>"}, "publishedAt": "2026-01-11T01:41:26.000Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06789.png", "numComments": 1, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07022", "authors": [{"_id": "6966474587c71000b5a910d2", "user": {"_id": "65446c938737c799e9ad6f83", "avatarUrl": "/avatars/6ade251e01442b14cbf8cd7888358fd1.svg", "isPro": false, "fullname": "Sungrae Park", "user": "sungrae-park", "type": "user"}, "name": "Sungrae Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:41:08.385Z", "hidden": false}, {"_id": "6966474587c71000b5a910d3", "name": "Sanghoon Kim", "hidden": false}, {"_id": "6966474587c71000b5a910d4", "name": "Jungho Cho", "hidden": false}, {"_id": "6966474587c71000b5a910d5", "name": "Gyoungjin Gim", "hidden": false}, {"_id": "6966474587c71000b5a910d6", "name": "Dawoon Jung", "hidden": false}, {"_id": "6966474587c71000b5a910d7", "name": "Mikyoung Cha", "hidden": false}, {"_id": "6966474587c71000b5a910d8", "name": "Eunhae Choo", "hidden": false}, {"_id": "6966474587c71000b5a910d9", "name": "Taekgyu Hong", "hidden": false}, {"_id": "6966474587c71000b5a910da", "name": "Minbyul Jeong", "hidden": false}, {"_id": "6966474587c71000b5a910db", "name": "SeHwan Joo", "hidden": false}, {"_id": "6966474587c71000b5a910dc", "name": "Minsoo Khang", "hidden": false}, {"_id": "6966474587c71000b5a910dd", "name": "Eunwon Kim", "hidden": false}, {"_id": "6966474587c71000b5a910de", "name": "Minjeong Kim", "hidden": false}, {"_id": "6966474587c71000b5a910df", "name": "Sujeong Kim", "hidden": false}, {"_id": "6966474587c71000b5a910e0", "name": "Yunsu Kim", "hidden": false}, {"_id": "6966474587c71000b5a910e1", "name": "Hyeonju Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e2", "name": "Seunghyun Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e3", "name": "Sukyung Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e4", "name": "Siyoung Park", "hidden": false}, {"_id": "6966474587c71000b5a910e5", "name": "Gyungin Shin", "hidden": false}, {"_id": "6966474587c71000b5a910e6", "user": {"_id": "64f04fa29a957782e2224dea", "avatarUrl": "/avatars/db853c30ceb59ddabc9a83dc25845690.svg", "isPro": false, "fullname": "Inseo Song", "user": "SSON9", "type": "user"}, "name": "Inseo Song", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:52:53.501Z", "hidden": false}, {"_id": "6966474587c71000b5a910e7", "name": "Wonho Song", "hidden": false}, {"_id": "6966474587c71000b5a910e8", "name": "Seonghoon Yang", "hidden": false}, {"_id": "6966474587c71000b5a910e9", "user": {"_id": "66e0d4bf290df82f137de44c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e0d4bf290df82f137de44c/RJ43RxY56_OvZmauF88Tw.jpeg", "isPro": false, "fullname": "Kyle Yi", "user": "younatics", "type": "user"}, "name": "Seungyoun Yi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T12:42:43.424Z", "hidden": false}, {"_id": "6966474587c71000b5a910ea", "name": "Sanghoon Yoon", "hidden": false}, {"_id": "6966474587c71000b5a910eb", "name": "Jeonghyun Ko", "hidden": false}, {"_id": "6966474587c71000b5a910ec", "name": "Seyoung Song", "hidden": false}, {"_id": "6966474587c71000b5a910ed", "name": "Keunwoo Choi", "hidden": false}, {"_id": "6966474587c71000b5a910ee", "name": "Hwalsuk Lee", "hidden": false}, {"_id": "6966474587c71000b5a910ef", "name": "Sunghun Kim", "hidden": false}, {"_id": "6966474587c71000b5a910f0", "name": "Du-Seong Chang", "hidden": false}, {"_id": "6966474587c71000b5a910f1", "name": "Kyunghyun Cho", "hidden": false}, {"_id": "6966474587c71000b5a910f2", "name": "Junsuk Choe", "hidden": false}, {"_id": "6966474587c71000b5a910f3", "name": "Hwaran Lee", "hidden": false}, {"_id": "6966474587c71000b5a910f4", "name": "Jae-Gil Lee", "hidden": false}, {"_id": "6966474587c71000b5a910f5", "name": "KyungTae Lim", "hidden": false}, {"_id": "6966474587c71000b5a910f6", "name": "Alice Oh", "hidden": false}], "publishedAt": "2026-01-11T18:33:09.000Z", "submittedOnDailyAt": "2026-01-14T02:22:03.363Z", "title": "Solar Open Technical Report", "submittedOnDailyBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "isPro": false, "fullname": "Minbyul Jeong", "user": "Minbyul", "type": "user"}, "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.", "upvotes": 50, "discussionId": "6966474587c71000b5a910f7", "ai_summary": "Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.", "ai_keywords": ["Mixture-of-Experts", "language model", "underserved languages", "data synthesis", "progressive curriculum", "reinforcement learning", "SnapPO", "domain-specific data", "quality thresholds", "composition optimization"], "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "summary_zh": "<ul>\n    <li>Solar Open\u662f\u4e00\u4e2a\u9488\u5bf9\u6b20\u53d1\u8fbe\u8bed\u8a00\u76841020\u4ebf\u53c2\u6570\u7684\u53cc\u8bed\u6df7\u5408\u4e13\u5bb6\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u5408\u62104.5\u4e07\u4ebf\u4e2a\u9ad8\u8d28\u91cf\u3001\u7279\u5b9a\u9886\u57df\u548c\u5f3a\u5316\u5b66\u4e60\u5bfc\u5411\u7684\u6570\u636e\u6765\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>Solar Open\u4f18\u5316\u4e86\u6570\u636e\u7684\u7ec4\u5408\u3001\u8d28\u91cf\u6807\u51c6\u548c\u9886\u57df\u8986\u76d6\uff0c\u5904\u7406\u4e8620\u4e07\u4ebf\u4e2a\u6570\u636e\u6807\u8bb0\u3002</li>\n    <li>\u4f7f\u7528SnapPO\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\uff0c\u4ee5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSolar Open\u5c55\u73b0\u4e86\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6b20\u53d1\u8fbe\u8bed\u8a00AI\u5f00\u53d1\u4e2d\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Solar Open is a large language model designed for languages that lack resources, with 102 billion parameters.</li>\n    <li>It tackles challenges like limited data for these languages by creating 4.5 trillion tokens of specialized training data.</li>\n    <li>The model organizes this data using a structured approach to ensure quality and coverage across various topics.</li>\n    <li>It uses a new optimization method called SnapPO to improve reasoning abilities through reinforcement learning.</li>\n    <li>Tests in English and Korean show that Solar Open performs well, proving this method works for developing AI in underserved languages.</li>\n</ul>"}, "publishedAt": "2026-01-11T13:33:09.000Z", "title": "Solar Open Technical Report", "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07022.png", "numComments": 1, "submittedBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "fullname": "Minbyul Jeong", "name": "Minbyul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.04745", "authors": [{"_id": "6964724e138cc47cbd765325", "user": {"_id": "68e4ba9bb3738c567535654e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e4ba9bb3738c567535654e/DmkMgEaKb3N3bnbJYk1cC.png", "isPro": false, "fullname": "wu", "user": "realty2333", "type": "user"}, "name": "Tingyu Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:53:39.763Z", "hidden": false}, {"_id": "6964724e138cc47cbd765326", "user": {"_id": "692d850486aa9dfeebcf10b5", "avatarUrl": "/avatars/6f7782844275f3eec7d8466fab787923.svg", "isPro": false, "fullname": "Zhisheng Chen", "user": "Zhisheng888", "type": "user"}, "name": "Zhisheng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:43.502Z", "hidden": false}, {"_id": "6964724e138cc47cbd765327", "name": "Ziyan Weng", "hidden": false}, {"_id": "6964724e138cc47cbd765328", "user": {"_id": "6776ce2f10eb0715dbb89df6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GH7VYlzgdrUEDlQfW60Ez.png", "isPro": false, "fullname": "Shuhe Wangv2", "user": "Super-shuhe-v2", "type": "user"}, "name": "Shuhe Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:52.050Z", "hidden": false}, {"_id": "6964724e138cc47cbd765329", "user": {"_id": "6411c9c71d87842eedc5ad23", "avatarUrl": "/avatars/b8a06aeafbbf7272a831534c2307d65e.svg", "isPro": false, "fullname": "Chenglong Li", "user": "ChenglongLi", "type": "user"}, "name": "Chenglong Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:57.344Z", "hidden": false}, {"_id": "6964724e138cc47cbd76532a", "name": "Shuo Zhang", "hidden": false}, {"_id": "6964724e138cc47cbd76532b", "name": "Sen Hu", "hidden": false}, {"_id": "6964724e138cc47cbd76532c", "name": "Silin Wu", "hidden": false}, {"_id": "6964724e138cc47cbd76532d", "user": {"_id": "68f287f2faba6f123f8a3b3c", "avatarUrl": "/avatars/58a34b0f45bb34d74f86a638eff7dc94.svg", "isPro": false, "fullname": "Qizhen Lan", "user": "lanqz7766", "type": "user"}, "name": "Qizhen Lan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:03.306Z", "hidden": false}, {"_id": "6964724e138cc47cbd76532e", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:08.549Z", "hidden": false}, {"_id": "6964724e138cc47cbd76532f", "user": {"_id": "6874f7f0f8e67e9b5714adf2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png", "isPro": false, "fullname": "RongHao Chen", "user": "SuPA4ki", "type": "user"}, "name": "Ronghao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:50:13.928Z", "hidden": false}], "publishedAt": "2026-01-08T09:11:33.000Z", "submittedOnDailyAt": "2026-01-14T05:31:09.992Z", "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.", "upvotes": 46, "discussionId": "6964724f138cc47cbd765330", "githubRepo": "https://github.com/QuantaAlpha/KnowMeBench", "githubRepoAddedBy": "auto", "ai_summary": "Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.", "ai_keywords": ["memory benchmarks", "autobiographical narratives", "retrieval-augmented systems", "factual recall", "subjective state attribution", "principle-level reasoning"], "githubStars": 83, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u7684\u957f\u671f\u8bb0\u5fc6\u57fa\u51c6\u4e3b\u8981\u4f7f\u7528\u591a\u8f6e\u5bf9\u8bdd\u6216\u5408\u6210\u7528\u6237\u5386\u53f2\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4e2a\u4eba\u7406\u89e3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\\BenchName\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u957f\u7bc7\u81ea\u4f20\u53d9\u8ff0\u7684\u516c\u5f00\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u63a8\u65ad\u7a33\u5b9a\u52a8\u673a\u548c\u51b3\u7b56\u539f\u5219\u7684\u4e30\u5bcc\u8bc1\u636e\u3002</li>\n    <li>\\BenchName\u5c06\u6bcf\u4e2a\u53d9\u8ff0\u91cd\u6784\u4e3a\u6709\u65f6\u95f4\u951a\u70b9\u7684\u95ea\u56de\u6d41\uff0c\u5e76\u901a\u8fc7\u4e0e\u8bc1\u636e\u76f8\u5173\u7684\u95ee\u9898\u8bc4\u4f30\u6a21\u578b\u3002</li>\n    <li>\u591a\u6837\u5316\u53d9\u8ff0\u6765\u6e90\u7684\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u4e3b\u8981\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u5728\u65f6\u95f4\u57fa\u7840\u7684\u89e3\u91ca\u548c\u66f4\u9ad8\u5c42\u6b21\u7684\u63a8\u7406\u4e0a\u4ecd\u5b58\u5728\u9519\u8bef\u3002</li>\n    <li>\u8fd9\u8868\u660e\u9700\u8981\u8d85\u8d8a\u68c0\u7d22\u7684\u8bb0\u5fc6\u673a\u5236\u6765\u6539\u8fdb\u6a21\u578b\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current memory benchmarks mostly use dialogues or fake histories, which don't fully test understanding of a person.</li>\n    <li>We introduce \\BenchName, a new benchmark using real-life autobiographical stories to better evaluate understanding.</li>\n    <li>This benchmark focuses on actions, context, and thoughts to reveal motivations and decision-making processes.</li>\n    <li>It includes questions about facts, personal feelings, and reasoning to assess model performance.</li>\n    <li>Our findings show that while some systems improve on factual answers, they struggle with explanations and deeper reasoning, indicating a need for better memory methods.</li>\n</ul>"}, "publishedAt": "2026-01-08T04:11:33.000Z", "title": "KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions", "summary": "Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04745.png", "numComments": 1, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08225", "authors": [{"_id": "6967202cc5e371f6b235d1cc", "name": "Jungho Cho", "hidden": false}, {"_id": "6967202cc5e371f6b235d1cd", "name": "Minbyul Jeong", "hidden": false}, {"_id": "6967202cc5e371f6b235d1ce", "user": {"_id": "65446c938737c799e9ad6f83", "avatarUrl": "/avatars/6ade251e01442b14cbf8cd7888358fd1.svg", "isPro": false, "fullname": "Sungrae Park", "user": "sungrae-park", "type": "user"}, "name": "Sungrae Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:29.339Z", "hidden": false}], "publishedAt": "2026-01-13T05:14:09.000Z", "submittedOnDailyAt": "2026-01-14T02:19:55.431Z", "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale", "submittedOnDailyBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "isPro": false, "fullname": "Minbyul Jeong", "user": "Minbyul", "type": "user"}, "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.", "upvotes": 40, "discussionId": "6967202dc5e371f6b235d1cf", "ai_summary": "Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.", "ai_keywords": ["large reasoning models", "multi-turn dialogue generation", "task-oriented simulation", "user simulator", "behavioral rules", "turn-by-turn feedback", "automated task generation", "high-density dataset", "human-agent interaction"], "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u9700\u6c42\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u3002</li>\n    <li>\u73b0\u6709\u7684\u6570\u636e\u96c6\u548c\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u5de5\u5177\u96c6\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u7684\u5f00\u653e\u5f0f\u4eba\u673a\u534f\u4f5c\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528LRM\u6a21\u62df\u5668\u52a8\u6001\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684\u5de5\u5177\u3002</li>\n    <li>\u4f20\u7edf\u7684\u4efb\u52a1\u5bfc\u5411\u8bbe\u8ba1\u5f80\u5f80\u5bfc\u81f4\u4e0e\u7528\u6237\u7684\u4e92\u52a8\u8f83\u5c11\uff0c\u65e0\u6cd5\u751f\u6210\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u5e38\u89c1\u7684\u9ad8\u8f6e\u6b21\u4ea4\u6d41\u3002</li>\n    <li>\u6211\u4eec\u8f6c\u5411\u7528\u6237\u5bfc\u5411\u7684\u6a21\u62df\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u4fc3\u8fdb\u66f4\u771f\u5b9e\u7684\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\uff0c\u9002\u5e94\u5b9e\u9645\u95ee\u9898\u89e3\u51b3\u7684\u9700\u6c42\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The shift to using large reasoning models (LRMs) as independent agents has increased the need for advanced multi-turn tool-use skills.</li>\n    <li>Current datasets are limited because they use fixed tools that can't adapt to complex human-agent collaborations.</li>\n    <li>We created a system to automatically generate realistic dialogues that include specific tools for completing tasks.</li>\n    <li>Our approach focuses on mimicking human interactions, allowing for longer and more meaningful conversations rather than just quick task completion.</li>\n    <li>This system can generate a rich dataset that mirrors real-world interactions by allowing multiple tasks to be completed in one conversation.</li>\n</ul>"}, "publishedAt": "2026-01-13T00:14:09.000Z", "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale", "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08225.png", "numComments": 2, "submittedBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "fullname": "Minbyul Jeong", "name": "Minbyul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06002", "authors": [{"_id": "6964644c138cc47cbd76529b", "user": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "name": "Qiguang Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:48.803Z", "hidden": false}, {"_id": "6964644c138cc47cbd76529c", "name": "Yantao Du", "hidden": false}, {"_id": "6964644c138cc47cbd76529d", "name": "Ziniu Li", "hidden": false}, {"_id": "6964644c138cc47cbd76529e", "name": "Jinhao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd76529f", "name": "Songyao Duan", "hidden": false}, {"_id": "6964644c138cc47cbd7652a0", "name": "Jiarui Guo", "hidden": false}, {"_id": "6964644c138cc47cbd7652a1", "name": "Minghao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a2", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a3", "name": "Tong Yang", "hidden": false}, {"_id": "6964644c138cc47cbd7652a4", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:51.170Z", "hidden": false}, {"_id": "6964644c138cc47cbd7652a5", "name": "Libo Qin", "hidden": false}, {"_id": "6964644c138cc47cbd7652a6", "name": "Wanxiang Che", "hidden": false}, {"_id": "6964644c138cc47cbd7652a7", "name": "Wenhao Huang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "publishedAt": "2026-01-09T18:39:01.000Z", "submittedOnDailyAt": "2026-01-12T01:02:27.368Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "submittedOnDailyBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "upvotes": 38, "discussionId": "6964644c138cc47cbd7652a8", "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.", "ai_keywords": ["chain-of-thought", "large language models", "Long CoT", "fine-tuning", "entropy convergence", "semantic isomers", "distribution-transfer-graph", "molecular-like structures", "deep reasoning", "self-reflection", "self-exploration"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n  <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u957f\u94fe\u601d\u7ef4\uff08Long CoT\uff09\u63a8\u7406\u3002</li>\n  <li>\u6211\u4eec\u63d0\u51fa\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u8f68\u8ff9\u5177\u6709\u7a33\u5b9a\u7684\u5206\u5b50\u7ed3\u6784\uff0c\u7531\u6df1\u5ea6\u63a8\u7406\u3001\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u63a2\u7d22\u4e09\u79cd\u4ea4\u4e92\u7c7b\u578b\u5f62\u6210\u3002</li>\n  <li>\u5206\u6790\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u7ed3\u6784\u6765\u81ea\u4e8e\u957f\u94fe\u601d\u7ef4\u7684\u5fae\u8c03\uff0c\u800c\u4e0d\u662f\u5173\u952e\u8bcd\u6a21\u4eff\u3002</li>\n  <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u6709\u6548\u7684\u8bed\u4e49\u5f02\u6784\u4f53\uff0c\u53d1\u73b0\u4fc3\u8fdb\u5feb\u901f\u71b5\u6536\u655b\u7684\u8054\u7cfb\u652f\u6301\u7a33\u5b9a\u7684\u957f\u94fe\u601d\u7ef4\u5b66\u4e60\u3002</li>\n  <li>\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51faMole-Syn\u65b9\u6cd5\uff0c\u5e2e\u52a9\u5408\u6210\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u7ed3\u6784\uff0c\u63d0\u9ad8\u6027\u80fd\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models struggle to learn effective long reasoning processes from other models or humans.</li>\n    <li>Effective long reasoning can be understood as having stable structures created by three types of interactions: deep reasoning, self-reflection, and self-exploration.</li>\n    <li>These stable structures are formed during fine-tuning, rather than just by imitating keywords.</li>\n    <li>We introduce a new method called Mole-Syn that helps create these effective reasoning structures, improving model performance and stability.</li>\n</ul>"}, "publishedAt": "2026-01-09T13:39:01.000Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png", "numComments": 1, "submittedBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "fullname": "Qiguang Chen", "name": "LightChen2333", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u73b0\u5b9e\u4e2d\u7684\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u5206\u6563\u5728\u4e92\u8054\u7f51\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u7684\u5f00\u653e\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u8981\u6c42\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u4e92\u52a8\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u201cAgentic\u201d\u65b9\u6cd5\u7684\u8868\u73b0\u4e0d\u603b\u662f\u4f18\u4e8e\u201cWorkflow\u201d\uff0c\u5176\u4f18\u52bf\u4f9d\u8d56\u4e8e\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u4e3a\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7814\u7a76\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u4e0b\u4e00\u4ee3\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Real-world video question answering needs to extract visual clues from videos and find answers from the web.</li>\n    <li>VideoDR is a new benchmark designed to help with video-based question answering, focusing on video and web interactions.</li>\n    <li>The benchmark includes high-quality samples across six different topics, created with careful human review.</li>\n    <li>Tests showed that two approaches, Workflow and Agentic, perform differently, with Agentic's success depending on how well it manages video clues during searches.</li>\n    <li>The study identifies challenges like maintaining focus and consistency over long searches as key issues for future video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u8feb\u5207\u9700\u8981\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u3002</li>\n    <li>\u5f53\u524d\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u591a\u4e3a\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u548c\u53ef\u91cd\u590d\u6027\uff0c\u9650\u5236\u4e86\u6570\u636e\u751f\u6210\u7684\u652f\u6301\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\uff0c\u4ee5\u53ca\u516d\u4e2a\u6db5\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u7684\u4e00\u822c\u7ba1\u9053\u3002</li>\n    <li>DataFlow\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38LLM\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u53ef\u91cd\u590d\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u7684\u6570\u636e\u4e2d\u5fc3AI\u53d1\u5c55\u5960\u5b9a\u4e86\u7cfb\u7edf\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The demand for high-quality data in Large Language Models (LLMs) is increasing, highlighting the need for better data preparation methods.</li>\n    <li>Current data preparation practices are often inconsistent and lack proper structure, making them hard to reproduce and optimize.</li>\n    <li>DataFlow is a new framework that provides organized and reusable data preparation tools, making it easier to build and debug data pipelines.</li>\n    <li>The framework includes around 200 tools and six different pipelines for various tasks, like text processing and code generation.</li>\n    <li>DataFlow improves the performance of LLMs in multiple tests, even outperforming curated datasets and demonstrating strong potential for future AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u4eba\u7c7b\u5728\u638c\u63e1\u8bed\u8a00\u4e4b\u524d\u5c31\u5177\u5907\u57fa\u672c\u7684\u89c6\u89c9\u6280\u80fd\uff0c\u4f46\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u5176\u8106\u5f31\u7684\u89c6\u89c9\u7406\u89e3\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u4e00\u4e9b\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4e0d\u53ca\u4eba\u7c7b\uff0c\u751a\u81f3\u4e09\u5c81\u7684\u5c0f\u5b69\u90fd\u80fd\u8f7b\u677e\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u4e86\u7814\u7a76\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u72ec\u7acb\u4e8e\u8bed\u8a00\u77e5\u8bc6\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\uff0c\u6db5\u76d6\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\uff0c\u7ed3\u679c\u663e\u793a\u9886\u5148\u7684MLLMs\u7684\u8868\u73b0\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\u3002</li>\n    <li>\u5c3d\u7ba1\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5f53\u524d\u7684MLLMs\u4ecd\u7f3a\u4e4f\u57fa\u672c\u7684\u89c6\u89c9\u80fd\u529b\uff0cBabyVision\u7684\u8fdb\u5c55\u662f\u671d\u5411\u4eba\u7c7b\u6c34\u5e73\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u4e00\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills early, but modern Multimodal LLMs struggle with basic visual tasks.</li>\n    <li>We created BabyVision, a benchmark to test visual abilities of MLLMs without relying on language.</li>\n    <li>BabyVision includes 388 tasks across 22 subclasses and four categories.</li>\n    <li>Top MLLMs, like Gemini3-Pro-Preview, score much lower than human averages, indicating poor visual understanding.</li>\n    <li>BabyVision aims to improve visual perception and reasoning in AI and includes a new tool for visual reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5b9a\u4f4d\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5ffd\u7565\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\uff0c\u8d4b\u4e88\u5176\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4ee3\u7406-\u5730\u56fe\u5faa\u73af\u3002</li>\n    <li>\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\uff0c\u4ee5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u548c\u63a2\u7d22\u591a\u4e2a\u5019\u9009\u8def\u5f84\u3002</li>\n    <li>\u5728\u5b9e\u9645\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The task of image geolocalization is to find out where a photo was taken on Earth using visual clues.</li>\n    <li>This study introduces a new way to help models use maps, which is a common human strategy, by creating a \"Thinking with Map\" ability.</li>\n    <li>The approach includes two steps: using reinforcement learning to improve decision-making and a method to test multiple options before making a final guess.</li>\n    <li>To test the new method, the researchers created MAPBench, a benchmark using real-world images for training and evaluation.</li>\n    <li>Results show that this new method performs better than existing models, especially increasing accuracy significantly in predictions.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u548c\u53c2\u8003\u56fe\u50cf\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\u6765\u652f\u6301\u591a\u6a21\u6001\u89c6\u9891\u521b\u4f5c\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>Kling-Omni \u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>\u5b83\u4e0d\u4ec5\u662f\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\uff0c\u4e5f\u662f\u671d\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u3001\u751f\u6210\u548c\u4e0e\u590d\u6742\u4e16\u754c\u4e92\u52a8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of inputs like text, images, and videos.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one integrated framework, unlike previous separate methods.</li>\n    <li>The system can handle various user inputs and turns them into a single representation for better video creation.</li>\n    <li>Kling-Omni has been trained with a large amount of data and optimized for efficient performance.</li>\n    <li>It shows strong abilities in generating content, editing based on reasoning, and following complex instructions, making it a step towards advanced multimodal simulations.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 99, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 87, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b56\u7565\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u4e3b\u8981\u662f\u88ab\u52a8\u5b58\u50a8\u5b64\u7acb\u4e8b\u5b9e\uff0c\u7f3a\u4e4f\u5bf9\u9ad8\u9636\u5173\u8054\u7684\u5173\u6ce8\uff0c\u9650\u5236\u4e86\u63a8\u7406\u548c\u77e5\u8bc6\u6f14\u53d8\u7684\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faHGMem\uff0c\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u8d85\u8d8a\u7b80\u5355\u5b58\u50a8\uff0c\u6784\u5efa\u52a8\u6001\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7ed3\u6784\u4ee5\u652f\u6301\u590d\u6742\u63a8\u7406\u3002</li>\n    <li>HGMem\u7684\u8bb0\u5fc6\u4ee5\u8d85\u56fe\u5f62\u5f0f\u8868\u793a\uff0c\u4fc3\u8fdb\u4e86\u8bb0\u5fc6\u4e2d\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u589e\u5f3a\u4e86\u5bf9\u95ee\u9898\u7684\u6574\u4f53\u7406\u89e3\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5168\u7403\u7406\u89e3\u7684\u6311\u6218\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) understand and reason better.</li>\n    <li>Current memory systems mostly just store facts, missing important connections between them.</li>\n    <li>This limitation leads to weak reasoning and understanding in complex tasks.</li>\n    <li>HGMem is a new memory system that uses a hypergraph structure for better connections between facts.</li>\n    <li>Testing shows that HGMem significantly improves performance on challenging reasoning tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u7528\u6237\u5e0c\u671b\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u5e76\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u5f15\u5165\u591a\u4e2a\u5956\u52b1\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u671d\u7740\u671f\u671b\u7684\u884c\u4e3a\u53d1\u5c55\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u76f4\u63a5\u5e94\u7528\u201c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u201d\uff08GRPO\uff09\u53ef\u80fd\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u4ef7\u503c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u201c\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316\u201d\uff08GDPO\uff09\uff0c\u5b83\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u5956\u52b1\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Language models need to provide accurate responses and behave according to different human preferences in various situations.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to guide models, but a method called Group Relative Policy Optimization (GRPO) may not work well for this.</li>\n    <li>Applying GRPO can lead to problems like reduced training effectiveness and failures during training.</li>\n    <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that avoids these issues by treating rewards separately.</li>\n    <li>GDPO is shown to perform better than GRPO in tasks like tool calling, math reasoning, and coding reasoning, improving both accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86MMFormalizer\uff0c\u65e8\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u7ffb\u8bd1\u4e3a\u5f62\u5f0f\u5316\u9648\u8ff0\uff0c\u4ee5\u4fbf\u673a\u5668\u63a8\u7406\u3002</li>\n    <li>MMFormalizer\u901a\u8fc7\u9002\u5e94\u6027\u57fa\u7840\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u5b66\u4e0e\u7269\u7406\u5b9e\u4f53\uff0c\u6269\u5c55\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u80fd\u529b\u3002</li>\n    <li>\u5229\u7528\u9012\u5f52\u57fa\u7840\u548c\u516c\u7406\u7ec4\u5408\u6784\u5efa\u5f62\u5f0f\u547d\u9898\uff0c\u6bcf\u4e2a\u62bd\u8c61\u90fd\u7531\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5PhyX-AF\u4e0a\u8bc4\u4f30MMFormalizer\uff0c\u7ed3\u679c\u663e\u793aGPT-5\u548cGemini-3-Pro\u5728\u7f16\u8bd1\u548c\u8bed\u4e49\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\u3002</li>\n    <li>\u8fd9\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization helps convert natural language math into formal statements for machines but struggles with real-world complexities.</li>\n    <li>MMFormalizer improves this process by combining visual elements with mathematical and physical concepts.</li>\n    <li>It builds formal propositions using visual evidence and defined mathematical principles, adapting as needed.</li>\n    <li>MMFormalizer was tested on a new benchmark, PhyX-AF, showing strong results with advanced models like GPT-5.</li>\n    <li>This method is the first to effectively handle complex topics like classical mechanics and quantum physics in multimodal settings.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aNeoVerse\u7684\u591a\u529f\u80fd4D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>NeoVerse\u80fd\u591f\u8fdb\u884c4D\u91cd\u5efa\u3001\u751f\u6210\u65b0\u7684\u89c6\u9891\u8f68\u8ff9\uff0c\u5e76\u652f\u6301\u591a\u79cd\u5e94\u7528\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u9650\u5236\uff0c\u9002\u7528\u4e8e\u5355\u773c\u89c6\u9891\u3002</li>\n    <li>\u5177\u6709\u65e0\u59ff\u6001\u3001\u524d\u9988\u76844D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u773c\u964d\u7ea7\u6a21\u5f0f\u6a21\u62df\u7b49\u7279\u70b9\u3002</li>\n    <li>\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeoVerse\u8868\u73b0\u4f18\u79c0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions and generate videos from new perspectives.</li>\n    <li>The model addresses the common problem of scalability in existing 4D modeling methods.</li>\n    <li>NeoVerse works well with regular monocular videos, avoiding the need for expensive multi-view data.</li>\n    <li>It includes features like pose-free reconstruction and online simulation to enhance its flexibility.</li>\n    <li>NeoVerse achieves top-tier results in various benchmarks for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u6311\u6218\u3002</li>\n    <li>Youtu-Agent \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316 LLM \u4ee3\u7406\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5177\u6709\u7075\u6d3b\u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u6807\u51c6\u4efb\u52a1\u548c\u590d\u6742\u9700\u6c42\u7684\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u548c\u914d\u7f6e\u3002</li>\n    <li>Youtu-Agent \u5305\u542b\u4e24\u79cd\u4f18\u5316\u6a21\u5757\uff1aAgent Practice \u548c Agent RL\uff0c\u5e2e\u52a9\u4ee3\u7406\u79ef\u7d2f\u7ecf\u9a8c\u5e76\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u81ea\u52a8\u751f\u6210\u5de5\u5177\u7684\u6210\u529f\u7387\u8d85\u8fc7 81%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents more easily.</li>\n    <li>It solves problems like high setup costs and limited flexibility by using a modular design for better tool integration and adaptability.</li>\n    <li>The framework has two modes: Workflow for standard tasks and Meta-Agent for complex tasks, allowing automatic generation of code and configurations.</li>\n    <li>Youtu-Agent includes a system to help agents learn and improve over time without needing constant updates, enhancing their performance in real situations.</li>\n    <li>Tests show Youtu-Agent outperforms previous models in various tasks and improves efficiency by speeding up training and enhancing problem-solving abilities.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 15, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2512.10430", "authors": [{"_id": "693bba8d9874a2a5e4ffb3ab", "user": {"_id": "64f4c8739ee58d48e8507e0e", "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg", "isPro": false, "fullname": "Dmitrii Stoianov", "user": "heylimon", "type": "user"}, "name": "Dmitrii Stoianov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:40.198Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ac", "user": {"_id": "64fb054ebb362cbf2fe53159", "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg", "isPro": false, "fullname": "Danil Taranets", "user": "taranetsdan", "type": "user"}, "name": "Danil Taranets", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:29.281Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ad", "user": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "name": "Olga Tsymboi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:38.180Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ae", "user": {"_id": "6780dcd6acf8d824c03864da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png", "isPro": false, "fullname": "Ramil Latypov", "user": "kylecr4ne", "type": "user"}, "name": "Ramil Latypov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:23.437Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3af", "user": {"_id": "6513f03e86d74f32ed65e3b8", "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg", "isPro": false, "fullname": "Almaz Dautov", "user": "the-hir0", "type": "user"}, "name": "Almaz Dautov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:30.990Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b0", "user": {"_id": "621a8daf325b927e60fcef08", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg", "isPro": false, "fullname": "Vladislav Kruglikov", "user": "vladislavkruglikov", "type": "user"}, "name": "Vladislav Kruglikov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:21.170Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b1", "name": "Nikita Surkov", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b2", "user": {"_id": "63188c428d698d8c1642a0d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg", "isPro": false, "fullname": "German Abramov", "user": "germanjke", "type": "user"}, "name": "German Abramov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:28.579Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b3", "name": "Pavel Gein", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b4", "user": {"_id": "636a9a07e3ad78bc68b1a5a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg", "isPro": false, "fullname": "Dmitry Abulkhanov", "user": "mponty", "type": "user"}, "name": "Dmitry Abulkhanov", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:02:42.107Z", "hidden": true}, {"_id": "693bba8d9874a2a5e4ffb3b5", "user": {"_id": "658bc20cfdf2279d4721f218", "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg", "isPro": false, "fullname": "Mikhail Gashkov", "user": "MikeGashkov", "type": "user"}, "name": "Mikhail Gashkov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:32.809Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b6", "name": "Viktor Zelenkovskiy", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b7", "user": {"_id": "644f64bc17b6189cda54cae8", "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg", "isPro": false, "fullname": "Artem Batalov", "user": "batalovme", "type": "user"}, "name": "Artem Batalov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:27.497Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b8", "user": {"_id": "62609d224e6e4b84475eb8d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg", "isPro": false, "fullname": "Alex Medvedev", "user": "kenkaneki", "type": "user"}, "name": "Aleksandr Medvedev", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:36.035Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b9", "user": {"_id": "63f26358be95ed4c9a9b0583", "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg", "isPro": false, "fullname": "Anatoly Potapov", "user": "AnatoliiPotapov", "type": "user"}, "name": "Anatolii Potapov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:25.666Z", "hidden": false}], "publishedAt": "2025-12-11T08:40:10.000Z", "submittedOnDailyAt": "2025-12-12T08:33:32.798Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "submittedOnDailyBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "upvotes": 60, "discussionId": "693bba8e9874a2a5e4ffb3ba", "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.", "ai_keywords": ["Cyrillic-dense tokenizer", "EAGLE speculative-decoding pipeline", "hybrid reasoning", "efficient inference", "reasoning-trace generation"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n  <li>\u63a8\u51fa\u4e86T-pro 2.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4fc4\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7528\u4e8e\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u65ad\u3002</li>\n  <li>\u6a21\u578b\u652f\u6301\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u548c\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4e86\u5bc6\u96c6\u7684\u897f\u91cc\u5c14\u5b57\u6bcd\u6807\u8bb0\u5668\u548c\u9002\u5e94\u7684EAGLE\u89e3\u7801\u6d41\u7a0b\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002</li>\n  <li>\u4e3a\u4fc3\u8fdb\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u7814\u7a76\uff0c\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001T-Wix 500k\u6307\u4ee4\u8bed\u6599\u5e93\u3001T-Math\u63a8\u7406\u57fa\u51c6\u548cEAGLE\u6743\u91cd\u3002</li>\n  <li>\u8fd9\u4e9b\u8d44\u6e90\u4f7f\u7528\u6237\u80fd\u591f\u7814\u7a76\u4fc4\u8bed\u63a8\u7406\uff0c\u5e76\u6269\u5c55\u6216\u8c03\u6574\u6a21\u578b\u548c\u63a8\u65ad\u6d41\u7a0b\u3002</li>\n  <li>\u63d0\u4f9b\u7684\u516c\u5171\u7f51\u7edc\u6f14\u793a\u5c55\u793a\u4e86\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u5f0f\uff0c\u8bf4\u660e\u4e86\u63a8\u65ad\u5806\u6808\u5728\u5404\u4e2a\u9886\u57df\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>T-pro 2.0 is a Russian language model designed for fast reasoning and answering questions.</li>\n    <li>It uses a special tokenizer and a new decoding method to reduce response time.</li>\n    <li>The model and supporting materials, like instruction datasets and benchmarks, are available for researchers on Hugging Face.</li>\n    <li>A public web demo shows how the model works in different modes and demonstrates its speed improvements.</li>\n    <li>T-pro 2.0 is aimed at helping users create and test efficient applications using Russian language processing.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:40:10.000Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png", "numComments": 1, "submittedBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "fullname": "Olga Tsymboi", "name": "oltsy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10739", "authors": [{"_id": "693b91d89874a2a5e4ffb329", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32a", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:56.632Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32b", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32c", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32d", "user": {"_id": "64e8505321540e1da3226b54", "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg", "isPro": false, "fullname": "Wenwei Zhang", "user": "ZwwWayne", "type": "user"}, "name": "Wenwei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:21.987Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32e", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32f", "name": "Fan Zheng", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb330", "user": {"_id": "670f8df2005a358fdc6c2fb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qw2yocepl2vhC5T2ae49b.png", "isPro": false, "fullname": "tianyou", "user": "matianyou", "type": "user"}, "name": "Tianyou Ma", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:57.205Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb331", "user": {"_id": "687f853bb39262ba84f3eeff", "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg", "isPro": false, "fullname": "Junhao Shen", "user": "shenjunhao", "type": "user"}, "name": "Junhao Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:00.496Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb332", "name": "Haiteng Zhao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb333", "user": {"_id": "6454b1073aaeff9f3d330ef6", "avatarUrl": "/avatars/331fcbbf18a72b0dc8419ca3a77299bb.svg", "isPro": false, "fullname": "Duanyang Zhang", "user": "KKKDaniel", "type": "user"}, "name": "Duanyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:12.911Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb334", "name": "Huilun Zhang", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb335", "user": {"_id": "63fd691794cc8f815d50c112", "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg", "isPro": false, "fullname": "liu", "user": "Harold-lkk", "type": "user"}, "name": "Kuikun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:58.650Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb336", "name": "Chengqi Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb337", "name": "Yanhui Duan", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb338", "name": "Chiyu Chen", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb339", "name": "Ningsheng Ma", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33a", "name": "Jianfei Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33b", "name": "Han Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33c", "user": {"_id": "636317ed80c1a705a6eff396", "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg", "isPro": false, "fullname": "Dahua Lin", "user": "lindahua", "type": "user"}, "name": "Dahua Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:20.346Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33d", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:26:28.000Z", "submittedOnDailyAt": "2025-12-12T01:27:55.307Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "upvotes": 37, "discussionId": "693b91d99874a2a5e4ffb33e", "ai_summary": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "long reasoning chains of thought (CoTs)", "iterative active learning", "Rejection Fine-Tuning (RFT)", "F1 score", "accuracy", "DeepSeek-R1-Distill-Qwen-32B", "AIME2025"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5f97\u76ca\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002</li>\n    <li>\u73b0\u6709\u7684\u7ed3\u679c\u57fa\u7840\u9a8c\u8bc1\u5668\u65e0\u6cd5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\uff0c\u800c\u8fc7\u7a0b\u57fa\u7840\u9a8c\u8bc1\u5668\u5728\u68c0\u6d4b\u590d\u6742\u957f\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u65f6\u9762\u4e34\u56f0\u96be\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u9a8c\u8bc1\u957f\u63a8\u7406\u7684\u603b\u7ed3\u7ed3\u679c\u3002</li>\n    <li>OPV\u91c7\u7528\u5faa\u73af\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6ce8\u91ca\u9010\u6b65\u63d0\u9ad8\u9a8c\u8bc1\u80fd\u529b\uff0c\u51cf\u5c11\u4eba\u5de5\u6ce8\u91ca\u6210\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPV\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u8bb8\u591a\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are improving at solving complex reasoning tasks using a method called Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>Current verification methods struggle to check unreliable steps in long reasoning processes, and there are not enough high-quality human annotations available.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) aims to verify the reasoning process more accurately and efficiently, using summarized outcomes from long reasoning chains.</li>\n    <li>OPV uses an iterative active learning approach to reduce annotation costs while improving its verification accuracy with expert input.</li>\n    <li>Tests show OPV performs better than larger models, achieving a high F1 score and significantly improving accuracy in collaboration with policy models.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:26:28.000Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10739.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10949", "authors": [{"_id": "693b895d9874a2a5e4ffb302", "user": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "name": "Yiwen Tang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:55.504Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb303", "user": {"_id": "642a8302d651bae3c11b72b1", "avatarUrl": "/avatars/4d2d422613e274d80482fed9a7d3f785.svg", "isPro": false, "fullname": "Zoey Guo", "user": "Purple1288", "type": "user"}, "name": "Zoey Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:01.705Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb304", "user": {"_id": "6708920aeae29d1cd41a703b", "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg", "isPro": false, "fullname": "kaixin zhu", "user": "czkk566", "type": "user"}, "name": "Kaixin Zhu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:10.275Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb305", "name": "Ray Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb306", "user": {"_id": "6535045a910b844786a6642f", "avatarUrl": "/avatars/37a94864a7a348151837b421ea6d77e3.svg", "isPro": false, "fullname": "Qizhi Chen", "user": "Tavish9", "type": "user"}, "name": "Qizhi Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:11.060Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb307", "user": {"_id": "6349214f8146350b3a4c5cdf", "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg", "isPro": false, "fullname": "Dongzhi Jiang", "user": "CaraJ", "type": "user"}, "name": "Dongzhi Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:06:02.910Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb308", "name": "Junli Liu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb309", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:08.733Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30a", "user": {"_id": "6662d2c9de4c4e1f04bd29c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QnnO_KOyZjFd-iXuPnHqR.png", "isPro": false, "fullname": "HaomingSong", "user": "HaomingSong", "type": "user"}, "name": "Haoming Song", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:30.062Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30b", "user": {"_id": "64daecec888b7e9c400f59b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png", "isPro": false, "fullname": "Delin Qu", "user": "delinqu", "type": "user"}, "name": "Delin Qu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:34.085Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30c", "name": "Tianyi Bai", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30d", "name": "Dan Xu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30e", "name": "Wentao Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30f", "name": "Bin Zhao", "hidden": false}], "publishedAt": "2025-12-11T18:59:52.000Z", "submittedOnDailyAt": "2025-12-12T00:49:43.850Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "submittedOnDailyBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "upvotes": 36, "discussionId": "693b895d9874a2a5e4ffb310", "githubRepo": "https://github.com/Ivan-Tang-3D/3DGen-R1", "githubRepoAddedBy": "user", "ai_summary": "This study investigates reinforcement learning for text-to-3D generation, focusing on reward designs, RL algorithms, benchmarking, and hierarchical optimization, introducing AR3D-R1 as the first RL-enhanced model for 3D generation.", "ai_keywords": ["reinforcement learning", "text-to-3D generation", "reward designs", "GRPO variants", "token-level optimization", "MME-3DR", "Hi-GRPO", "hierarchical 3D generation", "AR3D-R1"], "githubStars": 40, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5f3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5df2\u6210\u529f\u5e94\u7528\u4e8e2D\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u57283D\u751f\u6210\u65b9\u9762\u4ecd\u672a\u88ab\u5e7f\u6cdb\u63a2\u7d22\uff0c\u4e3b\u8981\u56e0\u4e3a3D\u7269\u4f53\u7684\u590d\u6742\u6027\u3002</li>\n    <li>\u6211\u4eec\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6587\u672c\u52303D\u81ea\u56de\u5f52\u751f\u6210\u7684RL\u5e94\u7528\uff0c\u5173\u6ce8\u5956\u52b1\u8bbe\u8ba1\u548c\u7b97\u6cd5\u3002</li>\n    <li>\u8bc4\u4f30\u5956\u52b1\u8bbe\u8ba1\u548c\u6a21\u578b\u9009\u62e9\uff0c\u53d1\u73b0\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u63d0\u51faMME-3DR\u57fa\u51c6\uff0c\u4ee5\u6d4b\u91cf3D\u751f\u6210\u6a21\u578b\u7684\u9690\u6027\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1\u4e86AR3D-R1\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u6587\u672c\u52303D\u6a21\u578b\uff0c\u4ece\u7c97\u7565\u5f62\u72b6\u5230\u7eb9\u7406\u7ec6\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is effective for 2D image generation, but its application to 3D generation is still under-researched due to the complexity of 3D objects.</li>\n    <li>The study systematically explores RL for creating 3D models from text, focusing on reward designs, RL algorithms, and benchmarks.</li>\n    <li>It emphasizes the importance of aligning rewards with human preferences and shows that multi-modal models can effectively assess 3D attributes.</li>\n    <li>New benchmarks, like MME-3DR, are introduced to evaluate reasoning abilities in 3D generation models.</li>\n    <li>The study also presents a new model, AR3D-R1, which improves 3D generation by optimizing the process from basic shape creation to detailed texture refinement.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:59:52.000Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10949.png", "numComments": 2, "submittedBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "fullname": "Ivan Tang", "name": "IvanTang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10756", "authors": [{"_id": "693b91539874a2a5e4ffb318", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91539874a2a5e4ffb319", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31a", "name": "Wenwei Zhang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31b", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31c", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:02.946Z", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31d", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31e", "name": "Tianyou Ma", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31f", "name": "Yuhong Liu", "hidden": false}, {"_id": "693b91539874a2a5e4ffb320", "name": "Zhi Wang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb321", "name": "Runyuan Ma", "hidden": false}, {"_id": "693b91539874a2a5e4ffb322", "name": "Guangyu Wang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb323", "name": "Wei Li", "hidden": false}, {"_id": "693b91539874a2a5e4ffb324", "name": "Conghui He", "hidden": false}, {"_id": "693b91539874a2a5e4ffb325", "name": "Dahua Lin", "hidden": false}, {"_id": "693b91539874a2a5e4ffb326", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:47:38.000Z", "submittedOnDailyAt": "2025-12-12T01:23:10.732Z", "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "upvotes": 30, "discussionId": "693b91539874a2a5e4ffb327", "ai_summary": "The Outcome-based Process Verifier (OPV) improves the verification of complex reasoning chains in large language models by combining outcome-based and process-based verification with iterative active learning and Rejection Fine-Tuning, achieving state-of-the-art performance on various benchmarks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "verifiers", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "CoTs", "iterative active learning", "Rejection Fine-Tuning (RFT)", "OPV-Bench", "AIME2025", "DeepSeek-R1-Distill-Qwen-32B"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u73b0\u6709\u57fa\u4e8e\u7ed3\u679c\u7684\u9a8c\u8bc1\u5668\u65e0\u6cd5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\uff0c\u800c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u9a8c\u8bc1\u5668\u5728\u957f\u671f\u63a8\u7406\u4e2d\u68c0\u6d4b\u9519\u8bef\u7684\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\uff0c\u5b83\u901a\u8fc7\u9a8c\u8bc1\u957f\u63a8\u7406\u7684\u603b\u7ed3\u7ed3\u679c\u6765\u63d0\u9ad8\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002</li>\n    <li>OPV\u91c7\u7528\u8fed\u4ee3\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\u9010\u6b65\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cOPV\u5728\u6211\u4eec\u7684\u6d4b\u8bd5\u57fa\u51c6OPV-Bench\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0cF1\u5206\u6570\u4e3a83.1\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) have improved in solving complex reasoning tasks using a method called Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>Current verification methods struggle with checking long reasoning processes due to unreliable intermediate steps and a lack of quality human annotations.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) aims to efficiently verify reasoning processes by summarizing outcomes from long chains of thought.</li>\n    <li>OPV uses an active learning approach to improve its verification ability while minimizing the need for expensive human annotations.</li>\n    <li>Tests show that OPV performs better than larger models, achieving an F1 score of 83.1, and improves accuracy in collaboration with other models.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:47:38.000Z", "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10756.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10534", "authors": [{"_id": "693b94359874a2a5e4ffb340", "name": "Haiteng Zhao", "hidden": false}, {"_id": "693b94359874a2a5e4ffb341", "user": {"_id": "687f853bb39262ba84f3eeff", "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg", "isPro": false, "fullname": "Junhao Shen", "user": "shenjunhao", "type": "user"}, "name": "Junhao Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:51.355Z", "hidden": false}, {"_id": "693b94359874a2a5e4ffb342", "user": {"_id": "64ccaa4687ec96aa4752e754", "avatarUrl": "/avatars/d2dd2040a521de4f55c7335cb7771c75.svg", "isPro": false, "fullname": "Yiming Zhang", "user": "ymzhang319", "type": "user"}, "name": "Yiming Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:27.118Z", "hidden": false}, {"_id": "693b94359874a2a5e4ffb343", "name": "Songyang Gao", "hidden": false}, {"_id": "693b94359874a2a5e4ffb344", "user": {"_id": "63fd691794cc8f815d50c112", "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg", "isPro": false, "fullname": "liu", "user": "Harold-lkk", "type": "user"}, "name": "Kuikun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:54.577Z", "hidden": false}, {"_id": "693b94359874a2a5e4ffb345", "name": "Tianyou Ma", "hidden": false}, {"_id": "693b94359874a2a5e4ffb346", "name": "Fan Zheng", "hidden": false}, {"_id": "693b94359874a2a5e4ffb347", "name": "Dahua Lin", "hidden": false}, {"_id": "693b94359874a2a5e4ffb348", "name": "Wenwei Zhang", "hidden": false}, {"_id": "693b94359874a2a5e4ffb349", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T11:05:04.000Z", "submittedOnDailyAt": "2025-12-12T03:29:52.050Z", "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.", "upvotes": 25, "discussionId": "693b94359874a2a5e4ffb34a", "ai_summary": "InternGeometry, an LLM agent, surpasses human performance on IMO geometry problems using a heuristic-driven approach with iterative proposition verification and a dynamic memory mechanism, significantly outperforming AlphaGeometry 2 with limited training data.", "ai_keywords": ["LLM agents", "mathematical problem-solving", "International Mathematical Olympiad", "AI for geometry", "AlphaGeometry 2", "large-scale data synthesis", "search", "heuristic limitations", "propositions", "auxiliary constructions", "symbolic engine", "dynamic memory mechanism", "Complexity-Boosting Reinforcement Learning", "CBRL", "training examples", "InternThinker-32B", "expert-level geometry tasks"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u51e0\u4f55\u95ee\u9898\u4ecd\u53d7\u9650\u4e8e\u4e13\u5bb6\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u540d\u4e3aInternGeometry\u7684\u65b0LLM\u4ee3\u7406\uff0c\u4e13\u6ce8\u4e8e\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u3002</li>\n    <li>InternGeometry\u901a\u8fc7\u8fed\u4ee3\u63d0\u51fa\u547d\u9898\u548c\u8f85\u52a9\u6784\u9020\uff0c\u5e76\u7528\u7b26\u53f7\u5f15\u64ce\u9a8c\u8bc1\uff0c\u514b\u670d\u51e0\u4f55\u95ee\u9898\u7684\u542f\u53d1\u5f0f\u9650\u5236\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u89e3\u51b3IMO\u51e0\u4f55\u95ee\u9898\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u89e3\u51b3\u4e8650\u9053\u9898\u4e2d\u768444\u9053\uff0c\u8d85\u8fc7\u5e73\u5747\u91d1\u724c\u5f97\u5206\u3002</li>\n    <li>\u6211\u4eec\u5c06\u53d1\u5e03\u8be5\u6a21\u578b\u3001\u6570\u636e\u548c\u7b26\u53f7\u5f15\u64ce\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models can solve complex math problems, including those at the International Mathematical Olympiad (IMO) level, especially with formal proofs.</li>\n    <li>Geometry problem-solving in AI is primarily led by expert models like AlphaGeometry 2, which use large datasets for training and evaluation.</li>\n    <li>This study introduces InternGeometry, a new model designed specifically for geometry that improves on existing methods by proposing and verifying geometric constructions iteratively.</li>\n    <li>InternGeometry interacts with a symbolic engine over 200 times per problem and uses a new training technique called Complexity-Boosting Reinforcement Learning to gradually increase problem difficulty.</li>\n    <li>It successfully solved 44 out of 50 IMO geometry problems while using far fewer training examples than AlphaGeometry 2, indicating its effectiveness and potential for future research.</li>\n</ul>"}, "publishedAt": "2025-12-11T06:05:04.000Z", "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning", "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10534.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2512.10881", "authors": [{"_id": "693b87039874a2a5e4ffb2f5", "user": {"_id": "6412d08e027aea38bc90c802", "avatarUrl": "/avatars/86e3fa33193305af591d7d3cc79feb5c.svg", "isPro": false, "fullname": "Gongkehong", "user": "kehong", "type": "user"}, "name": "Kehong Gong", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:11.844Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f6", "user": {"_id": "62f4b9edd6d189cbd25fc7f8", "avatarUrl": "/avatars/12428e2ca213bf64c86dc7ed26b4dbcc.svg", "isPro": false, "fullname": "Zhengyu Wen", "user": "wzy27", "type": "user"}, "name": "Zhengyu Wen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:21.767Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f7", "user": {"_id": "68faf7fcc8107da320682a57", "avatarUrl": "/avatars/324fb883e23acc65ce8a14af87a37c65.svg", "isPro": false, "fullname": "He Weixia", "user": "weixia111111", "type": "user"}, "name": "Weixia He", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:32.488Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f8", "name": "Mingxi Xu", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f9", "name": "Qi Wang", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fa", "name": "Ning Zhang", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fb", "name": "Zhengyu Li", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fc", "user": {"_id": "64ac10d2c08741fc97bda2e9", "avatarUrl": "/avatars/ffe028f28e2284a44b0a0617f4abe387.svg", "isPro": false, "fullname": "Dongze Lian", "user": "DonaldLian", "type": "user"}, "name": "Dongze Lian", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:42.284Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fd", "name": "Wei Zhao", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fe", "name": "Xiaoyu He", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2ff", "name": "Mingyuan Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xNfWwMFlhegi3RqwZxqPG.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xdSriPIZ-k8QLE8m0A8c0.qt"], "publishedAt": "2025-12-11T18:09:48.000Z", "submittedOnDailyAt": "2025-12-12T00:41:38.576Z", "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/", "upvotes": 20, "discussionId": "693b87049874a2a5e4ffb300", "ai_summary": "MoCapAnything is a reference-guided framework that reconstructs rotation-based animations from monocular video for arbitrary rigged 3D assets, enabling cross-species retargeting and scalable 3D motion capture.", "ai_keywords": ["Category-Agnostic Motion Capture", "CAMoCap", "MoCapAnything", "Reference Prompt Encoder", "Video Feature Extractor", "Unified Motion Decoder", "3D joint trajectories", "inverse kinematics", "BVH", "Truebones Zoo", "skeletal animations", "cross-species retargeting"], "summary_zh": "<ul>\n    <li>\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65e0\u7c7b\u522b\u8fd0\u52a8\u6355\u6349 (CAMoCap)\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u76843D\u8d44\u4ea7\u52a8\u753b\u3002</li>\n    <li>MoCapAnything\u6846\u67b6\u901a\u8fc7\u5206\u6790\u89c6\u9891\u548c3D\u8d44\u4ea7\uff0c\u91cd\u5efa\u57fa\u4e8e\u65cb\u8f6c\u7684\u52a8\u753b\uff0c\u652f\u6301\u591a\u79cd\u8d44\u4ea7\u7c7b\u578b\u3002</li>\n    <li>\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u53ef\u5b66\u4e60\u6a21\u5757\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u9006\u8fd0\u52a8\u5b66\u9636\u6bb5\uff0c\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u7684\u52a8\u753b\u8f68\u8ff9\u3002</li>\n    <li>\u7814\u7a76\u56e2\u961f\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1038\u4e2a\u52a8\u4f5c\u526a\u8f91\u7684\u6570\u636e\u5e93\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u9aa8\u9abc-\u7f51\u683c-\u6e32\u67d3\u4e09\u5143\u7ec4\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoCapAnything\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u9aa8\u9abc\u52a8\u753b\uff0c\u5e76\u652f\u6301\u8de8\u7269\u79cd\u7684\u52a8\u753b\u91cd\u5b9a\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces a new method for motion capture called Category-Agnostic Motion Capture (CAMoCap), which can work with any 3D model and video.</li>\n    <li>MoCapAnything is a system that uses a video and a 3D asset to create animations, specifically focusing on generating rotation-based animations.</li>\n    <li>The system has three main components: a prompt encoder for the 3D asset, a video feature extractor for analyzing the video, and a motion decoder to create smooth animations.</li>\n    <li>It includes a collection called Truebones Zoo with over 1,000 motion clips to help standardize the motion capture process.</li>\n    <li>Tests show that MoCapAnything produces high-quality animations and can adapt motions across different types of 3D models effectively.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:09:48.000Z", "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos", "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xNfWwMFlhegi3RqwZxqPG.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xdSriPIZ-k8QLE8m0A8c0.qt"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10881.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 182}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05439", "authors": [{"_id": "693c5619f516c69324680c76", "name": "Tarun Suresh", "hidden": false}, {"_id": "693c5619f516c69324680c77", "name": "Nalin Wadhwa", "hidden": false}, {"_id": "693c5619f516c69324680c78", "name": "Debangshu Banerjee", "hidden": false}, {"_id": "693c5619f516c69324680c79", "name": "Gagandeep Singh", "hidden": false}], "publishedAt": "2025-12-05T05:34:06.000Z", "submittedOnDailyAt": "2025-12-12T15:32:48.549Z", "title": "BEAVER: An Efficient Deterministic LLM Verifier", "submittedOnDailyBy": {"_id": "65e7bb35e5e78134ab049942", "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg", "isPro": false, "fullname": "Tarun Suresh", "user": "tarsur909", "type": "user"}, "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.", "upvotes": 13, "discussionId": "693c561af516c69324680c7a", "ai_summary": "BEAVER is a framework that provides deterministic and sound probability bounds for verifying constraints in large language models, achieving tighter bounds and identifying more high-risk instances than baseline methods.", "ai_keywords": ["large language models", "LLMs", "BEAVER", "prefix-closed semantic constraint", "token trie", "frontier data structures", "sound probability bounds", "correctness verification", "privacy verification", "secure code generation", "high-risk instances"], "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9700\u8981\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1\u5176\u8f93\u51fa\u662f\u5426\u6ee1\u8db3\u8981\u6c42\u7684\u7ea6\u675f\u3002</li>\n    <li>BEAVER\u662f\u7b2c\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u53ef\u4ee5\u8ba1\u7b97LLM\u7ea6\u675f\u6ee1\u8db3\u7684\u786e\u5b9a\u6027\u6982\u7387\u754c\u9650\u3002</li>\n    <li>BEAVER\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u7ed3\u6784\u7cfb\u7edf\u5730\u63a2\u7d22\u751f\u6210\u7a7a\u95f4\uff0c\u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u4fdd\u6301\u8bc1\u660e\u6709\u6548\u7684\u754c\u9650\u3002</li>\n    <li>\u7ecf\u8fc7\u8bc4\u4f30\uff0cBEAVER\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7d27\u7684\u6982\u7387\u754c\u9650\u548c\u66f4\u591a\u9ad8\u98ce\u9669\u5b9e\u4f8b\u7684\u8bc6\u522b\u3002</li>\n    <li>BEAVER\u80fd\u591f\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u7279\u5f81\u63cf\u8ff0\u548c\u98ce\u9669\u8bc4\u4f30\uff0c\u8d85\u8d8a\u4e86\u677e\u6563\u754c\u9650\u6216\u7ecf\u9a8c\u8bc4\u4f30\u7684\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>BEAVER is a new framework that helps check if large language models (LLMs) meet specific requirements.</li>\n    <li>It provides reliable, deterministic probability limits on whether models follow these requirements.</li>\n    <li>BEAVER uses advanced data structures to explore possible model outputs and keeps track of accurate bounds during the process.</li>\n    <li>The framework has been tested on various tasks and shows significant improvements over existing methods, identifying more high-risk issues.</li>\n    <li>It offers better precision for assessing model behavior compared to traditional sampling methods.</li>\n</ul>"}, "publishedAt": "2025-12-05T00:34:06.000Z", "title": "BEAVER: An Efficient Deterministic LLM Verifier", "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05439.png", "numComments": 1, "submittedBy": {"_id": "65e7bb35e5e78134ab049942", "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg", "fullname": "Tarun Suresh", "name": "tarsur909", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.10867", "authors": [{"_id": "693bb6089874a2a5e4ffb39e", "user": {"_id": "670f86e4d75f114352916a35", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g3zZ6TTbgV-Xu789lPaYN.png", "isPro": false, "fullname": "Li", "user": "zongzhao", "type": "user"}, "name": "Zongzhao Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:42.257Z", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb39f", "name": "Xiangzhe Kong", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a0", "name": "Jiahui Su", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a1", "name": "Zongyang Ma", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a2", "name": "Mingze Li", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a3", "name": "Songyou Li", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a4", "name": "Yuelin Zhang", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a5", "name": "Yu Rong", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a6", "name": "Tingyang Xu", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a7", "name": "Deli Zhao", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a8", "name": "Wenbing Huang", "hidden": false}], "publishedAt": "2025-12-11T18:00:21.000Z", "submittedOnDailyAt": "2025-12-12T04:05:11.878Z", "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models", "submittedOnDailyBy": {"_id": "61bb00f6c4ac95d207b25f1b", "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg", "isPro": false, "fullname": "Zongyang Ma", "user": "zyma", "type": "user"}, "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.", "upvotes": 11, "discussionId": "693bb6099874a2a5e4ffb3a9", "ai_summary": "A benchmark framework evaluates Vision-Language Models in understanding microscopic spatial relationships, showing potential but highlighting the need for domain-specific knowledge integration.", "ai_keywords": ["Vision-Language Models", "VLMs", "Microscopic Spatial Intelligence", "MiSI", "MiSI-Bench", "spatial transformations", "relational identifications", "hydrogen bond recognition", "scientific AGI"], "organization": {"_id": "622177ac43826d6f261f8208", "name": "RUC", "fullname": "Renmin University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86\u5fae\u89c2\u7a7a\u95f4\u667a\u80fd\uff08MiSI\uff09\u7684\u6982\u5ff5\uff0c\u6307\u7684\u662f\u611f\u77e5\u548c\u63a8\u7406\u4e0d\u53ef\u89c1\u5fae\u89c2\u5b9e\u4f53\u7684\u7a7a\u95f4\u5173\u7cfb\u7684\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8fd9\u4e00\u9886\u57df\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u57fa\u51c6\u6846\u67b6MiSI-Bench\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5305\u542b\u8d85\u8fc7163,000\u5bf9\u95ee\u7b54\u548c587,000\u5f20\u6765\u81ea\u7ea64,000\u79cd\u5206\u5b50\u7ed3\u6784\u7684\u56fe\u50cf\uff0c\u6db5\u76d6\u4e5d\u4e2a\u4e92\u8865\u4efb\u52a1\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u9876\u5c16\u7684VLM\u5728\u8fd9\u4e00\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u7ecf\u8fc7\u8c03\u4f18\u76847B\u6a21\u578b\u5728\u7a7a\u95f4\u8f6c\u6362\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u5728\u79d1\u5b66\u57fa\u7840\u4efb\u52a1\uff08\u5982\u6c22\u952e\u8bc6\u522b\uff09\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\uff0c\u5f3a\u8c03\u4e86\u5c06\u660e\u786e\u7684\u9886\u57df\u77e5\u8bc6\u6574\u5408\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63a8\u52a8\u79d1\u5b66AGI\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces Microscopic Spatial Intelligence (MiSI), which is the ability to understand the spatial relationships of tiny, invisible entities important for science.</li>\n    <li>It presents a new benchmark framework called MiSI-Bench, containing over 163,000 question-answer pairs and 587,000 images related to about 4,000 molecular structures.</li>\n    <li>This framework assesses various skills, from basic spatial tasks to complex relationship identification.</li>\n    <li>Current advanced Vision-Language Models (VLMs) do not perform as well as humans on this benchmark, although a fine-tuned model shows promise in certain tasks.</li>\n    <li>The research highlights the need for better integration of scientific knowledge to improve performance in tasks like recognizing hydrogen bonds.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:00:21.000Z", "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models", "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10867.png", "numComments": 1, "submittedBy": {"_id": "61bb00f6c4ac95d207b25f1b", "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg", "fullname": "Zongyang Ma", "name": "zyma", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "622177ac43826d6f261f8208", "name": "RUC", "fullname": "Renmin University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.23386", "authors": [{"_id": "693ba6879874a2a5e4ffb362", "name": "Sinan Du", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb363", "name": "Jiahao Guo", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb364", "user": {"_id": "68fed13597cae9c0f484ebed", "avatarUrl": "/avatars/8df380d81daab4fae8a52d0816bf8099.svg", "isPro": false, "fullname": "libo", "user": "libo31", "type": "user"}, "name": "Bo Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:49.405Z", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb365", "name": "Shuhao Cui", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb366", "name": "Zhengzhuo Xu", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb367", "name": "Yifu Luo", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb368", "name": "Yongxian Wei", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb369", "name": "Kun Gai", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb36a", "name": "Xinggang Wang", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb36b", "user": {"_id": "68e741ea3edb0ff47e20084e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg", "isPro": false, "fullname": "Wu Kai", "user": "KaiiWuu1993", "type": "user"}, "name": "Kai Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:47.442Z", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb36c", "name": "Chun Yuan", "hidden": false}], "publishedAt": "2025-11-28T17:26:34.000Z", "submittedOnDailyAt": "2025-12-12T02:54:20.848Z", "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction", "submittedOnDailyBy": {"_id": "68e741ea3edb0ff47e20084e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg", "isPro": false, "fullname": "Wu Kai", "user": "KaiiWuu1993", "type": "user"}, "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.", "upvotes": 10, "discussionId": "693ba6879874a2a5e4ffb36d", "ai_summary": "VQRAE, a Vector Quantization Representation AutoEncoder, unifies multimodal understanding, generation, and reconstruction using a unified tokenizer with continuous semantic features and discrete tokens.", "ai_keywords": ["multimodal understanding", "generation", "reconstruction", "tokenizer", "dual encoder paradigm", "VQRAE", "Representation AutoEncoders", "Vector Quantization", "symmetric ViT decoder", "two-stage training strategy", "encoder", "self-distillation constraints", "semantic VQ codebook", "pixel reconstruction objective", "multimodal understanding", "discrete tokens", "fine-grained reconstruction", "autoregressive paradigm"], "organization": {"_id": "665f02ce9f9e5b38d0a256a8", "name": "Kwai-Kolors", "fullname": "Kolors Team, Kuaishou Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"}, "summary_zh": "<ul>\n    <li> VQRAE\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5668\uff0c\u65e8\u5728\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u3001\u751f\u6210\u548c\u91cd\u5efa\u8868\u793a\u3002</li>\n    <li> \u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u8fde\u7eed\u8bed\u4e49\u7279\u5f81\u548c\u79bb\u6563\u6807\u8bb0\uff0c\u4f7f\u7528\u5355\u4e00\u7684\u6807\u8bb0\u5668\u8fdb\u884c\u5904\u7406\u3002</li>\n    <li> \u91c7\u7528\u4e86\u5bf9\u79f0\u7684ViT\u89e3\u7801\u5668\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u51bb\u7ed3\u7f16\u7801\u5668\u5b66\u4e60\u9ad8\u7ef4\u8bed\u4e49\u4ee3\u7801\u672c\u3002</li>\n    <li> \u8be5\u65b9\u6cd5\u80fd\u4fdd\u6301\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u751f\u6210\u9002\u5408\u751f\u6210\u548c\u7cbe\u7ec6\u91cd\u5efa\u7684\u79bb\u6563\u6807\u8bb0\u3002</li>\n    <li> VQRAE\u5728\u89c6\u89c9\u7406\u89e3\u3001\u751f\u6210\u548c\u91cd\u5efa\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Creating a unified model that understands and generates images using a single tokenizer is challenging.</li>\n    <li>Previous methods often used separate encoders for understanding and generating images.</li>\n    <li>VQRAE is a new approach that combines continuous semantic features for understanding and discrete tokens for generation.</li>\n    <li>It uses a two-stage training strategy and a high-dimensional codebook for better performance in visual tasks.</li>\n    <li>VQRAE shows strong results in visual understanding, generation, and reconstruction across different benchmarks.</li>\n</ul>"}, "publishedAt": "2025-11-28T12:26:34.000Z", "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction", "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23386.png", "numComments": 1, "submittedBy": {"_id": "68e741ea3edb0ff47e20084e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg", "fullname": "Wu Kai", "name": "KaiiWuu1993", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "665f02ce9f9e5b38d0a256a8", "name": "Kwai-Kolors", "fullname": "Kolors Team, Kuaishou Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10675", "authors": [{"_id": "693b83939874a2a5e4ffb2a4", "name": "Gemini Robotics Team", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a5", "name": "Coline Devin", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a6", "user": {"_id": "63c9bd445fdc575773c732fe", "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg", "isPro": false, "fullname": "Yilun Du", "user": "yilundu", "type": "user"}, "name": "Yilun Du", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:15.439Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a7", "name": "Debidatta Dwibedi", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a8", "name": "Ruiqi Gao", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a9", "name": "Abhishek Jindal", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2aa", "user": {"_id": "6413572b6cd62eb3ba2024e9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6413572b6cd62eb3ba2024e9/UCtMdeV8_N6w-SBnTDXBy.jpeg", "isPro": false, "fullname": "Thomas Kipf", "user": "tkipf", "type": "user"}, "name": "Thomas Kipf", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:33.451Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ab", "user": {"_id": "648c85f33ffc11989bddcce1", "avatarUrl": "/avatars/bdde73b95a36e32cb2975656cca46022.svg", "isPro": false, "fullname": "Sean Kirmani", "user": "skirmani", "type": "user"}, "name": "Sean Kirmani", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:39.448Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ac", "user": {"_id": "63609ef63605bd411c1c7509", "avatarUrl": "/avatars/3fadc3c4117cdf17d25f3efc545388cd.svg", "isPro": false, "fullname": "Fangchen Liu", "user": "fangchenliu", "type": "user"}, "name": "Fangchen Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:44.842Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ad", "name": "Anirudha Majumdar", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ae", "user": {"_id": "60ac2ec408805574fcf076f4", "avatarUrl": "/avatars/a8cd60f39ba89939c5841252dc9d742b.svg", "isPro": false, "fullname": "Andrew Marmon", "user": "acoadmarmon", "type": "user"}, "name": "Andrew Marmon", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:54.023Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2af", "user": {"_id": "65b7c3e87817e067a5b66cf1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/KysLXVoFmkPM2pQMgDlsb.jpeg", "isPro": false, "fullname": "Carolina Parada", "user": "CarolinaParada", "type": "user"}, "name": "Carolina Parada", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:59.800Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b0", "name": "Yulia Rubanova", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b1", "name": "Dhruv Shah", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b2", "name": "Vikas Sindhwani", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b3", "name": "Jie Tan", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b4", "name": "Fei Xia", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b5", "name": "Ted Xiao", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b6", "name": "Sherry Yang", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b7", "name": "Wenhao Yu", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b8", "name": "Allan Zhou", "hidden": false}], "publishedAt": "2025-12-11T14:22:14.000Z", "submittedOnDailyAt": "2025-12-12T00:23:17.104Z", "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.", "upvotes": 8, "discussionId": "693b83939874a2a5e4ffb2b9", "ai_summary": "A generative evaluation system using a frontier video model (Veo) enables comprehensive policy evaluation in robotics, including nominal performance, out-of-distribution generalization, and safety checks.", "ai_keywords": ["generative world models", "video models", "policy evaluation", "out-of-distribution (OOD) generalization", "generative evaluation system", "frontier video foundation model", "Veo", "robot action conditioning", "multi-view consistency", "generative image-editing", "multi-view completion", "bimanual manipulator"], "organization": {"_id": "60f6cbb2852126bac698c89e", "name": "deepmind", "fullname": "Deepmind", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"}, "summary_zh": "<ul>\n    <li>\u751f\u6210\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u6a21\u62df\u673a\u5668\u4eba\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u3002</li>\n    <li>\u89c6\u9891\u6a21\u578b\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u89c2\u5bdf\u548c\u73af\u5883\u4ea4\u4e92\uff0c\u4f46\u76ee\u524d\u4e3b\u8981\u7528\u4e8e\u76f8\u4f3c\u573a\u666f\u7684\u8bc4\u4f30\u3002</li>\n    <li>\u6211\u4eec\u5c55\u793a\u4e86\u89c6\u9891\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u673a\u5668\u4eba\u653f\u7b56\u8bc4\u4f30\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5305\u62ec\u6b63\u5e38\u6027\u80fd\u548c\u8d85\u51fa\u5206\u5e03\u7684\u6cdb\u5316\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u6cbf\u89c6\u9891\u6a21\u578b\uff08Veo\uff09\u7684\u751f\u6210\u8bc4\u4f30\u7cfb\u7edf\uff0c\u652f\u6301\u673a\u5668\u4eba\u52a8\u4f5c\u6761\u4ef6\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002</li>\n    <li>\u901a\u8fc71600\u591a\u6b21\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u5728\u4e0d\u540c\u653f\u7b56\u8868\u73b0\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Generative world models can simulate how robots interact with their environments using video models.</li>\n    <li>Current use of video models in robotics is mostly limited to familiar scenarios that match training data.</li>\n    <li>This report shows that video models can evaluate robot policies in a wide range of situations, including challenging or unfamiliar ones.</li>\n    <li>A new evaluation system, called Veo, was developed to help robots simulate different actions and environments accurately.</li>\n    <li>The system was tested with over 1,600 real-world evaluations, showing its effectiveness in predicting robot performance and identifying safety issues.</li>\n</ul>"}, "publishedAt": "2025-12-11T09:22:14.000Z", "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator", "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10675.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 182}, "organization": {"_id": "60f6cbb2852126bac698c89e", "name": "deepmind", "fullname": "Deepmind", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u7684\u8fd0\u52a8\u63a7\u5236\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u63a7\u5236\u7c97\u7cd9\u548c\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\u3002</li>\n    <li>\u5b83\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u573a\u666f\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u751f\u6210\u7684480p\u30015\u79d2\u89c6\u9891\u7684\u8fd0\u52a8\u53ef\u63a7\u6027\u4e0e\u5546\u4e1a\u8f6f\u4ef6Kling 1.5 Pro\u76f8\u5f53\u3002</li>\n    <li>\u63d0\u4f9b\u4e86MoveBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6837\u5185\u5bb9\u548c\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6ce8\u91ca\uff0c\u4fbf\u4e8e\u5168\u9762\u8bc4\u4f30\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over how objects move in videos.</li>\n    <li>The framework uses dense point trajectories to represent object motions and guides video synthesis.</li>\n    <li>Wan-Move can create 5-second, 480p videos with motion control that competes with existing commercial tools.</li>\n    <li>A new benchmark called MoveBench has been created to evaluate motion quality in videos, and all related resources are publicly available.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.08478", "authors": [{"_id": "6938e00fdfc35938ba129f4f", "name": "Yuning Gong", "hidden": false}, {"_id": "6938e00fdfc35938ba129f50", "name": "Yifei Liu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f51", "name": "Yifan Zhan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f52", "name": "Muyao Niu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f53", "name": "Xueying Li", "hidden": false}, {"_id": "6938e00fdfc35938ba129f54", "name": "Yuanjun Liao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f55", "name": "Jiaming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f56", "name": "Yuanyuan Gao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f57", "name": "Jiaqi Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f58", "name": "Minming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f59", "name": "Li Zhou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5a", "name": "Yuning Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5b", "name": "Wei Wang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5c", "name": "Xiaoqing Hou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5d", "name": "Huaxi Huang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5e", "name": "Shixiang Tang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5f", "name": "Le Ma", "hidden": false}, {"_id": "6938e00fdfc35938ba129f60", "name": "Dingwen Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f61", "name": "Xue Yang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f62", "name": "Junchi Yan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f63", "name": "Yanchi Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f64", "name": "Yinqiang Zheng", "hidden": false}, {"_id": "6938e00fdfc35938ba129f65", "name": "Xiao Sun", "hidden": false}, {"_id": "6938e00fdfc35938ba129f66", "user": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "name": "Zhihang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:56:28.162Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "publishedAt": "2025-12-09T10:54:58.000Z", "submittedOnDailyAt": "2025-12-10T07:43:37.566Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "submittedOnDailyBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "upvotes": 64, "discussionId": "6938e00fdfc35938ba129f67", "projectPage": "https://visionary-laboratory.github.io/visionary/", "githubRepo": "https://github.com/Visionary-Laboratory/visionary", "githubRepoAddedBy": "user", "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.", "ai_keywords": ["Neural rendering", "3D Gaussian Splatting", "3DGS", "WebGPU", "ONNX inference", "Gaussian Generator contract", "three.js", "TypeScript API", "MLP-based 3DGS", "4DGS", "neural avatars", "style transformation", "GPU-based primitive sorting", "World Model Carrier"], "githubStars": 162, "summary_zh": "<ul>\n    <li>\u795e\u7ecf\u6e32\u67d3\uff0c\u5c24\u5176\u662f3D\u9ad8\u65af\u70b9\u4e91\u6e32\u67d3\uff083DGS\uff09\uff0c\u5df2\u8fc5\u901f\u53d1\u5c55\uff0c\u6210\u4e3a\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\u90e8\u5206\u3002</li>\n    <li>\u73b0\u6709\u7684\u67e5\u770b\u5668\u89e3\u51b3\u65b9\u6848\u5206\u6563\u3001\u7b28\u91cd\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u52a8\u6001\u5185\u5bb9\u548c\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>Visionary\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u3001\u57fa\u4e8e\u7f51\u9875\u7684\u5e73\u53f0\uff0c\u652f\u6301\u5b9e\u65f6\u9ad8\u65af\u70b9\u4e91\u548c\u7f51\u683c\u6e32\u67d3\u3002</li>\n    <li>\u8be5\u5e73\u53f0\u4f7f\u7528\u9ad8\u6548\u7684WebGPU\u6e32\u67d3\u5668\uff0c\u652f\u6301\u6bcf\u5e27\u7684ONNX\u63a8\u7406\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u6d4f\u89c8\u5668\u4f53\u9a8c\u3002</li>\n    <li>Visionary\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u7f51\u9875\u5e94\u7528\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u4e14\u5728\u6e32\u67d3\u6548\u7387\u4e0a\u4f18\u4e8e\u5f53\u524d\u7684Web\u67e5\u770b\u5668\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visionary is a new open platform for real-time rendering of 3D Gaussian Splatting and meshes.</li>\n    <li>It uses an efficient WebGPU renderer, allowing dynamic processing with a simple browser-based experience.</li>\n    <li>The platform supports a standardized method for generating and updating Gaussian data each frame.</li>\n    <li>Visionary integrates easily with existing web applications through a TypeScript API and a three.js library.</li>\n    <li>It improves rendering efficiency and supports various advanced rendering techniques, making it easier to use 3D Gaussian Splatting methods.</li>\n</ul>"}, "publishedAt": "2025-12-09T05:54:58.000Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08478.png", "numComments": 3, "submittedBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "fullname": "Zhihang Zhong", "name": "Zuica96", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10430", "authors": [{"_id": "693bba8d9874a2a5e4ffb3ab", "user": {"_id": "64f4c8739ee58d48e8507e0e", "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg", "isPro": false, "fullname": "Dmitrii Stoianov", "user": "heylimon", "type": "user"}, "name": "Dmitrii Stoianov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:40.198Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ac", "user": {"_id": "64fb054ebb362cbf2fe53159", "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg", "isPro": false, "fullname": "Danil Taranets", "user": "taranetsdan", "type": "user"}, "name": "Danil Taranets", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:29.281Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ad", "user": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "name": "Olga Tsymboi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:38.180Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ae", "user": {"_id": "6780dcd6acf8d824c03864da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png", "isPro": false, "fullname": "Ramil Latypov", "user": "kylecr4ne", "type": "user"}, "name": "Ramil Latypov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:23.437Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3af", "user": {"_id": "6513f03e86d74f32ed65e3b8", "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg", "isPro": false, "fullname": "Almaz Dautov", "user": "the-hir0", "type": "user"}, "name": "Almaz Dautov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:30.990Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b0", "user": {"_id": "621a8daf325b927e60fcef08", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg", "isPro": false, "fullname": "Vladislav Kruglikov", "user": "vladislavkruglikov", "type": "user"}, "name": "Vladislav Kruglikov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:21.170Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b1", "name": "Nikita Surkov", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b2", "user": {"_id": "63188c428d698d8c1642a0d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg", "isPro": false, "fullname": "German Abramov", "user": "germanjke", "type": "user"}, "name": "German Abramov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:28.579Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b3", "name": "Pavel Gein", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b4", "user": {"_id": "636a9a07e3ad78bc68b1a5a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg", "isPro": false, "fullname": "Dmitry Abulkhanov", "user": "mponty", "type": "user"}, "name": "Dmitry Abulkhanov", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:02:42.107Z", "hidden": true}, {"_id": "693bba8d9874a2a5e4ffb3b5", "user": {"_id": "658bc20cfdf2279d4721f218", "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg", "isPro": false, "fullname": "Mikhail Gashkov", "user": "MikeGashkov", "type": "user"}, "name": "Mikhail Gashkov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:32.809Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b6", "name": "Viktor Zelenkovskiy", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b7", "user": {"_id": "644f64bc17b6189cda54cae8", "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg", "isPro": false, "fullname": "Artem Batalov", "user": "batalovme", "type": "user"}, "name": "Artem Batalov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:27.497Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b8", "user": {"_id": "62609d224e6e4b84475eb8d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg", "isPro": false, "fullname": "Alex Medvedev", "user": "kenkaneki", "type": "user"}, "name": "Aleksandr Medvedev", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:36.035Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b9", "user": {"_id": "63f26358be95ed4c9a9b0583", "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg", "isPro": false, "fullname": "Anatoly Potapov", "user": "AnatoliiPotapov", "type": "user"}, "name": "Anatolii Potapov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:25.666Z", "hidden": false}], "publishedAt": "2025-12-11T08:40:10.000Z", "submittedOnDailyAt": "2025-12-12T08:33:32.798Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "submittedOnDailyBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "upvotes": 60, "discussionId": "693bba8e9874a2a5e4ffb3ba", "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.", "ai_keywords": ["Cyrillic-dense tokenizer", "EAGLE speculative-decoding pipeline", "hybrid reasoning", "efficient inference", "reasoning-trace generation"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n  <li>\u63a8\u51fa\u4e86T-pro 2.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4fc4\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7528\u4e8e\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u65ad\u3002</li>\n  <li>\u6a21\u578b\u652f\u6301\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u548c\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4e86\u5bc6\u96c6\u7684\u897f\u91cc\u5c14\u5b57\u6bcd\u6807\u8bb0\u5668\u548c\u9002\u5e94\u7684EAGLE\u89e3\u7801\u6d41\u7a0b\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002</li>\n  <li>\u4e3a\u4fc3\u8fdb\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u7814\u7a76\uff0c\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001T-Wix 500k\u6307\u4ee4\u8bed\u6599\u5e93\u3001T-Math\u63a8\u7406\u57fa\u51c6\u548cEAGLE\u6743\u91cd\u3002</li>\n  <li>\u8fd9\u4e9b\u8d44\u6e90\u4f7f\u7528\u6237\u80fd\u591f\u7814\u7a76\u4fc4\u8bed\u63a8\u7406\uff0c\u5e76\u6269\u5c55\u6216\u8c03\u6574\u6a21\u578b\u548c\u63a8\u65ad\u6d41\u7a0b\u3002</li>\n  <li>\u63d0\u4f9b\u7684\u516c\u5171\u7f51\u7edc\u6f14\u793a\u5c55\u793a\u4e86\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u5f0f\uff0c\u8bf4\u660e\u4e86\u63a8\u65ad\u5806\u6808\u5728\u5404\u4e2a\u9886\u57df\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>T-pro 2.0 is a Russian language model designed for fast reasoning and answering questions.</li>\n    <li>It uses a special tokenizer and a new decoding method to reduce response time.</li>\n    <li>The model and supporting materials, like instruction datasets and benchmarks, are available for researchers on Hugging Face.</li>\n    <li>A public web demo shows how the model works in different modes and demonstrates its speed improvements.</li>\n    <li>T-pro 2.0 is aimed at helping users create and test efficient applications using Russian language processing.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:40:10.000Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png", "numComments": 1, "submittedBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "fullname": "Olga Tsymboi", "name": "oltsy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07461", "authors": [{"_id": "6937b96219d912300c34a398", "user": {"_id": "626b889ff451470f861d8c78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651214465695-noauth.jpeg", "isPro": false, "fullname": "victor wu", "user": "victor-wu", "type": "user"}, "name": "Tong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:22.731Z", "hidden": false}, {"_id": "6937b96219d912300c34a399", "user": {"_id": "6191cc9e6d34e827404cebab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg", "isPro": false, "fullname": "Yang", "user": "jacklanda", "type": "user"}, "name": "Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:20.278Z", "hidden": false}, {"_id": "6937b96219d912300c34a39a", "user": {"_id": "624505fcd083d28d314de3dd", "avatarUrl": "/avatars/92cf6b6a1d81d7958dbbd21f0bf63f8f.svg", "isPro": false, "fullname": "bai jun", "user": "ba1jun", "type": "user"}, "name": "Jun Bai", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:17.404Z", "hidden": false}, {"_id": "6937b96219d912300c34a39b", "name": "Zixia Jia", "hidden": false}, {"_id": "6937b96219d912300c34a39c", "name": "Shuyi Zhang", "hidden": false}, {"_id": "6937b96219d912300c34a39d", "name": "Ziyong Lin", "hidden": false}, {"_id": "6937b96219d912300c34a39e", "user": {"_id": "64b119c4372d43407723136b", "avatarUrl": "/avatars/d523e181993eea06b7f6a71a592c995e.svg", "isPro": false, "fullname": "YANTING WANG", "user": "Noane", "type": "user"}, "name": "Yanting Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:14.418Z", "hidden": false}, {"_id": "6937b96219d912300c34a39f", "name": "Song-Chun Zhu", "hidden": false}, {"_id": "6937b96219d912300c34a3a0", "name": "Zilong Zheng", "hidden": false}], "publishedAt": "2025-12-08T11:39:43.000Z", "submittedOnDailyAt": "2025-12-09T04:12:55.960Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "submittedOnDailyBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "isPro": false, "fullname": "Zilong Zheng", "user": "zlzheng", "type": "user"}, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "upvotes": 49, "discussionId": "6937b96219d912300c34a3a1", "projectPage": "https://bigai-nlco.github.io/Native-Parallel-Reasoner/", "githubRepo": "https://github.com/bigai-nlco/Native-Parallel-Reasoner", "ai_summary": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.", "ai_keywords": ["Native Parallel Reasoner", "Large Language Models", "self-evolve", "parallel reasoning", "self-distilled progressive training", "cold-start format discovery", "topological constraints", "Parallel-Aware Policy Optimization", "branching policies", "execution graph", "adaptive decomposition", "trial and error", "NPR Engine", "memory management", "flow control", "parallel RL training", "reasoning benchmarks", "Qwen3-4B", "genuine parallel execution", "autoregressive decoding", "agentic reasoning"], "githubStars": 18, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aNative Parallel Reasoner (NPR) \u7684\u65b0\u6846\u67b6\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u6211\u53d1\u5c55\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>NPR\u901a\u8fc7\u4e09\u9879\u5173\u952e\u521b\u65b0\u5c06\u6a21\u578b\u4ece\u987a\u5e8f\u6267\u884c\u8f6c\u53d8\u4e3a\u672c\u5730\u5e76\u884c\u8ba4\u77e5\u3002</li>\n    <li>\u91c7\u7528\u81ea\u6211\u84b8\u998f\u7684\u6e10\u8fdb\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u9010\u6b65\u53d1\u73b0\u683c\u5f0f\u5e76\u65bd\u52a0\u4e25\u683c\u7684\u62d3\u6251\u7ea6\u675f\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08PAPO\uff09\uff0c\u76f4\u63a5\u5728\u6267\u884c\u56fe\u4e2d\u4f18\u5316\u5206\u652f\u7b56\u7565\uff0c\u652f\u6301\u6a21\u578b\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u81ea\u9002\u5e94\u5206\u89e3\u3002</li>\n    <li>NPR\u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u81f34.6\u500d\uff0c\u5e76\u5b9e\u73b0\u4e86100%\u7684\u771f\u6b63\u5e76\u884c\u6267\u884c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The Native Parallel Reasoner (NPR) helps Large Language Models (LLMs) improve their reasoning abilities without needing a teacher.</li>\n    <li>NPR uses three main innovations: a training method that evolves the model without outside help, a new algorithm that allows the model to learn through experience, and a system that manages memory and control for efficient training.</li>\n    <li>NPR shows significant improvements, achieving up to 24.5% better performance and 4.6 times faster inference on reasoning tasks.</li>\n    <li>Unlike previous methods that rely on sequential processing, NPR can execute reasoning tasks in parallel, setting a new benchmark for efficient reasoning in AI.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:39:43.000Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07461.png", "numComments": 1, "submittedBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "fullname": "Zilong Zheng", "name": "zlzheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.09363", "authors": [{"_id": "693a379e74fced5bf9c32412", "user": {"_id": "6486ff6561053da6442fef1a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg", "isPro": false, "fullname": "KeXing", "user": "KXingLab", "type": "user"}, "name": "Ke Xing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:26.656Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32413", "name": "Longfei Li", "hidden": false}, {"_id": "693a379e74fced5bf9c32414", "user": {"_id": "64b7ab4c037d6452a31910eb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png", "isPro": false, "fullname": "yuyangyin", "user": "yuyangyin", "type": "user"}, "name": "Yuyang Yin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:43:30.256Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32415", "name": "Hanwen Liang", "hidden": false}, {"_id": "693a379e74fced5bf9c32416", "name": "Guixun Luo", "hidden": false}, {"_id": "693a379e74fced5bf9c32417", "name": "Chen Fang", "hidden": false}, {"_id": "693a379e74fced5bf9c32418", "name": "Jue Wang", "hidden": false}, {"_id": "693a379e74fced5bf9c32419", "name": "Konstantinos N. Plataniotis", "hidden": false}, {"_id": "693a379e74fced5bf9c3241a", "name": "Xiaojie Jin", "hidden": false}, {"_id": "693a379e74fced5bf9c3241b", "name": "Yao Zhao", "hidden": false}, {"_id": "693a379e74fced5bf9c3241c", "name": "Yunchao Wei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "publishedAt": "2025-12-10T06:50:16.000Z", "submittedOnDailyAt": "2025-12-11T00:46:52.612Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "upvotes": 44, "discussionId": "693a379e74fced5bf9c3241d", "projectPage": "https://ke-xing.github.io/StereoWorld/", "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.", "ai_keywords": ["stereo video", "monocular-to-stereo", "pretrained video generator", "geometry-aware regularization", "spatio-temporal tiling", "high-definition stereo video dataset", "natural human interpupillary distance (IPD)", "visual fidelity", "geometric consistency"], "summary_zh": "<ul>\n    <li>XR\u8bbe\u5907\u7684\u666e\u53ca\u589e\u52a0\u4e86\u5bf9\u9ad8\u8d28\u91cf\u7acb\u4f53\u89c6\u9891\u7684\u9700\u6c42\uff0c\u4f46\u751f\u4ea7\u6210\u672c\u9ad8\u4e14\u6613\u4ea7\u751f\u4f2a\u5f71\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86StereoWorld\uff0c\u4e00\u4e2a\u7528\u4e8e\u5c06\u5355\u76ee\u89c6\u9891\u751f\u6210\u7acb\u4f53\u89c6\u9891\u7684\u6846\u67b6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u51e0\u4f55\u611f\u77e5\u7684\u6b63\u5219\u5316\u6765\u786e\u4fdd3D\u7ed3\u6784\u7684\u771f\u5b9e\u611f\u3002</li>\n    <li>\u96c6\u6210\u4e86\u65f6\u7a7a\u5e73\u94fa\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1100\u4e07\u5e27\u7684\u9ad8\u6e05\u7acb\u4f53\u89c6\u9891\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u663e\u793aStereoWorld\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>XR devices need high-quality stereo video, but making it is expensive and can have issues.</li>\n  <li>StereoWorld is a new system that uses an existing video generator to create better stereo videos from regular (monocular) videos.</li>\n  <li>The system ensures the 3D structure is accurate by using special techniques to guide the video creation process.</li>\n  <li>It also includes a method to efficiently produce high-resolution videos.</li>\n  <li>A large dataset of over 11 million stereo video frames was created for training and testing, showing that StereoWorld produces much better videos than previous methods.</li>\n</ul>"}, "publishedAt": "2025-12-10T01:50:16.000Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09363.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07525", "authors": [{"_id": "693794d319d912300c34a291", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:38:01.257Z", "hidden": false}, {"_id": "693794d319d912300c34a292", "name": "Yuerong Song", "hidden": false}, {"_id": "693794d319d912300c34a293", "name": "Zhigeng Liu", "hidden": false}, {"_id": "693794d319d912300c34a294", "user": {"_id": "64805e6dde559d48dbb00627", "avatarUrl": "/avatars/29ca34546411dcc28bbc934e3c26a2ba.svg", "isPro": false, "fullname": "Zengfeng", "user": "ZengfengHuang", "type": "user"}, "name": "Zengfeng Huang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:11.754Z", "hidden": false}, {"_id": "693794d319d912300c34a295", "user": {"_id": "6491cd52b1e5d3444528edb1", "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg", "isPro": false, "fullname": "Qipeng Guo", "user": "QipengGuo", "type": "user"}, "name": "Qipeng Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:18.117Z", "hidden": false}, {"_id": "693794d319d912300c34a296", "name": "Zhaoxiang Liu", "hidden": false}, {"_id": "693794d319d912300c34a297", "name": "Shiguo Lian", "hidden": false}, {"_id": "693794d319d912300c34a298", "user": {"_id": "64de18f41d826d7355c285e7", "avatarUrl": "/avatars/23e2c44e3a593415becc02463980f6e8.svg", "isPro": false, "fullname": "Ziwei He", "user": "ziweihe", "type": "user"}, "name": "Ziwei He", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T20:17:41.271Z", "hidden": false}, {"_id": "693794d319d912300c34a299", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:28.411Z", "hidden": false}], "publishedAt": "2025-12-08T12:59:54.000Z", "submittedOnDailyAt": "2025-12-09T00:49:29.234Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "upvotes": 41, "discussionId": "693794d419d912300c34a29a", "githubRepo": "https://github.com/OpenMOSS/rope_pp", "ai_summary": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.", "ai_keywords": ["Rotary Position Embeddings", "Large Language Models", "complex-valued dot product", "attention score", "long-context dependencies", "positional information"], "githubStars": 12, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7f16\u7801\u5e8f\u5217\u987a\u5e8f\u3002</li>\n    <li>\u6807\u51c6\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u6709\u4ef7\u503c\u7684\u865a\u90e8\u4fe1\u606f\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u5f15\u5165\u88ab\u5ffd\u7565\u7684\u865a\u90e8\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u6765\u8ba1\u7b97\u53cc\u7ec4\u5206\u6ce8\u610f\u529b\u5206\u6570\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5efa\u6a21\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u4fdd\u7559\u4e86\u66f4\u591a\u4f4d\u7f6e\u76f8\u5173\u4fe1\u606f\u3002</li>\n    <li>\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u6807\u51c6RoPE\uff0c\u5c24\u5176\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u660e\u663e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Rotary Position Embeddings (RoPE) help Large Language Models understand the order of sequences using rotations in the complex plane.</li>\n    <li>Current methods only use the real part of complex numbers for calculating attention scores, ignoring the useful imaginary part.</li>\n    <li>This paper presents a new method that includes the imaginary component to improve attention scores and retain important positional information.</li>\n    <li>Tests show that this approach better models long-context relationships and improves performance on language tasks as context length increases.</li>\n    <li>The code for this new method is available online for others to use.</li>\n</ul>"}, "publishedAt": "2025-12-08T07:59:54.000Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07525.png", "numComments": 1, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07951", "authors": [{"_id": "6938e892dfc35938ba129ff5", "name": "Zekai Luo", "hidden": false}, {"_id": "6938e892dfc35938ba129ff6", "name": "Zongze Du", "hidden": false}, {"_id": "6938e892dfc35938ba129ff7", "name": "Zhouhang Zhu", "hidden": false}, {"_id": "6938e892dfc35938ba129ff8", "name": "Hao Zhong", "hidden": false}, {"_id": "6938e892dfc35938ba129ff9", "user": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "name": "Muzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T10:51:43.541Z", "hidden": false}, {"_id": "6938e892dfc35938ba129ffa", "name": "Wen Wang", "hidden": false}, {"_id": "6938e892dfc35938ba129ffb", "name": "Yuling Xi", "hidden": false}, {"_id": "6938e892dfc35938ba129ffc", "name": "Chenchen Jing", "hidden": false}, {"_id": "6938e892dfc35938ba129ffd", "name": "Hao Chen", "hidden": false}, {"_id": "6938e892dfc35938ba129ffe", "name": "Chunhua Shen", "hidden": false}], "publishedAt": "2025-12-08T19:00:04.000Z", "submittedOnDailyAt": "2025-12-10T00:59:39.366Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "submittedOnDailyBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "upvotes": 40, "discussionId": "6938e892dfc35938ba129fff", "projectPage": "https://aim-uofa.github.io/LivingSwap", "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.", "ai_keywords": ["keyframe conditioning", "video reference guidance", "temporal stitching", "identity preservation", "high-fidelity reconstruction", "Face2Face dataset"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u5728\u7535\u5f71\u548c\u5a31\u4e50\u5236\u4f5c\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5728\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LivingSwap\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u53c2\u8003\u7684\u89c6\u9891\u6362\u8138\u6a21\u578b\uff0c\u5229\u7528\u6e90\u89c6\u9891\u7684\u89c6\u89c9\u5c5e\u6027\u6765\u63d0\u5347\u6362\u8138\u6548\u679c\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u5173\u952e\u5e27\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u4ee5\u5b9e\u73b0\u7075\u6d3b\u548c\u53ef\u63a7\u7684\u8eab\u4efd\u7f16\u8f91\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u5408\u5173\u952e\u5e27\u6761\u4ef6\u548c\u89c6\u9891\u53c2\u8003\u6307\u5bfc\uff0c\u6a21\u578b\u786e\u4fdd\u4e86\u5728\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u7a33\u5b9a\u7684\u8eab\u4efd\u4fdd\u7559\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u914d\u5bf9\u6362\u8138\u6570\u636e\u96c6Face2Face\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6548\u679c\u4e0a\u5904\u4e8e\u9886\u5148\u6c34\u5e73\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5236\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video face swapping is important in movies but has challenges with quality and consistency in long videos.</li>\n    <li>The study introduces LivingSwap, a new model that uses keyframes to improve face swapping accuracy and control.</li>\n    <li>LivingSwap combines keyframe information with video references to maintain identity and quality throughout longer sequences.</li>\n    <li>A new dataset called Face2Face was created to help train the model effectively.</li>\n    <li>Tests show LivingSwap achieves high-quality results while making the production process easier and faster.</li>\n</ul>"}, "publishedAt": "2025-12-08T14:00:04.000Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07951.png", "numComments": 1, "submittedBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "fullname": "zhumuzhi", "name": "Z-MU-Z", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10739", "authors": [{"_id": "693b91d89874a2a5e4ffb329", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32a", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:56.632Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32b", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32c", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32d", "user": {"_id": "64e8505321540e1da3226b54", "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg", "isPro": false, "fullname": "Wenwei Zhang", "user": "ZwwWayne", "type": "user"}, "name": "Wenwei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:21.987Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32e", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32f", "name": "Fan Zheng", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb330", "user": {"_id": "670f8df2005a358fdc6c2fb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qw2yocepl2vhC5T2ae49b.png", "isPro": false, "fullname": "tianyou", "user": "matianyou", "type": "user"}, "name": "Tianyou Ma", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:57.205Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb331", "user": {"_id": "687f853bb39262ba84f3eeff", "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg", "isPro": false, "fullname": "Junhao Shen", "user": "shenjunhao", "type": "user"}, "name": "Junhao Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:00.496Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb332", "name": "Haiteng Zhao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb333", "user": {"_id": "6454b1073aaeff9f3d330ef6", "avatarUrl": "/avatars/331fcbbf18a72b0dc8419ca3a77299bb.svg", "isPro": false, "fullname": "Duanyang Zhang", "user": "KKKDaniel", "type": "user"}, "name": "Duanyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:12.911Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb334", "name": "Huilun Zhang", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb335", "user": {"_id": "63fd691794cc8f815d50c112", "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg", "isPro": false, "fullname": "liu", "user": "Harold-lkk", "type": "user"}, "name": "Kuikun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:58.650Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb336", "name": "Chengqi Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb337", "name": "Yanhui Duan", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb338", "name": "Chiyu Chen", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb339", "name": "Ningsheng Ma", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33a", "name": "Jianfei Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33b", "name": "Han Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33c", "user": {"_id": "636317ed80c1a705a6eff396", "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg", "isPro": false, "fullname": "Dahua Lin", "user": "lindahua", "type": "user"}, "name": "Dahua Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:20.346Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33d", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:26:28.000Z", "submittedOnDailyAt": "2025-12-12T01:27:55.307Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "upvotes": 37, "discussionId": "693b91d99874a2a5e4ffb33e", "ai_summary": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "long reasoning chains of thought (CoTs)", "iterative active learning", "Rejection Fine-Tuning (RFT)", "F1 score", "accuracy", "DeepSeek-R1-Distill-Qwen-32B", "AIME2025"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5f97\u76ca\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002</li>\n    <li>\u73b0\u6709\u7684\u7ed3\u679c\u57fa\u7840\u9a8c\u8bc1\u5668\u65e0\u6cd5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\uff0c\u800c\u8fc7\u7a0b\u57fa\u7840\u9a8c\u8bc1\u5668\u5728\u68c0\u6d4b\u590d\u6742\u957f\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u65f6\u9762\u4e34\u56f0\u96be\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u9a8c\u8bc1\u957f\u63a8\u7406\u7684\u603b\u7ed3\u7ed3\u679c\u3002</li>\n    <li>OPV\u91c7\u7528\u5faa\u73af\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6ce8\u91ca\u9010\u6b65\u63d0\u9ad8\u9a8c\u8bc1\u80fd\u529b\uff0c\u51cf\u5c11\u4eba\u5de5\u6ce8\u91ca\u6210\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPV\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u8bb8\u591a\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are improving at solving complex reasoning tasks using a method called Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>Current verification methods struggle to check unreliable steps in long reasoning processes, and there are not enough high-quality human annotations available.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) aims to verify the reasoning process more accurately and efficiently, using summarized outcomes from long reasoning chains.</li>\n    <li>OPV uses an iterative active learning approach to reduce annotation costs while improving its verification accuracy with expert input.</li>\n    <li>Tests show OPV performs better than larger models, achieving a high F1 score and significantly improving accuracy in collaboration with policy models.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:26:28.000Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10739.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10949", "authors": [{"_id": "693b895d9874a2a5e4ffb302", "user": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "name": "Yiwen Tang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:55.504Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb303", "user": {"_id": "642a8302d651bae3c11b72b1", "avatarUrl": "/avatars/4d2d422613e274d80482fed9a7d3f785.svg", "isPro": false, "fullname": "Zoey Guo", "user": "Purple1288", "type": "user"}, "name": "Zoey Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:01.705Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb304", "user": {"_id": "6708920aeae29d1cd41a703b", "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg", "isPro": false, "fullname": "kaixin zhu", "user": "czkk566", "type": "user"}, "name": "Kaixin Zhu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:10.275Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb305", "name": "Ray Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb306", "user": {"_id": "6535045a910b844786a6642f", "avatarUrl": "/avatars/37a94864a7a348151837b421ea6d77e3.svg", "isPro": false, "fullname": "Qizhi Chen", "user": "Tavish9", "type": "user"}, "name": "Qizhi Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:11.060Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb307", "user": {"_id": "6349214f8146350b3a4c5cdf", "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg", "isPro": false, "fullname": "Dongzhi Jiang", "user": "CaraJ", "type": "user"}, "name": "Dongzhi Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:06:02.910Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb308", "name": "Junli Liu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb309", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:08.733Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30a", "user": {"_id": "6662d2c9de4c4e1f04bd29c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QnnO_KOyZjFd-iXuPnHqR.png", "isPro": false, "fullname": "HaomingSong", "user": "HaomingSong", "type": "user"}, "name": "Haoming Song", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:30.062Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30b", "user": {"_id": "64daecec888b7e9c400f59b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png", "isPro": false, "fullname": "Delin Qu", "user": "delinqu", "type": "user"}, "name": "Delin Qu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:34.085Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30c", "name": "Tianyi Bai", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30d", "name": "Dan Xu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30e", "name": "Wentao Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30f", "name": "Bin Zhao", "hidden": false}], "publishedAt": "2025-12-11T18:59:52.000Z", "submittedOnDailyAt": "2025-12-12T00:49:43.850Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "submittedOnDailyBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "upvotes": 36, "discussionId": "693b895d9874a2a5e4ffb310", "githubRepo": "https://github.com/Ivan-Tang-3D/3DGen-R1", "githubRepoAddedBy": "user", "ai_summary": "This study investigates reinforcement learning for text-to-3D generation, focusing on reward designs, RL algorithms, benchmarking, and hierarchical optimization, introducing AR3D-R1 as the first RL-enhanced model for 3D generation.", "ai_keywords": ["reinforcement learning", "text-to-3D generation", "reward designs", "GRPO variants", "token-level optimization", "MME-3DR", "Hi-GRPO", "hierarchical 3D generation", "AR3D-R1"], "githubStars": 40, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5f3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5df2\u6210\u529f\u5e94\u7528\u4e8e2D\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u57283D\u751f\u6210\u65b9\u9762\u4ecd\u672a\u88ab\u5e7f\u6cdb\u63a2\u7d22\uff0c\u4e3b\u8981\u56e0\u4e3a3D\u7269\u4f53\u7684\u590d\u6742\u6027\u3002</li>\n    <li>\u6211\u4eec\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6587\u672c\u52303D\u81ea\u56de\u5f52\u751f\u6210\u7684RL\u5e94\u7528\uff0c\u5173\u6ce8\u5956\u52b1\u8bbe\u8ba1\u548c\u7b97\u6cd5\u3002</li>\n    <li>\u8bc4\u4f30\u5956\u52b1\u8bbe\u8ba1\u548c\u6a21\u578b\u9009\u62e9\uff0c\u53d1\u73b0\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u63d0\u51faMME-3DR\u57fa\u51c6\uff0c\u4ee5\u6d4b\u91cf3D\u751f\u6210\u6a21\u578b\u7684\u9690\u6027\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1\u4e86AR3D-R1\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u6587\u672c\u52303D\u6a21\u578b\uff0c\u4ece\u7c97\u7565\u5f62\u72b6\u5230\u7eb9\u7406\u7ec6\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is effective for 2D image generation, but its application to 3D generation is still under-researched due to the complexity of 3D objects.</li>\n    <li>The study systematically explores RL for creating 3D models from text, focusing on reward designs, RL algorithms, and benchmarks.</li>\n    <li>It emphasizes the importance of aligning rewards with human preferences and shows that multi-modal models can effectively assess 3D attributes.</li>\n    <li>New benchmarks, like MME-3DR, are introduced to evaluate reasoning abilities in 3D generation models.</li>\n    <li>The study also presents a new model, AR3D-R1, which improves 3D generation by optimizing the process from basic shape creation to detailed texture refinement.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:59:52.000Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10949.png", "numComments": 2, "submittedBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "fullname": "Ivan Tang", "name": "IvanTang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07469", "authors": [{"_id": "69379e0319d912300c34a2fb", "user": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "name": "Xiangpeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T10:34:33.919Z", "hidden": false}, {"_id": "69379e0319d912300c34a2fc", "name": "Ji Xie", "hidden": false}, {"_id": "69379e0319d912300c34a2fd", "name": "Yiyuan Yang", "hidden": false}, {"_id": "69379e0319d912300c34a2fe", "name": "Yan Huang", "hidden": false}, {"_id": "69379e0319d912300c34a2ff", "name": "Min Xu", "hidden": false}, {"_id": "69379e0319d912300c34a300", "name": "Qiang Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "publishedAt": "2025-12-08T11:50:18.000Z", "submittedOnDailyAt": "2025-12-09T01:34:28.853Z", "title": "Unified Video Editing with Temporal Reasoner", "submittedOnDailyBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "upvotes": 31, "discussionId": "69379e0319d912300c34a301", "projectPage": "https://videocof.github.io/", "githubRepo": "https://github.com/knightyxp/VideoCoF", "ai_summary": "VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.", "ai_keywords": ["chain-of-frames", "chain-of-thought reasoning", "video diffusion model", "reasoning tokens", "edit-region latents", "target video tokens", "instruction-to-region alignment", "fine-grained video editing", "RoPE alignment", "motion alignment", "length extrapolation"], "githubStars": 25, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u9762\u4e34\u7cbe\u786e\u6027\u4e0e\u7edf\u4e00\u6027\u7684\u6743\u8861\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5VideoCoF\uff0c\u901a\u8fc7\u201c\u770b\u3001\u63a8\u7406\u3001\u7f16\u8f91\u201d\u7684\u6b65\u9aa4\u6765\u6539\u5584\u89c6\u9891\u7f16\u8f91\u7cbe\u5ea6\u3002</li>\n    <li>VideoCoF\u4e0d\u9700\u8981\u7528\u6237\u63d0\u4f9b\u7684\u63a9\u7801\uff0c\u80fd\u51c6\u786e\u6620\u5c04\u6307\u4ee4\u5230\u89c6\u9891\u533a\u57df\u3002</li>\n    <li>\u5f15\u5165RoPE\u5bf9\u9f50\u7b56\u7565\uff0c\u786e\u4fdd\u8fd0\u52a8\u5bf9\u9f50\u5e76\u652f\u6301\u8d85\u51fa\u8bad\u7ec3\u65f6\u957f\u7684\u957f\u5ea6\u5ef6\u5c55\u3002</li>\n    <li>VideoCoF\u5728\u4ec5\u4f7f\u752850,000\u5bf9\u89c6\u9891\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u89c6\u9891\u7f16\u8f91\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video editing methods struggle between precision and flexibility, with expert models needing specific inputs like masks and unified models lacking spatial details.</li>\n    <li>VideoCoF is a new approach that uses a \"Chain-of-Frames\" method to improve video editing by predicting reasoning tokens before editing.</li>\n    <li>This method allows for precise editing without needing user-provided masks and enhances instruction-to-region mapping.</li>\n    <li>It includes a RoPE alignment strategy for better motion alignment and allows for extending video lengths beyond what was originally trained.</li>\n    <li>VideoCoF shows strong performance with just 50,000 video pairs, outperforming other methods in video editing tasks.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:50:18.000Z", "title": "Unified Video Editing with Temporal Reasoner", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07469.png", "numComments": 5, "submittedBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "fullname": "Xiangpeng Yang", "name": "XiangpengYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u8f6c\u5316\u4e3a\u529f\u80fd\u4ee3\u7801\uff0c\u6539\u53d8\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u3002</li>\n    <li>\u5546\u4e1a\u5de5\u5177\u5982Github Copilot\u548cClaude Code\u63a8\u52a8\u4e86LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6027\u80fd\u4ece\u4e2a\u4f4d\u6570\u63d0\u5347\u5230\u8d85\u8fc795%\u7684\u6210\u529f\u7387\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u5168\u9762\u5206\u6790\uff0c\u5305\u62ec\u6a21\u578b\u751f\u547d\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u5982\u6570\u636e\u5904\u7406\u3001\u8bad\u7ec3\u548c\u7f16\u7801\u4ee3\u7406\u3002</li>\n    <li>\u6211\u4eec\u6bd4\u8f83\u4e86\u901a\u7528LLMs\u4e0e\u4e13\u95e8\u9488\u5bf9\u4ee3\u7801\u7684LLMs\u7684\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u6280\u672f\u3001\u8bbe\u8ba1\u51b3\u7b56\u53ca\u5176\u6743\u8861\u3002</li>\n    <li>\u8fd8\u5206\u6790\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u4e0e\u5b9e\u9645\u9700\u6c42\u7684\u5339\u914d\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are changing how software is developed by turning natural language into code quickly and effectively.</li>\n    <li>Tools like Github Copilot and others have made LLMs popular in the software industry, achieving high success rates in coding tasks.</li>\n    <li>This work offers a detailed guide on how to use code LLMs, covering everything from data preparation to advanced coding techniques.</li>\n    <li>We compare general models like GPT-4 to specialized coding models and discuss their strengths, weaknesses, and design choices.</li>\n    <li>We also explore the gap between academic research and real-world applications in software development, suggesting future research directions to meet practical needs.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u517c\u5177\u8ba1\u7b97\u6548\u7387\u548c\u4f18\u79c0\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u672c\u5728\u591a\u4e2a\u7ade\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u80fd\u7cfb\u7edf\u6027\u5730\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines high efficiency with strong reasoning abilities and agent performance.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which reduces computational costs while maintaining effectiveness in long-context situations.</li>\n    <li>The model includes a scalable reinforcement learning framework, allowing it to perform similarly to GPT-5, with its special version outperforming GPT-5 and matching the reasoning skills of Gemini-3.0-Pro.</li>\n    <li>DeepSeek-V3.2 achieved top results in the 2025 International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It has a new data generation pipeline for training, improving its ability to follow instructions and perform in complex environments.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u4f8b\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u4e00\u4e9b\u5f00\u6e90\u66ff\u4ee3\u54c1\uff08\u5982Qwen-Image\u548cHunyuan-Image-3.0\uff09\u53c2\u6570\u6570\u91cf\u5de8\u5927\uff0c\u5bfc\u81f4\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u96be\u4ee5\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u57fa\u7840\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u7ecf\u8fc7\u4f18\u5316\uff0c\u53ef\u4ee5\u5728\u7ea6314K H800 GPU\u5c0f\u65f6\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u4e14\u652f\u6301\u666e\u901a\u786c\u4ef6\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u903c\u771f\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u53ef\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u5e76\u5df2\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u6f14\u793a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current image generation models are mostly proprietary and large, making them hard to use on regular hardware.</li>\n    <li>Z-Image is a new, efficient model with only 6 billion parameters, designed for better performance without needing excessive resources.</li>\n    <li>It was trained quickly and cost-effectively, using a special training method that allows it to run on both powerful and consumer-grade hardware.</li>\n    <li>Z-Image can create high-quality images and understand multiple languages, showing results that compete with the best commercial models.</li>\n    <li>All resources, including code and demos, are made publicly available to support the development of affordable and effective generative models.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u6613\u51fa\u73b0\u5e7b\u89c9\u73b0\u8c61\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u7406\u89e3\u957f\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u6d4f\u89c8\u548c\u7ec6\u8282\u68c0\u67e5\u6765\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>LongVT\u5229\u7528LMM\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u901a\u8fc7\u526a\u8f91\u89c6\u9891\u7247\u6bb5\u5e76\u91cd\u65b0\u91c7\u6837\u89c6\u9891\u5e27\u6765\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u8bad\u7ec3\u548c\u8bc4\u4f30\u7528\u7684247,900\u4e2a\u6837\u672c\uff0c\u65e8\u5728\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u3002</li>\n    <li>\u7ecf\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0cLongVT\u5728\u56db\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models can struggle with understanding long videos, often leading to errors called hallucinations.</li>\n    <li>The LongVT framework helps improve video reasoning by allowing a global-to-local approach, similar to how humans watch videos.</li>\n    <li>It uses a method that zooms in on specific video clips for better detail and understanding.</li>\n    <li>A new dataset called VideoSIAH will be released to support training and evaluation, containing a large number of samples for fine-tuning and testing.</li>\n    <li>LongVT has been shown to perform better than existing methods in understanding and reasoning with long videos through a well-structured training process.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u8ba1\u7b97\u987a\u5e8f\u548c\u957f\u671f\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u5229\u7528\u591aGPU\u8fdb\u884c\u53bb\u566a\u5904\u7406\uff0c\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u4e86\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u7ef4\u62a4\u5e8f\u5217\u7684\u4fdd\u771f\u5ea6\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620\u5e27\u6bcf\u79d2\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u6807\u5fd7\u7740\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u65b0\u8303\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Live Avatar is a new system that allows for real-time, high-quality avatar generation using advanced algorithms.</li>\n  <li>It uses a large 14-billion-parameter model and a technique called Timestep-forcing Pipeline Parallelism to speed up the generation process.</li>\n  <li>The system improves video consistency and reduces visual issues with a method called Rolling Sink Frame Mechanism.</li>\n  <li>It can generate video at 20 frames per second on 5 GPUs, making it suitable for practical use.</li>\n  <li>This work sets a new standard for using advanced models in creating long videos for applications like streaming and gaming.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u6d89\u53ca\u8bbe\u8ba1\u548c\u6784\u5efa\u591a\u9636\u6bb5SQL\u7ba1\u9053\uff0c\u5e76\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\u4e0b\u6539\u8fdb\u7cfb\u7edf\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u89e3\u51b3\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u5148\u8fdb\u6a21\u578b\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u5de5\u7a0b\u548c\u5206\u6790\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u771f\u6b63\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark consisting of 210 tasks that simulate real-world data workflows in businesses.</li>\n    <li>It includes data engineering tasks that require creating and modifying complex SQL pipelines, and data analysis tasks that involve solving open-ended business problems.</li>\n    <li>Performance on data engineering tasks is very low, with success rates under 20%, highlighting difficulties in managing complete data processes.</li>\n    <li>Data analysis tasks also show poor performance, with average success rates below 40%, indicating challenges in open-ended reasoning.</li>\n    <li>DAComp helps identify these issues, aiming to improve the development of autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.13254", "authors": [{"_id": "691c1c836bfd5965c0fd39e4", "user": {"_id": "6419b34a9a27800807c34a63", "avatarUrl": "/avatars/ee1391a9a153bae0dd04323b1fa5b5d6.svg", "isPro": false, "fullname": "Shalini M", "user": "shalinimaiti", "type": "user"}, "name": "Shalini Maiti", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:47.486Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e5", "user": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "name": "Amar Budhiraja", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:49.892Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e6", "user": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "isPro": false, "fullname": "Bhavul Gauri", "user": "bhavul", "type": "user"}, "name": "Bhavul Gauri", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:31.124Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e7", "user": {"_id": "691c6b9b660c15d270b5838a", "avatarUrl": "/avatars/2dc40e079bd8b7af7ae4a4ac43acc552.svg", "isPro": false, "fullname": "Gaurav Chaurasia", "user": "gchauras", "type": "user"}, "name": "Gaurav Chaurasia", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:45.558Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e8", "name": "Anton Protopopov", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e9", "name": "Alexis Audran-Reiss", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ea", "name": "Michael Slater", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39eb", "name": "Despoina Magka", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ec", "name": "Tatiana Shavrina", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ed", "name": "Roberta Raileanu", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ee", "name": "Yoram Bachrach", "hidden": false}], "publishedAt": "2025-11-17T11:13:34.000Z", "submittedOnDailyAt": "2025-11-18T04:47:00.815Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "submittedOnDailyBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "upvotes": 131, "discussionId": "691c1c846bfd5965c0fd39fc", "githubRepo": "https://github.com/facebookresearch/llm_souping", "githubStars": 60, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>\u6a21\u578b\u201c\u6df7\u5408\u201d\uff08model souping\uff09\u901a\u8fc7\u5bf9\u591a\u4e2a\u76f8\u540c\u67b6\u6784\u7684\u6a21\u578b\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u80fd\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86\u201c\u7c7b\u522b\u4e13\u5bb6\u6df7\u5408\u201d\uff08SoCE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u51c6\u7ec4\u5408\u6765\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u6765\u4f18\u5316\u6027\u80fd\u3002</li>\n    <li>SoCE\u65b9\u6cd5\u5229\u7528\u57fa\u51c6\u7c7b\u522b\u4e4b\u95f4\u7684\u4f4e\u76f8\u5173\u6027\u6765\u8bc6\u522b\u201c\u4e13\u5bb6\u201d\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4f18\u5316\u52a0\u6743\u5408\u5e76\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u80fd\u529b\u3001\u5de5\u5177\u8c03\u7528\u548c\u6570\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are powerful but require a lot of resources and time to train.</li>\n    <li>Model souping, or averaging weights from multiple models, can improve performance without needing extensive retraining.</li>\n    <li>This paper presents Soup Of Category Experts (SoCE), a new method for model souping that selects the best models based on benchmark categories.</li>\n    <li>SoCE uses weighted averaging to combine models, focusing on those that perform well in related areas instead of using equal weights for all models.</li>\n    <li>The method shows better performance and reliability in various tasks, achieving top results in certain benchmarks.</li>\n</ul>"}, "publishedAt": "2025-11-17T06:13:34.000Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13254.png", "numComments": 4, "submittedBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "fullname": "Amar Budhiraja", "name": "ambud26", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Dec 15, 2025";
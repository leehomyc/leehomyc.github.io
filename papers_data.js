window.trendingPapers = {
    "today": [{"paper": {"id": "2601.03252", "authors": [{"_id": "695dc956c03d6d81e4399ea4", "name": "Hao Yu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea5", "user": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "name": "Haotong Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:04.783Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea6", "name": "Jiawei Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea7", "name": "Jiaxin Li", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea8", "name": "Yida Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea9", "user": {"_id": "6791a6c19ce382eae861ed61", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg", "isPro": false, "fullname": "Xueyang Zhang", "user": "zhangxueyang001", "type": "user"}, "name": "Xueyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:39.946Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399eaa", "name": "Yue Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399eab", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "695dc956c03d6d81e4399eac", "name": "Ruizhen Hu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ead", "user": {"_id": "62986ca2b58e71e2ac9b8f01", "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg", "isPro": false, "fullname": "Sida Peng", "user": "pengsida", "type": "user"}, "name": "Sida Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:12.074Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "publishedAt": "2026-01-06T18:57:06.000Z", "submittedOnDailyAt": "2026-01-07T00:26:37.060Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "submittedOnDailyBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "upvotes": 71, "discussionId": "695dc956c03d6d81e4399eae", "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.", "ai_keywords": ["neural implicit fields", "local implicit decoder", "continuous 2D coordinates", "arbitrary-resolution depth estimation", "synthetic benchmark", "4K synthetic benchmark", "novel view synthesis", "viewpoint shifts"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u5728\u79bb\u6563\u7684\u56fe\u50cf\u7f51\u683c\u4e0a\u9884\u6d4b\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u7ec6\u8282\u6062\u590d\u80fd\u529b\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86InfiniDepth\uff0c\u5c06\u6df1\u5ea6\u8868\u793a\u4e3a\u795e\u7ecf\u9690\u5f0f\u573a\uff0c\u53ef\u4ee5\u5728\u8fde\u7eed\u76842D\u5750\u6807\u4e0a\u67e5\u8be2\u6df1\u5ea6\uff0c\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u548c\u7ec6\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76844K\u5408\u6210\u57fa\u51c6\uff0c\u6db5\u76d6\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u6e38\u620f\u7684\u4e30\u5bcc\u573a\u666f\uff0c\u4ee5\u8bc4\u4f30\u65b9\u6cd5\u7684\u80fd\u529b\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0cInfiniDepth\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u8282\u4e30\u5bcc\u7684\u533a\u57df\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u5927\u89c6\u89d2\u8f6c\u6362\u4e0b\u7684\u65b0\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u51cf\u5c11\u4e86\u7a7a\u6d1e\u548c\u4f2a\u5f71\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current depth estimation methods can only predict depth on fixed image grids, which limits their flexibility and detail.</li>\n    <li>The paper introduces InfiniDepth, which uses neural implicit fields to estimate depth at any point in a 2D space, allowing for high-resolution and detailed depth maps.</li>\n    <li>To test InfiniDepth, researchers created a high-quality 4K synthetic benchmark using scenes from five different video games.</li>\n    <li>InfiniDepth outperforms existing methods in both synthetic and real-world depth estimation tasks, especially in areas with fine details.</li>\n    <li>It also improves the quality of images when creating new views from different angles, resulting in fewer visual errors.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:57:06.000Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png", "numComments": 8, "submittedBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "fullname": "Haotong Lin", "name": "haotongl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01554", "authors": [{"_id": "695dcda5c03d6d81e4399eb8", "name": "MOSI. AI", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eb9", "name": "Donghua Yu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eba", "name": "Zhengyuan Lin", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebb", "user": {"_id": "660c345da15ab85523ad00d1", "avatarUrl": "/avatars/b0bfdee89a6c62ff12140b9e85de499a.svg", "isPro": false, "fullname": "Chen Yang", "user": "kiiic", "type": "user"}, "name": "Chen Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:58.894Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebc", "name": "Yiyang Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebd", "name": "Hanfu Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebe", "name": "Jingqi Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebf", "name": "Ke Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec0", "name": "Liwei Fan", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec1", "name": "Yi Jiang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec2", "name": "Jie Zhu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec3", "name": "Muchen Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec4", "name": "Wenxuan Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec5", "name": "Yang Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec6", "user": {"_id": "6443f7bf1bc692d87b25e234", "avatarUrl": "/avatars/fa9e62d96d0691a9a48e3db499a61557.svg", "isPro": false, "fullname": "Xu Zhe", "user": "Phospheneser", "type": "user"}, "name": "Zhe Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:55.950Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec7", "name": "Yitian Gong", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec8", "name": "Yuqian Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec9", "name": "Wenbo Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eca", "user": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "name": "Zhaoye Fei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:10.124Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecb", "user": {"_id": "695757e4fd9dc6e9bac27935", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_uZEu4oOlKJVYqrG763Z-.jpeg", "isPro": false, "fullname": "aa", "user": "qinyuancheng", "type": "user"}, "name": "Qinyuan Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:03.749Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecc", "name": "Shimin Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecd", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:50.004Z", "hidden": false}], "publishedAt": "2026-01-04T15:01:10.000Z", "submittedOnDailyAt": "2026-01-07T00:52:16.123Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "submittedOnDailyBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "upvotes": 45, "discussionId": "695dcda6c03d6d81e4399ece", "projectPage": "https://mosi.cn/models/moss-transcribe-diarize", "ai_summary": "A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.", "ai_keywords": ["multimodal large language model", "end-to-end paradigm", "speaker diarization", "time-stamped transcription", "context window", "robust generalization"], "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>SATS\uff08\u8bf4\u8bdd\u8005\u5f52\u5c5e\u3001\u65f6\u95f4\u6233\u8f6c\u5f55\uff09\u7528\u4e8e\u51c6\u786e\u8bb0\u5f55\u4f1a\u8bae\u53d1\u8a00\u53ca\u5176\u65f6\u95f4\u3002</li>\n    <li>\u73b0\u6709\u7684SATS\u7cfb\u7edf\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff0c\u5982\u4e0d\u80fd\u5b9e\u73b0\u7aef\u5230\u7aef\u8f6c\u5f55\u548c\u8f93\u51fa\u65f6\u95f4\u6233\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MOSS Transcribe Diarize\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5730\u8fdb\u884c\u8bf4\u8bdd\u8005\u5f52\u5c5e\u548c\u65f6\u95f4\u6233\u8f6c\u5f55\u3002</li>\n    <li>MOSS\u6a21\u578b\u7ecf\u8fc7\u5927\u91cf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u652f\u6301\u957f\u8fbe90\u5206\u949f\u7684\u8f93\u5165\u548c128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002</li>\n    <li>\u5728\u5404\u79cd\u8bc4\u4f30\u4e2d\uff0cMOSS\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5546\u4e1a\u7cfb\u7edf\uff0c\u8868\u73b0\u66f4\u52a0\u4f18\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MOSS Transcribe Diarize is a new tool for transcribing meetings, focusing on identifying who speaks and when.</li>\n    <li>Current systems have limitations, like not being able to handle long contexts or provide exact timestamps.</li>\n    <li>This new model can process up to 90 minutes of audio and is trained on a large amount of real-world data.</li>\n    <li>MOSS Transcribe Diarize is designed to work well in an end-to-end manner, improving efficiency and accuracy.</li>\n    <li>It has shown better performance than leading commercial transcription systems in various tests.</li>\n</ul>"}, "publishedAt": "2026-01-04T10:01:10.000Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01554.png", "numComments": 2, "submittedBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "fullname": "Zhaoye Fei", "name": "ngc7293", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03233", "authors": [{"_id": "695dc6d9c03d6d81e4399e85", "user": {"_id": "6303cc5e0547362a22a51af0", "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg", "isPro": false, "fullname": "Yoav HaCohen", "user": "yoavhacohen", "type": "user"}, "name": "Yoav HaCohen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:29.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e86", "user": {"_id": "6489c487b9e9258ba065418f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png", "isPro": false, "fullname": "Benny Brazowski", "user": "benibraz", "type": "user"}, "name": "Benny Brazowski", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:35.981Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e87", "user": {"_id": "62dd30a8d43078cd49ac8ad8", "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg", "isPro": false, "fullname": "Nisan Chiprut", "user": "nisan", "type": "user"}, "name": "Nisan Chiprut", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:41.634Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e88", "user": {"_id": "64a7adc087cbd4dc7301fdd6", "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg", "isPro": false, "fullname": "Yaki Bitterman", "user": "jacobitterman", "type": "user"}, "name": "Yaki Bitterman", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:46.749Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e89", "user": {"_id": "65897258509bcae23fa162c9", "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg", "isPro": false, "fullname": "Andrew Kvochko", "user": "kvochko", "type": "user"}, "name": "Andrew Kvochko", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:51.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8a", "name": "Avishai Berkowitz", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8b", "name": "Daniel Shalem", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8c", "user": {"_id": "681af83e2f4aaa88639e703d", "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg", "isPro": false, "fullname": "Daphna Lifschitz", "user": "Daphnal", "type": "user"}, "name": "Daphna Lifschitz", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:04.180Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8d", "user": {"_id": "636b97a57631fe5e86fe1fa2", "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg", "isPro": false, "fullname": "Dudu Moshe", "user": "dudumoshe", "type": "user"}, "name": "Dudu Moshe", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:13.512Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8e", "name": "Eitan Porat", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8f", "user": {"_id": "677a422979d3c32a5dd87a0a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png", "isPro": false, "fullname": "Eitan Richardson", "user": "eitanrich", "type": "user"}, "name": "Eitan Richardson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:22.677Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e90", "user": {"_id": "673f6911d83832a6ce15e7bf", "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg", "isPro": false, "fullname": "Guy Shiran", "user": "guysrn", "type": "user"}, "name": "Guy Shiran", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:28.250Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e91", "user": {"_id": "65744a2fe09de6aa74026d80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg", "isPro": false, "fullname": "Itay Chachy", "user": "ItayChachy", "type": "user"}, "name": "Itay Chachy", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:36.781Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e92", "name": "Jonathan Chetboun", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e93", "user": {"_id": "6678365ac411b340b32d6148", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg", "isPro": false, "fullname": "Michael Finkelson", "user": "MichaelFinkelson", "type": "user"}, "name": "Michael Finkelson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:53.574Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e94", "user": {"_id": "6318aa43cb4ca740c4c55651", "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg", "isPro": false, "fullname": "michael kupchick", "user": "michaellightricks", "type": "user"}, "name": "Michael Kupchick", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:59.945Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e95", "user": {"_id": "673f29b568595672b8d3e90e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png", "isPro": false, "fullname": "Nir Zabari", "user": "NirZabariLTX", "type": "user"}, "name": "Nir Zabari", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:07.297Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e96", "user": {"_id": "64ae89c043dda9449a1eb1ba", "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg", "isPro": false, "fullname": "Nitzan Guetta", "user": "nitzanguetta", "type": "user"}, "name": "Nitzan Guetta", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:14.712Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e97", "name": "Noa Kotler", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e98", "user": {"_id": "631f58935ba8c026340b377c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg", "isPro": false, "fullname": "Ofir Bibi", "user": "ofirbibi", "type": "user"}, "name": "Ofir Bibi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:27.196Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e99", "user": {"_id": "674348b46215a2c0878e219b", "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg", "isPro": false, "fullname": "Ori Gordon", "user": "origordon", "type": "user"}, "name": "Ori Gordon", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:34.878Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9a", "name": "Poriya Panet", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9b", "name": "Roi Benita", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9c", "name": "Shahar Armon", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9d", "name": "Victor Kulikov", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9e", "name": "Yaron Inger", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9f", "name": "Yonatan Shiftan", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea0", "name": "Zeev Melumian", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea1", "name": "Zeev Farbman", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "publishedAt": "2026-01-06T18:24:41.000Z", "submittedOnDailyAt": "2026-01-07T00:07:29.528Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "upvotes": 40, "discussionId": "695dc6d9c03d6d81e4399ea2", "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v", "githubRepo": "https://github.com/Lightricks/LTX-2", "githubRepoAddedBy": "user", "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.", "ai_keywords": ["text-to-video diffusion models", "audiovisual content", "dual-stream transformer", "cross-attention layers", "temporal positional embeddings", "AdaLN", "classifier-free guidance", "modality-aware classifier-free guidance", "multilingual text encoder", "diffusion models"], "githubStars": 922, "summary_zh": "<ul>\n    <li>LTX-2\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e0d\u5bf9\u79f0\u7684\u53cc\u6d41\u53d8\u6362\u5668\uff0c\u89c6\u9891\u6d41\u548c\u97f3\u9891\u6d41\u7684\u53c2\u6570\u5206\u522b\u4e3a14\u4ebf\u548c5\u4ebf\u3002</li>\n    <li>LTX-2\u901a\u8fc7\u53cc\u5411\u97f3\u89c6\u9891\u4ea4\u53c9\u6ce8\u610f\u5c42\u6765\u5b9e\u73b0\u97f3\u9891\u548c\u89c6\u9891\u7684\u9ad8\u6548\u7ed3\u5408\u3002</li>\n    <li>\u6a21\u578b\u4e0d\u4ec5\u80fd\u751f\u6210\u8bed\u97f3\uff0c\u8fd8\u80fd\u521b\u9020\u7b26\u5408\u573a\u666f\u7684\u4e30\u5bcc\u97f3\u8f68\uff0c\u5305\u62ec\u81ea\u7136\u80cc\u666f\u97f3\u548c\u97f3\u6548\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u4e2d\uff0cLTX-2\u7684\u89c6\u542c\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u6027\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u8fdc\u4f4e\u4e8e\u4e13\u6709\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LTX-2 is a new open-source model that creates high-quality videos with synchronized audio.</li>\n    <li>The model uses a dual-stream transformer with separate components for video (14 billion parameters) and audio (5 billion parameters).</li>\n    <li>It includes features for better understanding of text prompts and improved alignment of audio and video.</li>\n    <li>LTX-2 generates not just speech, but also background sounds and sound effects that match the video's scenes.</li>\n    <li>The model provides high-quality results similar to proprietary models but is more efficient and cost-effective.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:24:41.000Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22334", "authors": [{"_id": "695e0748c03d6d81e439a027", "user": {"_id": "67d2a001919e3b981ad45066", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d2a001919e3b981ad45066/KxaYKOGgLVbVhN6EsYGRG.jpeg", "isPro": false, "fullname": "wyh", "user": "naonaowyh", "type": "user"}, "name": "Yiheng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:26.201Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a028", "user": {"_id": "64c396def6fe448b1ad553d6", "avatarUrl": "/avatars/b2ce4739f42dc00ee974fff7ee1cb301.svg", "isPro": false, "fullname": "Yixin Chen", "user": "YixinChen", "type": "user"}, "name": "Yixin Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T14:25:35.753Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a029", "name": "Shuo Li", "hidden": false}, {"_id": "695e0748c03d6d81e439a02a", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:19.089Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a02b", "name": "Bo Liu", "hidden": false}, {"_id": "695e0748c03d6d81e439a02c", "user": {"_id": "66d19399b26010e571b1fcf0", "avatarUrl": "/avatars/9d0998fc38f6659305bcbcecaf0a1c96.svg", "isPro": false, "fullname": "Hengjian Gao", "user": "Talthy", "type": "user"}, "name": "Hengjian Gao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:28.254Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a02d", "user": {"_id": "64a3d1ddb3239f3e3892b24b", "avatarUrl": "/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg", "isPro": false, "fullname": "Jiakang Yuan", "user": "JiakangYuan", "type": "user"}, "name": "Jiakang Yuan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T14:25:23.256Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a02e", "name": "Jia Bu", "hidden": false}, {"_id": "695e0748c03d6d81e439a02f", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:21.127Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a030", "user": {"_id": "63bab9c1bb6a2fabd14421bd", "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg", "isPro": false, "fullname": "Yuhao Zhou", "user": "Soptq", "type": "user"}, "name": "Yuhao Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:18:03.373Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a031", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "695e0748c03d6d81e439a032", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "695e0748c03d6d81e439a033", "name": "Fengxiang Wang", "hidden": false}, {"_id": "695e0748c03d6d81e439a034", "user": {"_id": "63ee1379190ddd6214efd73a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png", "isPro": false, "fullname": "HAODONG DUAN", "user": "KennyUTC", "type": "user"}, "name": "Haodong Duan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:18:16.943Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a035", "name": "Songyang Zhang", "hidden": false}, {"_id": "695e0748c03d6d81e439a036", "name": "Jun Yao", "hidden": false}, {"_id": "695e0748c03d6d81e439a037", "name": "Han Deng", "hidden": false}, {"_id": "695e0748c03d6d81e439a038", "name": "Yizhou Wang", "hidden": false}, {"_id": "695e0748c03d6d81e439a039", "user": {"_id": "650d2f798ffe1f53bdd4ae71", "avatarUrl": "/avatars/4c733681f8301d801023105c0b3aba38.svg", "isPro": false, "fullname": "Xiao Jiabei", "user": "Xiao-Youth", "type": "user"}, "name": "Jiabei Xiao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T14:25:59.280Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a03a", "name": "Jiaqi Liu", "hidden": false}, {"_id": "695e0748c03d6d81e439a03b", "user": {"_id": "6784a79c5ffcc118504ed99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eL4kAIp_oi9Q7si5--uVl.png", "isPro": false, "fullname": "Encheng Su", "user": "EncSU", "type": "user"}, "name": "Encheng Su", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:18:31.162Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a03c", "name": "Yujie Liu", "hidden": false}, {"_id": "695e0748c03d6d81e439a03d", "user": {"_id": "661b9d96c153e4a0a25adc3e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg", "isPro": false, "fullname": "Weida Wang", "user": "weidawang", "type": "user"}, "name": "Weida Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:23.748Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a03e", "name": "Junchi Yao", "hidden": false}, {"_id": "695e0748c03d6d81e439a03f", "name": "Shenghe Zheng", "hidden": false}, {"_id": "695e0748c03d6d81e439a040", "name": "Haoran Sun", "hidden": false}, {"_id": "695e0748c03d6d81e439a041", "name": "Runmin Ma", "hidden": false}, {"_id": "695e0748c03d6d81e439a042", "name": "Xiangchao Yan", "hidden": false}, {"_id": "695e0748c03d6d81e439a043", "name": "Bo Zhang", "hidden": false}, {"_id": "695e0748c03d6d81e439a044", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "695e0748c03d6d81e439a045", "user": {"_id": "68f8924f4278c8968587dfea", "avatarUrl": "/avatars/b48d7899881a14ca68cc9feb2643ad8c.svg", "isPro": false, "fullname": "shufei zhang", "user": "ShufeiZhang", "type": "user"}, "name": "Shufei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:18:24.428Z", "hidden": false}, {"_id": "695e0748c03d6d81e439a046", "name": "Peng Ye", "hidden": false}, {"_id": "695e0748c03d6d81e439a047", "name": "Xiaosong Wang", "hidden": false}, {"_id": "695e0748c03d6d81e439a048", "name": "Shixiang Tang", "hidden": false}, {"_id": "695e0748c03d6d81e439a049", "name": "Wenlong Zhang", "hidden": false}, {"_id": "695e0748c03d6d81e439a04a", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-26T17:36:02.000Z", "submittedOnDailyAt": "2026-01-07T05:09:43.722Z", "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence", "submittedOnDailyBy": {"_id": "63bab9c1bb6a2fabd14421bd", "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg", "isPro": false, "fullname": "Yuhao Zhou", "user": "Soptq", "type": "user"}, "summary": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.", "upvotes": 28, "discussionId": "695e0748c03d6d81e439a04b", "projectPage": "https://opencompass.org.cn/Intern-Discovery-Eval/rank", "githubRepo": "https://github.com/InternScience/SciEvalKit", "githubRepoAddedBy": "user", "ai_summary": "SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.", "ai_keywords": ["AI models", "scientific intelligence", "Scientific Multimodal Perception", "Scientific Multimodal Reasoning", "Scientific Multimodal Understanding", "Scientific Symbolic Reasoning", "Scientific Code Generation", "Science Hypothesis Generation", "Scientific Knowledge Understanding", "scientific foundation models", "intelligent agents"], "githubStars": 56, "organization": {"_id": "690af7a885f71496ea396393", "name": "InternScience", "fullname": "Intern Science", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faSciEvalKit\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u79d1\u5b66\u9886\u57dfAI\u6a21\u578b\u7684\u7edf\u4e00\u57fa\u51c6\u5de5\u5177\u5305\u3002</li>\n    <li>SciEvalKit\u4e13\u6ce8\u4e8e\u79d1\u5b66\u667a\u80fd\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5982\u591a\u6a21\u6001\u611f\u77e5\u3001\u63a8\u7406\u548c\u7406\u89e3\u7b49\u3002</li>\n    <li>\u652f\u6301\u5305\u62ec\u7269\u7406\u3001\u5316\u5b66\u3001\u5929\u6587\u5b66\u548c\u6750\u6599\u79d1\u5b66\u5728\u5185\u7684\u516d\u5927\u79d1\u5b66\u9886\u57df\u3002</li>\n    <li>\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\uff0c\u786e\u4fdd\u4efb\u52a1\u53cd\u6620\u771f\u5b9e\u7684\u79d1\u5b66\u6311\u6218\u3002</li>\n    <li>\u5de5\u5177\u5305\u662f\u5f00\u6e90\u7684\uff0c\u652f\u6301\u793e\u533a\u9a71\u52a8\u7684\u53d1\u5c55\uff0c\u65e8\u5728\u63a8\u52a8AI\u5728\u79d1\u5b66\u9886\u57df\u7684\u8fdb\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SciEvalKit is a toolkit for evaluating AI models specifically in scientific fields.</li>\n    <li>It focuses on key scientific skills like perception, reasoning, code generation, and knowledge understanding.</li>\n    <li>The toolkit covers six major science areas, including physics, chemistry, and astronomy.</li>\n    <li>It uses real-world datasets to create expert-standard benchmarks that reflect real scientific challenges.</li>\n    <li>SciEvalKit is open-source and encourages community contributions to improve AI in science.</li>\n</ul>"}, "publishedAt": "2025-12-26T12:36:02.000Z", "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence", "summary": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22334.png", "numComments": 1, "submittedBy": {"_id": "63bab9c1bb6a2fabd14421bd", "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg", "fullname": "Yuhao Zhou", "name": "Soptq", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "690af7a885f71496ea396393", "name": "InternScience", "fullname": "Intern Science", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03193", "authors": [{"_id": "695de87dc03d6d81e4399fd2", "user": {"_id": "6723369dc09be95d8c49c605", "avatarUrl": "/avatars/797df409537c07cbf894f1c027cddbb1.svg", "isPro": false, "fullname": "Ruiyan Han", "user": "Hungryyan", "type": "user"}, "name": "Ruiyan Han", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:53.416Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd3", "user": {"_id": "64b0a5037a475fba70a7260d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg", "isPro": false, "fullname": "Zhen Fang", "user": "CostaliyA", "type": "user"}, "name": "Zhen Fang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T13:13:51.100Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd4", "name": "XinYu Sun", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd5", "name": "Yuchen Ma", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd6", "name": "Ziheng Wang", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd7", "user": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "name": "Yu Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:35.308Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd8", "user": {"_id": "64892d31cbda0d1cdb956897", "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg", "isPro": false, "fullname": "Zehui Chen", "user": "lovesnowbest", "type": "user"}, "name": "Zehui Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:22:13.382Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd9", "user": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "name": "Lin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:37.096Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fda", "user": {"_id": "67dc162ec8c00778e8689f42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png", "isPro": false, "fullname": "Wenxuan Huang", "user": "Osilly", "type": "user"}, "name": "Wenxuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T13:37:57.684Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fdb", "name": "Wei-Jie Xu", "hidden": false}, {"_id": "695de87dc03d6d81e4399fdc", "name": "Yi Cao", "hidden": false}, {"_id": "695de87dc03d6d81e4399fdd", "name": "Feng Zhao", "hidden": false}], "publishedAt": "2026-01-06T17:15:50.000Z", "submittedOnDailyAt": "2026-01-07T02:31:26.744Z", "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision", "submittedOnDailyBy": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.", "upvotes": 25, "discussionId": "695de87dc03d6d81e4399fde", "projectPage": "https://costaliya.github.io/UniCorn.github.io/", "githubRepo": "https://github.com/Hungryyan1/UniCorn", "githubRepoAddedBy": "user", "ai_summary": "UniCorn, a self-improvement framework for unified multimodal models, addresses generation gaps through self-play and cognitive pattern reconstruction, achieving state-of-the-art results in text-to-image generation.", "ai_keywords": ["Unified Multimodal Models", "Conduction Aphasia", "UniCorn", "self-improvement framework", "self-play", "cognitive pattern reconstruction", "Text to Image", "cycle-consistency", "UniCycle", "T2I generation"], "githubStars": 25, "summary_zh": "<ul>\n    <li>\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u6a21\u6001\u7406\u89e3\u4e0a\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u9ad8\u8d28\u91cf\u751f\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002</li>\n    <li>\u6211\u4eec\u5c06\u8fd9\u79cd\u5dee\u8ddd\u79f0\u4e3a\u201c\u4f20\u5bfc\u5931\u8bed\u201d\uff0c\u5373\u6a21\u578b\u80fd\u7406\u89e3\u591a\u6a21\u6001\u8f93\u5165\uff0c\u4f46\u96be\u4ee5\u5c06\u7406\u89e3\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u751f\u6210\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86UniCorn\uff0c\u4e00\u4e2a\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u6559\u5e08\u76d1\u7763\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\u3002</li>\n    <li>UniCorn\u5c06\u5355\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5206\u4e3a\u4e09\u79cd\u534f\u4f5c\u89d2\u8272\uff1a\u63d0\u8bae\u8005\u3001\u89e3\u51b3\u8005\u548c\u8bc4\u5224\u8005\uff0c\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08\u751f\u6210\u9ad8\u8d28\u91cf\u4e92\u52a8\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cUniCorn\u5728\u591a\u4e2a\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u5728TIIF\u3001DPG\u548cCompBench\u7b49\u9879\u76ee\u4e0a\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u6027\u80fd\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Unified Multimodal Models (UMMs) can understand different types of inputs well but have trouble creating high-quality outputs from that understanding.</li>\n    <li>This issue is termed Conduction Aphasia, where models can interpret inputs but fail to generate coherent results.</li>\n    <li>The proposed solution, UniCorn, uses a self-improvement method without needing extra data or supervision by splitting the model into three roles: Proposer, Solver, and Judge.</li>\n    <li>UniCorn improves quality by using self-play and reconstructing cognitive patterns to make better outputs.</li>\n    <li>Tests show that UniCorn greatly outperforms the original model in generating images while keeping strong comprehension, achieving top results in several benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-06T12:15:50.000Z", "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision", "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03193.png", "numComments": 2, "submittedBy": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "fullname": "Lin Chen", "name": "Lin-Chen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 91, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02427", "authors": [{"_id": "695dce13c03d6d81e4399ed0", "name": "Lo\u00efc Magne", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed1", "name": "Anas Awadalla", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed2", "name": "Guanzhi Wang", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed3", "name": "Yinzhen Xu", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed4", "name": "Joshua Belofsky", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed5", "name": "Fengyuan Hu", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed6", "name": "Joohwan Kim", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed7", "name": "Ludwig Schmidt", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed8", "name": "Georgia Gkioxari", "hidden": false}, {"_id": "695dce13c03d6d81e4399ed9", "name": "Jan Kautz", "hidden": false}, {"_id": "695dce13c03d6d81e4399eda", "name": "Yisong Yue", "hidden": false}, {"_id": "695dce13c03d6d81e4399edb", "name": "Yejin Choi", "hidden": false}, {"_id": "695dce13c03d6d81e4399edc", "name": "Yuke Zhu", "hidden": false}, {"_id": "695dce13c03d6d81e4399edd", "name": "Linxi \"Jim\" Fan", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/wxFZ7NVsObT5w9DBp9Gdu.mp4"], "publishedAt": "2026-01-04T16:24:50.000Z", "submittedOnDailyAt": "2026-01-07T00:38:46.198Z", "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.", "upvotes": 22, "discussionId": "695dce14c03d6d81e4399ede", "projectPage": "https://nitrogen.minedojo.org/", "githubRepo": "https://github.com/MineDojo/NitroGen", "githubRepoAddedBy": "user", "ai_summary": "NitroGen is a vision-action foundation model trained on extensive gameplay data that demonstrates strong cross-game generalization and effective transfer learning capabilities.", "ai_keywords": ["vision-action foundation model", "large-scale behavior cloning", "cross-game generalization", "transfer learning", "embodied agents"], "githubStars": 1443, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86NitroGen\uff0c\u4e00\u4e2a\u7528\u4e8e\u901a\u7528\u6e38\u620f\u4ee3\u7406\u7684\u89c6\u89c9-\u884c\u52a8\u57fa\u7840\u6a21\u578b\uff0c\u8bad\u7ec3\u6570\u636e\u6765\u6e90\u4e8e\u8d85\u8fc71000\u6b3e\u6e38\u620f\u76844\u4e07\u5c0f\u65f6\u6e38\u620f\u89c6\u9891\u3002</li>\n    <li>\u6a21\u578b\u7684\u4e09\u4e2a\u5173\u952e\u8981\u7d20\u5305\u62ec\uff1a\u5927\u89c4\u6a21\u89c6\u9891-\u884c\u52a8\u6570\u636e\u96c6\u3001\u8de8\u6e38\u620f\u901a\u7528\u6027\u6d4b\u8bc4\u73af\u5883\uff0c\u4ee5\u53ca\u7edf\u4e00\u7684\u89c6\u89c9-\u884c\u52a8\u6a21\u578b\u3002</li>\n    <li>NitroGen\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u59823D\u52a8\u4f5c\u6e38\u620f\u7684\u6218\u6597\u30012D\u5e73\u53f0\u6e38\u620f\u7684\u9ad8\u7cbe\u5ea6\u63a7\u5236\u548c\u7a0b\u5e8f\u751f\u6210\u4e16\u754c\u7684\u63a2\u7d22\u3002</li>\n    <li>\u8be5\u6a21\u578b\u80fd\u6709\u6548\u8f6c\u79fb\u5230\u672a\u89c1\u8fc7\u7684\u6e38\u620f\uff0c\u76f8\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8652%\u3002</li>\n    <li>\u6211\u4eec\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u6a21\u578b\u6743\u91cd\uff0c\u4ee5\u63a8\u52a8\u901a\u7528\u5177\u8eab\u4ee3\u7406\u7684\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NitroGen is a new model for gaming agents, trained on 40,000 hours of gameplay from over 1,000 games.</li>\n    <li>It uses a large dataset of player actions extracted from publicly available gameplay videos.</li>\n    <li>The model is tested in a multi-game environment to see how well it performs across different games.</li>\n    <li>NitroGen shows strong skills in various gaming tasks, like combat, control, and exploration.</li>\n    <li>The creators are sharing the dataset and model to help improve research on gaming agents.</li>\n</ul>"}, "publishedAt": "2026-01-04T11:24:50.000Z", "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents", "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/wxFZ7NVsObT5w9DBp9Gdu.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02427.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03044", "authors": [{"_id": "695dc422c03d6d81e4399e60", "user": {"_id": "655ecd7c56e5ceaf05344b24", "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg", "isPro": false, "fullname": "MingjieP", "user": "pmj110119", "type": "user"}, "name": "Mingjie Pan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:27:27.004Z", "hidden": false}, {"_id": "695dc422c03d6d81e4399e61", "user": {"_id": "620326e962b2b0e46e79971b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620326e962b2b0e46e79971b/1FVPRpsWng5q3An4qbuYQ.jpeg", "isPro": false, "fullname": "Siyuan Feng", "user": "Eralien", "type": "user"}, "name": "Siyuan Feng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:26:03.276Z", "hidden": false}, {"_id": "695dc422c03d6d81e4399e62", "name": "Qinglin Zhang", "hidden": false}, {"_id": "695dc422c03d6d81e4399e63", "name": "Xinchen Li", "hidden": false}, {"_id": "695dc422c03d6d81e4399e64", "user": {"_id": "64993b27f8069251837b81ed", "avatarUrl": "/avatars/9aa3956dc527dccfdc6b6014dedf761f.svg", "isPro": false, "fullname": "Song Jianheng", "user": "JJH1998", "type": "user"}, "name": "Jianheng Song", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:28:32.580Z", "hidden": false}, {"_id": "695dc422c03d6d81e4399e65", "name": "Chendi Qu", "hidden": false}, {"_id": "695dc422c03d6d81e4399e66", "name": "Yi Wang", "hidden": false}, {"_id": "695dc422c03d6d81e4399e67", "name": "Chuankang Li", "hidden": false}, {"_id": "695dc422c03d6d81e4399e68", "user": {"_id": "64d107229617774ce41b9467", "avatarUrl": "/avatars/10d4dff031f2d8dd7047d7ea5ec0d4c1.svg", "isPro": false, "fullname": "Ziyu Xiong", "user": "JennyZiyu", "type": "user"}, "name": "Ziyu Xiong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:28:14.043Z", "hidden": false}, {"_id": "695dc422c03d6d81e4399e69", "name": "Zhi Chen", "hidden": false}, {"_id": "695dc422c03d6d81e4399e6a", "name": "Yi Liu", "hidden": false}, {"_id": "695dc422c03d6d81e4399e6b", "user": {"_id": "64f8cb8ed04a890f5380d9a4", "avatarUrl": "/avatars/d6fdfdbb0c10141aa3b4c832d928121b.svg", "isPro": false, "fullname": "Jianlan Luo", "user": "jianlanluo", "type": "user"}, "name": "Jianlan Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T13:13:52.912Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/655ecd7c56e5ceaf05344b24/AEQ64pAi4UD3rvJ33Qeag.mp4"], "publishedAt": "2026-01-06T14:25:11.000Z", "submittedOnDailyAt": "2026-01-07T03:50:19.522Z", "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models", "submittedOnDailyBy": {"_id": "655ecd7c56e5ceaf05344b24", "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg", "isPro": false, "fullname": "MingjieP", "user": "pmj110119", "type": "user"}, "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.", "upvotes": 19, "discussionId": "695dc422c03d6d81e4399e6c", "ai_summary": "A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.", "ai_keywords": ["Vision-language-action models", "post-training", "online learning", "distributed learning", "multi-task learning", "closed-loop architecture", "on-policy experience", "interactive imitation learning", "reinforcement learning", "fleet-scale deployment"], "organization": {"_id": "6959ca2481c675360361a275", "name": "KineMind", "fullname": "AgiBot Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69548eb63499b9bcd6ee0574/g02RWir_9quNCKL40jiV0.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u540e\u8bad\u7ec3\u7cfb\u7edf\uff08SOP\uff09\uff0c\u4f7f\u901a\u7528\u7684VLA\u6a21\u578b\u80fd\u591f\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8fdb\u884c\u5728\u7ebf\u3001\u591a\u4efb\u52a1\u7684\u540e\u8bad\u7ec3\u3002</li>\n    <li>SOP\u901a\u8fc7\u5c06\u6267\u884c\u548c\u5b66\u4e60\u7d27\u5bc6\u7ed3\u5408\uff0c\u5141\u8bb8\u4e00\u7fa4\u673a\u5668\u4eba\u4e0d\u65ad\u6536\u96c6\u7ecf\u9a8c\u5e76\u5411\u4e91\u7aef\u5b66\u4e60\u7cfb\u7edf\u53d1\u9001\u4fe1\u53f7\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u652f\u6301\u5feb\u901f\u7684\u653f\u7b56\u4fee\u6b63\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u90e8\u7f72\u63d0\u9ad8\u7ecf\u9a8c\u6536\u96c6\u6548\u7387\uff0c\u540c\u65f6\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u901a\u7528\u6027\u3002</li>\n    <li>\u5728\u591a\u79cd\u5b9e\u9645\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff08\u5982\u6298\u53e0\u8863\u7269\u3001\u7ec4\u88c5\u76d2\u5b50\u548c\u8865\u5145\u98df\u54c1\uff09\uff0cSOP\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u9884\u8bad\u7ec3VLA\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n    <li>\u6709\u6548\u7684\u540e\u8bad\u7ec3\u53ef\u4ee5\u5728\u6570\u5c0f\u65f6\u5185\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u4e92\u52a8\u5b9e\u73b0\uff0c\u5e76\u4e14\u673a\u5668\u4eba\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u51e0\u4e4e\u5448\u7ebf\u6027\u589e\u957f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VLA models are good at general tasks but need more specific skills for real-world use.</li>\n    <li>The new Scalable Online Post-training (SOP) system allows robots to learn and adapt while they work in real-time.</li>\n    <li>SOP connects robot actions and learning, enabling quick updates and improvements based on their experiences.</li>\n    <li>This system can use different learning methods and has been tested on tasks like folding cloth and assembling boxes.</li>\n    <li>With SOP, robots can improve their skills quickly, and more robots can lead to better performance.</li>\n</ul>"}, "publishedAt": "2026-01-06T09:25:11.000Z", "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models", "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/655ecd7c56e5ceaf05344b24/AEQ64pAi4UD3rvJ33Qeag.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03044.png", "numComments": 1, "submittedBy": {"_id": "655ecd7c56e5ceaf05344b24", "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg", "fullname": "MingjieP", "name": "pmj110119", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6959ca2481c675360361a275", "name": "KineMind", "fullname": "AgiBot Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69548eb63499b9bcd6ee0574/g02RWir_9quNCKL40jiV0.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02785", "authors": [{"_id": "695dd030c03d6d81e4399ee8", "user": {"_id": "6805bdfb344d6d8a8fd5b07a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png", "isPro": false, "fullname": "Mengtian Li", "user": "LemonSky1995", "type": "user"}, "name": "Mengtian Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:22:38.847Z", "hidden": false}, {"_id": "695dd030c03d6d81e4399ee9", "name": "Jinshu Chen", "hidden": false}, {"_id": "695dd030c03d6d81e4399eea", "name": "Songtao Zhao", "hidden": false}, {"_id": "695dd030c03d6d81e4399eeb", "user": {"_id": "64462aec86679b753ff7ece0", "avatarUrl": "/avatars/ff0e1e693b984c9d569dc462fa7fe4ef.svg", "isPro": false, "fullname": "Wanquan Feng", "user": "wanquan", "type": "user"}, "name": "Wanquan Feng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:23:08.687Z", "hidden": false}, {"_id": "695dd030c03d6d81e4399eec", "user": {"_id": "66151dae18269ebfe9702abf", "avatarUrl": "/avatars/cd10619fe843a75c0ff29a7c0d295e88.svg", "isPro": false, "fullname": "tupengqi", "user": "tupengqi", "type": "user"}, "name": "Pengqi Tu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:23:19.334Z", "hidden": false}, {"_id": "695dd030c03d6d81e4399eed", "name": "Qian He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1B89POtz-ncYIp_iw_rCT.qt"], "publishedAt": "2026-01-06T07:42:12.000Z", "submittedOnDailyAt": "2026-01-07T00:48:26.802Z", "title": "DreamStyle: A Unified Framework for Video Stylization", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.", "upvotes": 17, "discussionId": "695dd030c03d6d81e4399eee", "projectPage": "https://lemonsky1995.github.io/dreamstyle/", "ai_summary": "DreamStyle is a unified video stylization framework that supports multiple style conditions while addressing style inconsistency and temporal flicker through a specialized data curation pipeline and LoRA training approach.", "ai_keywords": ["video stylization", "Image-to-Video (I2V) model", "Low-Rank Adaptation (LoRA)", "token-specific up matrices", "style consistency", "temporal flicker"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u98ce\u683c\u5316\u662f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u4f46\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u7684\u98ce\u683c\u6761\u4ef6\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002</li>\n    <li>\u63d0\u51fa\u4e86DreamStyle\u6846\u67b6\uff0c\u652f\u6301\u6587\u672c\u3001\u98ce\u683c\u56fe\u50cf\u548c\u9996\u5e27\u6307\u5bfc\u7684\u89c6\u9891\u98ce\u683c\u5316\u3002</li>\n    <li>DreamStyle\u914d\u6709\u9ad8\u8d28\u91cf\u89c6\u9891\u6570\u636e\u7684\u6536\u96c6\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u98ce\u683c\u4e0d\u4e00\u81f4\u548c\u65f6\u95f4\u95ea\u70c1\u7684\u95ee\u9898\u3002</li>\n    <li>DreamStyle\u5728\u591a\u4e2a\u89c6\u9891\u98ce\u683c\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u5728\u98ce\u683c\u4e00\u81f4\u6027\u548c\u89c6\u9891\u8d28\u91cf\u4e0a\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video stylization is an important area of video generation that has not been fully explored.</li>\n    <li>Current methods usually focus on one type of style input, which limits their versatility.</li>\n    <li>DreamStyle is a new framework that supports three types of style conditions: text, style images, and first frames.</li>\n    <li>It uses a special method to gather high-quality video data and improve consistency in style.</li>\n    <li>DreamStyle shows better performance in style consistency and video quality compared to existing methods.</li>\n</ul>"}, "publishedAt": "2026-01-06T02:42:12.000Z", "title": "DreamStyle: A Unified Framework for Video Stylization", "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1B89POtz-ncYIp_iw_rCT.qt"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02785.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02780", "authors": [{"_id": "695dd171c03d6d81e4399ef4", "user": {"_id": "6732130e07f893d9c185c908", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C31Me-ujgNy5k2KLL5hh6.png", "isPro": false, "fullname": "bangjun xiao", "user": "xbjpku", "type": "user"}, "name": "Bangjun Xiao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:25:11.197Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399ef5", "user": {"_id": "6348c3cda6aa28fa6313e906", "avatarUrl": "/avatars/2c0f0b08b1371689ebb4df18ddf45e54.svg", "isPro": false, "fullname": "Bingquan Xia", "user": "xiabingquan", "type": "user"}, "name": "Bingquan Xia", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:25:19.374Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399ef6", "user": {"_id": "693ee055a14dfa27729d6684", "avatarUrl": "/avatars/3ae4edc04cc740ca61f6f8e4ad011413.svg", "isPro": false, "fullname": "mi-yb", "user": "ami-yb", "type": "user"}, "name": "Bo Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:53.834Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399ef7", "user": {"_id": "65ae21adabf6d1ccb795e9a4", "avatarUrl": "/avatars/b5dced62c6a3564095a8fa0959bc06cb.svg", "isPro": false, "fullname": "Bofei Gao", "user": "KbsdJames", "type": "user"}, "name": "Bofei Gao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:25:37.497Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399ef8", "name": "Bowen Shen", "hidden": false}, {"_id": "695dd171c03d6d81e4399ef9", "name": "Chen Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399efa", "user": {"_id": "6829e9379292b4796b6d9124", "avatarUrl": "/avatars/838acf681fce69703737d31845f4bef0.svg", "isPro": false, "fullname": "chen honghe", "user": "chenhonghe", "type": "user"}, "name": "Chenhong He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:26:21.940Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399efb", "user": {"_id": "6528be15578679aac79e925f", "avatarUrl": "/avatars/bd5dbb3ff43b2faa034c4cd381263d9c.svg", "isPro": false, "fullname": "Chiheng Lou", "user": "gjghfd", "type": "user"}, "name": "Chiheng Lou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:26:29.953Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399efc", "user": {"_id": "6538815d1bdb3c40db94fbfa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6538815d1bdb3c40db94fbfa/id7aSY8JUgKK2agKWLERt.jpeg", "isPro": false, "fullname": "Fuli Luo", "user": "luofuli", "type": "user"}, "name": "Fuli Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:26:36.566Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399efd", "name": "Gang Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399efe", "name": "Gang Xie", "hidden": false}, {"_id": "695dd171c03d6d81e4399eff", "name": "Hailin Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f00", "name": "Hanglong Lv", "hidden": false}, {"_id": "695dd171c03d6d81e4399f01", "name": "Hanyu Li", "hidden": false}, {"_id": "695dd171c03d6d81e4399f02", "name": "Heyu Chen", "hidden": false}, {"_id": "695dd171c03d6d81e4399f03", "name": "Hongshen Xu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f04", "name": "Houbin Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f05", "name": "Huaqiu Liu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f06", "name": "Jiangshan Duo", "hidden": false}, {"_id": "695dd171c03d6d81e4399f07", "name": "Jianyu Wei", "hidden": false}, {"_id": "695dd171c03d6d81e4399f08", "name": "Jiebao Xiao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f09", "name": "Jinhao Dong", "hidden": false}, {"_id": "695dd171c03d6d81e4399f0a", "name": "Jun Shi", "hidden": false}, {"_id": "695dd171c03d6d81e4399f0b", "name": "Junhao Hu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f0c", "name": "Kainan Bao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f0d", "name": "Kang Zhou", "hidden": false}, {"_id": "695dd171c03d6d81e4399f0e", "name": "Lei Li", "hidden": false}, {"_id": "695dd171c03d6d81e4399f0f", "name": "Liang Zhao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f10", "name": "Linghao Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f11", "name": "Peidian Li", "hidden": false}, {"_id": "695dd171c03d6d81e4399f12", "name": "Qianli Chen", "hidden": false}, {"_id": "695dd171c03d6d81e4399f13", "name": "Shaohui Liu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f14", "name": "Shihua Yu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f15", "name": "Shijie Cao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f16", "user": {"_id": "64f5216e3c9b5e07f85582e3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5216e3c9b5e07f85582e3/tWe8_9R2nNxQpPv-wa4nB.jpeg", "isPro": false, "fullname": "Ezio Chen", "user": "Ezioii", "type": "user"}, "name": "Shimao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:44.714Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399f17", "name": "Shouqiu Yu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f18", "name": "Shuo Liu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f19", "name": "Tianling Zhou", "hidden": false}, {"_id": "695dd171c03d6d81e4399f1a", "name": "Weijiang Su", "hidden": false}, {"_id": "695dd171c03d6d81e4399f1b", "name": "Weikun Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f1c", "name": "Wenhan Ma", "hidden": false}, {"_id": "695dd171c03d6d81e4399f1d", "name": "Xiangwei Deng", "hidden": false}, {"_id": "695dd171c03d6d81e4399f1e", "name": "Bohan Mao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f1f", "name": "Bowen Ye", "hidden": false}, {"_id": "695dd171c03d6d81e4399f20", "name": "Can Cai", "hidden": false}, {"_id": "695dd171c03d6d81e4399f21", "name": "Chenghua Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f22", "name": "Chengxuan Zhu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f23", "name": "Chong Ma", "hidden": false}, {"_id": "695dd171c03d6d81e4399f24", "name": "Chun Chen", "hidden": false}, {"_id": "695dd171c03d6d81e4399f25", "name": "Chunan Li", "hidden": false}, {"_id": "695dd171c03d6d81e4399f26", "name": "Dawei Zhu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f27", "name": "Deshan Xiao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f28", "name": "Dong Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f29", "name": "Duo Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f2a", "name": "Fangyue Liu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f2b", "name": "Feiyu Yang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f2c", "name": "Fengyuan Shi", "hidden": false}, {"_id": "695dd171c03d6d81e4399f2d", "name": "Guoan Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f2e", "name": "Hao Tian", "hidden": false}, {"_id": "695dd171c03d6d81e4399f2f", "name": "Hao Wu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f30", "name": "Heng Qu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f31", "name": "Hongfei Yi", "hidden": false}, {"_id": "695dd171c03d6d81e4399f32", "name": "Hongxu An", "hidden": false}, {"_id": "695dd171c03d6d81e4399f33", "name": "Hongyi Guan", "hidden": false}, {"_id": "695dd171c03d6d81e4399f34", "name": "Xing Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f35", "name": "Yifan Song", "hidden": false}, {"_id": "695dd171c03d6d81e4399f36", "name": "Yihan Yan", "hidden": false}, {"_id": "695dd171c03d6d81e4399f37", "name": "Yihao Zhao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f38", "name": "Yingchun Lai", "hidden": false}, {"_id": "695dd171c03d6d81e4399f39", "name": "Yizhao Gao", "hidden": false}, {"_id": "695dd171c03d6d81e4399f3a", "name": "Yu Cheng", "hidden": false}, {"_id": "695dd171c03d6d81e4399f3b", "name": "Yuanyuan Tian", "hidden": false}, {"_id": "695dd171c03d6d81e4399f3c", "name": "Yudong Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f3d", "name": "Zhen Tang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f3e", "name": "Zhengju Tang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f3f", "name": "Zhengtao Wen", "hidden": false}, {"_id": "695dd171c03d6d81e4399f40", "name": "Zhichao Song", "hidden": false}, {"_id": "695dd171c03d6d81e4399f41", "name": "Zhixian Zheng", "hidden": false}, {"_id": "695dd171c03d6d81e4399f42", "user": {"_id": "665ee916b9e194a20c4769da", "avatarUrl": "/avatars/cfb3ff26979bed1a0c65a6fa318af3ad.svg", "isPro": false, "fullname": "Zihan Jiang", "user": "Jumbo0715", "type": "user"}, "name": "Zihan Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:47.154Z", "hidden": false}, {"_id": "695dd171c03d6d81e4399f43", "name": "Jian Wen", "hidden": false}, {"_id": "695dd171c03d6d81e4399f44", "name": "Jiarui Sun", "hidden": false}, {"_id": "695dd171c03d6d81e4399f45", "name": "Jiawei Li", "hidden": false}, {"_id": "695dd171c03d6d81e4399f46", "name": "Jinlong Xue", "hidden": false}, {"_id": "695dd171c03d6d81e4399f47", "name": "Jun Xia", "hidden": false}, {"_id": "695dd171c03d6d81e4399f48", "name": "Kai Fang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f49", "name": "Menghang Zhu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f4a", "name": "Nuo Chen", "hidden": false}, {"_id": "695dd171c03d6d81e4399f4b", "name": "Qian Tu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f4c", "name": "Qihao Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f4d", "name": "Qiying Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f4e", "name": "Rang Li", "hidden": false}, {"_id": "695dd171c03d6d81e4399f4f", "name": "Rui Ma", "hidden": false}, {"_id": "695dd171c03d6d81e4399f50", "name": "Shaolei Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f51", "name": "Shengfan Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f52", "name": "Shicheng Li", "hidden": false}, {"_id": "695dd171c03d6d81e4399f53", "name": "Shuhao Gu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f54", "name": "Shuhuai Ren", "hidden": false}, {"_id": "695dd171c03d6d81e4399f55", "name": "Sirui Deng", "hidden": false}, {"_id": "695dd171c03d6d81e4399f56", "name": "Tao Guo", "hidden": false}, {"_id": "695dd171c03d6d81e4399f57", "name": "Tianyang Lu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f58", "name": "Weiji Zhuang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f59", "name": "Weikang Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f5a", "name": "Weimin Xiong", "hidden": false}, {"_id": "695dd171c03d6d81e4399f5b", "name": "Wenshan Huang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f5c", "name": "Wenyu Yang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f5d", "name": "Xin Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f5e", "name": "Xing Yong", "hidden": false}, {"_id": "695dd171c03d6d81e4399f5f", "name": "Xu Wang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f60", "name": "Xueyang Xie", "hidden": false}, {"_id": "695dd171c03d6d81e4399f61", "name": "Yilin Jiang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f62", "name": "Yixin Yang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f63", "name": "Yongzhe He", "hidden": false}, {"_id": "695dd171c03d6d81e4399f64", "name": "Yu Tu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f65", "name": "Yuanliang Dong", "hidden": false}, {"_id": "695dd171c03d6d81e4399f66", "name": "Yuchen Liu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f67", "name": "Yue Ma", "hidden": false}, {"_id": "695dd171c03d6d81e4399f68", "name": "Yue Yu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f69", "name": "Yuxing Xiang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f6a", "name": "Zhaojun Huang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f6b", "name": "Zhenru Lin", "hidden": false}, {"_id": "695dd171c03d6d81e4399f6c", "name": "Zhipeng Xu", "hidden": false}, {"_id": "695dd171c03d6d81e4399f6d", "name": "Zhiyang Chen", "hidden": false}, {"_id": "695dd171c03d6d81e4399f6e", "name": "Zhonghua Deng", "hidden": false}, {"_id": "695dd171c03d6d81e4399f6f", "name": "Zihan Zhang", "hidden": false}, {"_id": "695dd171c03d6d81e4399f70", "name": "Zihao Yue", "hidden": false}], "publishedAt": "2026-01-06T07:31:47.000Z", "submittedOnDailyAt": "2026-01-07T00:52:38.331Z", "title": "MiMo-V2-Flash Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.", "upvotes": 16, "discussionId": "695dd172c03d6d81e4399f71", "projectPage": "https://mimo.xiaomi.com/blog/mimo-v2-flash", "githubRepo": "https://github.com/XiaomiMiMo/MiMo-V2-Flash", "githubRepoAddedBy": "user", "ai_summary": "MiMo-V2-Flash is a sparse Mixture-of-Experts model with hybrid attention architecture and efficient distillation technique that achieves strong performance with reduced parameters and improved inference speed.", "ai_keywords": ["Mixture-of-Experts", "Sliding Window Attention", "global attention", "Multi-Token Prediction", "speculative decoding", "Multi-Teacher On-Policy Distillation"], "githubStars": 957, "organization": {"_id": "680cb4c37f289defb2210940", "name": "XiaomiMiMo", "fullname": "Xiaomi MiMo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"}, "summary_zh": "<ul>\n    <li>MiMo-V2-Flash \u662f\u4e00\u4e2a\u5177\u6709 3090 \u4ebf\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u4e13\u4e3a\u5feb\u901f\u63a8\u7406\u548c\u667a\u80fd\u80fd\u529b\u8bbe\u8ba1\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u6df7\u5408\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u5168\u5c40\u6ce8\u610f\u529b\u3002</li>\n    <li>\u6a21\u578b\u5728 27 \u4e07\u4ebf\u4e2a\u6807\u8bb0\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u652f\u6301 32k \u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u540e\u6765\u6269\u5c55\u5230 256k\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65b0\u7684\u591a\u6559\u5e08\u5728\u7ebf\u84b8\u998f\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u3002</li>\n    <li>MiMo-V2-Flash \u5728\u63a8\u7406\u65f6\u901f\u5ea6\u63d0\u5347\u660e\u663e\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u6743\u91cd\u4ee5\u4fc3\u8fdb\u7814\u7a76\u4e0e\u5408\u4f5c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MiMo-V2-Flash is a large AI model with 309 billion parameters, designed for fast reasoning and strong performance.</li>\n    <li>The model uses a mix of different attention methods to improve efficiency, including a sliding window approach.</li>\n    <li>It was trained on a massive dataset of 27 trillion tokens and can handle longer texts with a context length of up to 256,000 tokens.</li>\n    <li>MiMo-V2-Flash introduces a new training method that helps it learn effectively from expert models.</li>\n    <li>It performs comparably to top models while using fewer parameters, and it is open-sourced for public use and collaboration.</li>\n</ul>"}, "publishedAt": "2026-01-06T02:31:47.000Z", "title": "MiMo-V2-Flash Technical Report", "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02780.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "organization": {"_id": "680cb4c37f289defb2210940", "name": "XiaomiMiMo", "fullname": "Xiaomi MiMo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01874", "authors": [{"_id": "695cee316aa73bc11f09163b", "name": "Shuhang Chen", "hidden": false}, {"_id": "695cee316aa73bc11f09163c", "user": {"_id": "646c77911ee398a4e9404b8b", "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg", "isPro": false, "fullname": "Yunqiu Xu", "user": "Yunqiu", "type": "user"}, "name": "Yunqiu Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:27:07.129Z", "hidden": false}, {"_id": "695cee316aa73bc11f09163d", "name": "Junjie Xie", "hidden": false}, {"_id": "695cee316aa73bc11f09163e", "name": "Aojun Lu", "hidden": false}, {"_id": "695cee316aa73bc11f09163f", "name": "Tao Feng", "hidden": false}, {"_id": "695cee316aa73bc11f091640", "name": "Zeying Huang", "hidden": false}, {"_id": "695cee316aa73bc11f091641", "name": "Ning Zhang", "hidden": false}, {"_id": "695cee316aa73bc11f091642", "name": "Yi Sun", "hidden": false}, {"_id": "695cee316aa73bc11f091643", "name": "Yi Yang", "hidden": false}, {"_id": "695cee316aa73bc11f091644", "user": {"_id": "649d54b314afbb10ce2a9eeb", "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg", "isPro": false, "fullname": "Hangjie Yuan", "user": "JacobYuan", "type": "user"}, "name": "Hangjie Yuan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:24:05.922Z", "hidden": false}], "publishedAt": "2026-01-05T08:02:18.000Z", "submittedOnDailyAt": "2026-01-07T01:06:12.554Z", "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving", "submittedOnDailyBy": {"_id": "646c77911ee398a4e9404b8b", "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg", "isPro": false, "fullname": "Yunqiu Xu", "user": "Yunqiu", "type": "user"}, "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.", "upvotes": 16, "discussionId": "695cee326aa73bc11f091645", "projectPage": "https://shchen233.github.io/cogflow/", "ai_summary": "Visual mathematical problem solving remains challenging for multimodal large language models, prompting the development of CogFlow, a cognitive-inspired three-stage framework that enhances perception, internalization, and reasoning through synergistic rewards and visual-gated policy optimization.", "ai_keywords": ["multimodal large language models", "visual mathematical problem solving", "visual perception", "visual reasoning", "cognitive-inspired framework", "knowledge internalization", "visual-gated policy optimization", "Synergistic Visual Rewards", "Knowledge Internalization Reward model"], "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u5df2\u6709\u8fdb\u5c55\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8f93\u5165\u7684\u63d0\u53d6\u548c\u89e3\u91ca\uff0c\u4f46\u5ffd\u89c6\u4e86\u5982\u4f55\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u89c6\u89c9\u4fe1\u606f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86CogFlow\uff0c\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u8ba4\u77e5\u542f\u53d1\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u7684\u5c42\u7ea7\u6d41\u7a0b\u3002</li>\n    <li>\u901a\u8fc7\u8bbe\u8ba1\u201c\u534f\u540c\u89c6\u89c9\u5956\u52b1\u201d\uff0c\u63d0\u5347\u89c6\u89c9\u4fe1\u606f\u7684\u63d0\u53d6\u80fd\u529b\uff0c\u786e\u4fdd\u89c6\u89c9\u4fe1\u606f\u5728\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6574\u5408\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u521b\u5efa\u4e86MathCog\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc712\u4e07\u6761\u9ad8\u8d28\u91cf\u7684\u611f\u77e5-\u63a8\u7406\u5bf9\u9f50\u6ce8\u91ca\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CogFlow\u7684\u4f18\u8d8a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal large language models have difficulty solving visual math problems due to issues with visual perception.</li>\n    <li>Current solutions focus on improving how visual inputs are extracted and interpreted but overlook how these inputs are used in reasoning.</li>\n    <li>The new framework called CogFlow simulates human reasoning through three stages: perception, internalization, and reasoning.</li>\n    <li>CogFlow includes methods to enhance visual perception and ensure that visual information is effectively integrated into reasoning.</li>\n    <li>A new dataset, MathCog, was created with over 120,000 annotated examples for training and testing the model's capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-05T03:02:18.000Z", "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving", "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01874.png", "numComments": 2, "submittedBy": {"_id": "646c77911ee398a4e9404b8b", "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg", "fullname": "Yunqiu Xu", "name": "Yunqiu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u8fdb\u884c4D\u91cd\u5efa\u548c\u65b0\u8f68\u8ff9\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5f53\u524d4D\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u6602\u8d35\u6216\u8bad\u7ec3\u8fc7\u7a0b\u7e41\u7410\u3002</li>\n    <li>NeoVerse\u7684\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5404\u79cd\u5355\u955c\u5934\u89c6\u9891\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>NeoVerse\u5b9e\u73b0\u4e86\u65e0\u59ff\u6001\u524d\u99884D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u955c\u5934\u964d\u7ea7\u6a21\u5f0f\u6a21\u62df\u3002</li>\n    <li>\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeoVerse\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions, generate videos from new perspectives, and support various applications.</li>\n    <li>Current 4D modeling methods struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse is designed to work well with common monocular videos, making it easier to use.</li>\n    <li>It includes features like pose-free 4D reconstruction and online simulation of video quality issues.</li>\n    <li>NeoVerse performs very well in standard tests for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03252", "authors": [{"_id": "695dc956c03d6d81e4399ea4", "name": "Hao Yu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea5", "user": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "name": "Haotong Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:04.783Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea6", "name": "Jiawei Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea7", "name": "Jiaxin Li", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea8", "name": "Yida Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea9", "user": {"_id": "6791a6c19ce382eae861ed61", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg", "isPro": false, "fullname": "Xueyang Zhang", "user": "zhangxueyang001", "type": "user"}, "name": "Xueyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:39.946Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399eaa", "name": "Yue Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399eab", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "695dc956c03d6d81e4399eac", "name": "Ruizhen Hu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ead", "user": {"_id": "62986ca2b58e71e2ac9b8f01", "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg", "isPro": false, "fullname": "Sida Peng", "user": "pengsida", "type": "user"}, "name": "Sida Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:12.074Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "publishedAt": "2026-01-06T18:57:06.000Z", "submittedOnDailyAt": "2026-01-07T00:26:37.060Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "submittedOnDailyBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "upvotes": 71, "discussionId": "695dc956c03d6d81e4399eae", "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.", "ai_keywords": ["neural implicit fields", "local implicit decoder", "continuous 2D coordinates", "arbitrary-resolution depth estimation", "synthetic benchmark", "4K synthetic benchmark", "novel view synthesis", "viewpoint shifts"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u5728\u79bb\u6563\u7684\u56fe\u50cf\u7f51\u683c\u4e0a\u9884\u6d4b\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u7ec6\u8282\u6062\u590d\u80fd\u529b\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86InfiniDepth\uff0c\u5c06\u6df1\u5ea6\u8868\u793a\u4e3a\u795e\u7ecf\u9690\u5f0f\u573a\uff0c\u53ef\u4ee5\u5728\u8fde\u7eed\u76842D\u5750\u6807\u4e0a\u67e5\u8be2\u6df1\u5ea6\uff0c\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u548c\u7ec6\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76844K\u5408\u6210\u57fa\u51c6\uff0c\u6db5\u76d6\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u6e38\u620f\u7684\u4e30\u5bcc\u573a\u666f\uff0c\u4ee5\u8bc4\u4f30\u65b9\u6cd5\u7684\u80fd\u529b\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0cInfiniDepth\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u8282\u4e30\u5bcc\u7684\u533a\u57df\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u5927\u89c6\u89d2\u8f6c\u6362\u4e0b\u7684\u65b0\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u51cf\u5c11\u4e86\u7a7a\u6d1e\u548c\u4f2a\u5f71\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current depth estimation methods can only predict depth on fixed image grids, which limits their flexibility and detail.</li>\n    <li>The paper introduces InfiniDepth, which uses neural implicit fields to estimate depth at any point in a 2D space, allowing for high-resolution and detailed depth maps.</li>\n    <li>To test InfiniDepth, researchers created a high-quality 4K synthetic benchmark using scenes from five different video games.</li>\n    <li>InfiniDepth outperforms existing methods in both synthetic and real-world depth estimation tasks, especially in areas with fine details.</li>\n    <li>It also improves the quality of images when creating new views from different angles, resulting in fewer visual errors.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:57:06.000Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png", "numComments": 8, "submittedBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "fullname": "Haotong Lin", "name": "haotongl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01554", "authors": [{"_id": "695dcda5c03d6d81e4399eb8", "name": "MOSI. AI", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eb9", "name": "Donghua Yu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eba", "name": "Zhengyuan Lin", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebb", "user": {"_id": "660c345da15ab85523ad00d1", "avatarUrl": "/avatars/b0bfdee89a6c62ff12140b9e85de499a.svg", "isPro": false, "fullname": "Chen Yang", "user": "kiiic", "type": "user"}, "name": "Chen Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:58.894Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebc", "name": "Yiyang Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebd", "name": "Hanfu Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebe", "name": "Jingqi Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebf", "name": "Ke Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec0", "name": "Liwei Fan", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec1", "name": "Yi Jiang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec2", "name": "Jie Zhu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec3", "name": "Muchen Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec4", "name": "Wenxuan Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec5", "name": "Yang Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec6", "user": {"_id": "6443f7bf1bc692d87b25e234", "avatarUrl": "/avatars/fa9e62d96d0691a9a48e3db499a61557.svg", "isPro": false, "fullname": "Xu Zhe", "user": "Phospheneser", "type": "user"}, "name": "Zhe Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:55.950Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec7", "name": "Yitian Gong", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec8", "name": "Yuqian Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec9", "name": "Wenbo Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eca", "user": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "name": "Zhaoye Fei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:10.124Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecb", "user": {"_id": "695757e4fd9dc6e9bac27935", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_uZEu4oOlKJVYqrG763Z-.jpeg", "isPro": false, "fullname": "aa", "user": "qinyuancheng", "type": "user"}, "name": "Qinyuan Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:03.749Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecc", "name": "Shimin Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecd", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:50.004Z", "hidden": false}], "publishedAt": "2026-01-04T15:01:10.000Z", "submittedOnDailyAt": "2026-01-07T00:52:16.123Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "submittedOnDailyBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "upvotes": 45, "discussionId": "695dcda6c03d6d81e4399ece", "projectPage": "https://mosi.cn/models/moss-transcribe-diarize", "ai_summary": "A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.", "ai_keywords": ["multimodal large language model", "end-to-end paradigm", "speaker diarization", "time-stamped transcription", "context window", "robust generalization"], "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>SATS\uff08\u8bf4\u8bdd\u8005\u5f52\u5c5e\u3001\u65f6\u95f4\u6233\u8f6c\u5f55\uff09\u7528\u4e8e\u51c6\u786e\u8bb0\u5f55\u4f1a\u8bae\u53d1\u8a00\u53ca\u5176\u65f6\u95f4\u3002</li>\n    <li>\u73b0\u6709\u7684SATS\u7cfb\u7edf\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff0c\u5982\u4e0d\u80fd\u5b9e\u73b0\u7aef\u5230\u7aef\u8f6c\u5f55\u548c\u8f93\u51fa\u65f6\u95f4\u6233\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MOSS Transcribe Diarize\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5730\u8fdb\u884c\u8bf4\u8bdd\u8005\u5f52\u5c5e\u548c\u65f6\u95f4\u6233\u8f6c\u5f55\u3002</li>\n    <li>MOSS\u6a21\u578b\u7ecf\u8fc7\u5927\u91cf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u652f\u6301\u957f\u8fbe90\u5206\u949f\u7684\u8f93\u5165\u548c128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002</li>\n    <li>\u5728\u5404\u79cd\u8bc4\u4f30\u4e2d\uff0cMOSS\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5546\u4e1a\u7cfb\u7edf\uff0c\u8868\u73b0\u66f4\u52a0\u4f18\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MOSS Transcribe Diarize is a new tool for transcribing meetings, focusing on identifying who speaks and when.</li>\n    <li>Current systems have limitations, like not being able to handle long contexts or provide exact timestamps.</li>\n    <li>This new model can process up to 90 minutes of audio and is trained on a large amount of real-world data.</li>\n    <li>MOSS Transcribe Diarize is designed to work well in an end-to-end manner, improving efficiency and accuracy.</li>\n    <li>It has shown better performance than leading commercial transcription systems in various tests.</li>\n</ul>"}, "publishedAt": "2026-01-04T10:01:10.000Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01554.png", "numComments": 2, "submittedBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "fullname": "Zhaoye Fei", "name": "ngc7293", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02204", "authors": [{"_id": "695c7d0d6aa73bc11f091433", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091434", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:57.686Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091435", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091436", "name": "Hang Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091437", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091438", "name": "Yongsheng Dong", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091439", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143a", "name": "Xian Li", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143b", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143c", "user": {"_id": "6344dcb1cd37e44d9ed46508", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg", "isPro": false, "fullname": "Yi Jiang", "user": "JiangYi", "type": "user"}, "name": "Yi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:55.158Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143d", "name": "Hu Ye", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143e", "name": "Bo Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143f", "name": "Yiming Gao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091440", "name": "Peng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091441", "name": "Akide Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091442", "name": "Zhipeng Yang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091443", "name": "Qili Deng", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091444", "name": "Linjie Xing", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091445", "name": "Jiyang Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091446", "name": "Zhao Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091447", "name": "Yang Zhou", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091448", "name": "Mingcong Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091449", "name": "Yi Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144a", "name": "Qian He", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144b", "name": "Xiwei Hu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144c", "name": "Zhongqi Qi", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144d", "name": "Jie Shao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144e", "name": "Zhiye Fu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144f", "name": "Shuai Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091450", "name": "Fangmin Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091451", "name": "Xuezhi Chai", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091452", "name": "Zhihua Wu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091453", "name": "Yitong Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091454", "name": "Zehuan Yuan", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091455", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091456", "name": "Xinglong Wu", "hidden": false}], "publishedAt": "2026-01-05T15:27:04.000Z", "submittedOnDailyAt": "2026-01-06T00:52:35.953Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "upvotes": 45, "discussionId": "695c7d0d6aa73bc11f091457", "githubRepo": "https://github.com/ByteVisionLab/NextFlow", "githubRepoAddedBy": "user", "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.", "ai_keywords": ["decoder-only autoregressive transformer", "interleaved text-image discrete tokens", "unified vision representation", "multimodal understanding", "multimodal generation", "next-token prediction", "next-scale prediction", "raster-scan methods", "visual generation", "prefix-tuning strategy", "reinforcement learning", "diffusion baselines"], "githubStars": 60, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>NextFlow\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u8bad\u7ec3\u4e866\u4e07\u4ebf\u4e2a\u6587\u672c-\u56fe\u50cf\u79bb\u6563\u6807\u8bb0\u3002</li>\n    <li>NextFlow\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u591a\u79cd\u6a21\u5f0f\u7684\u5185\u5bb9\uff0c\u5982\u56fe\u50cf\u7f16\u8f91\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5b83\u91c7\u7528\u4e0d\u540c\u7684\u9884\u6d4b\u65b9\u5f0f\uff1a\u6587\u672c\u4f7f\u7528\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u56fe\u50cf\u4f7f\u7528\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u9884\u6d4b\u3002</li>\n    <li>\u751f\u62101024x1024\u56fe\u50cf\u53ea\u97005\u79d2\uff0c\u6bd4\u4f20\u7edf\u6a21\u578b\u5feb\u5f97\u591a\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cNextFlow\u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u4e13\u4e1a\u6269\u6563\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NextFlow is a new AI model that combines text and images using a single architecture, trained on a massive dataset of 6 trillion text-image pairs.</li>\n    <li>This model can understand and create both text and images, allowing for tasks like image editing and video generation.</li>\n    <li>NextFlow uses a different approach for generating text and images, enabling it to create high-quality 1024x1024 images in just 5 seconds.</li>\n    <li>The model addresses challenges in generating images at multiple scales through effective training methods.</li>\n    <li>Experiments show that NextFlow performs exceptionally well, competing with specialized models in terms of visual quality.</li>\n</ul>"}, "publishedAt": "2026-01-05T10:27:04.000Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01739", "authors": [{"_id": "695c72346aa73bc11f0913bf", "user": {"_id": "6044fd39e6aa3e130cb92867", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg", "isPro": false, "fullname": "Eunbi Choi", "user": "unbiarirang", "type": "user"}, "name": "Eunbi Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:17.472Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c0", "user": {"_id": "64d31ca9465b6039259838df", "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg", "isPro": false, "fullname": "kibong choi", "user": "bongchoi", "type": "user"}, "name": "Kibong Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:11.322Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c1", "name": "Seokhee Hong", "hidden": false}, {"_id": "695c72346aa73bc11f0913c2", "user": {"_id": "63c50e590c24c8b53958f75e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png", "isPro": false, "fullname": "Junwon Hwang", "user": "nuxlear", "type": "user"}, "name": "Junwon Hwang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:09.333Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c3", "name": "Hyojin Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913c4", "user": {"_id": "66a9e066a203add977948988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg", "isPro": false, "fullname": "hyunjik.jo", "user": "switiz87", "type": "user"}, "name": "Hyunjik Jo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:13.551Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c5", "name": "Joonkee Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c6", "name": "Seonghwan Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c7", "name": "Soyeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c8", "name": "Sunkyoung Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c9", "name": "Yireun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ca", "name": "Yongil Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913cb", "name": "Haeju Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cc", "name": "Jinsik Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cd", "name": "Kyungmin Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ce", "name": "Sangha Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913cf", "name": "Heuiyeen Yeen", "hidden": false}, {"_id": "695c72346aa73bc11f0913d0", "name": "Hwan Chang", "hidden": false}, {"_id": "695c72346aa73bc11f0913d1", "name": "Stanley Jungkyu Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d2", "name": "Yejin Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d3", "name": "Jiwon Ham", "hidden": false}, {"_id": "695c72346aa73bc11f0913d4", "name": "Kijeong Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913d5", "name": "Geunyeong Jeong", "hidden": false}, {"_id": "695c72346aa73bc11f0913d6", "name": "Gerrard Jeongwon Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d7", "name": "Yonghwan Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d8", "name": "Jiyeon Jung", "hidden": false}, {"_id": "695c72346aa73bc11f0913d9", "name": "Naeun Kang", "hidden": false}, {"_id": "695c72346aa73bc11f0913da", "name": "Dohoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913db", "name": "Euisoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dc", "name": "Hayeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dd", "name": "Hyosang Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913de", "name": "Hyunseo Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913df", "name": "Jieun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e0", "name": "Minu Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e1", "name": "Myoungshin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e2", "name": "Unsol Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e3", "name": "Youchul Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e4", "name": "YoungJin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e5", "name": "Chaeeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e6", "name": "Chaeyoon Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e7", "user": {"_id": "6399ab9e92e12136b99ef60e", "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg", "isPro": false, "fullname": "Changhun Lee", "user": "xvyaward", "type": "user"}, "name": "Changhun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:15.420Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913e8", "name": "Dahm Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e9", "name": "Edward Hwayoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ea", "name": "Honglak Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913eb", "name": "Jinsang Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ec", "name": "Jiyoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ed", "name": "Sangeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ee", "name": "Seungwon Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ef", "name": "Solji Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f0", "name": "Woohyung Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f1", "name": "Chanwoo Moon", "hidden": false}, {"_id": "695c72346aa73bc11f0913f2", "name": "Jaewoo Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f3", "name": "Jinho Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f4", "name": "Yongmin Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f5", "name": "Hyerin Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f6", "name": "Wooseok Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f7", "name": "Yongwoo Song", "hidden": false}, {"_id": "695c72346aa73bc11f0913f8", "name": "Sejong Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913f9", "name": "Sihoon Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913fa", "name": "Chang En Yea", "hidden": false}, {"_id": "695c72346aa73bc11f0913fb", "name": "Sihyuk Yi", "hidden": false}, {"_id": "695c72346aa73bc11f0913fc", "name": "Chansik Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fd", "name": "Dongkeun Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fe", "name": "Sangyeon Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913ff", "name": "Hyeongu Yun", "hidden": false}], "publishedAt": "2026-01-05T02:30:59.000Z", "submittedOnDailyAt": "2026-01-06T01:03:14.011Z", "title": "K-EXAONE Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "upvotes": 44, "discussionId": "695c72356aa73bc11f091400", "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE", "githubRepoAddedBy": "user", "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.", "ai_keywords": ["Mixture-of-Experts", "256K-token context window", "multilingual language model", "parameter-efficient fine-tuning"], "githubStars": 39, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "summary_zh": "<ul>\n    <li>K-EXAONE\u662fLG AI\u7814\u7a76\u9662\u5f00\u53d1\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u67092360\u4ebf\u53c2\u6570\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\uff0c\u5e76\u652f\u6301256K\u4ee4\u724c\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002</li>\n    <li>K-EXAONE\u8986\u76d6\u516d\u79cd\u8bed\u8a00\uff1a\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u63a8\u7406\u3001\u4ee3\u7406\u3001\u4e00\u822c\u80fd\u529b\u3001\u97e9\u8bed\u548c\u591a\u8bed\u8a00\u80fd\u529b\u7b49\u591a\u9879\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>K-EXAONE\u65e8\u5728\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4e1a\u548c\u7814\u7a76\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>K-EXAONE is a large multilingual language model created by LG AI Research.</li>\n    <li>It has 236 billion parameters, using 23 billion at a time when providing answers.</li>\n    <li>The model can handle a context of 256,000 tokens and supports six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.</li>\n    <li>K-EXAONE has been tested on various skills and performs similarly to other large models available to the public.</li>\n    <li>It is designed to enhance AI applications in both industrial and research settings.</li>\n</ul>"}, "publishedAt": "2026-01-04T21:30:59.000Z", "title": "K-EXAONE Technical Report", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00664", "authors": [{"_id": "695b237a832867f253525d70", "user": {"_id": "66b57c77778c98d29446c8ec", "avatarUrl": "/avatars/c176bb7c072f3093f6a0786c87d384d8.svg", "isPro": false, "fullname": "Taekyung Ki", "user": "taekyungki", "type": "user"}, "name": "Taekyung Ki", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:19.100Z", "hidden": false}, {"_id": "695b237a832867f253525d71", "name": "Sangwon Jang", "hidden": false}, {"_id": "695b237a832867f253525d72", "name": "Jaehyeong Jo", "hidden": false}, {"_id": "695b237a832867f253525d73", "user": {"_id": "652066649004117947e46ed6", "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg", "isPro": false, "fullname": "Jaehong Yoon", "user": "jaehong31", "type": "user"}, "name": "Jaehong Yoon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:17.228Z", "hidden": false}, {"_id": "695b237a832867f253525d74", "name": "Sung Ju Hwang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "publishedAt": "2026-01-02T11:58:48.000Z", "submittedOnDailyAt": "2026-01-05T00:05:44.498Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "upvotes": 43, "discussionId": "695b237a832867f253525d75", "projectPage": "https://taekyungki.github.io/AvatarForcing/", "githubRepo": "https://github.com/TaekyungKi/AvatarForcing", "githubRepoAddedBy": "user", "ai_summary": "Avatar Forcing framework enables real-time interactive head avatar generation with low latency and expressive motion through diffusion forcing and label-free preference optimization.", "ai_keywords": ["diffusion forcing", "real-time interaction", "multimodal inputs", "audio", "motion", "causal constraints", "synthetic losing samples", "direct preference optimization", "interactive head avatar generation"], "githubStars": 65, "summary_zh": "<ul>\n    <li>\u8c08\u8bdd\u5934\u50cf\u751f\u6210\u6280\u672f\u53ef\u4ee5\u5c06\u9759\u6001\u8096\u50cf\u8f6c\u5316\u4e3a\u903c\u771f\u7684\u865a\u62df\u5934\u50cf\uff0c\u7528\u4e8e\u4ea4\u6d41\u548c\u5185\u5bb9\u521b\u4f5c\u3002</li>\n    <li>\u76ee\u524d\u7684\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u4e92\u52a8\u4ea4\u6d41\uff0c\u751f\u6210\u7684\u56de\u5e94\u5f80\u5f80\u5355\u5411\uff0c\u7f3a\u4e4f\u60c5\u611f\u5171\u9e23\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cAvatar Forcing\u201d\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5b9e\u65f6\u8fd0\u52a8\u751f\u6210\u548c\u65e0\u6807\u7b7e\u6570\u636e\u5b66\u4e60\u751f\u52a8\u53cd\u5e94\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u5904\u7406\u7528\u6237\u7684\u97f3\u9891\u548c\u52a8\u4f5c\u8f93\u5165\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u5373\u65f6\u53cd\u5e94\uff0c\u53cd\u5e94\u65f6\u95f4\u7ea6\u4e3a500\u6beb\u79d2\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u53cd\u5e94\u8fc5\u901f\u548c\u8868\u73b0\u751f\u52a8\u7684\u5934\u50cf\u8fd0\u52a8\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u83b7\u5f97\u4e86\u8d85\u8fc780%\u7684\u504f\u597d\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Talking head generation creates realistic avatars from photos for virtual communication but lacks true interactivity.</li>\n    <li>Current models struggle with real-time motion and learning expressive reactions without extra labeled data.</li>\n    <li>Avatar Forcing is a new framework that allows avatars to interact in real-time using user inputs like audio and motion.</li>\n    <li>The system responds quickly (about 500ms) and is 6.8 times faster than previous models, making interactions more engaging.</li>\n    <li>Experiments show that this new method produces more expressive avatar movements, preferred by users over 80% of the time compared to older models.</li>\n</ul>"}, "publishedAt": "2026-01-02T06:58:48.000Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00664.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 198}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03233", "authors": [{"_id": "695dc6d9c03d6d81e4399e85", "user": {"_id": "6303cc5e0547362a22a51af0", "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg", "isPro": false, "fullname": "Yoav HaCohen", "user": "yoavhacohen", "type": "user"}, "name": "Yoav HaCohen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:29.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e86", "user": {"_id": "6489c487b9e9258ba065418f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png", "isPro": false, "fullname": "Benny Brazowski", "user": "benibraz", "type": "user"}, "name": "Benny Brazowski", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:35.981Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e87", "user": {"_id": "62dd30a8d43078cd49ac8ad8", "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg", "isPro": false, "fullname": "Nisan Chiprut", "user": "nisan", "type": "user"}, "name": "Nisan Chiprut", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:41.634Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e88", "user": {"_id": "64a7adc087cbd4dc7301fdd6", "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg", "isPro": false, "fullname": "Yaki Bitterman", "user": "jacobitterman", "type": "user"}, "name": "Yaki Bitterman", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:46.749Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e89", "user": {"_id": "65897258509bcae23fa162c9", "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg", "isPro": false, "fullname": "Andrew Kvochko", "user": "kvochko", "type": "user"}, "name": "Andrew Kvochko", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:51.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8a", "name": "Avishai Berkowitz", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8b", "name": "Daniel Shalem", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8c", "user": {"_id": "681af83e2f4aaa88639e703d", "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg", "isPro": false, "fullname": "Daphna Lifschitz", "user": "Daphnal", "type": "user"}, "name": "Daphna Lifschitz", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:04.180Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8d", "user": {"_id": "636b97a57631fe5e86fe1fa2", "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg", "isPro": false, "fullname": "Dudu Moshe", "user": "dudumoshe", "type": "user"}, "name": "Dudu Moshe", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:13.512Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8e", "name": "Eitan Porat", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8f", "user": {"_id": "677a422979d3c32a5dd87a0a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png", "isPro": false, "fullname": "Eitan Richardson", "user": "eitanrich", "type": "user"}, "name": "Eitan Richardson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:22.677Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e90", "user": {"_id": "673f6911d83832a6ce15e7bf", "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg", "isPro": false, "fullname": "Guy Shiran", "user": "guysrn", "type": "user"}, "name": "Guy Shiran", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:28.250Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e91", "user": {"_id": "65744a2fe09de6aa74026d80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg", "isPro": false, "fullname": "Itay Chachy", "user": "ItayChachy", "type": "user"}, "name": "Itay Chachy", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:36.781Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e92", "name": "Jonathan Chetboun", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e93", "user": {"_id": "6678365ac411b340b32d6148", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg", "isPro": false, "fullname": "Michael Finkelson", "user": "MichaelFinkelson", "type": "user"}, "name": "Michael Finkelson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:53.574Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e94", "user": {"_id": "6318aa43cb4ca740c4c55651", "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg", "isPro": false, "fullname": "michael kupchick", "user": "michaellightricks", "type": "user"}, "name": "Michael Kupchick", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:59.945Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e95", "user": {"_id": "673f29b568595672b8d3e90e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png", "isPro": false, "fullname": "Nir Zabari", "user": "NirZabariLTX", "type": "user"}, "name": "Nir Zabari", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:07.297Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e96", "user": {"_id": "64ae89c043dda9449a1eb1ba", "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg", "isPro": false, "fullname": "Nitzan Guetta", "user": "nitzanguetta", "type": "user"}, "name": "Nitzan Guetta", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:14.712Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e97", "name": "Noa Kotler", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e98", "user": {"_id": "631f58935ba8c026340b377c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg", "isPro": false, "fullname": "Ofir Bibi", "user": "ofirbibi", "type": "user"}, "name": "Ofir Bibi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:27.196Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e99", "user": {"_id": "674348b46215a2c0878e219b", "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg", "isPro": false, "fullname": "Ori Gordon", "user": "origordon", "type": "user"}, "name": "Ori Gordon", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:34.878Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9a", "name": "Poriya Panet", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9b", "name": "Roi Benita", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9c", "name": "Shahar Armon", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9d", "name": "Victor Kulikov", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9e", "name": "Yaron Inger", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9f", "name": "Yonatan Shiftan", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea0", "name": "Zeev Melumian", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea1", "name": "Zeev Farbman", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "publishedAt": "2026-01-06T18:24:41.000Z", "submittedOnDailyAt": "2026-01-07T00:07:29.528Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "upvotes": 40, "discussionId": "695dc6d9c03d6d81e4399ea2", "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v", "githubRepo": "https://github.com/Lightricks/LTX-2", "githubRepoAddedBy": "user", "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.", "ai_keywords": ["text-to-video diffusion models", "audiovisual content", "dual-stream transformer", "cross-attention layers", "temporal positional embeddings", "AdaLN", "classifier-free guidance", "modality-aware classifier-free guidance", "multilingual text encoder", "diffusion models"], "githubStars": 922, "summary_zh": "<ul>\n    <li>LTX-2\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e0d\u5bf9\u79f0\u7684\u53cc\u6d41\u53d8\u6362\u5668\uff0c\u89c6\u9891\u6d41\u548c\u97f3\u9891\u6d41\u7684\u53c2\u6570\u5206\u522b\u4e3a14\u4ebf\u548c5\u4ebf\u3002</li>\n    <li>LTX-2\u901a\u8fc7\u53cc\u5411\u97f3\u89c6\u9891\u4ea4\u53c9\u6ce8\u610f\u5c42\u6765\u5b9e\u73b0\u97f3\u9891\u548c\u89c6\u9891\u7684\u9ad8\u6548\u7ed3\u5408\u3002</li>\n    <li>\u6a21\u578b\u4e0d\u4ec5\u80fd\u751f\u6210\u8bed\u97f3\uff0c\u8fd8\u80fd\u521b\u9020\u7b26\u5408\u573a\u666f\u7684\u4e30\u5bcc\u97f3\u8f68\uff0c\u5305\u62ec\u81ea\u7136\u80cc\u666f\u97f3\u548c\u97f3\u6548\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u4e2d\uff0cLTX-2\u7684\u89c6\u542c\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u6027\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u8fdc\u4f4e\u4e8e\u4e13\u6709\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LTX-2 is a new open-source model that creates high-quality videos with synchronized audio.</li>\n    <li>The model uses a dual-stream transformer with separate components for video (14 billion parameters) and audio (5 billion parameters).</li>\n    <li>It includes features for better understanding of text prompts and improved alignment of audio and video.</li>\n    <li>LTX-2 generates not just speech, but also background sounds and sound effects that match the video's scenes.</li>\n    <li>The model provides high-quality results similar to proprietary models but is more efficient and cost-effective.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:24:41.000Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01425", "authors": [{"_id": "695c765d6aa73bc11f091402", "name": "Xu Guo", "hidden": false}, {"_id": "695c765d6aa73bc11f091403", "name": "Fulong Ye", "hidden": false}, {"_id": "695c765d6aa73bc11f091404", "name": "Xinghui Li", "hidden": false}, {"_id": "695c765d6aa73bc11f091405", "name": "Pengqi Tu", "hidden": false}, {"_id": "695c765d6aa73bc11f091406", "name": "Pengze Zhang", "hidden": false}, {"_id": "695c765d6aa73bc11f091407", "user": {"_id": "674566cb79d6f3a9da7be0de", "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg", "isPro": false, "fullname": "Qichao Sun", "user": "Simons212", "type": "user"}, "name": "Qichao Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:07.336Z", "hidden": false}, {"_id": "695c765d6aa73bc11f091408", "name": "Songtao Zhao", "hidden": false}, {"_id": "695c765d6aa73bc11f091409", "name": "Xiangwang Hou", "hidden": false}, {"_id": "695c765d6aa73bc11f09140a", "name": "Qian He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "publishedAt": "2026-01-04T08:07:11.000Z", "submittedOnDailyAt": "2026-01-06T00:30:06.666Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "submittedOnDailyBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "upvotes": 33, "discussionId": "695c765d6aa73bc11f09140b", "projectPage": "https://guoxu1233.github.io/DreamID-V/", "githubRepo": "https://github.com/bytedance/DreamID-V", "githubRepoAddedBy": "user", "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.", "ai_keywords": ["Video Face Swapping", "Image Face Swapping", "diffusion transformer", "Modality-Aware Conditioning", "Synthetic-to-Real Curriculum", "Identity-Coherence Reinforcement Learning", "IDBench-V", "Identity-Anchored Video Synthesizer", "bidirectional ID quadruplets", "multi-model conditions"], "githubStars": 86, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u6280\u672f\u9700\u8981\u5c06\u4e00\u4e2a\u4eba\u7684\u5f62\u8c61\u65e0\u7f1d\u5730\u878d\u5165\u53e6\u4e00\u4e2a\u89c6\u9891\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u7684\u59ff\u52bf\u3001\u8868\u60c5\u3001\u5149\u7167\u548c\u80cc\u666f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u56fe\u50cf\u6362\u8138\u7684\u4f18\u52bf\u6709\u6548\u8f6c\u79fb\u5230\u89c6\u9891\u9886\u57df\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u65b0\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0bSyncID-Pipe\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6846\u67b6DreamID-V\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5efa\u7acb\u4e86IDBench-V\u57fa\u51c6\uff0c\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793aDreamID-V\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on improving video face swapping, which involves inserting one person's face into a video while keeping all other details like expression and background the same.</li>\n    <li>Current methods have trouble keeping the face identity consistent and maintaining good quality over time.</li>\n    <li>The researchers propose a new framework called DreamID-V that uses a special data pipeline and a unique model to better manage face swapping in videos.</li>\n    <li>They also introduce a new benchmark called IDBench-V to help test and compare different face swapping techniques.</li>\n    <li>Tests show that DreamID-V performs better than existing methods and can adapt well to different tasks related to face swapping.</li>\n</ul>"}, "publishedAt": "2026-01-04T03:07:11.000Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01425.png", "numComments": 2, "submittedBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "fullname": "xuguo", "name": "XuGuo699", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02256", "authors": [{"_id": "695c7d256aa73bc11f091459", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d256aa73bc11f09145a", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:53.171Z", "hidden": false}, {"_id": "695c7d256aa73bc11f09145b", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d256aa73bc11f09145c", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d256aa73bc11f09145d", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d256aa73bc11f09145e", "name": "Xian Li", "hidden": false}, {"_id": "695c7d256aa73bc11f09145f", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d256aa73bc11f091460", "name": "Yi Jiang", "hidden": false}, {"_id": "695c7d256aa73bc11f091461", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d256aa73bc11f091462", "name": "Xinglong Wu", "hidden": false}, {"_id": "695c7d256aa73bc11f091463", "name": "Jia Jia", "hidden": false}], "publishedAt": "2026-01-05T16:36:40.000Z", "submittedOnDailyAt": "2026-01-06T00:51:20.686Z", "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "upvotes": 28, "discussionId": "695c7d266aa73bc11f091464", "ai_summary": "Visual autoregressive models face training instability due to asynchronous policy conflicts, which are addressed through a novel framework enhancing group relative policy optimization with intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms.", "ai_keywords": ["AutoRegressive", "diffusion", "Visual AutoRegressive", "reinforcement learning", "Group Relative Policy Optimization", "intermediate reward", "dynamic time-step reweighting", "mask propagation", "Reward Feedback Learning", "visual generation"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u751f\u6210\u6709\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u81ea\u56de\u5f52\uff08AR\uff09\u3001\u6269\u6563\u548c\u89c6\u89c9\u81ea\u56de\u5f52\uff08VAR\uff09\u6a21\u578b\u3002</li>\n    <li>VAR\u6a21\u578b\u5728\u751f\u6210\u6b65\u9aa4\u4e2d\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u8f93\u5165\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u5f02\u6b65\u7b56\u7565\u51b2\u7a81\u3002</li>\n    <li>\u8fd9\u79cd\u51b2\u7a81\u5728\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u5c24\u4e3a\u660e\u663e\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6548\u679c\u4e0d\u4f73\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u6539\u8fdb\u7fa4\u4f53\u76f8\u5bf9\u653f\u7b56\u4f18\u5316\uff08GRPO\uff09\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u51b2\u7a81\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e09\u4e2a\u7ec4\u4ef6\u63d0\u5347\u6027\u80fd\uff1a\u7a33\u5b9a\u7684\u4e2d\u95f4\u5956\u52b1\u3001\u52a8\u6001\u65f6\u95f4\u6b65\u91cd\u52a0\u6743\u65b9\u6848\u548c\u65b0\u9896\u7684\u63a9\u7801\u4f20\u64ad\u7b97\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visual generation uses three main methods: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models.</li>\n    <li>VAR models face issues with asynchronous conflicts during generation, especially in reinforcement learning, leading to unstable training.</li>\n    <li>To address these conflicts, a new framework is proposed to improve Group Relative Policy Optimization (GRPO).</li>\n    <li>This framework includes three key components: a stabilizing reward, a dynamic reweighting scheme, and a mask propagation algorithm.</li>\n    <li>The new approach shows better sample quality and alignment compared to the standard GRPO, making VAR model optimization more effective.</li>\n</ul>"}, "publishedAt": "2026-01-05T11:36:40.000Z", "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02256.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03193", "authors": [{"_id": "695de87dc03d6d81e4399fd2", "user": {"_id": "6723369dc09be95d8c49c605", "avatarUrl": "/avatars/797df409537c07cbf894f1c027cddbb1.svg", "isPro": false, "fullname": "Ruiyan Han", "user": "Hungryyan", "type": "user"}, "name": "Ruiyan Han", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:53.416Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd3", "user": {"_id": "64b0a5037a475fba70a7260d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg", "isPro": false, "fullname": "Zhen Fang", "user": "CostaliyA", "type": "user"}, "name": "Zhen Fang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T13:13:51.100Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd4", "name": "XinYu Sun", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd5", "name": "Yuchen Ma", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd6", "name": "Ziheng Wang", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd7", "user": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "name": "Yu Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:35.308Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd8", "user": {"_id": "64892d31cbda0d1cdb956897", "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg", "isPro": false, "fullname": "Zehui Chen", "user": "lovesnowbest", "type": "user"}, "name": "Zehui Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:22:13.382Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fd9", "user": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "name": "Lin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:37.096Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fda", "user": {"_id": "67dc162ec8c00778e8689f42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png", "isPro": false, "fullname": "Wenxuan Huang", "user": "Osilly", "type": "user"}, "name": "Wenxuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T13:37:57.684Z", "hidden": false}, {"_id": "695de87dc03d6d81e4399fdb", "name": "Wei-Jie Xu", "hidden": false}, {"_id": "695de87dc03d6d81e4399fdc", "name": "Yi Cao", "hidden": false}, {"_id": "695de87dc03d6d81e4399fdd", "name": "Feng Zhao", "hidden": false}], "publishedAt": "2026-01-06T17:15:50.000Z", "submittedOnDailyAt": "2026-01-07T02:31:26.744Z", "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision", "submittedOnDailyBy": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.", "upvotes": 25, "discussionId": "695de87dc03d6d81e4399fde", "projectPage": "https://costaliya.github.io/UniCorn.github.io/", "githubRepo": "https://github.com/Hungryyan1/UniCorn", "githubRepoAddedBy": "user", "ai_summary": "UniCorn, a self-improvement framework for unified multimodal models, addresses generation gaps through self-play and cognitive pattern reconstruction, achieving state-of-the-art results in text-to-image generation.", "ai_keywords": ["Unified Multimodal Models", "Conduction Aphasia", "UniCorn", "self-improvement framework", "self-play", "cognitive pattern reconstruction", "Text to Image", "cycle-consistency", "UniCycle", "T2I generation"], "githubStars": 25, "summary_zh": "<ul>\n    <li>\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u6a21\u6001\u7406\u89e3\u4e0a\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u9ad8\u8d28\u91cf\u751f\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002</li>\n    <li>\u6211\u4eec\u5c06\u8fd9\u79cd\u5dee\u8ddd\u79f0\u4e3a\u201c\u4f20\u5bfc\u5931\u8bed\u201d\uff0c\u5373\u6a21\u578b\u80fd\u7406\u89e3\u591a\u6a21\u6001\u8f93\u5165\uff0c\u4f46\u96be\u4ee5\u5c06\u7406\u89e3\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u751f\u6210\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86UniCorn\uff0c\u4e00\u4e2a\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u6559\u5e08\u76d1\u7763\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\u3002</li>\n    <li>UniCorn\u5c06\u5355\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5206\u4e3a\u4e09\u79cd\u534f\u4f5c\u89d2\u8272\uff1a\u63d0\u8bae\u8005\u3001\u89e3\u51b3\u8005\u548c\u8bc4\u5224\u8005\uff0c\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08\u751f\u6210\u9ad8\u8d28\u91cf\u4e92\u52a8\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cUniCorn\u5728\u591a\u4e2a\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u5728TIIF\u3001DPG\u548cCompBench\u7b49\u9879\u76ee\u4e0a\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u6027\u80fd\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Unified Multimodal Models (UMMs) can understand different types of inputs well but have trouble creating high-quality outputs from that understanding.</li>\n    <li>This issue is termed Conduction Aphasia, where models can interpret inputs but fail to generate coherent results.</li>\n    <li>The proposed solution, UniCorn, uses a self-improvement method without needing extra data or supervision by splitting the model into three roles: Proposer, Solver, and Judge.</li>\n    <li>UniCorn improves quality by using self-play and reconstructing cognitive patterns to make better outputs.</li>\n    <li>Tests show that UniCorn greatly outperforms the original model in generating images while keeping strong comprehension, achieving top results in several benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-06T12:15:50.000Z", "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision", "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03193.png", "numComments": 2, "submittedBy": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "fullname": "Lin Chen", "name": "Lin-Chen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 91, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u9700\u6c42\u7684\u589e\u52a0\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u3002</li>\n    <li>\u76ee\u524d\u7684\u505a\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7ed3\u6784\uff0c\u96be\u4ee5\u91cd\u590d\u4f7f\u7528\u548c\u652f\u6301\u6a21\u578b\u751f\u6210\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u5177\u6709\u6a21\u5757\u5316\u548c\u53ef\u590d\u7528\u7684\u6570\u636e\u8f6c\u6362\u80fd\u529b\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u4e86\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7ba1\u9053\uff0c\u9002\u7528\u4e8e\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>\u901a\u8fc7DataFlow\u81ea\u52a8\u751f\u6210\u7684\u6570\u636e\u7ba1\u9053\uff0c\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4f20\u7edf\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u4e13\u95e8\u5408\u6210\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data for Large Language Models (LLMs), but current methods are often messy and inconsistent.</li>\n    <li>DataFlow is a new framework that helps create better data preparation pipelines, making them more organized and easier to use.</li>\n    <li>It includes nearly 200 reusable tools and supports various tasks like text processing, coding, and knowledge extraction.</li>\n    <li>DataFlow-Agent can automatically turn simple language instructions into working data pipelines, improving usability.</li>\n    <li>Tests show that DataFlow improves LLM performance significantly, outperforming traditional datasets and methods.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u6210\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u5206\u79bb\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u591a\u6a21\u6001\u89c6\u9891\u521b\u4f5c\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u9075\u5faa\u591a\u6a21\u6001\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual language inputs.</li>\n    <li>It combines various video tasks like generation, editing, and reasoning into one unified system.</li>\n    <li>The framework can use text, images, and video as inputs, allowing for diverse and creative video content.</li>\n    <li>Kling-Omni is built on a solid data system and uses advanced training methods for better performance.</li>\n    <li>It shows strong abilities in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u52a8\u6001\u63a7\u5236\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u4e4b\u5904\u3002</li>\n    <li>Wan-Move\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u573a\u666f\u7684\u7ec6\u81f4\u63a7\u5236\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u7b80\u5316\u4e86\u8fd0\u52a8\u6307\u5bfc\u8fc7\u7a0b\u3002</li>\n    <li>\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cWan-Move\u5728\u8fd0\u52a8\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control of motion, addressing issues in existing methods.</li>\n    <li>The framework uses dense point trajectories to represent object motions for better scene control.</li>\n    <li>It can be easily integrated into existing models without needing changes to their architecture.</li>\n    <li>Wan-Move produces high-quality 5-second videos and has been tested against other methods, showing superior results.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 92, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 75, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u662f\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u5185\u5b58\u673a\u5236HGMem\uff0c\u57fa\u4e8e\u8d85\u56fe\u7ed3\u6784\uff0c\u80fd\u591f\u52a8\u6001\u5f62\u6210\u66f4\u590d\u6742\u7684\u63a8\u7406\u5173\u7cfb\u3002</li>\n    <li>HGMem\u5c06\u4fe1\u606f\u548c\u601d\u7ef4\u8fde\u63a5\u5728\u4e00\u8d77\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u63a8\u7406\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u6df1\u5ea6\u7406\u89e3\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u5728\u591a\u4e2a\u5168\u7403\u6027\u7406\u89e3\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30HGMem\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u6b65\u9aa4RAG\u4e2d\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) understand and reason better.</li>\n    <li>Current memory systems mainly store information but do not connect facts well, leading to weak reasoning.</li>\n    <li>HGMem is a new memory system that uses hypergraphs to create dynamic and interconnected memory structures.</li>\n    <li>This new approach allows for better connections between facts, improving reasoning and understanding.</li>\n    <li>Tests show that HGMem significantly outperforms existing systems on challenging tasks requiring global understanding.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u8fdb\u884c4D\u91cd\u5efa\u548c\u65b0\u8f68\u8ff9\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5f53\u524d4D\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u6602\u8d35\u6216\u8bad\u7ec3\u8fc7\u7a0b\u7e41\u7410\u3002</li>\n    <li>NeoVerse\u7684\u8bbe\u8ba1\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5404\u79cd\u5355\u955c\u5934\u89c6\u9891\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>NeoVerse\u5b9e\u73b0\u4e86\u65e0\u59ff\u6001\u524d\u99884D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u955c\u5934\u964d\u7ea7\u6a21\u5f0f\u6a21\u62df\u3002</li>\n    <li>\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeoVerse\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions, generate videos from new perspectives, and support various applications.</li>\n    <li>Current 4D modeling methods struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse is designed to work well with common monocular videos, making it easier to use.</li>\n    <li>It includes features like pose-free 4D reconstruction and online simulation of video quality issues.</li>\n    <li>NeoVerse performs very well in standard tests for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u95ee\u9898\u3002</li>\n    <li>Youtu-Agent \u662f\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u6f14\u5316 LLM \u4ee3\u7406\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5177\u6709\u7075\u6d3b\u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u6807\u51c6\u4efb\u52a1\u548c\u590d\u6742\u9700\u6c42\u7684\u81ea\u52a8\u751f\u6210\u3002</li>\n    <li>Youtu-Agent \u5305\u542b\u7ecf\u9a8c\u79ef\u7d2f\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\uff0c\u80fd\u63d0\u9ad8\u4ee3\u7406\u7684\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u5347\u4e86\u7f16\u7801\u3001\u63a8\u7406\u548c\u641c\u7d22\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents more easily.</li>\n    <li>It solves problems like high setup costs and limited adaptability of existing agents by using a modular design.</li>\n    <li>The framework allows for automatic generation of tools and configurations through two modes: Workflow for simple tasks and Meta-Agent for complex ones.</li>\n    <li>It includes a system that helps agents learn and improve over time without needing to change their underlying models.</li>\n    <li>Youtu-Agent has shown impressive results, achieving high performance on specific benchmarks and speeding up training processes significantly.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u80fd\u591f\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u548c\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u9075\u5faa\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u5982Frechet\u89c6\u9891\u8ddd\u79bb\uff08FVD\uff09\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u63a8\u7406\u5931\u8d25\uff0c\u5982\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u6cd5\u5219\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u4e2d\u8bc4\u4f30\u751f\u6210\u63a8\u7406\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u548c\u957f\u65f6\u95f4\u7a7a\u95f4\u89c4\u5212\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Video models can create realistic videos, but their ability to accurately simulate the world depends on their understanding of physical and logical rules.</li>\n  <li>Current evaluation methods focus on visual quality but miss important reasoning errors, like ignoring causality and physics.</li>\n  <li>MMGR is a new framework that assesses five reasoning skills: physical, logical, 3D spatial, 2D spatial, and temporal.</li>\n  <li>It tests models on different tasks, such as solving puzzles, navigating in 3D spaces, and understanding physical interactions.</li>\n  <li>Results show that while models do okay in physical tasks, they struggle significantly with abstract reasoning and spatial planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u6846\u67b6\u6765\u652f\u6301\u81ea\u4e3b\u79d1\u5b66\u7814\u7a76\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u6765\u5b9e\u73b0\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u5e2e\u52a9\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a\u6df1\u5ea6\u7814\u7a76\u7684\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current AI lacks a clear definition and framework for Scientific General Intelligence (SGI), which is the ability to think and work like a scientist.</li>\n    <li>We propose a definition based on the Practical Inquiry Model (PIM) and create four tasks to evaluate SGI: deep research, idea generation, experiments, and reasoning.</li>\n    <li>Our SGI-Bench includes over 1,000 curated examples based on important scientific questions, helping to test advanced AI models.</li>\n    <li>Results show significant weaknesses in AI performance, such as low accuracy in research, impractical ideas, and challenges in executing experiments properly.</li>\n    <li>We introduce Test-Time Reinforcement Learning (TTRL) to improve AI's novelty in hypotheses without needing reference answers, aiming to enhance AI's role in scientific discovery.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578bSemanticGen\uff0c\u65e8\u5728\u63d0\u9ad8\u89c6\u9891\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\uff0c\u5148\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u518d\u6dfb\u52a0\u7ec6\u8282\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5904\u7406\u4f4e\u7ea7\u89c6\u9891\u4fe1\u606f\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff0c\u9996\u5148\u751f\u6210\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u66f4\u5feb\uff0c\u5c24\u5176\u5728\u957f\u89c6\u9891\u751f\u6210\u65f6\u66f4\u5177\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u57fa\u51c6\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new way to create videos that improves on existing methods which can be slow and expensive.</li>\n    <li>It works by first generating a basic outline of the video in a high-level semantic space before adding details.</li>\n    <li>The process has two stages: first, it creates compact semantic features, then it generates detailed video content based on those features.</li>\n    <li>This approach allows for faster video generation and is better for creating longer videos.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and is better than current leading methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u53d8\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002</li>\n    <li>\u65b0\u6a21\u578bStep-GUI\u8fbe\u5230\u6700\u4f73\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u8868\u73b0\uff0c\u4e14\u6210\u672c\u964d\u4f4e10-100\u500d\uff0c\u6ce8\u91ca\u51c6\u786e\u7387\u8d85\u8fc790%\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GUI-MCP\uff0c\u9996\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u786e\u4fdd\u7528\u6237\u9690\u79c1\u5e76\u652f\u6301\u4e0d\u540c\u8bbe\u5907\u95f4\u7684\u6807\u51c6\u5316\u63a5\u53e3\u3002</li>\n    <li>\u5f15\u5165AndroidDaily\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u624b\u673a\u4f7f\u7528\u6a21\u5f0f\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u5f00\u53d1\uff0c\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5f3a\u5927\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New multimodal large language models can automate graphical user interfaces (GUIs), but getting good training data is a challenge.</li>\n    <li>A new training method, called the Calibrated Step Reward System, allows for efficient data collection with high accuracy and low cost.</li>\n    <li>The Step-GUI models achieve top performance in GUI tasks while remaining versatile for other uses.</li>\n    <li>To ensure privacy, the GUI-MCP protocol allows sensitive data to be processed on the device rather than being sent to the cloud.</li>\n    <li>The AndroidDaily benchmark tests these models with real-world mobile usage, showing promising results for everyday tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 08, 2026";
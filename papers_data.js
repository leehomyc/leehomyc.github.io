window.trendingPapers = {
    "today": [{"paper": {"id": "2512.16093", "authors": [{"_id": "6944be16fbf17e708e186002", "user": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "name": "Jintao Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:52:30.866Z", "hidden": false}, {"_id": "6944be16fbf17e708e186003", "name": "Kaiwen Zheng", "hidden": false}, {"_id": "6944be16fbf17e708e186004", "name": "Kai Jiang", "hidden": false}, {"_id": "6944be16fbf17e708e186005", "name": "Haoxu Wang", "hidden": false}, {"_id": "6944be16fbf17e708e186006", "name": "Ion Stoica", "hidden": false}, {"_id": "6944be16fbf17e708e186007", "name": "Joseph E. Gonzalez", "hidden": false}, {"_id": "6944be16fbf17e708e186008", "name": "Jianfei Chen", "hidden": false}, {"_id": "6944be16fbf17e708e186009", "name": "Jun Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "publishedAt": "2025-12-18T02:21:30.000Z", "submittedOnDailyAt": "2025-12-25T00:44:44.469Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "submittedOnDailyBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "upvotes": 48, "discussionId": "6944be16fbf17e708e18600a", "projectPage": "https://github.com/thu-ml/TurboDiffusion", "githubRepo": "https://github.com/thu-ml/TurboDiffusion", "githubRepoAddedBy": "user", "ai_summary": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.", "ai_keywords": ["SageAttention", "Sparse-Linear Attention", "rCM", "W8A8 quantization", "diffusion generation", "video generation", "RTX 5090 GPU"], "githubStars": 1958, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "summary_zh": "<ul>\n    <li>TurboDiffusion\u662f\u4e00\u4e2a\u89c6\u9891\u751f\u6210\u52a0\u901f\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u751f\u6210\u901f\u5ea6\u63d0\u9ad8100-200\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\u3002</li>\n    <li>\u5176\u52a0\u901f\u4e3b\u8981\u4f9d\u9760\u51e0\u4e2a\u7ec4\u4ef6\uff1a\u4f4e\u4f4dSageAttention\u548c\u53ef\u8bad\u7ec3\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\uff08SLA\uff09\u52a0\u5feb\u6ce8\u610f\u529b\u8ba1\u7b97\u3002</li>\n    <li>\u4f7f\u7528rCM\u8fdb\u884c\u6709\u6548\u7684\u6b65\u9aa4\u84b8\u998f\u3002</li>\n    <li>\u5c06\u6a21\u578b\u53c2\u6570\u548c\u6fc0\u6d3b\u91cf\u5316\u4e3a8\u4f4d\uff0c\u4ee5\u52a0\u901f\u7ebf\u6027\u5c42\u5e76\u538b\u7f29\u6a21\u578b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cTurboDiffusion\u5728\u5355\u4e2aRTX 5090 GPU\u4e0a\u4e5f\u80fd\u5b9e\u73b0100-200\u500d\u7684\u751f\u6210\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u89c6\u9891\u8d28\u91cf\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>TurboDiffusion is a new framework that speeds up video generation by 100-200 times while keeping the video quality high.</li>\n    <li>It uses special techniques like low-bit SageAttention and Sparse-Linear Attention for faster attention calculations.</li>\n    <li>TurboDiffusion employs step distillation and compresses model data to 8 bits to improve performance.</li>\n    <li>Tests on various models show significant speed improvements, even on a single RTX 5090 GPU.</li>\n    <li>Code and model files are available on GitHub for easy access and use.</li>\n</ul>"}, "publishedAt": "2025-12-17T21:21:30.000Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16093.png", "numComments": 2, "submittedBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "fullname": "Jintao Zhang", "name": "jt-zhang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u5957\u4ef6\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u74063D\u7a7a\u95f4\u4e2d\u7269\u4f53\u51e0\u4f55\u548c\u5173\u7cfb\u7684\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u751f\u6210\u591a\u9879\u9009\u62e9\u9898\u548c\u7b54\u6848\uff0c\u63d0\u53d6\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\u3002</li>\n    <li>\u65b0\u6570\u636e\u96c6\u5f3a\u8c03\u4e86\u771f\u5b9e\u89c6\u9891\u6765\u6e90\u30013D\u5bf9\u8c61\u548c\u573a\u666f\u9700\u6c42\u3001\u89c6\u89d2\u8f6c\u6362\u3001\u591a\u7269\u4f53\u4e92\u52a8\u548c\u7ec6\u81f4\u7684\u7a0b\u5e8f\u6027\u7b54\u6848\u3002</li>\n    <li>\u5f15\u5165\u4e86\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\uff0c\u5c06\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u6574\u5408\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5e2e\u52a9\u7cbe\u7b80\u95ee\u9898\u8bed\u4e49\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06DSR-Train\u548cGSM\u6574\u5408\u5230Qwen2.5-VL-7B\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e00\u822c\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models are good at understanding but struggle with dynamic spatial reasoning (DSR), which involves understanding how objects change in 3D space over time.</li>\n    <li>To improve DSR, a new tool called DSR Suite is introduced, which includes an automated system that creates question-answer pairs from videos.</li>\n    <li>This system uses advanced vision models to gather important 3D information like the position of cameras and the movement of objects.</li>\n    <li>DSR Suite focuses on real-world videos, requires 3D understanding of objects and scenes, and incorporates complex interactions between multiple objects.</li>\n    <li>A new Geometry Selection Module (GSM) helps VLMs use relevant 3D information without being overloaded with unnecessary details, improving their ability to reason about dynamic spatial changes.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u201c\u4e00\u955c\u5230\u5e95\u201d\u6280\u672f\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5177\u6709\u72ec\u7279\u7684\u7f8e\u5b66\uff0c\u4f46\u5b9e\u73b0\u6210\u672c\u9ad8\u4e14\u9762\u4e34\u590d\u6742\u9650\u5236\u3002</li>\n    <li>DreaMontage\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u6237\u8f93\u5165\u5408\u6210\u65e0\u7f1d\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u957f\u65f6\u6bb5\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u65b9\u9762\u63d0\u5347\u89c6\u9891\u8d28\u91cf\uff1a\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u6761\u4ef6\u673a\u5236\u3001\u89c6\u89c9\u8868\u8fbe\u4f18\u5316\u548c\u5b9a\u5236\u7684DPO\u65b9\u6848\u3002</li>\n    <li>\u91c7\u7528\u6bb5\u843d\u81ea\u56de\u5f52\u63a8\u7406\u7b56\u7565\uff0c\u4f7f\u5f97\u751f\u6210\u957f\u89c6\u9891\u66f4\u9ad8\u6548\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u6548\u679c\u4e0a\u51fa\u8272\uff0c\u5e76\u4e14\u63d0\u9ad8\u4e86\u751f\u6210\u5185\u5bb9\u7684\u8fde\u8d2f\u6027\u548c\u5b9e\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" filmmaking technique is unique but expensive and difficult to implement in real life.</li>\n    <li>Current video generation methods often produce choppy videos due to simple clip joining.</li>\n    <li>DreaMontage is a new framework that creates smooth, expressive one-shot videos from user inputs.</li>\n    <li>It uses advanced methods to control video frames, improve visual quality, and ensure smooth transitions.</li>\n    <li>The system is efficient and allows users to turn short clips into engaging, seamless one-shot films.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21094", "authors": [{"_id": "694ca45e746a34b55dd542de", "name": "Zhe Cao", "hidden": false}, {"_id": "694ca45e746a34b55dd542df", "name": "Tao Wang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e0", "user": {"_id": "68355c5ec0003bc40230b3f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WZQ5f8oqpni0i-D3a5R-P.png", "isPro": false, "fullname": "jasmineWang", "user": "Jessamine", "type": "user"}, "name": "Jiaming Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:43.024Z", "hidden": false}, {"_id": "694ca45e746a34b55dd542e1", "name": "Yanghai Wang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e2", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e3", "name": "Jialu Chen", "hidden": false}, {"_id": "694ca45e746a34b55dd542e4", "name": "Miao Deng", "hidden": false}, {"_id": "694ca45e746a34b55dd542e5", "user": {"_id": "68abfd1ba1f07af43fbbf3f1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ivRfEWMAo1GQWw3x06LQp.png", "isPro": false, "fullname": "jiahaowang", "user": "wang-jiahao", "type": "user"}, "name": "Jiahao Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:40.267Z", "hidden": false}, {"_id": "694ca45e746a34b55dd542e6", "name": "Yubin Guo", "hidden": false}, {"_id": "694ca45e746a34b55dd542e7", "name": "Chenxi Liao", "hidden": false}, {"_id": "694ca45e746a34b55dd542e8", "name": "Yize Zhang", "hidden": false}, {"_id": "694ca45e746a34b55dd542e9", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "694ca45e746a34b55dd542ea", "name": "Jiaheng Liu", "hidden": false}], "publishedAt": "2025-12-24T10:30:35.000Z", "submittedOnDailyAt": "2025-12-25T00:12:49.258Z", "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation", "submittedOnDailyBy": {"_id": "65377c30e48353201e6fdda0", "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg", "isPro": false, "fullname": "Jiaheng Liu", "user": "CheeryLJH", "type": "user"}, "summary": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.", "upvotes": 22, "discussionId": "694ca45e746a34b55dd542eb", "projectPage": "https://nju-link.github.io/T2AV-Compass/", "githubRepo": "https://github.com/NJU-LINK/T2AV-Compass/", "githubRepoAddedBy": "user", "githubStars": 5, "organization": {"_id": "68edc767abe005ac1b354573", "name": "NJU-LINK", "fullname": "NJU-LINK Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"}, "summary_zh": "<ul>\n    <li>T2AV\u751f\u6210\u65e8\u5728\u6839\u636e\u81ea\u7136\u8bed\u8a00\u5408\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u548c\u8bed\u4e49\u540c\u6b65\u7684\u97f3\u9891\uff0c\u4f46\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u591f\u5168\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86T2AV-Compass\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u5305\u542b500\u4e2a\u591a\u6837\u4e14\u590d\u6742\u7684\u63d0\u793a\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30T2AV\u7cfb\u7edf\u3002</li>\n    <li>T2AV-Compass\u5f15\u5165\u4e86\u53cc\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5ba2\u89c2\u4fe1\u53f7\u7ea7\u6307\u6807\u548c\u4e3b\u89c2\u8bc4\u4f30\u534f\u8bae\u3002</li>\n    <li>\u5bf911\u4e2a\u4ee3\u8868\u6027T2AV\u7cfb\u7edf\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5f3a\u7684\u6a21\u578b\u5728\u771f\u5b9e\u611f\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002</li>\n    <li>\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u672a\u6765\u6a21\u578b\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u5e76\u5f3a\u8c03\u4e86T2AV-Compass\u5728\u63a8\u52a8\u6587\u672c\u5230\u97f3\u9891\u89c6\u9891\u751f\u6210\u4e2d\u7684\u91cd\u8981\u4ef7\u503c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text-to-Audio-Video (T2AV) generation creates videos and audio from text but lacks effective evaluation methods.</li>\n    <li>T2AV-Compass is a new benchmark with 500 complex prompts for better evaluation of T2AV systems.</li>\n    <li>The benchmark uses a two-part evaluation system: objective metrics for quality and subjective assessments for realism.</li>\n    <li>Tests on 11 T2AV systems show that even the best models struggle with producing human-like realism and audio-video synchronization.</li>\n    <li>T2AV-Compass helps identify areas where T2AV models can improve and serves as a useful tool for future research.</li>\n</ul>"}, "publishedAt": "2025-12-24T05:30:35.000Z", "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation", "summary": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21094.png", "numComments": 1, "submittedBy": {"_id": "65377c30e48353201e6fdda0", "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg", "fullname": "Jiaheng Liu", "name": "CheeryLJH", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 25}, "organization": {"_id": "68edc767abe005ac1b354573", "name": "NJU-LINK", "fullname": "NJU-LINK Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67f9d060395fb1a0d7e4ae21/O3V4UZjcSGnOivcQqTcXW.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21337", "authors": [{"_id": "694ccc03746a34b55dd54372", "name": "Li-Zhong Szu-Tu", "hidden": false}, {"_id": "694ccc03746a34b55dd54373", "name": "Ting-Lin Wu", "hidden": false}, {"_id": "694ccc03746a34b55dd54374", "name": "Chia-Jui Chang", "hidden": false}, {"_id": "694ccc03746a34b55dd54375", "name": "He Syu", "hidden": false}, {"_id": "694ccc03746a34b55dd54376", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KzxafmNkf4IHasqHjp3DV.jpeg"], "publishedAt": "2025-12-24T18:59:54.000Z", "submittedOnDailyAt": "2025-12-25T03:01:52.321Z", "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/", "upvotes": 20, "discussionId": "694ccc03746a34b55dd54377", "projectPage": "https://sytwu.github.io/BeyondMemo/", "githubRepo": "https://github.com/Sytwu/BeyondMemo", "githubRepoAddedBy": "user", "githubStars": 3, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u53d1\u73b0\u4e86\u6700\u65b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5b58\u5728\u663e\u8457\u7684\u6d41\u884c\u6027\u504f\u5dee\uff0c\u8457\u540d\u5efa\u7b51\u7684\u51c6\u786e\u7387\u6bd4\u666e\u901a\u5efa\u7b51\u9ad8\u51fa34%\u3002</li>\n    <li>\u4e3a\u4e86\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\uff0c\u6211\u4eec\u63a8\u51fa\u4e86\u6700\u5927\u7684\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff1aYearGuessr\uff0c\u5305\u542b\u6765\u81ea157\u4e2a\u56fd\u5bb6\u768455,546\u5f20\u5efa\u7b51\u56fe\u7247\u3002</li>\n    <li>\u8be5\u6570\u636e\u96c6\u6807\u6ce8\u4e86\u5efa\u7b51\u7684\u5efa\u8bbe\u5e74\u4efd\uff081001-2024\uff09\u3001GPS\u6570\u636e\u4ee5\u53ca\u9875\u9762\u8bbf\u95ee\u91cf\uff0c\u4ee5\u4f5c\u4e3a\u6d41\u884c\u6027\u7684\u4ee3\u7406\u6307\u6807\u3002</li>\n    <li>\u6211\u4eec\u5c06\u5efa\u8bbe\u5e74\u4efd\u9884\u6d4b\u4efb\u52a1\u89c6\u4e3a\u5e8f\u6570\u56de\u5f52\uff0c\u5e76\u5f15\u5165\u4e86\u8003\u8651\u6d41\u884c\u6027\u7684\u533a\u95f4\u51c6\u786e\u6027\u6307\u6807\u6765\u91cf\u5316\u8fd9\u79cd\u504f\u5dee\u3002</li>\n    <li>\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cVLMs\u5728\u6d41\u884c\u3001\u88ab\u8bb0\u4f4f\u7684\u9879\u76ee\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0d\u88ab\u8bc6\u522b\u7684\u5bf9\u8c61\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u66b4\u9732\u4e86\u5b83\u4eec\u63a8\u7406\u80fd\u529b\u7684\u4e25\u91cd\u7f3a\u9677\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) perform 34% better on famous buildings than ordinary ones, showing a bias towards popular items.</li>\n    <li>The YearGuessr dataset is introduced, containing 55,546 building images from 157 countries, with data on construction years, GPS, and popularity.</li>\n    <li>The dataset allows for a new way to predict construction years using ordinal regression and measures popularity bias with new accuracy metrics.</li>\n    <li>Tests with over 30 models, including the YearCLIP model, reveal that VLMs are good at recognizing popular buildings but struggle with less known ones.</li>\n    <li>This highlights a major limitation in VLMs' reasoning skills, as they rely on memorization rather than true understanding.</li>\n</ul>"}, "publishedAt": "2025-12-24T13:59:54.000Z", "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models", "summary": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/KzxafmNkf4IHasqHjp3DV.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21337.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21338", "authors": [{"_id": "694ca7a7746a34b55dd542ed", "name": "Haonan Qiu", "hidden": false}, {"_id": "694ca7a7746a34b55dd542ee", "name": "Shikun Liu", "hidden": false}, {"_id": "694ca7a7746a34b55dd542ef", "name": "Zijian Zhou", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f0", "name": "Zhaochong An", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f1", "name": "Weiming Ren", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f2", "name": "Zhiheng Liu", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f3", "name": "Jonas Schult", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f4", "name": "Sen He", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f5", "user": {"_id": "6412a33900634c4fe9873652", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6412a33900634c4fe9873652/Nmn_yRA1gGD2VO1YbSOYF.jpeg", "isPro": false, "fullname": "Shoufa Chen", "user": "ShoufaChen", "type": "user"}, "name": "Shoufa Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:37.443Z", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f6", "name": "Yuren Cong", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f7", "name": "Tao Xiang", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f8", "name": "Ziwei Liu", "hidden": false}, {"_id": "694ca7a7746a34b55dd542f9", "name": "Juan-Manuel Perez-Rua", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63023b6ab002e9a4a2152890/ytnMYj5hXZ70LoDfcSZyA.jpeg"], "publishedAt": "2025-12-24T18:59:58.000Z", "submittedOnDailyAt": "2025-12-25T00:30:37.939Z", "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming", "submittedOnDailyBy": {"_id": "63023b6ab002e9a4a2152890", "avatarUrl": "/avatars/cae8ba0a8d61fb4e576934431f43991b.svg", "isPro": false, "fullname": "Haonan Qiu", "user": "MoonQiu", "type": "user"}, "summary": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.", "upvotes": 15, "discussionId": "694ca7a8746a34b55dd542fa", "projectPage": "https://arthur-qiu.github.io/projects/HiStream", "githubRepo": "https://github.com/arthur-qiu/HiStream", "githubRepoAddedBy": "user", "githubStars": 21, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>HiStream\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u7a7a\u95f4\u538b\u7f29\u3001\u65f6\u95f4\u538b\u7f29\u548c\u65f6\u95f4\u6b65\u538b\u7f29\u4e09\u4e2a\u65b9\u9762\u51cf\u5c11\u5197\u4f59\uff0c\u63d0\u9ad8\u751f\u6210\u6548\u7387\u3002</li>\n    <li>\u57281080p\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHiStream\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u4e14\u53bb\u566a\u901f\u5ea6\u6bd4Wan2.1\u5feb76.2\u500d\u3002</li>\n    <li>\u5176\u5feb\u901f\u53d8\u4f53HiStream+\u7ed3\u5408\u4e86\u6240\u6709\u4e09\u79cd\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u5feb107.5\u500d\u7684\u52a0\u901f\u3002</li>\n    <li>HiStream\u4f7f\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u53d8\u5f97\u66f4\u5b9e\u7528\u548c\u53ef\u6269\u5c55\uff0c\u540c\u65f6\u5728\u901f\u5ea6\u548c\u8d28\u91cf\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>HiStream is a new system designed to generate high-resolution videos more efficiently.</li>\n    <li>It reduces processing time by compressing data in three ways: low-resolution denoising, chunk-based processing, and fewer steps for later processing.</li>\n    <li>HiStream achieves top visual quality while running up to 76.2 times faster than previous methods.</li>\n    <li>A faster version, HiStream+, uses all three optimizations to be 107.5 times faster, balancing speed and quality effectively.</li>\n    <li>This makes it easier and more practical to create high-resolution videos for digital media and film.</li>\n</ul>"}, "publishedAt": "2025-12-24T13:59:58.000Z", "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming", "summary": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63023b6ab002e9a4a2152890/ytnMYj5hXZ70LoDfcSZyA.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21338.png", "numComments": 1, "submittedBy": {"_id": "63023b6ab002e9a4a2152890", "avatarUrl": "/avatars/cae8ba0a8d61fb4e576934431f43991b.svg", "fullname": "Haonan Qiu", "name": "MoonQiu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20848", "authors": [{"_id": "694ca2e9746a34b55dd5419b", "name": "NVIDIA", "hidden": false}, {"_id": "694ca2e9746a34b55dd5419d", "name": "Aaron Blakeman", "hidden": false}, {"_id": "694ca2e9746a34b55dd5419e", "name": "Aaron Grattafiori", "hidden": false}, {"_id": "694ca2e9746a34b55dd5419f", "name": "Aarti Basant", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a0", "name": "Abhibha Gupta", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a1", "name": "Abhinav Khattar", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a2", "name": "Adi Renduchintala", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a3", "name": "Aditya Vavre", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a4", "name": "Akanksha Shukla", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a5", "name": "Akhiad Bercovich", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a6", "name": "Aleksander Ficek", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a7", "name": "Aleksandr Shaposhnikov", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a8", "name": "Alex Kondratenko", "hidden": false}, {"_id": "694ca2e9746a34b55dd541a9", "name": "Alexander Bukharin", "hidden": false}, {"_id": "694ca2e9746a34b55dd541aa", "name": "Alexandre Milesi", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ab", "name": "Ali Taghibakhshi", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ac", "name": "Alisa Liu", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ad", "name": "Amelia Barton", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ae", "name": "Ameya Sunil Mahabaleshwarkar", "hidden": false}, {"_id": "694ca2e9746a34b55dd541af", "name": "Amir Klein", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b0", "name": "Amit Zuker", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b1", "name": "Amnon Geifman", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b2", "name": "Amy Shen", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b3", "name": "Anahita Bhiwandiwalla", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b4", "name": "Andrew Tao", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b5", "name": "Ann Guan", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b6", "name": "Anubhav Mandarwal", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b7", "name": "Arham Mehta", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b8", "name": "Ashwath Aithal", "hidden": false}, {"_id": "694ca2e9746a34b55dd541b9", "name": "Ashwin Poojary", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ba", "name": "Asif Ahamed", "hidden": false}, {"_id": "694ca2e9746a34b55dd541bb", "name": "Asma Kuriparambil Thekkumpate", "hidden": false}, {"_id": "694ca2e9746a34b55dd541bc", "name": "Ayush Dattagupta", "hidden": false}, {"_id": "694ca2e9746a34b55dd541bd", "name": "Banghua Zhu", "hidden": false}, {"_id": "694ca2e9746a34b55dd541be", "name": "Bardiya Sadeghi", "hidden": false}, {"_id": "694ca2e9746a34b55dd541bf", "name": "Barnaby Simkin", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c0", "name": "Ben Lanir", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c1", "name": "Benedikt Schifferer", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c2", "name": "Besmira Nushi", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c3", "name": "Bilal Kartal", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c4", "name": "Bita Darvish Rouhani", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c5", "name": "Boris Ginsburg", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c6", "name": "Brandon Norick", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c7", "name": "Brandon Soubasis", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c8", "name": "Branislav Kisacanin", "hidden": false}, {"_id": "694ca2e9746a34b55dd541c9", "name": "Brian Yu", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ca", "name": "Bryan Catanzaro", "hidden": false}, {"_id": "694ca2e9746a34b55dd541cb", "name": "Carlo del Mundo", "hidden": false}, {"_id": "694ca2e9746a34b55dd541cc", "name": "Chantal Hwang", "hidden": false}, {"_id": "694ca2e9746a34b55dd541cd", "name": "Charles Wang", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ce", "name": "Cheng-Ping Hsieh", "hidden": false}, {"_id": "694ca2e9746a34b55dd541cf", "name": "Chenghao Zhang", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d0", "name": "Chenhan Yu", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d1", "name": "Chetan Mungekar", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d2", "name": "Chintan Patel", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d3", "name": "Chris Alexiuk", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d4", "name": "Christopher Parisien", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d5", "name": "Collin Neale", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d6", "name": "Damon Mosk-Aoyama", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d7", "name": "Dan Su", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d8", "name": "Dane Corneil", "hidden": false}, {"_id": "694ca2e9746a34b55dd541d9", "name": "Daniel Afrimi", "hidden": false}, {"_id": "694ca2e9746a34b55dd541da", "name": "Daniel Rohrer", "hidden": false}, {"_id": "694ca2e9746a34b55dd541db", "name": "Daniel Serebrenik", "hidden": false}, {"_id": "694ca2e9746a34b55dd541dc", "name": "Daria Gitman", "hidden": false}, {"_id": "694ca2e9746a34b55dd541dd", "name": "Daria Levy", "hidden": false}, {"_id": "694ca2e9746a34b55dd541de", "name": "Darko Stosic", "hidden": false}, {"_id": "694ca2e9746a34b55dd541df", "name": "David Mosallanezhad", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e0", "name": "Deepak Narayanan", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e1", "name": "Dhruv Nathawani", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e2", "name": "Dima Rekesh", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e3", "name": "Dina Yared", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e4", "name": "Divyanshu Kakwani", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e5", "name": "Dong Ahn", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e6", "name": "Duncan Riach", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e7", "name": "Dusan Stosic", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e8", "name": "Edgar Minasyan", "hidden": false}, {"_id": "694ca2e9746a34b55dd541e9", "name": "Edward Lin", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ea", "name": "Eileen Long", "hidden": false}, {"_id": "694ca2e9746a34b55dd541eb", "name": "Eileen Peters Long", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ec", "name": "Elena Lantz", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ed", "name": "Ellie Evans", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ee", "name": "Elliott Ning", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ef", "name": "Eric Chung", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f0", "name": "Eric Harper", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f1", "name": "Eric Tramel", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f2", "name": "Erick Galinkin", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f3", "name": "Erik Pounds", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f4", "name": "Evan Briones", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f5", "name": "Evelina Bakhturina", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f6", "name": "Faisal Ladhak", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f7", "name": "Fay Wang", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f8", "name": "Fei Jia", "hidden": false}, {"_id": "694ca2e9746a34b55dd541f9", "name": "Felipe Soares", "hidden": false}, {"_id": "694ca2e9746a34b55dd541fa", "name": "Feng Chen", "hidden": false}, {"_id": "694ca2e9746a34b55dd541fb", "name": "Ferenc Galko", "hidden": false}, {"_id": "694ca2e9746a34b55dd541fc", "name": "Frankie Siino", "hidden": false}, {"_id": "694ca2e9746a34b55dd541fd", "name": "Gal Hubara Agam", "hidden": false}, {"_id": "694ca2e9746a34b55dd541fe", "name": "Ganesh Ajjanagadde", "hidden": false}, {"_id": "694ca2e9746a34b55dd541ff", "name": "Gantavya Bhatt", "hidden": false}, {"_id": "694ca2e9746a34b55dd54200", "name": "Gargi Prasad", "hidden": false}, {"_id": "694ca2e9746a34b55dd54201", "name": "George Armstrong", "hidden": false}, {"_id": "694ca2e9746a34b55dd54202", "name": "Gerald Shen", "hidden": false}, {"_id": "694ca2e9746a34b55dd54203", "name": "Gorkem Batmaz", "hidden": false}, {"_id": "694ca2e9746a34b55dd54204", "name": "Grigor Nalbandyan", "hidden": false}, {"_id": "694ca2e9746a34b55dd54205", "name": "Haifeng Qian", "hidden": false}, {"_id": "694ca2e9746a34b55dd54206", "name": "Harsh Sharma", "hidden": false}, {"_id": "694ca2e9746a34b55dd54207", "name": "Hayley Ross", "hidden": false}, {"_id": "694ca2e9746a34b55dd54208", "name": "Helen Ngo", "hidden": false}, {"_id": "694ca2e9746a34b55dd54209", "name": "Herman Sahota", "hidden": false}, {"_id": "694ca2e9746a34b55dd5420a", "name": "Hexin Wang", "hidden": false}, {"_id": "694ca2e9746a34b55dd5420b", "name": "Himanshu Soni", "hidden": false}, {"_id": "694ca2e9746a34b55dd5420c", "name": "Hiren Upadhyay", "hidden": false}, {"_id": "694ca2e9746a34b55dd5420d", "name": "Huizi Mao", "hidden": false}, {"_id": "694ca2e9746a34b55dd5420e", "name": "Huy C Nguyen", "hidden": false}, {"_id": "694ca2e9746a34b55dd5420f", "name": "Huy Q Nguyen", "hidden": false}, {"_id": "694ca2e9746a34b55dd54210", "name": "Iain Cunningham", "hidden": false}, {"_id": "694ca2e9746a34b55dd54211", "name": "Ido Shahaf", "hidden": false}, {"_id": "694ca2e9746a34b55dd54212", "name": "Igor Gitman", "hidden": false}, {"_id": "694ca2e9746a34b55dd54213", "name": "Ilya Loshchilov", "hidden": false}, {"_id": "694ca2e9746a34b55dd54214", "name": "Ivan Moshkov", "hidden": false}, {"_id": "694ca2e9746a34b55dd54215", "name": "Izzy Putterman", "hidden": false}, {"_id": "694ca2e9746a34b55dd54216", "name": "Jan Kautz", "hidden": false}, {"_id": "694ca2e9746a34b55dd54217", "name": "Jane Polak Scowcroft", "hidden": false}, {"_id": "694ca2e9746a34b55dd54218", "name": "Jared Casper", "hidden": false}, {"_id": "694ca2e9746a34b55dd54219", "name": "Jatin Mitra", "hidden": false}, {"_id": "694ca2e9746a34b55dd5421a", "name": "Jeffrey Glick", "hidden": false}, {"_id": "694ca2e9746a34b55dd5421b", "name": "Jenny Chen", "hidden": false}, {"_id": "694ca2e9746a34b55dd5421c", "name": "Jesse Oliver", "hidden": false}, {"_id": "694ca2e9746a34b55dd5421d", "name": "Jian Zhang", "hidden": false}, {"_id": "694ca2e9746a34b55dd5421e", "name": "Jiaqi Zeng", "hidden": false}, {"_id": "694ca2e9746a34b55dd5421f", "name": "Jie Lou", "hidden": false}, {"_id": "694ca2e9746a34b55dd54220", "name": "Jimmy Zhang", "hidden": false}, {"_id": "694ca2e9746a34b55dd54221", "name": "Jining Huang", "hidden": false}, {"_id": "694ca2e9746a34b55dd54222", "name": "Joey Conway", "hidden": false}, {"_id": "694ca2e9746a34b55dd54223", "name": "Joey Guman", "hidden": false}, {"_id": "694ca2e9746a34b55dd54224", "name": "John Kamalu", "hidden": false}, {"_id": "694ca2e9746a34b55dd54225", "name": "Johnny Greco", "hidden": false}, {"_id": "694ca2e9746a34b55dd54226", "name": "Jonathan Cohen", "hidden": false}, {"_id": "694ca2e9746a34b55dd54227", "name": "Joseph Jennings", "hidden": false}, {"_id": "694ca2e9746a34b55dd54228", "name": "Joyjit Daw", "hidden": false}, {"_id": "694ca2e9746a34b55dd54229", "name": "Julien Veron Vialard", "hidden": false}, {"_id": "694ca2e9746a34b55dd5422a", "name": "Junkeun Yi", "hidden": false}, {"_id": "694ca2e9746a34b55dd5422b", "name": "Jupinder Parmar", "hidden": false}, {"_id": "694ca2e9746a34b55dd5422c", "name": "Kai Xu", "hidden": false}, {"_id": "694ca2e9746a34b55dd5422d", "name": "Kan Zhu", "hidden": false}, {"_id": "694ca2e9746a34b55dd5422e", "name": "Kari Briski", "hidden": false}, {"_id": "694ca2e9746a34b55dd5422f", "name": "Katherine Cheung", "hidden": false}, {"_id": "694ca2e9746a34b55dd54230", "name": "Katherine Luna", "hidden": false}, {"_id": "694ca2e9746a34b55dd54231", "name": "Keshav Santhanam", "hidden": false}, {"_id": "694ca2e9746a34b55dd54232", "name": "Kevin Shih", "hidden": false}, {"_id": "694ca2e9746a34b55dd54233", "name": "Kezhi Kong", "hidden": false}, {"_id": "694ca2e9746a34b55dd54234", "name": "Khushi Bhardwaj", "hidden": false}, {"_id": "694ca2e9746a34b55dd54235", "name": "Krishna C. Puvvada", "hidden": false}, {"_id": "694ca2e9746a34b55dd54236", "name": "Krzysztof Pawelec", "hidden": false}, {"_id": "694ca2e9746a34b55dd54237", "name": "Kumar Anik", "hidden": false}, {"_id": "694ca2e9746a34b55dd54238", "name": "Lawrence McAfee", "hidden": false}, {"_id": "694ca2e9746a34b55dd54239", "name": "Laya Sleiman", "hidden": false}, {"_id": "694ca2e9746a34b55dd5423a", "name": "Leon Derczynski", "hidden": false}, {"_id": "694ca2e9746a34b55dd5423b", "name": "Li Ding", "hidden": false}, {"_id": "694ca2e9746a34b55dd5423c", "name": "Lucas Liebenwein", "hidden": false}, {"_id": "694ca2e9746a34b55dd5423d", "name": "Luis Vega", "hidden": false}, {"_id": "694ca2e9746a34b55dd5423e", "name": "Maanu Grover", "hidden": false}, {"_id": "694ca2e9746a34b55dd5423f", "name": "Maarten Van Segbroeck", "hidden": false}, {"_id": "694ca2e9746a34b55dd54240", "name": "Maer Rodrigues de Melo", "hidden": false}, {"_id": "694ca2e9746a34b55dd54241", "name": "Makesh Narsimhan Sreedhar", "hidden": false}, {"_id": "694ca2e9746a34b55dd54242", "name": "Manoj Kilaru", "hidden": false}, {"_id": "694ca2e9746a34b55dd54243", "name": "Maor Ashkenazi", "hidden": false}, {"_id": "694ca2e9746a34b55dd54244", "name": "Marc Romeijn", "hidden": false}, {"_id": "694ca2e9746a34b55dd54245", "name": "Mark Cai", "hidden": false}, {"_id": "694ca2e9746a34b55dd54246", "name": "Markus Kliegl", "hidden": false}, {"_id": "694ca2e9746a34b55dd54247", "name": "Maryam Moosaei", "hidden": false}, {"_id": "694ca2e9746a34b55dd54248", "name": "Matvei Novikov", "hidden": false}, {"_id": "694ca2e9746a34b55dd54249", "name": "Mehrzad Samadi", "hidden": false}, {"_id": "694ca2e9746a34b55dd5424a", "name": "Melissa Corpuz", "hidden": false}, {"_id": "694ca2e9746a34b55dd5424b", "name": "Mengru Wang", "hidden": false}, {"_id": "694ca2e9746a34b55dd5424c", "name": "Meredith Price", "hidden": false}, {"_id": "694ca2e9746a34b55dd5424d", "name": "Michael Boone", "hidden": false}, {"_id": "694ca2e9746a34b55dd5424e", "name": "Michael Evans", "hidden": false}, {"_id": "694ca2e9746a34b55dd5424f", "name": "Miguel Martinez", "hidden": false}, {"_id": "694ca2e9746a34b55dd54250", "name": "Mike Chrzanowski", "hidden": false}, {"_id": "694ca2e9746a34b55dd54251", "name": "Mohammad Shoeybi", "hidden": false}, {"_id": "694ca2e9746a34b55dd54252", "name": "Mostofa Patwary", "hidden": false}, {"_id": "694ca2e9746a34b55dd54253", "name": "Nabin Mulepati", "hidden": false}, {"_id": "694ca2e9746a34b55dd54254", "name": "Natalie Hereth", "hidden": false}, {"_id": "694ca2e9746a34b55dd54255", "name": "Nave Assaf", "hidden": false}, {"_id": "694ca2e9746a34b55dd54256", "name": "Negar Habibi", "hidden": false}, {"_id": "694ca2e9746a34b55dd54257", "name": "Neta Zmora", "hidden": false}, {"_id": "694ca2e9746a34b55dd54258", "name": "Netanel Haber", "hidden": false}, {"_id": "694ca2e9746a34b55dd54259", "name": "Nicola Sessions", "hidden": false}, {"_id": "694ca2e9746a34b55dd5425a", "name": "Nidhi Bhatia", "hidden": false}, {"_id": "694ca2e9746a34b55dd5425b", "name": "Nikhil Jukar", "hidden": false}, {"_id": "694ca2e9746a34b55dd5425c", "name": "Nikki Pope", "hidden": false}, {"_id": "694ca2e9746a34b55dd5425d", "name": "Nikolai Ludwig", "hidden": false}, {"_id": "694ca2e9746a34b55dd5425e", "name": "Nima Tajbakhsh", "hidden": false}, {"_id": "694ca2e9746a34b55dd5425f", "name": "Nirmal Juluru", "hidden": false}, {"_id": "694ca2e9746a34b55dd54260", "name": "Oleksii Hrinchuk", "hidden": false}, {"_id": "694ca2e9746a34b55dd54261", "name": "Oleksii Kuchaiev", "hidden": false}, {"_id": "694ca2e9746a34b55dd54262", "name": "Olivier Delalleau", "hidden": false}, {"_id": "694ca2e9746a34b55dd54263", "name": "Oluwatobi Olabiyi", "hidden": false}, {"_id": "694ca2e9746a34b55dd54264", "name": "Omer Ullman Argov", "hidden": false}, {"_id": "694ca2e9746a34b55dd54265", "name": "Ouye Xie", "hidden": false}, {"_id": "694ca2e9746a34b55dd54266", "name": "Parth Chadha", "hidden": false}, {"_id": "694ca2e9746a34b55dd54267", "name": "Pasha Shamis", "hidden": false}, {"_id": "694ca2e9746a34b55dd54268", "name": "Pavlo Molchanov", "hidden": false}, {"_id": "694ca2e9746a34b55dd54269", "name": "Pawel Morkisz", "hidden": false}, {"_id": "694ca2e9746a34b55dd5426a", "name": "Peter Dykas", "hidden": false}, {"_id": "694ca2e9746a34b55dd5426b", "name": "Peter Jin", "hidden": false}, {"_id": "694ca2e9746a34b55dd5426c", "name": "Pinky Xu", "hidden": false}, {"_id": "694ca2e9746a34b55dd5426d", "name": "Piotr Januszewski", "hidden": false}, {"_id": "694ca2e9746a34b55dd5426e", "name": "Pranav Prashant Thombre", "hidden": false}, {"_id": "694ca2e9746a34b55dd5426f", "name": "Prasoon Varshney", "hidden": false}, {"_id": "694ca2e9746a34b55dd54270", "name": "Pritam Gundecha", "hidden": false}, {"_id": "694ca2e9746a34b55dd54271", "name": "Qing Miao", "hidden": false}, {"_id": "694ca2e9746a34b55dd54272", "name": "Rabeeh Karimi Mahabadi", "hidden": false}, {"_id": "694ca2e9746a34b55dd54273", "name": "Ran El-Yaniv", "hidden": false}, {"_id": "694ca2e9746a34b55dd54274", "name": "Ran Zilberstein", "hidden": false}, {"_id": "694ca2e9746a34b55dd54275", "name": "Rasoul Shafipour", "hidden": false}, {"_id": "694ca2e9746a34b55dd54276", "name": "Rich Harang", "hidden": false}, {"_id": "694ca2e9746a34b55dd54277", "name": "Rick Izzo", "hidden": false}, {"_id": "694ca2e9746a34b55dd54278", "name": "Rima Shahbazyan", "hidden": false}, {"_id": "694ca2e9746a34b55dd54279", "name": "Rishabh Garg", "hidden": false}, {"_id": "694ca2e9746a34b55dd5427a", "name": "Ritika Borkar", "hidden": false}, {"_id": "694ca2e9746a34b55dd5427b", "name": "Ritu Gala", "hidden": false}, {"_id": "694ca2e9746a34b55dd5427c", "name": "Riyad Islam", "hidden": false}, {"_id": "694ca2e9746a34b55dd5427d", "name": "Roger Waleffe", "hidden": false}, {"_id": "694ca2e9746a34b55dd5427e", "name": "Rohit Watve", "hidden": false}, {"_id": "694ca2e9746a34b55dd5427f", "name": "Roi Koren", "hidden": false}, {"_id": "694ca2e9746a34b55dd54280", "name": "Ruoxi Zhang", "hidden": false}, {"_id": "694ca2e9746a34b55dd54281", "name": "Russell J. Hewett", "hidden": false}, {"_id": "694ca2e9746a34b55dd54282", "name": "Ryan Prenger", "hidden": false}, {"_id": "694ca2e9746a34b55dd54283", "name": "Ryan Timbrook", "hidden": false}, {"_id": "694ca2e9746a34b55dd54284", "name": "Sadegh Mahdavi", "hidden": false}, {"_id": "694ca2e9746a34b55dd54285", "name": "Sahil Modi", "hidden": false}, {"_id": "694ca2e9746a34b55dd54286", "name": "Samuel Kriman", "hidden": false}, {"_id": "694ca2e9746a34b55dd54287", "name": "Sanjay Kariyappa", "hidden": false}, {"_id": "694ca2e9746a34b55dd54288", "name": "Sanjeev Satheesh", "hidden": false}, {"_id": "694ca2e9746a34b55dd54289", "name": "Saori Kaji", "hidden": false}, {"_id": "694ca2e9746a34b55dd5428a", "name": "Satish Pasumarthi", "hidden": false}, {"_id": "694ca2e9746a34b55dd5428b", "name": "Sean Narentharen", "hidden": false}, {"_id": "694ca2e9746a34b55dd5428c", "name": "Sean Narenthiran", "hidden": false}, {"_id": "694ca2e9746a34b55dd5428d", "name": "Seonmyeong Bak", "hidden": false}, {"_id": "694ca2e9746a34b55dd5428e", "name": "Sergey Kashirsky", "hidden": false}, {"_id": "694ca2e9746a34b55dd5428f", "name": "Seth Poulos", "hidden": false}, {"_id": "694ca2e9746a34b55dd54290", "name": "Shahar Mor", "hidden": false}, {"_id": "694ca2e9746a34b55dd54291", "name": "Shanmugam Ramasamy", "hidden": false}, {"_id": "694ca2e9746a34b55dd54292", "name": "Shantanu Acharya", "hidden": false}, {"_id": "694ca2e9746a34b55dd54293", "name": "Shaona Ghosh", "hidden": false}, {"_id": "694ca2e9746a34b55dd54294", "name": "Sharath Turuvekere Sreenivas", "hidden": false}, {"_id": "694ca2e9746a34b55dd54295", "name": "Shelby Thomas", "hidden": false}, {"_id": "694ca2e9746a34b55dd54296", "name": "Shiqing Fan", "hidden": false}, {"_id": "694ca2e9746a34b55dd54297", "name": "Shreya Gopal", "hidden": false}, {"_id": "694ca2e9746a34b55dd54298", "name": "Shrimai Prabhumoye", "hidden": false}, {"_id": "694ca2e9746a34b55dd54299", "name": "Shubham Pachori", "hidden": false}, {"_id": "694ca2e9746a34b55dd5429a", "name": "Shubham Toshniwal", "hidden": false}, {"_id": "694ca2e9746a34b55dd5429b", "name": "Shuoyang Ding", "hidden": false}, {"_id": "694ca2e9746a34b55dd5429c", "name": "Siddharth Singh", "hidden": false}, {"_id": "694ca2e9746a34b55dd5429d", "name": "Simeng Sun", "hidden": false}, {"_id": "694ca2e9746a34b55dd5429e", "name": "Smita Ithape", "hidden": false}, {"_id": "694ca2e9746a34b55dd5429f", "name": "Somshubra Majumdar", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a0", "name": "Soumye Singhal", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a1", "name": "Stefania Alborghetti", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a2", "name": "Stephen Ge", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a3", "name": "Sugam Dipak Devare", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a4", "name": "Sumeet Kumar Barua", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a5", "name": "Suseella Panguluri", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a6", "name": "Suyog Gupta", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a7", "name": "Sweta Priyadarshi", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a8", "name": "Syeda Nahida Akter", "hidden": false}, {"_id": "694ca2e9746a34b55dd542a9", "name": "Tan Bui", "hidden": false}, {"_id": "694ca2e9746a34b55dd542aa", "name": "Teodor-Dumitru Ene", "hidden": false}, {"_id": "694ca2e9746a34b55dd542ab", "name": "Terry Kong", "hidden": false}, {"_id": "694ca2e9746a34b55dd542ac", "name": "Thanh Do", "hidden": false}, {"_id": "694ca2e9746a34b55dd542ad", "name": "Tijmen Blankevoort", "hidden": false}, {"_id": "694ca2e9746a34b55dd542ae", "name": "Tom Balough", "hidden": false}, {"_id": "694ca2e9746a34b55dd542af", "name": "Tomer Asida", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b0", "name": "Tomer Bar Natan", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b1", "name": "Tugrul Konuk", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b2", "name": "Twinkle Vashishth", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b3", "name": "Udi Karpas", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b4", "name": "Ushnish De", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b5", "name": "Vahid Noorozi", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b6", "name": "Vahid Noroozi", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b7", "name": "Venkat Srinivasan", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b8", "name": "Venmugil Elango", "hidden": false}, {"_id": "694ca2e9746a34b55dd542b9", "name": "Vijay Korthikanti", "hidden": false}, {"_id": "694ca2e9746a34b55dd542ba", "name": "Vitaly Kurin", "hidden": false}, {"_id": "694ca2e9746a34b55dd542bb", "name": "Vitaly Lavrukhin", "hidden": false}, {"_id": "694ca2e9746a34b55dd542bc", "name": "Wanli Jiang", "hidden": false}, {"_id": "694ca2e9746a34b55dd542bd", "name": "Wasi Uddin Ahmad", "hidden": false}, {"_id": "694ca2e9746a34b55dd542be", "name": "Wei Du", "hidden": false}, {"_id": "694ca2e9746a34b55dd542bf", "name": "Wei Ping", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c0", "name": "Wenfei Zhou", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c1", "name": "Will Jennings", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c2", "name": "William Zhang", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c3", "name": "Wojciech Prazuch", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c4", "name": "Xiaowei Ren", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c5", "name": "Yashaswi Karnati", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c6", "name": "Yejin Choi", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c7", "name": "Yev Meyer", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c8", "name": "Yi-Fu Wu", "hidden": false}, {"_id": "694ca2e9746a34b55dd542c9", "name": "Yian Zhang", "hidden": false}, {"_id": "694ca2e9746a34b55dd542ca", "name": "Ying Lin", "hidden": false}, {"_id": "694ca2e9746a34b55dd542cb", "name": "Yonatan Geifman", "hidden": false}, {"_id": "694ca2e9746a34b55dd542cc", "name": "Yonggan Fu", "hidden": false}, {"_id": "694ca2e9746a34b55dd542cd", "name": "Yoshi Subara", "hidden": false}, {"_id": "694ca2e9746a34b55dd542ce", "name": "Yoshi Suhara", "hidden": false}, {"_id": "694ca2e9746a34b55dd542cf", "name": "Yubo Gao", "hidden": false}, {"_id": "694ca2e9746a34b55dd542d0", "name": "Zach Moshe", "hidden": false}, {"_id": "694ca2e9746a34b55dd542d1", "name": "Zhen Dong", "hidden": false}, {"_id": "694ca2e9746a34b55dd542d2", "name": "Zihan Liu", "hidden": false}, {"_id": "694ca2e9746a34b55dd542d3", "name": "Zijia Chen", "hidden": false}, {"_id": "694ca2e9746a34b55dd542d4", "name": "Zijie Yan", "hidden": false}], "publishedAt": "2025-12-23T23:54:32.000Z", "submittedOnDailyAt": "2025-12-25T00:05:41.130Z", "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.", "upvotes": 14, "discussionId": "694ca2e9746a34b55dd542d5", "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>Nemotron 3 Nano 30B-A3B \u662f\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u7ecf\u8fc7\u5927\u91cf\u6587\u672c\u8bad\u7ec3\u3002</li>\n    <li>\u5b83\u5728 25 \u4e07\u4ebf\u4e2a\u6587\u672c\u6807\u8bb0\u4e0a\u9884\u8bad\u7ec3\uff0c\u6bd4\u524d\u4e00\u4ee3\u589e\u52a0\u4e86\u8d85\u8fc7 3 \u4e07\u4ebf\u4e2a\u65b0\u7684\u72ec\u7279\u6807\u8bb0\u3002</li>\n    <li>\u4e0e Nemotron 2 Nano \u76f8\u6bd4\uff0c\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u4f46\u6bcf\u6b21\u63a8\u7406\u65f6\u6fc0\u6d3b\u7684\u53c2\u6570\u4e0d\u5230\u4e00\u534a\u3002</li>\n    <li>\u63a8\u7406\u901f\u5ea6\u6bd4\u540c\u7c7b\u6a21\u578b\u5feb 3.3 \u500d\uff0c\u540c\u65f6\u5728\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\u3002</li>\n    <li>\u652f\u6301\u6700\u957f 1M \u6807\u8bb0\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728 Hugging Face \u4e0a\u53d1\u5e03\u4e86\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u7684\u7248\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Nemotron 3 Nano 30B-A3B is a new language model that uses a hybrid Mamba-Transformer design.</li>\n    <li>It was trained on a massive dataset of 25 trillion text tokens, including over 3 trillion new unique tokens.</li>\n    <li>This model is more accurate than its predecessor, Nemotron 2 Nano, while using less than half the parameters for each prediction.</li>\n    <li>Nemotron 3 Nano is up to 3.3 times faster than similar models and performs better on popular tests.</li>\n    <li>It can handle very long texts of up to 1 million tokens and is available for download on Hugging Face.</li>\n</ul>"}, "publishedAt": "2025-12-23T18:54:32.000Z", "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning", "summary": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20848.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 192}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20856", "authors": [{"_id": "694ca2d2746a34b55dd54032", "name": "NVIDIA", "hidden": false}, {"_id": "694ca2d2746a34b55dd54034", "name": "Aaron Blakeman", "hidden": false}, {"_id": "694ca2d2746a34b55dd54035", "name": "Aaron Grattafiori", "hidden": false}, {"_id": "694ca2d2746a34b55dd54036", "name": "Aarti Basant", "hidden": false}, {"_id": "694ca2d2746a34b55dd54037", "name": "Abhibha Gupta", "hidden": false}, {"_id": "694ca2d2746a34b55dd54038", "name": "Abhinav Khattar", "hidden": false}, {"_id": "694ca2d2746a34b55dd54039", "name": "Adi Renduchintala", "hidden": false}, {"_id": "694ca2d2746a34b55dd5403a", "name": "Aditya Vavre", "hidden": false}, {"_id": "694ca2d2746a34b55dd5403b", "name": "Akanksha Shukla", "hidden": false}, {"_id": "694ca2d2746a34b55dd5403c", "name": "Akhiad Bercovich", "hidden": false}, {"_id": "694ca2d2746a34b55dd5403d", "name": "Aleksander Ficek", "hidden": false}, {"_id": "694ca2d2746a34b55dd5403e", "name": "Aleksandr Shaposhnikov", "hidden": false}, {"_id": "694ca2d2746a34b55dd5403f", "name": "Alex Kondratenko", "hidden": false}, {"_id": "694ca2d2746a34b55dd54040", "name": "Alexander Bukharin", "hidden": false}, {"_id": "694ca2d2746a34b55dd54041", "name": "Alexandre Milesi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54042", "name": "Ali Taghibakhshi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54043", "name": "Alisa Liu", "hidden": false}, {"_id": "694ca2d2746a34b55dd54044", "name": "Amelia Barton", "hidden": false}, {"_id": "694ca2d2746a34b55dd54045", "name": "Ameya Sunil Mahabaleshwarkar", "hidden": false}, {"_id": "694ca2d2746a34b55dd54046", "name": "Amir Klein", "hidden": false}, {"_id": "694ca2d2746a34b55dd54047", "name": "Amit Zuker", "hidden": false}, {"_id": "694ca2d2746a34b55dd54048", "name": "Amnon Geifman", "hidden": false}, {"_id": "694ca2d2746a34b55dd54049", "name": "Amy Shen", "hidden": false}, {"_id": "694ca2d2746a34b55dd5404a", "name": "Anahita Bhiwandiwalla", "hidden": false}, {"_id": "694ca2d2746a34b55dd5404b", "name": "Andrew Tao", "hidden": false}, {"_id": "694ca2d2746a34b55dd5404c", "name": "Anjulie Agrusa", "hidden": false}, {"_id": "694ca2d2746a34b55dd5404d", "name": "Ankur Verma", "hidden": false}, {"_id": "694ca2d2746a34b55dd5404e", "name": "Ann Guan", "hidden": false}, {"_id": "694ca2d2746a34b55dd5404f", "name": "Anubhav Mandarwal", "hidden": false}, {"_id": "694ca2d2746a34b55dd54050", "name": "Arham Mehta", "hidden": false}, {"_id": "694ca2d2746a34b55dd54051", "name": "Ashwath Aithal", "hidden": false}, {"_id": "694ca2d2746a34b55dd54052", "name": "Ashwin Poojary", "hidden": false}, {"_id": "694ca2d2746a34b55dd54053", "name": "Asif Ahamed", "hidden": false}, {"_id": "694ca2d2746a34b55dd54054", "name": "Asit Mishra", "hidden": false}, {"_id": "694ca2d2746a34b55dd54055", "name": "Asma Kuriparambil Thekkumpate", "hidden": false}, {"_id": "694ca2d2746a34b55dd54056", "name": "Ayush Dattagupta", "hidden": false}, {"_id": "694ca2d2746a34b55dd54057", "name": "Banghua Zhu", "hidden": false}, {"_id": "694ca2d2746a34b55dd54058", "name": "Bardiya Sadeghi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54059", "name": "Barnaby Simkin", "hidden": false}, {"_id": "694ca2d2746a34b55dd5405a", "name": "Ben Lanir", "hidden": false}, {"_id": "694ca2d2746a34b55dd5405b", "name": "Benedikt Schifferer", "hidden": false}, {"_id": "694ca2d2746a34b55dd5405c", "name": "Besmira Nushi", "hidden": false}, {"_id": "694ca2d2746a34b55dd5405d", "name": "Bilal Kartal", "hidden": false}, {"_id": "694ca2d2746a34b55dd5405e", "name": "Bita Darvish Rouhani", "hidden": false}, {"_id": "694ca2d2746a34b55dd5405f", "name": "Boris Ginsburg", "hidden": false}, {"_id": "694ca2d2746a34b55dd54060", "name": "Brandon Norick", "hidden": false}, {"_id": "694ca2d2746a34b55dd54061", "name": "Brandon Soubasis", "hidden": false}, {"_id": "694ca2d2746a34b55dd54062", "name": "Branislav Kisacanin", "hidden": false}, {"_id": "694ca2d2746a34b55dd54063", "name": "Brian Yu", "hidden": false}, {"_id": "694ca2d2746a34b55dd54064", "name": "Bryan Catanzaro", "hidden": false}, {"_id": "694ca2d2746a34b55dd54065", "name": "Carlo del Mundo", "hidden": false}, {"_id": "694ca2d2746a34b55dd54066", "name": "Chantal Hwang", "hidden": false}, {"_id": "694ca2d2746a34b55dd54067", "name": "Charles Wang", "hidden": false}, {"_id": "694ca2d2746a34b55dd54068", "name": "Cheng-Ping Hsieh", "hidden": false}, {"_id": "694ca2d2746a34b55dd54069", "name": "Chenghao Zhang", "hidden": false}, {"_id": "694ca2d2746a34b55dd5406a", "name": "Chenhan Yu", "hidden": false}, {"_id": "694ca2d2746a34b55dd5406b", "name": "Chetan Mungekar", "hidden": false}, {"_id": "694ca2d2746a34b55dd5406c", "name": "Chintan Patel", "hidden": false}, {"_id": "694ca2d2746a34b55dd5406d", "name": "Chris Alexiuk", "hidden": false}, {"_id": "694ca2d2746a34b55dd5406e", "name": "Christopher Parisien", "hidden": false}, {"_id": "694ca2d2746a34b55dd5406f", "name": "Collin Neale", "hidden": false}, {"_id": "694ca2d2746a34b55dd54070", "name": "Cyril Meurillon", "hidden": false}, {"_id": "694ca2d2746a34b55dd54071", "name": "Damon Mosk-Aoyama", "hidden": false}, {"_id": "694ca2d2746a34b55dd54072", "name": "Dan Su", "hidden": false}, {"_id": "694ca2d2746a34b55dd54073", "name": "Dane Corneil", "hidden": false}, {"_id": "694ca2d2746a34b55dd54074", "name": "Daniel Afrimi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54075", "name": "Daniel Lo", "hidden": false}, {"_id": "694ca2d2746a34b55dd54076", "name": "Daniel Rohrer", "hidden": false}, {"_id": "694ca2d2746a34b55dd54077", "name": "Daniel Serebrenik", "hidden": false}, {"_id": "694ca2d2746a34b55dd54078", "name": "Daria Gitman", "hidden": false}, {"_id": "694ca2d2746a34b55dd54079", "name": "Daria Levy", "hidden": false}, {"_id": "694ca2d2746a34b55dd5407a", "name": "Darko Stosic", "hidden": false}, {"_id": "694ca2d2746a34b55dd5407b", "name": "David Mosallanezhad", "hidden": false}, {"_id": "694ca2d2746a34b55dd5407c", "name": "Deepak Narayanan", "hidden": false}, {"_id": "694ca2d2746a34b55dd5407d", "name": "Dhruv Nathawani", "hidden": false}, {"_id": "694ca2d2746a34b55dd5407e", "name": "Dima Rekesh", "hidden": false}, {"_id": "694ca2d2746a34b55dd5407f", "name": "Dina Yared", "hidden": false}, {"_id": "694ca2d2746a34b55dd54080", "name": "Divyanshu Kakwani", "hidden": false}, {"_id": "694ca2d2746a34b55dd54081", "name": "Dong Ahn", "hidden": false}, {"_id": "694ca2d2746a34b55dd54082", "name": "Duncan Riach", "hidden": false}, {"_id": "694ca2d2746a34b55dd54083", "name": "Dusan Stosic", "hidden": false}, {"_id": "694ca2d2746a34b55dd54084", "name": "Edgar Minasyan", "hidden": false}, {"_id": "694ca2d2746a34b55dd54085", "name": "Edward Lin", "hidden": false}, {"_id": "694ca2d2746a34b55dd54086", "name": "Eileen Long", "hidden": false}, {"_id": "694ca2d2746a34b55dd54087", "name": "Eileen Peters Long", "hidden": false}, {"_id": "694ca2d2746a34b55dd54088", "name": "Elad Segal", "hidden": false}, {"_id": "694ca2d2746a34b55dd54089", "name": "Elena Lantz", "hidden": false}, {"_id": "694ca2d2746a34b55dd5408a", "name": "Ellie Evans", "hidden": false}, {"_id": "694ca2d2746a34b55dd5408b", "name": "Elliott Ning", "hidden": false}, {"_id": "694ca2d2746a34b55dd5408c", "name": "Eric Chung", "hidden": false}, {"_id": "694ca2d2746a34b55dd5408d", "name": "Eric Harper", "hidden": false}, {"_id": "694ca2d2746a34b55dd5408e", "name": "Eric Tramel", "hidden": false}, {"_id": "694ca2d2746a34b55dd5408f", "name": "Erick Galinkin", "hidden": false}, {"_id": "694ca2d2746a34b55dd54090", "name": "Erik Pounds", "hidden": false}, {"_id": "694ca2d2746a34b55dd54091", "name": "Evan Briones", "hidden": false}, {"_id": "694ca2d2746a34b55dd54092", "name": "Evelina Bakhturina", "hidden": false}, {"_id": "694ca2d2746a34b55dd54093", "name": "Evgeny Tsykunov", "hidden": false}, {"_id": "694ca2d2746a34b55dd54094", "name": "Faisal Ladhak", "hidden": false}, {"_id": "694ca2d2746a34b55dd54095", "name": "Fay Wang", "hidden": false}, {"_id": "694ca2d2746a34b55dd54096", "name": "Fei Jia", "hidden": false}, {"_id": "694ca2d2746a34b55dd54097", "name": "Felipe Soares", "hidden": false}, {"_id": "694ca2d2746a34b55dd54098", "name": "Feng Chen", "hidden": false}, {"_id": "694ca2d2746a34b55dd54099", "name": "Ferenc Galko", "hidden": false}, {"_id": "694ca2d2746a34b55dd5409a", "name": "Frank Sun", "hidden": false}, {"_id": "694ca2d2746a34b55dd5409b", "name": "Frankie Siino", "hidden": false}, {"_id": "694ca2d2746a34b55dd5409c", "name": "Gal Hubara Agam", "hidden": false}, {"_id": "694ca2d2746a34b55dd5409d", "name": "Ganesh Ajjanagadde", "hidden": false}, {"_id": "694ca2d2746a34b55dd5409e", "name": "Gantavya Bhatt", "hidden": false}, {"_id": "694ca2d2746a34b55dd5409f", "name": "Gargi Prasad", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a0", "name": "George Armstrong", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a1", "name": "Gerald Shen", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a2", "name": "Gorkem Batmaz", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a3", "name": "Grigor Nalbandyan", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a4", "name": "Haifeng Qian", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a5", "name": "Harsh Sharma", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a6", "name": "Hayley Ross", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a7", "name": "Helen Ngo", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a8", "name": "Herbert Hum", "hidden": false}, {"_id": "694ca2d2746a34b55dd540a9", "name": "Herman Sahota", "hidden": false}, {"_id": "694ca2d2746a34b55dd540aa", "name": "Hexin Wang", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ab", "name": "Himanshu Soni", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ac", "name": "Hiren Upadhyay", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ad", "name": "Huizi Mao", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ae", "name": "Huy C Nguyen", "hidden": false}, {"_id": "694ca2d2746a34b55dd540af", "name": "Huy Q Nguyen", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b0", "name": "Iain Cunningham", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b1", "name": "Ido Galil", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b2", "name": "Ido Shahaf", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b3", "name": "Igor Gitman", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b4", "name": "Ilya Loshchilov", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b5", "name": "Itamar Schen", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b6", "name": "Itay Levy", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b7", "name": "Ivan Moshkov", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b8", "name": "Izik Golan", "hidden": false}, {"_id": "694ca2d2746a34b55dd540b9", "name": "Izzy Putterman", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ba", "name": "Jan Kautz", "hidden": false}, {"_id": "694ca2d2746a34b55dd540bb", "name": "Jane Polak Scowcroft", "hidden": false}, {"_id": "694ca2d2746a34b55dd540bc", "name": "Jared Casper", "hidden": false}, {"_id": "694ca2d2746a34b55dd540bd", "name": "Jatin Mitra", "hidden": false}, {"_id": "694ca2d2746a34b55dd540be", "name": "Jeffrey Glick", "hidden": false}, {"_id": "694ca2d2746a34b55dd540bf", "name": "Jenny Chen", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c0", "name": "Jesse Oliver", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c1", "name": "Jian Zhang", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c2", "name": "Jiaqi Zeng", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c3", "name": "Jie Lou", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c4", "name": "Jimmy Zhang", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c5", "name": "Jinhang Choi", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c6", "name": "Jining Huang", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c7", "name": "Joey Conway", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c8", "name": "Joey Guman", "hidden": false}, {"_id": "694ca2d2746a34b55dd540c9", "name": "John Kamalu", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ca", "name": "Johnny Greco", "hidden": false}, {"_id": "694ca2d2746a34b55dd540cb", "name": "Jonathan Cohen", "hidden": false}, {"_id": "694ca2d2746a34b55dd540cc", "name": "Joseph Jennings", "hidden": false}, {"_id": "694ca2d2746a34b55dd540cd", "name": "Joyjit Daw", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ce", "name": "Julien Veron Vialard", "hidden": false}, {"_id": "694ca2d2746a34b55dd540cf", "name": "Junkeun Yi", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d0", "name": "Jupinder Parmar", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d1", "name": "Kai Xu", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d2", "name": "Kan Zhu", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d3", "name": "Kari Briski", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d4", "name": "Katherine Cheung", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d5", "name": "Katherine Luna", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d6", "name": "Keith Wyss", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d7", "name": "Keshav Santhanam", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d8", "name": "Kevin Shih", "hidden": false}, {"_id": "694ca2d2746a34b55dd540d9", "name": "Kezhi Kong", "hidden": false}, {"_id": "694ca2d2746a34b55dd540da", "name": "Khushi Bhardwaj", "hidden": false}, {"_id": "694ca2d2746a34b55dd540db", "name": "Kirthi Shankar", "hidden": false}, {"_id": "694ca2d2746a34b55dd540dc", "name": "Krishna C. Puvvada", "hidden": false}, {"_id": "694ca2d2746a34b55dd540dd", "name": "Krzysztof Pawelec", "hidden": false}, {"_id": "694ca2d2746a34b55dd540de", "name": "Kumar Anik", "hidden": false}, {"_id": "694ca2d2746a34b55dd540df", "name": "Lawrence McAfee", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e0", "name": "Laya Sleiman", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e1", "name": "Leon Derczynski", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e2", "name": "Li Ding", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e3", "name": "Lizzie Wei", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e4", "name": "Lucas Liebenwein", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e5", "name": "Luis Vega", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e6", "name": "Maanu Grover", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e7", "name": "Maarten Van Segbroeck", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e8", "name": "Maer Rodrigues de Melo", "hidden": false}, {"_id": "694ca2d2746a34b55dd540e9", "name": "Mahdi Nazemi", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ea", "name": "Makesh Narsimhan Sreedhar", "hidden": false}, {"_id": "694ca2d2746a34b55dd540eb", "name": "Manoj Kilaru", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ec", "name": "Maor Ashkenazi", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ed", "name": "Marc Romeijn", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ee", "name": "Marcin Chochowski", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ef", "name": "Mark Cai", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f0", "name": "Markus Kliegl", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f1", "name": "Maryam Moosaei", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f2", "name": "Matt Kulka", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f3", "name": "Matvei Novikov", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f4", "name": "Mehrzad Samadi", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f5", "name": "Melissa Corpuz", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f6", "name": "Mengru Wang", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f7", "name": "Meredith Price", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f8", "name": "Michael Andersch", "hidden": false}, {"_id": "694ca2d2746a34b55dd540f9", "name": "Michael Boone", "hidden": false}, {"_id": "694ca2d2746a34b55dd540fa", "name": "Michael Evans", "hidden": false}, {"_id": "694ca2d2746a34b55dd540fb", "name": "Miguel Martinez", "hidden": false}, {"_id": "694ca2d2746a34b55dd540fc", "name": "Mikail Khona", "hidden": false}, {"_id": "694ca2d2746a34b55dd540fd", "name": "Mike Chrzanowski", "hidden": false}, {"_id": "694ca2d2746a34b55dd540fe", "name": "Minseok Lee", "hidden": false}, {"_id": "694ca2d2746a34b55dd540ff", "name": "Mohammad Dabbah", "hidden": false}, {"_id": "694ca2d2746a34b55dd54100", "name": "Mohammad Shoeybi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54101", "name": "Mostofa Patwary", "hidden": false}, {"_id": "694ca2d2746a34b55dd54102", "name": "Nabin Mulepati", "hidden": false}, {"_id": "694ca2d2746a34b55dd54103", "name": "Najeeb Nabwani", "hidden": false}, {"_id": "694ca2d2746a34b55dd54104", "name": "Natalie Hereth", "hidden": false}, {"_id": "694ca2d2746a34b55dd54105", "name": "Nave Assaf", "hidden": false}, {"_id": "694ca2d2746a34b55dd54106", "name": "Negar Habibi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54107", "name": "Neta Zmora", "hidden": false}, {"_id": "694ca2d2746a34b55dd54108", "name": "Netanel Haber", "hidden": false}, {"_id": "694ca2d2746a34b55dd54109", "name": "Nicola Sessions", "hidden": false}, {"_id": "694ca2d2746a34b55dd5410a", "name": "Nidhi Bhatia", "hidden": false}, {"_id": "694ca2d2746a34b55dd5410b", "name": "Nikhil Jukar", "hidden": false}, {"_id": "694ca2d2746a34b55dd5410c", "name": "Nikki Pope", "hidden": false}, {"_id": "694ca2d2746a34b55dd5410d", "name": "Nikolai Ludwig", "hidden": false}, {"_id": "694ca2d2746a34b55dd5410e", "name": "Nima Tajbakhsh", "hidden": false}, {"_id": "694ca2d2746a34b55dd5410f", "name": "Nir Ailon", "hidden": false}, {"_id": "694ca2d2746a34b55dd54110", "name": "Nirmal Juluru", "hidden": false}, {"_id": "694ca2d2746a34b55dd54111", "name": "Nishant Sharma", "hidden": false}, {"_id": "694ca2d2746a34b55dd54112", "name": "Oleksii Hrinchuk", "hidden": false}, {"_id": "694ca2d2746a34b55dd54113", "name": "Oleksii Kuchaiev", "hidden": false}, {"_id": "694ca2d2746a34b55dd54114", "name": "Olivier Delalleau", "hidden": false}, {"_id": "694ca2d2746a34b55dd54115", "name": "Oluwatobi Olabiyi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54116", "name": "Omer Ullman Argov", "hidden": false}, {"_id": "694ca2d2746a34b55dd54117", "name": "Omri Puny", "hidden": false}, {"_id": "694ca2d2746a34b55dd54118", "name": "Oren Tropp", "hidden": false}, {"_id": "694ca2d2746a34b55dd54119", "name": "Ouye Xie", "hidden": false}, {"_id": "694ca2d2746a34b55dd5411a", "name": "Parth Chadha", "hidden": false}, {"_id": "694ca2d2746a34b55dd5411b", "name": "Pasha Shamis", "hidden": false}, {"_id": "694ca2d2746a34b55dd5411c", "name": "Paul Gibbons", "hidden": false}, {"_id": "694ca2d2746a34b55dd5411d", "name": "Pavlo Molchanov", "hidden": false}, {"_id": "694ca2d2746a34b55dd5411e", "name": "Pawel Morkisz", "hidden": false}, {"_id": "694ca2d2746a34b55dd5411f", "name": "Peter Dykas", "hidden": false}, {"_id": "694ca2d2746a34b55dd54120", "name": "Peter Jin", "hidden": false}, {"_id": "694ca2d2746a34b55dd54121", "name": "Pinky Xu", "hidden": false}, {"_id": "694ca2d2746a34b55dd54122", "name": "Piotr Januszewski", "hidden": false}, {"_id": "694ca2d2746a34b55dd54123", "name": "Pranav Prashant Thombre", "hidden": false}, {"_id": "694ca2d2746a34b55dd54124", "name": "Prasoon Varshney", "hidden": false}, {"_id": "694ca2d2746a34b55dd54125", "name": "Pritam Gundecha", "hidden": false}, {"_id": "694ca2d2746a34b55dd54126", "name": "Przemek Tredak", "hidden": false}, {"_id": "694ca2d2746a34b55dd54127", "name": "Qing Miao", "hidden": false}, {"_id": "694ca2d2746a34b55dd54128", "name": "Qiyu Wan", "hidden": false}, {"_id": "694ca2d2746a34b55dd54129", "name": "Rabeeh Karimi Mahabadi", "hidden": false}, {"_id": "694ca2d2746a34b55dd5412a", "name": "Rachit Garg", "hidden": false}, {"_id": "694ca2d2746a34b55dd5412b", "name": "Ran El-Yaniv", "hidden": false}, {"_id": "694ca2d2746a34b55dd5412c", "name": "Ran Zilberstein", "hidden": false}, {"_id": "694ca2d2746a34b55dd5412d", "name": "Rasoul Shafipour", "hidden": false}, {"_id": "694ca2d2746a34b55dd5412e", "name": "Rich Harang", "hidden": false}, {"_id": "694ca2d2746a34b55dd5412f", "name": "Rick Izzo", "hidden": false}, {"_id": "694ca2d2746a34b55dd54130", "name": "Rima Shahbazyan", "hidden": false}, {"_id": "694ca2d2746a34b55dd54131", "name": "Rishabh Garg", "hidden": false}, {"_id": "694ca2d2746a34b55dd54132", "name": "Ritika Borkar", "hidden": false}, {"_id": "694ca2d2746a34b55dd54133", "name": "Ritu Gala", "hidden": false}, {"_id": "694ca2d2746a34b55dd54134", "name": "Riyad Islam", "hidden": false}, {"_id": "694ca2d2746a34b55dd54135", "name": "Robert Hesse", "hidden": false}, {"_id": "694ca2d2746a34b55dd54136", "name": "Roger Waleffe", "hidden": false}, {"_id": "694ca2d2746a34b55dd54137", "name": "Rohit Watve", "hidden": false}, {"_id": "694ca2d2746a34b55dd54138", "name": "Roi Koren", "hidden": false}, {"_id": "694ca2d2746a34b55dd54139", "name": "Ruoxi Zhang", "hidden": false}, {"_id": "694ca2d2746a34b55dd5413a", "name": "Russell Hewett", "hidden": false}, {"_id": "694ca2d2746a34b55dd5413b", "name": "Russell J. Hewett", "hidden": false}, {"_id": "694ca2d2746a34b55dd5413c", "name": "Ryan Prenger", "hidden": false}, {"_id": "694ca2d2746a34b55dd5413d", "name": "Ryan Timbrook", "hidden": false}, {"_id": "694ca2d2746a34b55dd5413e", "name": "Sadegh Mahdavi", "hidden": false}, {"_id": "694ca2d2746a34b55dd5413f", "name": "Sahil Modi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54140", "name": "Samuel Kriman", "hidden": false}, {"_id": "694ca2d2746a34b55dd54141", "name": "Sangkug Lim", "hidden": false}, {"_id": "694ca2d2746a34b55dd54142", "name": "Sanjay Kariyappa", "hidden": false}, {"_id": "694ca2d2746a34b55dd54143", "name": "Sanjeev Satheesh", "hidden": false}, {"_id": "694ca2d2746a34b55dd54144", "name": "Saori Kaji", "hidden": false}, {"_id": "694ca2d2746a34b55dd54145", "name": "Satish Pasumarthi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54146", "name": "Saurav Muralidharan", "hidden": false}, {"_id": "694ca2d2746a34b55dd54147", "name": "Sean Narentharen", "hidden": false}, {"_id": "694ca2d2746a34b55dd54148", "name": "Sean Narenthiran", "hidden": false}, {"_id": "694ca2d2746a34b55dd54149", "name": "Seonmyeong Bak", "hidden": false}, {"_id": "694ca2d2746a34b55dd5414a", "name": "Sergey Kashirsky", "hidden": false}, {"_id": "694ca2d2746a34b55dd5414b", "name": "Seth Poulos", "hidden": false}, {"_id": "694ca2d2746a34b55dd5414c", "name": "Shahar Mor", "hidden": false}, {"_id": "694ca2d2746a34b55dd5414d", "name": "Shanmugam Ramasamy", "hidden": false}, {"_id": "694ca2d2746a34b55dd5414e", "name": "Shantanu Acharya", "hidden": false}, {"_id": "694ca2d2746a34b55dd5414f", "name": "Shaona Ghosh", "hidden": false}, {"_id": "694ca2d2746a34b55dd54150", "name": "Sharath Turuvekere Sreenivas", "hidden": false}, {"_id": "694ca2d2746a34b55dd54151", "name": "Shelby Thomas", "hidden": false}, {"_id": "694ca2d2746a34b55dd54152", "name": "Shiqing Fan", "hidden": false}, {"_id": "694ca2d2746a34b55dd54153", "name": "Shreya Gopal", "hidden": false}, {"_id": "694ca2d2746a34b55dd54154", "name": "Shrimai Prabhumoye", "hidden": false}, {"_id": "694ca2d2746a34b55dd54155", "name": "Shubham Pachori", "hidden": false}, {"_id": "694ca2d2746a34b55dd54156", "name": "Shubham Toshniwal", "hidden": false}, {"_id": "694ca2d2746a34b55dd54157", "name": "Shuoyang Ding", "hidden": false}, {"_id": "694ca2d2746a34b55dd54158", "name": "Siddharth Singh", "hidden": false}, {"_id": "694ca2d2746a34b55dd54159", "name": "Simeng Sun", "hidden": false}, {"_id": "694ca2d2746a34b55dd5415a", "name": "Smita Ithape", "hidden": false}, {"_id": "694ca2d2746a34b55dd5415b", "name": "Somshubra Majumdar", "hidden": false}, {"_id": "694ca2d2746a34b55dd5415c", "name": "Soumye Singhal", "hidden": false}, {"_id": "694ca2d2746a34b55dd5415d", "name": "Stas Sergienko", "hidden": false}, {"_id": "694ca2d2746a34b55dd5415e", "name": "Stefania Alborghetti", "hidden": false}, {"_id": "694ca2d2746a34b55dd5415f", "name": "Stephen Ge", "hidden": false}, {"_id": "694ca2d2746a34b55dd54160", "name": "Sugam Dipak Devare", "hidden": false}, {"_id": "694ca2d2746a34b55dd54161", "name": "Sumeet Kumar Barua", "hidden": false}, {"_id": "694ca2d2746a34b55dd54162", "name": "Suseella Panguluri", "hidden": false}, {"_id": "694ca2d2746a34b55dd54163", "name": "Suyog Gupta", "hidden": false}, {"_id": "694ca2d2746a34b55dd54164", "name": "Sweta Priyadarshi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54165", "name": "Syeda Nahida Akter", "hidden": false}, {"_id": "694ca2d2746a34b55dd54166", "name": "Tan Bui", "hidden": false}, {"_id": "694ca2d2746a34b55dd54167", "name": "Teodor-Dumitru Ene", "hidden": false}, {"_id": "694ca2d2746a34b55dd54168", "name": "Terry Kong", "hidden": false}, {"_id": "694ca2d2746a34b55dd54169", "name": "Thanh Do", "hidden": false}, {"_id": "694ca2d2746a34b55dd5416a", "name": "Tijmen Blankevoort", "hidden": false}, {"_id": "694ca2d2746a34b55dd5416b", "name": "Tim Moon", "hidden": false}, {"_id": "694ca2d2746a34b55dd5416c", "name": "Tom Balough", "hidden": false}, {"_id": "694ca2d2746a34b55dd5416d", "name": "Tomer Asida", "hidden": false}, {"_id": "694ca2d2746a34b55dd5416e", "name": "Tomer Bar Natan", "hidden": false}, {"_id": "694ca2d2746a34b55dd5416f", "name": "Tomer Ronen", "hidden": false}, {"_id": "694ca2d2746a34b55dd54170", "name": "Tugrul Konuk", "hidden": false}, {"_id": "694ca2d2746a34b55dd54171", "name": "Twinkle Vashishth", "hidden": false}, {"_id": "694ca2d2746a34b55dd54172", "name": "Udi Karpas", "hidden": false}, {"_id": "694ca2d2746a34b55dd54173", "name": "Ushnish De", "hidden": false}, {"_id": "694ca2d2746a34b55dd54174", "name": "Vahid Noorozi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54175", "name": "Vahid Noroozi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54176", "name": "Venkat Srinivasan", "hidden": false}, {"_id": "694ca2d2746a34b55dd54177", "name": "Venmugil Elango", "hidden": false}, {"_id": "694ca2d2746a34b55dd54178", "name": "Victor Cui", "hidden": false}, {"_id": "694ca2d2746a34b55dd54179", "name": "Vijay Korthikanti", "hidden": false}, {"_id": "694ca2d2746a34b55dd5417a", "name": "Vinay Rao", "hidden": false}, {"_id": "694ca2d2746a34b55dd5417b", "name": "Vitaly Kurin", "hidden": false}, {"_id": "694ca2d2746a34b55dd5417c", "name": "Vitaly Lavrukhin", "hidden": false}, {"_id": "694ca2d2746a34b55dd5417d", "name": "Vladimir Anisimov", "hidden": false}, {"_id": "694ca2d2746a34b55dd5417e", "name": "Wanli Jiang", "hidden": false}, {"_id": "694ca2d2746a34b55dd5417f", "name": "Wasi Uddin Ahmad", "hidden": false}, {"_id": "694ca2d2746a34b55dd54180", "name": "Wei Du", "hidden": false}, {"_id": "694ca2d2746a34b55dd54181", "name": "Wei Ping", "hidden": false}, {"_id": "694ca2d2746a34b55dd54182", "name": "Wenfei Zhou", "hidden": false}, {"_id": "694ca2d2746a34b55dd54183", "name": "Will Jennings", "hidden": false}, {"_id": "694ca2d2746a34b55dd54184", "name": "William Zhang", "hidden": false}, {"_id": "694ca2d2746a34b55dd54185", "name": "Wojciech Prazuch", "hidden": false}, {"_id": "694ca2d2746a34b55dd54186", "name": "Xiaowei Ren", "hidden": false}, {"_id": "694ca2d2746a34b55dd54187", "name": "Yashaswi Karnati", "hidden": false}, {"_id": "694ca2d2746a34b55dd54188", "name": "Yejin Choi", "hidden": false}, {"_id": "694ca2d2746a34b55dd54189", "name": "Yev Meyer", "hidden": false}, {"_id": "694ca2d2746a34b55dd5418a", "name": "Yi-Fu Wu", "hidden": false}, {"_id": "694ca2d2746a34b55dd5418b", "name": "Yian Zhang", "hidden": false}, {"_id": "694ca2d2746a34b55dd5418c", "name": "Yigong Qin", "hidden": false}, {"_id": "694ca2d2746a34b55dd5418d", "name": "Ying Lin", "hidden": false}, {"_id": "694ca2d2746a34b55dd5418e", "name": "Yonatan Geifman", "hidden": false}, {"_id": "694ca2d2746a34b55dd5418f", "name": "Yonggan Fu", "hidden": false}, {"_id": "694ca2d2746a34b55dd54190", "name": "Yoshi Subara", "hidden": false}, {"_id": "694ca2d2746a34b55dd54191", "name": "Yoshi Suhara", "hidden": false}, {"_id": "694ca2d2746a34b55dd54192", "name": "Yubo Gao", "hidden": false}, {"_id": "694ca2d2746a34b55dd54193", "name": "Zach Moshe", "hidden": false}, {"_id": "694ca2d2746a34b55dd54194", "name": "Zhen Dong", "hidden": false}, {"_id": "694ca2d2746a34b55dd54195", "name": "Zhongbo Zhu", "hidden": false}, {"_id": "694ca2d2746a34b55dd54196", "name": "Zihan Liu", "hidden": false}, {"_id": "694ca2d2746a34b55dd54197", "name": "Zijia Chen", "hidden": false}, {"_id": "694ca2d2746a34b55dd54198", "name": "Zijie Yan", "hidden": false}], "publishedAt": "2025-12-24T00:24:05.000Z", "submittedOnDailyAt": "2025-12-25T00:05:14.241Z", "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.", "upvotes": 12, "discussionId": "694ca2d2746a34b55dd54199", "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faNemotron 3\u7cfb\u5217\u6a21\u578b\uff0c\u5305\u62ecNano\u3001Super\u548cUltra\uff0c\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u548c\u5bf9\u8bdd\u80fd\u529b\u3002</li>\n    <li>\u91c7\u7528\u6df7\u5408\u4e13\u5bb6Mamba-Transformer\u67b6\u6784\uff0c\u652f\u6301\u9ad8\u8fbe100\u4e07\u6807\u8bb0\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002</li>\n    <li>Super\u548cUltra\u6a21\u578b\u4f7f\u7528\u65b0\u65b9\u6cd5LatentMoE\u4f18\u5316\u8d28\u91cf\uff0c\u5e76\u52a0\u5165MTP\u5c42\u52a0\u901f\u6587\u672c\u751f\u6210\u3002</li>\n    <li>\u6240\u6709\u6a21\u578b\u901a\u8fc7\u591a\u73af\u5883\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u540e\u671f\u8bad\u7ec3\uff0c\u652f\u6301\u63a8\u7406\u548c\u591a\u6b65\u9aa4\u5de5\u5177\u4f7f\u7528\u3002</li>\n    <li>Nano\u6a21\u578b\u5c0f\u5de7\u4e14\u9ad8\u6548\uff0cSuper\u9002\u5408\u9ad8\u8d1f\u8f7d\u5de5\u4f5c\uff0cUltra\u63d0\u4f9b\u9876\u7ea7\u51c6\u786e\u6027\u548c\u63a8\u7406\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The Nemotron 3 family includes three models: Nano, Super, and Ultra, with strong reasoning and conversational abilities.</li>\n    <li>These models use a special architecture called Mixture-of-Experts hybrid Mamba-Transformer for efficient processing and can handle up to 1 million tokens.</li>\n    <li>Super and Ultra models have improved quality through a method called LatentMoE and are faster due to MTP layers.</li>\n    <li>Nano is the smallest model, known for high accuracy and low cost, while Ultra offers top performance and reasoning.</li>\n    <li>Model weights and training software will be openly shared for community use, with Nano already available and Super and Ultra set to follow soon.</li>\n</ul>"}, "publishedAt": "2025-12-23T19:24:05.000Z", "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence", "summary": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20856.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 192}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20757", "authors": [{"_id": "694cd450746a34b55dd54396", "user": {"_id": "6462472fd1ccd517f46737a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/sqHi525isv9wsDXxHBbUa.png", "isPro": false, "fullname": "G\u00fcl Sena Alt\u0131nta\u015f", "user": "gsaltintas", "type": "user"}, "name": "G\u00fcl Sena Alt\u0131nta\u015f", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:20.164Z", "hidden": false}, {"_id": "694cd450746a34b55dd54397", "user": {"_id": "63cacbc502ee13c2af9d6759", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cacbc502ee13c2af9d6759/dQCSTb6IOPIQDK-DJmIgs.jpeg", "isPro": false, "fullname": "Malikeh Ehghaghi", "user": "Malikeh1375", "type": "user"}, "name": "Malikeh Ehghaghi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:26.290Z", "hidden": false}, {"_id": "694cd450746a34b55dd54398", "name": "Brian Lester", "hidden": false}, {"_id": "694cd450746a34b55dd54399", "name": "Fengyuan Liu", "hidden": false}, {"_id": "694cd450746a34b55dd5439a", "name": "Wanru Zhao", "hidden": false}, {"_id": "694cd450746a34b55dd5439b", "user": {"_id": "662fbe32246247eeadd0e1f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662fbe32246247eeadd0e1f4/h0bsU30qrzYjC9e4d6BdD.jpeg", "isPro": true, "fullname": "Marco Ciccone", "user": "mciccone", "type": "user"}, "name": "Marco Ciccone", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:17.328Z", "hidden": false}, {"_id": "694cd450746a34b55dd5439c", "user": {"_id": "6079c29765b9d0165cb18392", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1618592397610-noauth.jpeg", "isPro": false, "fullname": "Colin Raffel", "user": "craffel", "type": "user"}, "name": "Colin Raffel", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:23.045Z", "hidden": false}], "publishedAt": "2025-12-23T20:43:06.000Z", "submittedOnDailyAt": "2025-12-25T03:39:53.477Z", "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior", "submittedOnDailyBy": {"_id": "63cacbc502ee13c2af9d6759", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cacbc502ee13c2af9d6759/dQCSTb6IOPIQDK-DJmIgs.jpeg", "isPro": false, "fullname": "Malikeh Ehghaghi", "user": "Malikeh1375", "type": "user"}, "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.", "upvotes": 11, "discussionId": "694cd450746a34b55dd5439d", "githubRepo": "https://github.com/r-three/Tokenizers", "githubRepoAddedBy": "user", "githubStars": 1, "organization": {"_id": "69023f3216bd45305a54a401", "name": "toksuite", "fullname": "TokSuite", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63cacbc502ee13c2af9d6759/Yt1Q3EHGg_HiYdwRUkfvH.png"}, "summary_zh": "<ul>\n    <li>Tokenizers \u662f\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5904\u7406\u6587\u672c\u7684\u57fa\u7840\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86 TokSuite\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7814\u7a76 tokenization \u5f71\u54cd\u7684\u6a21\u578b\u548c\u57fa\u51c6\u96c6\u5408\u3002</li>\n    <li>\u8be5\u7814\u7a76\u8bad\u7ec3\u4e86\u5341\u56db\u4e2a\u4f7f\u7528\u4e0d\u540c tokenizers \u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u67b6\u6784\u3001\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u9884\u7b97\u548c\u521d\u59cb\u5316\u4e0a\u5b8c\u5168\u76f8\u540c\u3002</li>\n    <li>TokSuite \u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u4e13\u95e8\u6d4b\u91cf\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u53ef\u80fd\u5f71\u54cd tokenization \u7684\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7 TokSuite\uff0c\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u66f4\u6e05\u6670\u5730\u7406\u89e3\u4e0d\u540c tokenizers \u7684\u4f18\u7f3a\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Tokenizers are crucial for how text is processed by language models, but their impact on performance is not well understood.</li>\n    <li>TokSuite is a new tool that helps researchers study how different tokenizers affect language models.</li>\n    <li>It includes fourteen models that use different tokenizers but are otherwise the same in design and training.</li>\n    <li>TokSuite also provides a benchmark to test model performance under conditions that affect tokenization.</li>\n    <li>This tool helps clarify the strengths and weaknesses of various popular tokenizers.</li>\n</ul>"}, "publishedAt": "2025-12-23T15:43:06.000Z", "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior", "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20757.png", "numComments": 1, "submittedBy": {"_id": "63cacbc502ee13c2af9d6759", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63cacbc502ee13c2af9d6759/dQCSTb6IOPIQDK-DJmIgs.jpeg", "fullname": "Malikeh Ehghaghi", "name": "Malikeh1375", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 27}, "organization": {"_id": "69023f3216bd45305a54a401", "name": "toksuite", "fullname": "TokSuite", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63cacbc502ee13c2af9d6759/Yt1Q3EHGg_HiYdwRUkfvH.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21004", "authors": [{"_id": "694cd697746a34b55dd543a9", "name": "Jinghan Li", "hidden": false}, {"_id": "694cd697746a34b55dd543aa", "name": "Yang Jin", "hidden": false}, {"_id": "694cd697746a34b55dd543ab", "name": "Hao Jiang", "hidden": false}, {"_id": "694cd697746a34b55dd543ac", "name": "Yadong Mu", "hidden": false}, {"_id": "694cd697746a34b55dd543ad", "name": "Yang Song", "hidden": false}, {"_id": "694cd697746a34b55dd543ae", "name": "Kun Xu", "hidden": false}], "publishedAt": "2025-12-24T07:07:08.000Z", "submittedOnDailyAt": "2025-12-25T03:46:35.640Z", "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations", "submittedOnDailyBy": {"_id": "62fc758172a7ab50b4b89c8c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677251053045-62fc758172a7ab50b4b89c8c.jpeg", "isPro": false, "fullname": "Zhicheng Sun", "user": "feifeiobama", "type": "user"}, "summary": "Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.", "upvotes": 7, "discussionId": "694cd698746a34b55dd543af", "githubRepo": "https://github.com/Singularity0104/NExT-Vid", "githubRepoAddedBy": "user", "githubStars": 7, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u8fd1\u671f\uff0c\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8fdb\u5c55\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u3002</li>\n    <li>\u5927\u591a\u6570\u89c6\u89c9\u751f\u6210\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u4e8eBERT\u5f0f\u7684\u63a9\u7801\u5efa\u6a21\uff0c\u5ffd\u89c6\u4e86\u89c6\u9891\u5206\u6790\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u7684\u81ea\u56de\u5f52\u89c6\u89c9\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u5b9a\u4f4d\u4e0d\u51c6\u786e\u548c\u751f\u6210\u8d28\u91cf\u5dee\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86NExT-Vid\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u63a9\u7801\u4e0b\u4e00\u5e27\u9884\u6d4b\u6765\u5171\u540c\u5efa\u6a21\u56fe\u50cf\u548c\u89c6\u9891\u3002</li>\n    <li>NExT-Vid\u901a\u8fc7\u4e0a\u4e0b\u6587\u9694\u79bb\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u5668\u548c\u6761\u4ef6\u6d41\u5339\u914d\u89e3\u7801\u5668\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u4e0a\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New advancements in general foundation models have improved performance in various tasks.</li>\n    <li>Current visual generative models mainly use BERT-style methods, which often overlook important time information in videos.</li>\n    <li>Existing autoregressive visual models face challenges like poor semantic accuracy and low-quality generation.</li>\n    <li>NExT-Vid is a new framework that predicts the next video frame while handling both images and videos effectively.</li>\n    <li>Experiments show that NExT-Vid outperforms previous methods in visual representation learning for classification tasks.</li>\n</ul>"}, "publishedAt": "2025-12-24T02:07:08.000Z", "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations", "summary": "Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21004.png", "numComments": 1, "submittedBy": {"_id": "62fc758172a7ab50b4b89c8c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677251053045-62fc758172a7ab50b4b89c8c.jpeg", "fullname": "Zhicheng Sun", "name": "feifeiobama", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n<li>SemanticGen\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u7684\u6162\u6536\u655b\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002</li>\n<li>\u8be5\u6a21\u578b\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\uff0c\u9996\u5148\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u7136\u540e\u6dfb\u52a0\u9ad8\u9891\u7ec6\u8282\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5904\u7406\u4f4e\u7ea7\u89c6\u9891\u6807\u8bb0\u3002</li>\n<li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n<li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\uff0c\u4e14\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u66f4\u6709\u6548\u7387\u3002</li>\n<li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new video generation method that improves on existing models.</li>\n    <li>It generates videos in a simplified semantic space instead of directly in pixel space.</li>\n    <li>The process has two stages: first, it creates a high-level video layout, then adds detailed features.</li>\n    <li>This approach is faster and more efficient, especially for generating long videos.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and beats other leading methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19693", "authors": [{"_id": "694a0ffa335742716e93227d", "name": "Weichen Fan", "hidden": false}, {"_id": "694a0ffa335742716e93227e", "name": "Haiwen Diao", "hidden": false}, {"_id": "694a0ffa335742716e93227f", "name": "Quan Wang", "hidden": false}, {"_id": "694a0ffa335742716e932280", "name": "Dahua Lin", "hidden": false}, {"_id": "694a0ffa335742716e932281", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-22T18:59:57.000Z", "submittedOnDailyAt": "2025-12-23T01:15:14.379Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "upvotes": 51, "discussionId": "694a0ffa335742716e932282", "githubRepo": "https://github.com/WeichenFan/UAE", "githubRepoAddedBy": "user", "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.", "ai_keywords": ["spectral characteristics", "semantic encoders", "pixel encoders", "feature spectrum", "low-frequency components", "high-frequency information", "Prism Hypothesis", "Unified Autoencoding", "frequency-band modulator", "ImageNet", "MS-COCO", "latent space"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4e86\u4e0d\u540c\u8bed\u4e49\u548c\u50cf\u7d20\u7f16\u7801\u5668\u7684\u5149\u8c31\u7279\u6027\u3002</li>\n    <li>\u53d1\u73b0\u8bed\u4e49\u7f16\u7801\u5668\u4e3b\u8981\u6355\u6349\u4f4e\u9891\u6210\u5206\uff0c\u50cf\u7d20\u7f16\u7801\u5668\u8fd8\u4fdd\u7559\u9ad8\u9891\u4fe1\u606f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u201c\u68f1\u955c\u5047\u8bbe\u201d\uff0c\u5c06\u6570\u636e\u6a21\u6001\u89c6\u4e3a\u81ea\u7136\u4e16\u754c\u5728\u5171\u540c\u7279\u5f81\u5149\u8c31\u4e0a\u7684\u6295\u5f71\u3002</li>\n    <li>\u63d0\u51fa\u7edf\u4e00\u81ea\u7f16\u7801 (UAE) \u6a21\u578b\uff0c\u7ed3\u5408\u8bed\u4e49\u7ed3\u6784\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n    <li>\u5728ImageNet\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86UAE\u7684\u9ad8\u6548\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This study looks at how different types of data encoders work together in capturing information.</li>\n    <li>It reveals that semantic encoders focus on low-frequency information for abstract meaning, while pixel encoders capture high-frequency details for finer visuals.</li>\n    <li>The authors introduce the \"Prism Hypothesis,\" suggesting that different data types reflect the natural world in a shared way, similar to how a prism works.</li>\n    <li>They propose a new model called Unified Autoencoding (UAE) that combines abstract meanings and detailed visuals using a special frequency modulator.</li>\n    <li>Tests on well-known datasets show that UAE successfully merges semantic and pixel information, achieving top performance results.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:59:57.000Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19673", "authors": [{"_id": "694ac3ad746a34b55dd53b6c", "name": "Yuqiao Tan", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6d", "name": "Minzheng Wang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6e", "name": "Shizhu He", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6f", "name": "Huanxuan Liao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b70", "name": "Chengfeng Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b71", "name": "Qiunan Lu", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b72", "name": "Tian Liang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b73", "name": "Jun Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b74", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-22T18:51:48.000Z", "submittedOnDailyAt": "2025-12-24T00:28:45.252Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "submittedOnDailyBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "isPro": false, "fullname": "mz.w", "user": "iiiiwis", "type": "user"}, "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "upvotes": 49, "discussionId": "694ac3ad746a34b55dd53b75", "githubRepo": "https://github.com/Trae1ounG/BuPO", "githubRepoAddedBy": "user", "ai_summary": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.", "ai_keywords": ["reinforcement learning", "large language models", "Transformer residual stream", "unembedding matrix", "Internal Layer Policies", "Internal Modular Policies", "self-attention", "feed-forward network", "entropy", "Bottom-up Policy Optimization"], "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u5ffd\u89c6\u4e86\u5176\u5185\u90e8\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u5206\u6790Transformer\u7684\u6b8b\u5dee\u6d41\uff0c\u63ed\u793a\u4e86\u5185\u90e8\u5c42\u7b56\u7565\u548c\u6a21\u5757\u7b56\u7565\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u65e9\u671f\u5c42\u4fdd\u6301\u9ad8\u71b5\u4ee5\u4fc3\u8fdb\u63a2\u7d22\uff0c\u800c\u9876\u5c42\u7684\u71b5\u63a5\u8fd1\u96f6\u4ee5\u8fdb\u884c\u7cbe\u7ec6\u5316\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u201c\u81ea\u4e0b\u800c\u4e0a\u7684\u7b56\u7565\u4f18\u5316\u201d\uff08BuPO\uff09\uff0c\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u5c42\u7b56\u7565\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBuPO\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current reinforcement learning methods view large language models as one whole, missing how they work internally.</li>\n    <li>This paper breaks down the language model into Internal Layer Policies and Internal Modular Policies for better understanding and optimization.</li>\n    <li>It finds that early layers explore a lot while top layers refine outputs, with different patterns in different models.</li>\n    <li>It introduces a new approach called Bottom-up Policy Optimization (BuPO) that improves training by focusing on early layers.</li>\n    <li>Experiments show that BuPO leads to better performance on complex reasoning tasks, and the code is available online.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:51:48.000Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19673.png", "numComments": 4, "submittedBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "fullname": "mz.w", "name": "iiiiwis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17901", "authors": [{"_id": "6948af7534f46eaf46cbb1da", "user": {"_id": "6719bfd07c6e6c83a388aeae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png", "isPro": false, "fullname": "Junyu Zhang", "user": "jyzhang1208", "type": "user"}, "name": "Junyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:53.009Z", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1db", "name": "Yifan Sun", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dc", "name": "Tianang Leng", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dd", "name": "Jingyan Shen", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1de", "name": "Liu Ziyin", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1df", "name": "Paul Pu Liang", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1e0", "name": "Huan Zhang", "hidden": false}], "publishedAt": "2025-12-19T18:59:11.000Z", "submittedOnDailyAt": "2025-12-22T00:10:07.525Z", "title": "When Reasoning Meets Its Laws", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "upvotes": 48, "discussionId": "6948af7534f46eaf46cbb1e1", "projectPage": "https://lore-project.github.io/", "githubRepo": "https://github.com/ASTRAL-Group/LoRe", "githubRepoAddedBy": "user", "ai_summary": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.", "ai_keywords": ["Laws of Reasoning", "LoRe", "compute law", "accuracy law", "question complexity", "monotonicity", "compositionality", "LoRe-Bench", "finetuning approach", "compute-law compositionality"], "githubStars": 15, "summary_zh": "<ul>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u63a8\u7406\u6cd5\u5219\uff08LoRe\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u884c\u4e3a\u3002</li>\n    <li>\u63a8\u7406\u6cd5\u5219\u5305\u62ec\u8ba1\u7b97\u6cd5\u5219\uff0c\u5047\u8bbe\u63a8\u7406\u8ba1\u7b97\u5e94\u8be5\u4e0e\u95ee\u9898\u590d\u6742\u6027\u7ebf\u6027\u76f8\u5173\u3002</li>\n    <li>\u7531\u4e8e\u95ee\u9898\u590d\u6742\u6027\u96be\u4ee5\u91cf\u5316\uff0c\u7814\u7a76\u901a\u8fc7\u5355\u8c03\u6027\u548c\u7ec4\u5408\u6027\u4e24\u4e2a\u6027\u8d28\u6765\u68c0\u9a8c\u8fd9\u4e9b\u5047\u8bbe\u3002</li>\n    <li>\u5f15\u5165\u4e86LoRe-Bench\u57fa\u51c6\uff0c\u7cfb\u7edf\u6d4b\u91cf\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5355\u8c03\u6027\u548c\u7ec4\u5408\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7b26\u5408\u8ba1\u7b97\u6cd5\u5219\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u66f4\u597d\u7684\u63a8\u7406\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Reasoning Models (LRMs) often have confusing reasoning behaviors that are not very effective.</li>\n    <li>This paper introduces the Laws of Reasoning (LoRe), a framework to define good reasoning patterns in LRMs.</li>\n    <li>LoRe includes a compute law suggesting that reasoning should become more efficient as question complexity increases.</li>\n    <li>The paper also discusses a supplementary accuracy law and introduces LoRe-Bench to measure key properties of LRMs.</li>\n    <li>Results show that while most models are reasonably consistent, they often struggle with compositionality; a new finetuning method can help improve this.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:59:11.000Z", "title": "When Reasoning Meets Its Laws", "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u5957\u4ef6\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u74063D\u7a7a\u95f4\u4e2d\u7269\u4f53\u51e0\u4f55\u548c\u5173\u7cfb\u7684\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u751f\u6210\u591a\u9879\u9009\u62e9\u9898\u548c\u7b54\u6848\uff0c\u63d0\u53d6\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\u3002</li>\n    <li>\u65b0\u6570\u636e\u96c6\u5f3a\u8c03\u4e86\u771f\u5b9e\u89c6\u9891\u6765\u6e90\u30013D\u5bf9\u8c61\u548c\u573a\u666f\u9700\u6c42\u3001\u89c6\u89d2\u8f6c\u6362\u3001\u591a\u7269\u4f53\u4e92\u52a8\u548c\u7ec6\u81f4\u7684\u7a0b\u5e8f\u6027\u7b54\u6848\u3002</li>\n    <li>\u5f15\u5165\u4e86\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\uff0c\u5c06\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\u6574\u5408\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5e2e\u52a9\u7cbe\u7b80\u95ee\u9898\u8bed\u4e49\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06DSR-Train\u548cGSM\u6574\u5408\u5230Qwen2.5-VL-7B\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e00\u822c\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models are good at understanding but struggle with dynamic spatial reasoning (DSR), which involves understanding how objects change in 3D space over time.</li>\n    <li>To improve DSR, a new tool called DSR Suite is introduced, which includes an automated system that creates question-answer pairs from videos.</li>\n    <li>This system uses advanced vision models to gather important 3D information like the position of cameras and the movement of objects.</li>\n    <li>DSR Suite focuses on real-world videos, requires 3D understanding of objects and scenes, and incorporates complex interactions between multiple objects.</li>\n    <li>A new Geometry Selection Module (GSM) helps VLMs use relevant 3D information without being overloaded with unnecessary details, improving their ability to reason about dynamic spatial changes.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20618", "authors": [{"_id": "694ba02a746a34b55dd53e8b", "name": "Runtao Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8c", "name": "Ziyi Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8d", "name": "Jiaqi Tang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8e", "name": "Yue Ma", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8f", "name": "Renjie Pi", "hidden": false}, {"_id": "694ba02a746a34b55dd53e90", "name": "Jipeng Zhang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e91", "name": "Qifeng Chen", "hidden": false}], "publishedAt": "2025-12-23T18:59:49.000Z", "submittedOnDailyAt": "2025-12-24T05:57:23.776Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "submittedOnDailyBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "isPro": true, "fullname": "Jiaqi Tang", "user": "Jiaqi-hkust", "type": "user"}, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "upvotes": 38, "discussionId": "694ba02a746a34b55dd53e92", "ai_summary": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.", "ai_keywords": ["multimodal LLMs", "long-video QA", "multi-agent framework", "grounding agent", "vision agent", "reinforcement learning", "temporal grounding", "LongTVQA", "LongTVQA+"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7531\u4e3bLLM\u534f\u8c03\u5176\u4ed6\u4ee3\u7406\uff0c\u589e\u5f3a\u957f\u89c6\u9891\u95ee\u7b54\u7684\u80fd\u529b\u3002</li>\n    <li>\u6846\u67b6\u4e2d\u5305\u62ec\u4e00\u4e2a\u5b9a\u4f4d\u4ee3\u7406\u548c\u4e00\u4e2a\u89c6\u89c9\u4ee3\u7406\uff0c\u5206\u522b\u8d1f\u8d23\u627e\u5230\u76f8\u5173\u7247\u6bb5\u548c\u63d0\u53d6\u6587\u672c\u4fe1\u606f\u3002</li>\n    <li>\u4e3b\u4ee3\u7406\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u4ee3\u7406\u5408\u4f5c\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002</li>\n    <li>\u5728LongTVQA\u548cLongTVQA+\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8868\u73b0\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u4ee3\u7406\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>New multimodal LLMs can analyze long videos for question answering, but many methods oversimplify content.</li>\n  <li>We introduce a multi-agent system with a master LLM that works with a grounding agent and a vision agent.</li>\n  <li>The system helps find relevant video segments and adds visual details to subtitles.</li>\n  <li>Our approach shows better performance on LongTVQA and LongTVQA+ datasets compared to existing methods.</li>\n  <li>Using reinforcement learning improves the planning and reasoning skills of the agents.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:49.000Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png", "numComments": 1, "submittedBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "fullname": "Jiaqi Tang", "name": "Jiaqi-hkust", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17650", "authors": [{"_id": "6948c8f334f46eaf46cbb325", "name": "Zhongwei Zhang", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb326", "name": "Fuchen Long", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb327", "name": "Wei Li", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb328", "name": "Zhaofan Qiu", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb329", "name": "Wu Liu", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb32a", "name": "Ting Yao", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb32b", "name": "Tao Mei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"], "publishedAt": "2025-12-19T14:49:30.000Z", "submittedOnDailyAt": "2025-12-23T01:38:37.820Z", "title": "Region-Constraint In-Context Generation for Instructional Video Editing", "submittedOnDailyBy": {"_id": "6496f5754a3c31df8e3139f6", "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg", "isPro": false, "fullname": "Zhongwei Zhang", "user": "zzwustc", "type": "user"}, "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.", "upvotes": 38, "discussionId": "6948c8f334f46eaf46cbb32c", "projectPage": "https://zhw-zhang.github.io/ReCo-page/", "githubRepo": "https://github.com/HiDream-ai/ReCo", "githubRepoAddedBy": "user", "ai_summary": "ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.", "ai_keywords": ["in-context generation", "instructional video editing", "denoising", "ReCo", "constraint modeling", "latent regularization", "attention regularization", "backward denoised latents", "attention maps", "ReCo-Data", "video diffusion learning", "instruction-video pairs", "video editing tasks"], "githubStars": 32, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "summary_zh": "<ul>\n    <li>ReCo\u662f\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u7f16\u8f91\u533a\u57df\u4e0e\u975e\u7f16\u8f91\u533a\u57df\u4e4b\u95f4\u7684\u7ea6\u675f\u5efa\u6a21\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bbd\u5ea6\u62fc\u63a5\u6e90\u89c6\u9891\u548c\u76ee\u6807\u89c6\u9891\u6765\u8fdb\u884c\u8054\u5408\u53bb\u566a\u3002</li>\n    <li>ReCo\u4f7f\u7528\u4e24\u79cd\u6b63\u5219\u5316\u6280\u672f\u6765\u63d0\u9ad8\u89c6\u9891\u53bb\u566a\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5206\u522b\u662f\u6f5c\u5728\u6b63\u5219\u5316\u548c\u6ce8\u610f\u529b\u6b63\u5219\u5316\u3002</li>\n    <li>\u6f5c\u5728\u6b63\u5219\u5316\u589e\u5f3a\u4e86\u7f16\u8f91\u533a\u57df\u4e0e\u975e\u7f16\u8f91\u533a\u57df\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u6ce8\u610f\u529b\u6b63\u5219\u5316\u51cf\u5c11\u4e86\u7f16\u8f91\u533a\u57df\u4e0e\u6e90\u89c6\u9891\u4e4b\u95f4\u7684\u5e72\u6270\u3002</li>\n    <li>ReCo-Data\u662f\u4e00\u4e2a\u5305\u542b50\u4e07\u4e2a\u6307\u4ee4\u89c6\u9891\u5bf9\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ReCo is a new approach for instructional video editing that improves editing quality and efficiency.</li>\n    <li>It addresses issues with unclear editing regions and unwanted interference during the editing process.</li>\n    <li>ReCo combines source and target videos to enhance the editing process and uses two regularization techniques to improve results.</li>\n    <li>It focuses on clearly defining the editing area to reduce mistakes and unwanted content in the final video.</li>\n    <li>A large dataset, ReCo-Data, was created with 500,000 instruction-video pairs to support the training of the model.</li>\n</ul>"}, "publishedAt": "2025-12-19T09:49:30.000Z", "title": "Region-Constraint In-Context Generation for Instructional Video Editing", "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17650.png", "numComments": 2, "submittedBy": {"_id": "6496f5754a3c31df8e3139f6", "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg", "fullname": "Zhongwei Zhang", "name": "zzwustc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17260", "authors": [{"_id": "6948afc434f46eaf46cbb1f1", "name": "Jiangjie Chen", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f2", "name": "Wenxiang Chen", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f3", "name": "Jiacheng Du", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f4", "user": {"_id": "637a06580a77f602dc4ac922", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637a06580a77f602dc4ac922/qOJAhHOEE2N-HzRcZOu1L.jpeg", "isPro": false, "fullname": "Jinyi Hu", "user": "JamesHujy", "type": "user"}, "name": "Jinyi Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:50.739Z", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f5", "name": "Zhicheng Jiang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f6", "user": {"_id": "5f729da2f1e7ef6e919a1c27", "avatarUrl": "/avatars/bdf48f0d3290fab54d7e9636a3d14158.svg", "isPro": false, "fullname": "Zhanming (Allan) Jie", "user": "allanjie", "type": "user"}, "name": "Allan Jie", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:49.005Z", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f7", "name": "Xiaoran Jin", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f8", "name": "Xing Jin", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f9", "name": "Chenggang Li", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fa", "name": "Wenlei Shi", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fb", "name": "Zhihong Wang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fc", "name": "Mingxuan Wang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fd", "name": "Chenrui Wei", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fe", "name": "Shufa Wei", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1ff", "name": "Huajian Xin", "hidden": false}, {"_id": "6948afc434f46eaf46cbb200", "name": "Fan Yang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb201", "name": "Weihao Gao", "hidden": false}, {"_id": "6948afc434f46eaf46cbb202", "name": "Zheng Yuan", "hidden": false}, {"_id": "6948afc434f46eaf46cbb203", "name": "Tianyang Zhan", "hidden": false}, {"_id": "6948afc434f46eaf46cbb204", "name": "Zeyu Zheng", "hidden": false}, {"_id": "6948afc434f46eaf46cbb205", "name": "Tianxi Zhou", "hidden": false}, {"_id": "6948afc434f46eaf46cbb206", "name": "Thomas Hanwen Zhu", "hidden": false}], "publishedAt": "2025-12-19T06:19:55.000Z", "submittedOnDailyAt": "2025-12-22T00:11:16.085Z", "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.", "upvotes": 37, "discussionId": "6948afc534f46eaf46cbb207", "ai_summary": "Seed-Prover 1.5, a formal theorem-proving model using large-scale agentic reinforcement learning and an efficient test-time scaling workflow, demonstrates superior performance in solving mathematical problems across various levels with reduced computational resources.", "ai_keywords": ["large language models", "theorem proving", "formal languages", "Lean", "reinforcement learning", "test-time scaling", "PutnamBench", "Fate-H", "Fate-X", "formal mathematical reasoning"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e25\u683c\u6570\u5b66\u8bc1\u660e\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>Seed-Prover 1.5\u662f\u4e00\u79cd\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u4e0eLean\u548c\u5176\u4ed6\u5de5\u5177\u7684\u4e92\u52a8\u4e2d\u4e0d\u65ad\u79ef\u7d2f\u7ecf\u9a8c\uff0c\u63d0\u9ad8\u4e86\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u7684\u80fd\u529b\u548c\u6548\u7387\u3002</li>\n    <li>Seed-Prover 1.5\u5728\u8ba1\u7b97\u8d44\u6e90\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u4e8688%\u7684\u672c\u79d1\u6c34\u5e73\u300180%\u7684\u7814\u7a76\u751f\u6c34\u5e73\u548c33%\u7684\u535a\u58eb\u751f\u6c34\u5e73\u95ee\u9898\u3002</li>\n    <li>\u4f7f\u7528\u8be5\u7cfb\u7edf\uff0c\u6211\u4eec\u57289\u5c0f\u65f6\u5185\u89e3\u51b3\u4e862025\u5e74Putnam\u7ade\u8d5b\u768412\u4e2a\u95ee\u9898\u4e2d\u768411\u4e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are improving in generating mathematical proofs, but using them for formal languages like Lean is still tough and costly.</li>\n    <li>Seed-Prover 1.5 is a new model that uses reinforcement learning to improve theorem proving efficiency.</li>\n    <li>This model learns from its interactions with Lean and other tools to get better over time.</li>\n    <li>It connects natural language and formal language proving effectively, leading to better performance with less computing power.</li>\n    <li>Seed-Prover 1.5 solved a high percentage of problems across different academic levels and performed well on specific challenging problems from Putnam 2025.</li>\n</ul>"}, "publishedAt": "2025-12-19T01:19:55.000Z", "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience", "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17260.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20617", "authors": [{"_id": "694b58e3746a34b55dd53cff", "name": "Yuxi Xiao", "hidden": false}, {"_id": "694b58e3746a34b55dd53d00", "name": "Longfei Li", "hidden": false}, {"_id": "694b58e3746a34b55dd53d01", "name": "Shen Yan", "hidden": false}, {"_id": "694b58e3746a34b55dd53d02", "name": "Xinhang Liu", "hidden": false}, {"_id": "694b58e3746a34b55dd53d03", "name": "Sida Peng", "hidden": false}, {"_id": "694b58e3746a34b55dd53d04", "name": "Yunchao Wei", "hidden": false}, {"_id": "694b58e3746a34b55dd53d05", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "694b58e3746a34b55dd53d06", "name": "Bingyi Kang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "publishedAt": "2025-12-23T18:59:46.000Z", "submittedOnDailyAt": "2025-12-24T00:38:28.003Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "upvotes": 34, "discussionId": "694b58e4746a34b55dd53d07", "projectPage": "https://spatialtree.github.io/", "ai_summary": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.", "ai_keywords": ["SpatialTree", "low-level perception", "mental mapping", "simulation", "agentic competence", "capability-centric hierarchical benchmark", "targeted supervised fine-tuning", "negative transfer", "cross-level transfer", "naive RL", "auto-think strategy"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SpatialTree\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u7a7a\u95f4\u80fd\u529b\u5c42\u6b21\u7ed3\u6784\uff0c\u5206\u4e3a\u56db\u4e2a\u7ea7\u522b\uff1a\u4f4e\u7ea7\u611f\u77e5\uff08L1\uff09\u3001\u5fc3\u7406\u6620\u5c04\uff08L2\uff09\u3001\u6a21\u62df\uff08L3\uff09\u548c\u80fd\u52a8\u80fd\u529b\uff08L4\uff09\u3002</li>\n    <li>\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u4ee5\u80fd\u529b\u4e3a\u4e2d\u5fc3\u7684\u5c42\u6b21\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u572827\u4e2a\u5b50\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cL1\u6280\u80fd\u4e4b\u95f4\u51e0\u4e4e\u6ca1\u6709\u76f8\u5173\u6027\uff0c\u800c\u66f4\u9ad8\u7ea7\u522b\u7684\u6280\u80fd\u4e4b\u95f4\u5b58\u5728\u8f83\u5f3a\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002</li>\n    <li>\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u6211\u4eec\u53d1\u73b0L1\u5b58\u5728\u8d1f\u8fc1\u79fb\uff0c\u4f46\u4f4e\u7ea7\u5230\u9ad8\u7ea7\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5f3a\u5927\u7684\u8de8\u5c42\u8fc1\u79fb\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u81ea\u52a8\u601d\u8003\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u5404\u5c42\u7ea7\u7684\u8868\u73b0\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u6df1\u601d\u719f\u8651\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Cognitive science shows that spatial ability develops in stages, but this is not well understood in multimodal language models (MLLMs).</li>\n    <li>We created SpatialTree, a framework that organizes spatial abilities into four levels: perception, mental mapping, simulation, and agentic competence.</li>\n    <li>We evaluated popular MLLMs on 27 sub-abilities, finding that lower-level skills are independent while higher-level skills depend on each other.</li>\n    <li>Fine-tuning showed that low-level skills can negatively affect each other, but higher skills benefit from lower ones.</li>\n    <li>We suggest a new strategy, auto-think, to improve performance across all levels without harming intuitive perception.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:46.000Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17909", "authors": [{"_id": "6948d11034f46eaf46cbb338", "name": "Shilong Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb339", "name": "He Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33a", "name": "Zhifei Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33b", "name": "Chongjian Ge", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33c", "name": "Shuchen Xue", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33d", "name": "Shaoteng Liu", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33e", "name": "Mengwei Ren", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33f", "name": "Soo Ye Kim", "hidden": false}, {"_id": "6948d11034f46eaf46cbb340", "name": "Yuqian Zhou", "hidden": false}, {"_id": "6948d11034f46eaf46cbb341", "name": "Qing Liu", "hidden": false}, {"_id": "6948d11034f46eaf46cbb342", "name": "Daniil Pakhomov", "hidden": false}, {"_id": "6948d11034f46eaf46cbb343", "name": "Kai Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb344", "name": "Zhe Lin", "hidden": false}, {"_id": "6948d11034f46eaf46cbb345", "name": "Ping Luo", "hidden": false}], "publishedAt": "2025-12-19T18:59:57.000Z", "submittedOnDailyAt": "2025-12-22T03:13:08.364Z", "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing", "submittedOnDailyBy": {"_id": "6424ffce46d202ad3d918a67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg", "isPro": false, "fullname": "Shilong Zhang", "user": "shilongz", "type": "user"}, "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.", "upvotes": 29, "discussionId": "6948d11034f46eaf46cbb346", "projectPage": "https://jshilong.github.io/PS-VAE-PAGE/", "ai_summary": "A systematic framework adapts understanding-oriented encoder features for generative tasks by introducing a semantic-pixel reconstruction objective, enabling state-of-the-art image reconstruction and generation.", "ai_keywords": ["Latent Diffusion Models", "VAE", "latent space", "generative latents", "discriminative feature space", "off-manifold latents", "pixel-level reconstruction", "semantic-pixel reconstruction objective", "Text-to-Image", "image editing", "feature spaces"], "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e3b\u8981\u5728\u4f4e\u7ea7VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u4f18\u5316\u50cf\u7d20\u91cd\u5efa\u3002</li>\n    <li>\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u91c7\u7528\u9ad8\u7ef4\u7279\u5f81\u4f5c\u4e3a\u751f\u6210\u6f5c\u5728\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u7279\u5f81\u7a7a\u95f4\u7f3a\u4e4f\u7d27\u51d1\u6b63\u5219\u5316\uff0c\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u7269\u4f53\u7ed3\u6784\u3002</li>\n    <li>\u7f16\u7801\u5668\u7684\u50cf\u7d20\u91cd\u5efa\u80fd\u529b\u5f31\uff0c\u59a8\u788d\u751f\u6210\u5668\u5b66\u4e60\u51c6\u786e\u7684\u51e0\u4f55\u548c\u7eb9\u7406\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u50cf\u7d20\u91cd\u5efa\u76ee\u6807\u6765\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u786e\u4fdd\u8bed\u4e49\u4fe1\u606f\u548c\u7ec6\u8282\u7684\u538b\u7f29\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u8868\u793a\u7f16\u7801\u5668\u53ef\u4ee5\u6709\u6548\u9002\u5e94\u751f\u6210\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Modern Latent Diffusion Models (LDMs) usually work in simple pixel-based spaces but researchers are exploring the use of more complex features from representation encoders.</li>\n    <li>Two main challenges with these complex features are: they can lead to inaccurate object shapes and they struggle with fine details in images.</li>\n    <li>This paper presents a new framework that improves how these encoder features can be used for generating images.</li>\n    <li>It introduces a method to better combine semantic understanding and detailed pixel information, resulting in a compact and effective representation.</li>\n    <li>The new method shows significant improvements in image quality, speed, and performance for tasks like Text-to-Image generation and image editing.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:59:57.000Z", "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing", "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17909.png", "numComments": 2, "submittedBy": {"_id": "6424ffce46d202ad3d918a67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg", "fullname": "Shilong Zhang", "name": "shilongz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86DeepSeek-V3.2\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u548c\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\uff08DSA\uff09\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u5728\u591a\u9879\u8d5b\u4e8b\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-5\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a8\u5e7f\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines fast processing with great reasoning and performance.</li>\n    <li>It uses a new attention method called DeepSeek Sparse Attention (DSA) that makes it more efficient for handling long texts.</li>\n    <li>The model works well with reinforcement learning, performing similarly to GPT-5, and its advanced version outperforms GPT-5 and matches the reasoning skills of Gemini-3.0-Pro.</li>\n    <li>DeepSeek-V3.2 achieved top results in prestigious competitions like the 2025 International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a new system for generating training data, improving its ability to follow instructions and adapt in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u578b\u5982Qwen-Image\u548cHunyuan-Image-3.0\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u53ef\u6269\u5c55\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff08S3-DiT\uff09\u3002</li>\n    <li>Z-Image\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5728\u4f01\u4e1a\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u517c\u5bb9\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\u3002</li>\n    <li>\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u6a21\u578b\u6743\u91cd\u548c\u5728\u7ebf\u6f14\u793a\uff0c\u4ee5\u63a8\u52a8\u53ef\u8bbf\u95ee\u7684\u3001\u7ecf\u6d4e\u5b9e\u60e0\u7684\u524d\u6cbf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The current top image generation models are mostly proprietary, making them hard to access.</li>\n    <li>Open-source models have very large sizes (20B to 80B parameters), which are not practical for most users.</li>\n    <li>We introduce Z-Image, a smaller 6B-parameter model that uses a new architecture for better efficiency.</li>\n    <li>Z-Image can be trained in a shorter time and is capable of fast image generation even on consumer-grade hardware.</li>\n    <li>Our model performs as well or better than leading models and is available for public use, promoting affordable access to advanced technology.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u6570\u636e\u51c6\u5907\u7684\u65b9\u5f0f\u9700\u8981\u66f4\u52a0\u53ef\u9760\u548c\u53ef\u6269\u5c55\u3002</li>\n    <li>\u5f53\u524d\u7684\u6570\u636e\u51c6\u5907\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\uff0c\u5f71\u54cd\u4e86\u53ef\u91cd\u590d\u6027\u548c\u6570\u636e\u751f\u6210\u7684\u652f\u6301\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u652f\u6301\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>\u901a\u8fc7DataFlow\uff0c\u6a21\u578b\u5728\u591a\u4e2a\u7528\u4f8b\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u63d0\u5347\uff0c\u5c24\u5176\u5728\u6570\u636e\u8d28\u91cf\u548c\u6267\u884c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data for Large Language Models (LLMs), but current data preparation methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create better data preparation processes by allowing users to build modular and reusable data transformations.</li>\n    <li>It includes nearly 200 reusable tools and various pipelines for handling different types of data, such as text and code.</li>\n    <li>DataFlow-Agent can automatically turn natural language instructions into working data pipelines, making it easier to use.</li>\n    <li>DataFlow improves LLM performance in various tasks, achieving better results than existing datasets and setting a strong foundation for future AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u5728\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u5934\u50cf\u5408\u6210\u4e2d\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u5934\u50cf\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u8fdb\u884c\u53bb\u566a\u6b65\u9aa4\u7684\u7ba1\u9053\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u74f6\u9888\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u52a8\u6001\u91cd\u6821\u51c6\u5916\u89c2\u7684\u201c\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\u201d\uff08RSFM\uff09\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u5e76\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5b9e\u65f6\u5934\u50cf\u751f\u6210\uff0c\u63a8\u52a8\u4e86\u5de5\u4e1a\u7ea7\u957f\u89c6\u9891\u5408\u6210\u5e94\u7528\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for generating realistic avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>The system uses a method called Timestep-forcing Pipeline Parallelism (TPP) to speed up the generation process by using multiple GPUs.</li>\n    <li>To improve the quality of the avatars, it includes a technique called Rolling Sink Frame Mechanism (RSFM) that keeps the appearance consistent over time.</li>\n    <li>Live Avatar can generate avatars at 20 frames per second using five GPUs, making it practical for real-time applications.</li>\n    <li>This work sets a new standard for using advanced models in long video synthesis, allowing for high-quality streaming of avatars.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4e86\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u67b6\u6784\u4e0a\u8fdb\u884c\u591a\u9636\u6bb5SQL\u7ba1\u9053\u7684\u8bbe\u8ba1\u548c\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u9700\u8981\u89e3\u51b3\u5f00\u653e\u6027\u5546\u4e1a\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6280\u672f\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u4e3a\u5f00\u53d1\u771f\u6b63\u6709\u6548\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u73af\u5883\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks that reflect real-world data workflows, including data engineering and data analysis.</li>\n    <li>Data engineering tasks involve creating and modifying SQL pipelines and require in-depth work with database schemas.</li>\n    <li>Data analysis tasks focus on solving business problems through exploration, coding, and formulating actionable insights.</li>\n    <li>Performance on these tasks is low, with data engineering tasks having success rates under 20% and data analysis tasks averaging below 40%.</li>\n    <li>DAComp helps identify challenges in data processing and supports the development of better autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u7b49\u529f\u80fd\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u6846\u67b6\u4f9d\u8d56\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\u548c\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u591a\u6a21\u6001\u89c6\u9891\u521b\u4f5c\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cKling-Omni \u5728\u751f\u6210\u3001\u7f16\u8f91\u548c\u9075\u5faa\u591a\u6a21\u6001\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one unified framework, unlike traditional methods that separate these processes.</li>\n    <li>The system can take various input forms, such as text instructions, images, and existing videos, to produce cinematic-quality content.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training and optimization techniques for better performance.</li>\n    <li>It shows great skills in generating content, editing based on reasoning, and following complex instructions, aiming to be a significant step toward creating interactive multimodal simulations.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21631", "authors": [{"_id": "692ffb1a26742347f61daf38", "user": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "name": "Shuai Bai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:34:29.118Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf39", "name": "Yuxuan Cai", "hidden": false}, {"_id": "692ffb1a26742347f61daf3a", "name": "Ruizhe Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3b", "name": "Keqin Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3c", "user": {"_id": "63f30b870a16587ea970edfe", "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg", "isPro": false, "fullname": "Xiong-Hui Chen", "user": "xionghuichen", "type": "user"}, "name": "Xionghui Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:42.689Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3d", "user": {"_id": "65b2529285b6c21448a10d65", "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg", "isPro": false, "fullname": "Zesen Cheng", "user": "ClownRat", "type": "user"}, "name": "Zesen Cheng", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:51.365Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3e", "name": "Lianghao Deng", "hidden": false}, {"_id": "692ffb1a26742347f61daf3f", "name": "Wei Ding", "hidden": false}, {"_id": "692ffb1a26742347f61daf40", "name": "Chang Gao", "hidden": false}, {"_id": "692ffb1a26742347f61daf41", "name": "Chunjiang Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf42", "name": "Wenbin Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf43", "name": "Zhifang Guo", "hidden": false}, {"_id": "692ffb1a26742347f61daf44", "user": {"_id": "656f1b21b075b63c90ba02ee", "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg", "isPro": false, "fullname": "Huang Qidong", "user": "shikiw", "type": "user"}, "name": "Qidong Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:49.065Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf45", "name": "Jie Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf46", "name": "Fei Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf47", "name": "Binyuan Hui", "hidden": false}, {"_id": "692ffb1a26742347f61daf48", "name": "Shutong Jiang", "hidden": false}, {"_id": "692ffb1a26742347f61daf49", "name": "Zhaohai Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4a", "name": "Mingsheng Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4b", "name": "Mei Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4c", "user": {"_id": "6346be8f7fb9f11870c63998", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png", "isPro": false, "fullname": "Kaixin Li", "user": "likaixin", "type": "user"}, "name": "Kaixin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:53.648Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4d", "user": {"_id": "67a31313cf9d856beb7f9afb", "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg", "isPro": false, "fullname": "Zicheng Lin", "user": "etonlin", "type": "user"}, "name": "Zicheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:50.803Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4e", "name": "Junyang Lin", "hidden": false}, {"_id": "692ffb1a26742347f61daf4f", "name": "Xuejing Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf50", "name": "Jiawei Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf51", "name": "Chenglong Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf52", "name": "Yang Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf53", "name": "Dayiheng Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf54", "user": {"_id": "64e72776e9fc9d0475ef5188", "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg", "isPro": false, "fullname": "Shixuan Liu", "user": "liusx", "type": "user"}, "name": "Shixuan Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:58.470Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf55", "name": "Dunjie Lu", "hidden": false}, {"_id": "692ffb1a26742347f61daf56", "name": "Ruilin Luo", "hidden": false}, {"_id": "692ffb1a26742347f61daf57", "name": "Chenxu Lv", "hidden": false}, {"_id": "692ffb1a26742347f61daf58", "name": "Rui Men", "hidden": false}, {"_id": "692ffb1a26742347f61daf59", "name": "Lingchen Meng", "hidden": false}, {"_id": "692ffb1a26742347f61daf5a", "name": "Xuancheng Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5b", "name": "Xingzhang Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5c", "name": "Sibo Song", "hidden": false}, {"_id": "692ffb1a26742347f61daf5d", "name": "Yuchong Sun", "hidden": false}, {"_id": "692ffb1a26742347f61daf5e", "name": "Jun Tang", "hidden": false}, {"_id": "692ffb1a26742347f61daf5f", "name": "Jianhong Tu", "hidden": false}, {"_id": "692ffb1a26742347f61daf60", "name": "Jianqiang Wan", "hidden": false}, {"_id": "692ffb1a26742347f61daf61", "name": "Peng Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf62", "name": "Pengfei Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf63", "name": "Qiuyue Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf64", "name": "Yuxuan Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf65", "name": "Tianbao Xie", "hidden": false}, {"_id": "692ffb1a26742347f61daf66", "name": "Yiheng Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf67", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:51.583Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf68", "name": "Jin Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf69", "name": "Zhibo Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6a", "name": "Mingkun Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6b", "name": "Jianxin Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6c", "name": "An Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6d", "name": "Bowen Yu", "hidden": false}, {"_id": "692ffb1a26742347f61daf6e", "name": "Fei Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6f", "name": "Hang Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf70", "name": "Xi Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf71", "name": "Bo Zheng", "hidden": false}, {"_id": "692ffb1a26742347f61daf72", "name": "Humen Zhong", "hidden": false}, {"_id": "692ffb1a26742347f61daf73", "name": "Jingren Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf74", "name": "Fan Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf75", "name": "Jing Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf76", "user": {"_id": "627d2723401f42c57b6b7c0c", "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg", "isPro": false, "fullname": "Yuanzhi Zhu", "user": "Yuanzhi", "type": "user"}, "name": "Yuanzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:59.879Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf77", "name": "Ke Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "publishedAt": "2025-11-26T17:59:08.000Z", "submittedOnDailyAt": "2025-12-04T01:02:46.772Z", "title": "Qwen3-VL Technical Report", "submittedOnDailyBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "upvotes": 110, "discussionId": "692ffb1b26742347f61daf78", "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.", "ai_keywords": ["vision-language model", "interleaved contexts", "multimodal benchmarks", "dense variants", "mixture-of-experts", "pure-text understanding", "long-context comprehension", "multimodal reasoning", "MMMU", "visual-math benchmarks", "interleaved-MRoPE", "DeepStack", "text-based time alignment", "T-RoPE", "explicit textual timestamp alignment", "vision-language alignment", "image-grounded reasoning", "agentic decision-making", "multimodal code intelligence"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>Qwen3-VL \u662f Qwen \u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u5353\u8d8a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u6700\u591a 256K \u7684\u4ea4\u9519\u4e0a\u4e0b\u6587\uff0c\u53ef\u4ee5\u540c\u65f6\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002</li>\n    <li>\u63d0\u4f9b\u591a\u79cd\u7248\u672c\uff0c\u5305\u62ec\u5bc6\u96c6\u578b\u548c\u4e13\u5bb6\u6df7\u5408\u578b\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7684\u5ef6\u8fdf\u548c\u8d28\u91cf\u9700\u6c42\u3002</li>\n    <li>\u5177\u6709\u66f4\u5f3a\u7684\u6587\u672c\u7406\u89e3\u80fd\u529b\u3001\u957f\u6587\u672c\u548c\u591a\u6a21\u6001\u8f93\u5165\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u6a21\u578b\u67b6\u6784\u6709\u4e09\u5927\u5347\u7ea7\uff0c\u4f18\u5316\u4e86\u7a7a\u95f4\u65f6\u95f4\u5efa\u6a21\u3001\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u4e0e\u89c6\u9891\u7684\u65f6\u95f4\u5bf9\u9f50\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-VL is the latest and most powerful vision-language model in the Qwen series, performing well on many tests.</li>\n    <li>It can handle up to 256,000 tokens, allowing it to work with text, images, and videos together.</li>\n    <li>The model comes in different sizes to match various needs for speed and quality.</li>\n    <li>Qwen3-VL has improved text understanding, long-context comprehension, and strong reasoning abilities for images and videos.</li>\n    <li>Key architectural upgrades enhance how it processes spatial and temporal information, improving its overall performance.</li>\n</ul>"}, "publishedAt": "2025-11-26T12:59:08.000Z", "title": "Qwen3-VL Technical Report", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png", "numComments": 3, "submittedBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "fullname": "shuai bai", "name": "ShuaiBai623", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.21689", "authors": [{"_id": "692f11ffbfc6eea1b6fb6900", "name": "Hongjin Su", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6901", "name": "Shizhe Diao", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6902", "name": "Ximing Lu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6903", "name": "Mingjie Liu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6904", "name": "Jiacheng Xu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6905", "name": "Xin Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6906", "name": "Yonggan Fu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6907", "name": "Peter Belcak", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6908", "name": "Hanrong Ye", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6909", "name": "Hongxu Yin", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690a", "name": "Yi Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690b", "name": "Evelina Bakhturina", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690c", "name": "Tao Yu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690d", "name": "Yejin Choi", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690e", "name": "Jan Kautz", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690f", "name": "Pavlo Molchanov", "hidden": false}], "publishedAt": "2025-11-26T18:59:46.000Z", "submittedOnDailyAt": "2025-12-03T13:46:43.945Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "submittedOnDailyBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "isPro": false, "fullname": "Shizhe Diao", "user": "shizhediao", "type": "user"}, "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "upvotes": 96, "discussionId": "692f11ffbfc6eea1b6fb6910", "projectPage": "https://research.nvidia.com/labs/lpr/ToolOrchestra/", "githubRepo": "https://github.com/NVlabs/ToolOrchestra/", "ai_summary": "A small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models.", "ai_keywords": ["large language models", "ToolOrchestra", "reinforcement learning", "outcome-aware rewards", "efficiency-aware rewards", "user-preference-aware rewards", "Orchestrator", "tool-use agents", "humanity's last exam", "tau2-bench", "FRAMES", "tool-augmented reasoning systems"], "githubStars": 297, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ToolOrchestra\uff0c\u4e00\u79cd\u8bad\u7ec3\u5c0f\u578b\u534f\u8c03\u8005\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ba1\u7406\u667a\u80fd\u5de5\u5177\u3002</li>\n    <li>ToolOrchestra\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u8003\u8651\u7ed3\u679c\u3001\u6548\u7387\u548c\u7528\u6237\u504f\u597d\u6765\u4f18\u5316\u5956\u52b1\u3002</li>\n    <li>\u901a\u8fc7ToolOrchestra\uff0c\u6211\u4eec\u5f00\u53d1\u4e86Orchestrator\uff0c\u8fd9\u662f\u4e00\u79cd8B\u6a21\u578b\uff0c\u51c6\u786e\u5ea6\u66f4\u9ad8\u4e14\u6210\u672c\u66f4\u4f4e\u3002</li>\n    <li>Orchestrator\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u534f\u8c03\u6a21\u578b\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Small orchestrators can effectively manage other models and tools to solve complex problems more efficiently.</li>\n    <li>ToolOrchestra is a new method for training these small orchestrators using rewards that consider outcomes, efficiency, and user preferences.</li>\n    <li>The Orchestrator model, which is 8 billion parameters, outperforms previous models like GPT-5 in accuracy while being more cost-effective.</li>\n    <li>On the Humanity's Last Exam, Orchestrator scored 37.1%, compared to GPT-5's 35.1%, and is 2.5 times more efficient.</li>\n    <li>Orchestrator shows strong performance across various tasks and adapts well to new tools, making it a promising solution for tool-augmented reasoning systems.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:59:46.000Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21689.png", "numComments": 3, "submittedBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "fullname": "Shizhe Diao", "name": "shizhediao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u63a7\u5236\u7cbe\u5ea6\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u800cWan-Move\u5b9e\u73b0\u4e86\u7cbe\u786e\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u573a\u666f\u7684\u7ec6\u81f4\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u751f\u6210\u4e94\u79d2\u3001480p\u7684\u89c6\u9891\uff0c\u5176\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0e\u5546\u4e1a\u4ea7\u54c1Kling 1.5 Pro\u7684Motion Brush\u76f8\u5f53\u3002</li>\n    <li>\u4e3a\u4e86\u5168\u9762\u8bc4\u4f30\uff0cWan-Move\u8fd8\u8bbe\u8ba1\u4e86MoveBench\uff0c\u4e00\u4e2a\u5305\u542b\u591a\u6837\u5185\u5bb9\u7c7b\u522b\u548c\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6807\u6ce8\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework designed to improve motion control in video generation models.</li>\n    <li>It offers precise and high-quality motion control, addressing limitations in existing methods.</li>\n    <li>The framework uses dense point trajectories to represent object movements, allowing for detailed scene control.</li>\n    <li>Wan-Move integrates smoothly with existing video models without needing major changes to their architecture.</li>\n    <li>It includes a new benchmark called MoveBench for evaluating motion quality across various video types.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01816", "authors": [{"_id": "692e5c0537312eaa83fd87b8", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:43.760Z", "hidden": false}, {"_id": "692e5c0537312eaa83fd87b9", "name": "Siyuan Li", "hidden": false}, {"_id": "692e5c0537312eaa83fd87ba", "name": "Conghui He", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bb", "name": "Lijun Wu", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bc", "user": {"_id": "64be296a46cc3cdfbb057f7e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg", "isPro": false, "fullname": "Cheng Tan", "user": "chengtan9907", "type": "user"}, "name": "Cheng Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:41.755Z", "hidden": false}], "publishedAt": "2025-12-01T15:52:31.000Z", "submittedOnDailyAt": "2025-12-02T01:31:46.625Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "submittedOnDailyBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "upvotes": 87, "discussionId": "692e5c0537312eaa83fd87bd", "projectPage": "https://opendatalab-raiser.github.io/Envision/", "githubRepo": "https://github.com/opendatalab-raiser/Envision", "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.", "ai_keywords": ["multimodal models", "text-to-image (T2I)", "causal event progression", "spatiotemporal causality", "Envision-a", "Envision-Score", "multi-dimensional consistency", "physicality", "aesthetics", "causal narrative coherence", "spatiotemporal consistency", "multi-frame reasoning", "dynamic world modeling"], "githubStars": 27, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bd5\u56fe\u901a\u8fc7\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6765\u514b\u670d\u5355\u4e00\u6a21\u6001\u7684\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u4efb\u52a1\u6765\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u4f9d\u8d56\u9759\u6001\u5355\u56fe\u751f\u6210\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u9759\u6001\u6a21\u5f0f\u5339\u914d\u548c\u8bed\u4e49\u878d\u5408\uff0c\u9650\u5236\u4e86\u5bf9\u52a8\u6001\u8fc7\u7a0b\u7684\u5efa\u6a21\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Envision\uff0c\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u5230\u591a\u56fe\u751f\u6210\u7684\u56e0\u679c\u4e8b\u4ef6\u8fdb\u5c55\u57fa\u51c6\uff0c\u5305\u542b1000\u4e2a\u56db\u9636\u6bb5\u63d0\u793a\uff0c\u8986\u76d6\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u3002</li>\n    <li>\u5f15\u5165Envision-Score\uff0c\u8fd9\u662f\u4e00\u79cd\u7efc\u5408\u6307\u6807\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u7ef4\u4e00\u81f4\u6027\u3001\u7269\u7406\u6027\u548c\u7f8e\u5b66\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4ee5\u89e3\u51b3\u4ece\u5355\u56fe\u5230\u987a\u5e8f\u5e27\u7684\u8bc4\u4f30\u8f6c\u53d8\u3002</li>\n    <li>\u5bf915\u4e2a\u6a21\u578b\u7684\u5168\u9762\u8bc4\u4f30\u663e\u793a\uff0c\u4e13\u95e8\u7684T2I\u6a21\u578b\u5728\u7f8e\u5b66\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5185\u5728\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u800c\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u4e8b\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current models struggle with understanding and generating images from text because they often focus on single images instead of sequences over time.</li>\n    <li>To improve this, researchers created a new benchmark called Envision, which tests how well models can generate a series of images based on text prompts related to events.</li>\n    <li>Envision includes 1,000 prompts across various fields and uses a new scoring system, Envision-Score, to evaluate the models' understanding of time and causality.</li>\n    <li>Tests showed that specialized models are good at creating aesthetically pleasing images but lack deeper world knowledge, while unified models perform better in creating coherent narratives.</li>\n    <li>Despite improvements, all models still face challenges with maintaining consistency over time and struggle with understanding dynamic processes in the world.</li>\n</ul>"}, "publishedAt": "2025-12-01T10:52:31.000Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png", "numComments": 4, "submittedBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "fullname": "Juanxi Tian", "name": "Juanxi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13}, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 26, 2025";
window.trendingPapers = {
    "today": [{"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u5e76\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u5f3a\u5316\u5b66\u4e60(RL)\u5f00\u59cb\u91c7\u7528\u591a\u79cd\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u5b9e\u73b0\u8fd9\u4e9b\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u4f1a\u5bfc\u81f4\u4e0d\u540c\u5956\u52b1\u7ec4\u5408\u7684\u6548\u679c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316(GDPO)\uff0c\u65e8\u5728\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u7684\u5f52\u4e00\u5316\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0cGDPO\u5728\u51c6\u786e\u6027\u548c\u9075\u5faa\u7ea6\u675f\u65b9\u9762\u90fd\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate responses and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to help models learn these preferred behaviors.</li>\n    <li>Current methods like Group Relative Policy Optimization (GRPO) can lead to problems, making the training less effective.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves training by keeping the differences between rewards clearer.</li>\n    <li>GDPO shows better performance than GRPO in three tasks: tool calling, math reasoning, and coding reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04890", "authors": [{"_id": "69608e7c5b7998385e639583", "user": {"_id": "64670db15993aa7666cc6022", "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg", "isPro": false, "fullname": "Maksim Velikanov", "user": "yellowvm", "type": "user"}, "name": "Maksim Velikanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:44.975Z", "hidden": false}, {"_id": "69608e7c5b7998385e639584", "user": {"_id": "6697a9fb6d173ec7382e0392", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg", "isPro": false, "fullname": "Ilyas Chahed", "user": "IChahed", "type": "user"}, "name": "Ilyas Chahed", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:46:04.314Z", "hidden": false}, {"_id": "69608e7c5b7998385e639585", "user": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "name": "Jingwei Zuo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:49.874Z", "hidden": false}, {"_id": "69608e7c5b7998385e639586", "name": "Dhia Eddine Rhaiem", "hidden": false}, {"_id": "69608e7c5b7998385e639587", "user": {"_id": "62441d1d9fdefb55a0b7d12c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png", "isPro": false, "fullname": "Younes B", "user": "ybelkada", "type": "user"}, "name": "Younes Belkada", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:47.914Z", "hidden": false}, {"_id": "69608e7c5b7998385e639588", "user": {"_id": "6471d727a2b0a376b8b6a4ed", "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg", "isPro": false, "fullname": "Hakim Hacid", "user": "HakimHacid", "type": "user"}, "name": "Hakim Hacid", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:56.651Z", "hidden": false}], "publishedAt": "2026-01-08T12:41:49.000Z", "submittedOnDailyAt": "2026-01-09T02:55:50.938Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "submittedOnDailyBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "upvotes": 28, "discussionId": "69608e7c5b7998385e639589", "projectPage": "https://tiiuae.github.io/Falcon-H1/", "githubRepo": "https://github.com/tiiuae/falcon-h1", "githubRepoAddedBy": "user", "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.", "ai_keywords": ["weight decay", "stochastic gradient noise", "Brownian-like expansion", "WD-noise equilibrium", "learnable multipliers", "matrix layers", "weight norm", "muP multipliers", "Adam optimizer", "Muon optimizer"], "githubStars": 98, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "summary_zh": "<ul>\n    <li>\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4e2d\uff0c\u5e94\u7528\u6743\u91cd\u8870\u51cf\uff08WD\uff09\u662f\u5e38\u89c1\u505a\u6cd5\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u4f1a\u5bfc\u81f4\u6743\u91cd\u77e9\u9635\u7684\u6269\u5c55\uff0c\u800c\u6743\u91cd\u8870\u51cf\u5219\u53ef\u4ee5\u62b5\u6d88\u8fd9\u79cd\u589e\u957f\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4e58\u6570\u6765\u4f18\u5316\u6743\u91cd\u7684\u89c4\u6a21\uff0c\u4ece\u800c\u6539\u8fdb\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6743\u91cd\u89c4\u8303\u3002</li>\n    <li>\u901a\u8fc7\u4e3a\u6bcf\u4e00\u884c\u548c\u6bcf\u4e00\u5217\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4e58\u6570\uff0c\u8fdb\u4e00\u6b65\u91ca\u653e\u4e86\u6743\u91cd\u7684\u89c4\u6a21\u7ea6\u675f\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0b\u6e38\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684muP\u57fa\u7ebf\uff0c\u5e76\u51cf\u5c11\u4e86\u8c03\u6574\u4e58\u6570\u7684\u8ba1\u7b97\u5f00\u9500\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Weight decay (WD) is commonly used in training large language models to control the growth of weight matrices.</li>\n  <li>This study identifies the equilibrium weight norm created by WD as a problem and proposes a solution using learnable multipliers.</li>\n  <li>A scalar multiplier is attached to the weights, improving performance by adapting to the data instead of relying on a fixed scale.</li>\n  <li>The method introduces learnable multipliers for each row and column of the weight matrix, enhancing flexibility and effectiveness compared to traditional approaches.</li>\n  <li>The approach shows better results than a standard tuning method, requiring less computational effort and improving performance with popular optimizers.</li>\n</ul>"}, "publishedAt": "2026-01-08T07:41:49.000Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png", "numComments": 1, "submittedBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "fullname": "Jingwei Zuo", "name": "JingweiZuo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 37, "isUserFollowing": false}, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05249", "authors": [{"_id": "6960a8ce5b7998385e639615", "user": {"_id": "676ce504027822ead2b5f193", "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg", "isPro": false, "fullname": "YuanKangNeilLee", "user": "NeilLeeNTU", "type": "user"}, "name": "Yuan-Kang Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:24:03.428Z", "hidden": false}, {"_id": "6960a8ce5b7998385e639616", "name": "Kuan-Lin Chen", "hidden": false}, {"_id": "6960a8ce5b7998385e639617", "name": "Chia-Che Chang", "hidden": false}, {"_id": "6960a8ce5b7998385e639618", "user": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "name": "Yu-Lun Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:50:20.802Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"], "publishedAt": "2026-01-08T18:59:55.000Z", "submittedOnDailyAt": "2026-01-09T04:35:57.571Z", "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/", "upvotes": 24, "discussionId": "6960a8ce5b7998385e639619", "projectPage": "https://ntuneillee.github.io/research/rl-awb/", "githubRepo": "https://github.com/BrianChen1120/RL-AWB", "githubRepoAddedBy": "user", "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.", "ai_keywords": ["deep reinforcement learning", "color constancy", "white balance", "statistical algorithms", "illumination estimation", "multi-sensor dataset"], "githubStars": 12, "summary_zh": "<ul>\n    <li>\u591c\u95f4\u767d\u5e73\u8861\u662f\u8ba1\u7b97\u6444\u5f71\u4e2d\u7684\u4e00\u4e2a\u96be\u9898\uff0c\u56e0\u4f4e\u5149\u566a\u58f0\u548c\u590d\u6742\u7684\u5149\u7167\u6761\u4ef6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RL-AWB\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ed3\u5408\u7edf\u8ba1\u65b9\u6cd5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528\u7edf\u8ba1\u7b97\u6cd5\u8fdb\u884c\u591c\u95f4\u573a\u666f\u5904\u7406\uff0c\u7ed3\u5408\u663e\u8457\u7070\u8272\u50cf\u7d20\u68c0\u6d4b\u548c\u5149\u7167\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u9996\u4e2a\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u989c\u8272\u6052\u5e38\u6027\u65b9\u6cd5\uff0c\u6a21\u4eff\u4e13\u4e1a\u767d\u5e73\u8861\u8c03\u6574\u4e13\u5bb6\u7684\u52a8\u6001\u4f18\u5316\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u591a\u4f20\u611f\u5668\u591c\u95f4\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f4e\u5149\u548c\u826f\u597d\u7167\u660e\u7684\u56fe\u50cf\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Nighttime color constancy is hard to achieve because of low light and complex lighting conditions.</li>\n    <li>We introduced RL-AWB, a new method that combines statistical techniques with deep reinforcement learning for adjusting white balance at night.</li>\n    <li>Our approach starts with a statistical method that detects important gray pixels and estimates light conditions for nighttime images.</li>\n    <li>We developed a reinforcement learning system that adjusts settings for each image, similar to how professional experts do it.</li>\n    <li>We also created a new dataset to test our method across different cameras, showing it works well in various lighting situations.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:55.000Z", "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes", "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05106", "authors": [{"_id": "696073d35b7998385e6394c4", "name": "Nuoya Xiong", "hidden": false}, {"_id": "696073d35b7998385e6394c5", "user": {"_id": "64887eb15cf73a16e767b56a", "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg", "isPro": false, "fullname": "Yuhang Zhou", "user": "zyhang1998", "type": "user"}, "name": "Yuhang Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T16:50:49.484Z", "hidden": false}, {"_id": "696073d35b7998385e6394c6", "name": "Hanqing Zeng", "hidden": false}, {"_id": "696073d35b7998385e6394c7", "name": "Zhaorun Chen", "hidden": false}, {"_id": "696073d35b7998385e6394c8", "name": "Furong Huang", "hidden": false}, {"_id": "696073d35b7998385e6394c9", "name": "Shuchao Bi", "hidden": false}, {"_id": "696073d35b7998385e6394ca", "name": "Lizhu Zhang", "hidden": false}, {"_id": "696073d35b7998385e6394cb", "name": "Zhuokai Zhao", "hidden": false}], "publishedAt": "2026-01-08T16:53:16.000Z", "submittedOnDailyAt": "2026-01-09T00:49:52.202Z", "title": "Token-Level LLM Collaboration via FusionRoute", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "upvotes": 23, "discussionId": "696073d45b7998385e6394cc", "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.", "ai_keywords": ["large language models", "token-level collaboration", "multi-LLM collaboration", "lightweight router", "expert selection", "logit addition", "complementary generator", "optimal decoding policy", "model merging", "direct fine-tuning"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u5de8\u5927\u7684\u8d44\u6e90\u6765\u8bad\u7ec3\u548c\u90e8\u7f72\u3002</li>\n    <li>\u8f83\u5c0f\u7684\u9886\u57df\u4e13\u7528\u6a21\u578b\u6548\u7387\u9ad8\uff0c\u4f46\u96be\u4ee5\u8d85\u51fa\u5176\u8bad\u7ec3\u8303\u56f4\u8fdb\u884c\u6cdb\u5316\u3002</li>\n    <li>FusionRoute \u662f\u4e00\u79cd\u65b0\u7684\u591aLLM\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u9009\u62e9\u6700\u5408\u9002\u7684\u4e13\u5bb6\uff0c\u5e76\u4fee\u6b63\u4e13\u5bb6\u7684\u8f93\u51fa\u3002</li>\n    <li>\u8be5\u6846\u67b6\u7684\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5355\u7eaf\u4f9d\u8d56\u4e13\u5bb6\u8f93\u51fa\u7684\u65b9\u5f0f\u5b58\u5728\u5c40\u9650\u6027\u3002</li>\n    <li>FusionRoute \u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u6307\u4ee4\u8ddf\u968f\u7b49\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0e\u9886\u57df\u4e13\u5bb6\u7ade\u4e89\u529b\u5f3a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are powerful but expensive to train and use as general-purpose models.</li>\n    <li>Smaller, domain-specific models are cheaper but struggle to handle tasks outside their training areas.</li>\n    <li>The FusionRoute framework helps combine multiple LLMs by using a lightweight router to choose the best expert model for each step and improve its output.</li>\n    <li>This method goes beyond previous collaboration techniques by adding a trainable generator, which enhances the model's ability to perform well.</li>\n    <li>FusionRoute has shown better performance on various tasks compared to other methods while still being competitive with specialized models.</li>\n</ul>"}, "publishedAt": "2026-01-08T11:53:16.000Z", "title": "Token-Level LLM Collaboration via FusionRoute", "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05241", "authors": [{"_id": "6960775a5b7998385e6394ff", "user": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "isPro": true, "fullname": "Boyang Wang", "user": "HikariDawn", "type": "user"}, "name": "Boyang Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:11.481Z", "hidden": false}, {"_id": "6960775a5b7998385e639500", "name": "Haoran Zhang", "hidden": false}, {"_id": "6960775a5b7998385e639501", "name": "Shujie Zhang", "hidden": false}, {"_id": "6960775a5b7998385e639502", "user": {"_id": "64edb581067fbb625f893628", "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg", "isPro": false, "fullname": "hao", "user": "wuzhi-hao", "type": "user"}, "name": "Jinkun Hao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:09.450Z", "hidden": false}, {"_id": "6960775a5b7998385e639503", "name": "Mingda Jia", "hidden": false}, {"_id": "6960775a5b7998385e639504", "name": "Qi Lv", "hidden": false}, {"_id": "6960775a5b7998385e639505", "user": {"_id": "65de9c6cf68c3d3bac330509", "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg", "isPro": false, "fullname": "Yucheng Mao", "user": "matthewmao", "type": "user"}, "name": "Yucheng Mao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:47.293Z", "hidden": false}, {"_id": "6960775a5b7998385e639506", "user": {"_id": "63f2ec797ddf724fbcc75aee", "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg", "isPro": false, "fullname": "Zhaoyang Lyu", "user": "ZhaoyangLyu", "type": "user"}, "name": "Zhaoyang Lyu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:53.025Z", "hidden": false}, {"_id": "6960775a5b7998385e639507", "user": {"_id": "685d08b9fc7a0ff2f338dbd0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png", "isPro": false, "fullname": "Jia Zeng", "user": "Jia-Zeng", "type": "user"}, "name": "Jia Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:46:06.251Z", "hidden": false}, {"_id": "6960775a5b7998385e639508", "name": "Xudong Xu", "hidden": false}, {"_id": "6960775a5b7998385e639509", "user": {"_id": "65783ee6ee33d547aecc3ffc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg", "isPro": false, "fullname": "Jiangmiao Pang", "user": "Jiangmiao", "type": "user"}, "name": "Jiangmiao Pang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:00.637Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:22.000Z", "submittedOnDailyAt": "2026-01-09T02:54:13.651Z", "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "submittedOnDailyBy": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "isPro": true, "fullname": "Boyang Wang", "user": "HikariDawn", "type": "user"}, "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "upvotes": 19, "discussionId": "6960775a5b7998385e63950a", "projectPage": "https://robovip.github.io/RoboVIP/", "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM", "githubRepoAddedBy": "user", "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.", "ai_keywords": ["image diffusion models", "visual identity prompting", "manipulation data", "vision-language-action models", "visuomotor policy models", "visual identity pool"], "githubStars": 7, "summary_zh": "<ul>\n    <li>\u64cd\u63a7\u6570\u636e\u7684\u591a\u6837\u6027\u3001\u6570\u91cf\u548c\u8d28\u91cf\u5bf9\u8bad\u7ec3\u6709\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u7531\u4e8e\u786c\u4ef6\u548c\u7269\u7406\u8bbe\u7f6e\u7684\u9650\u5236\uff0c\u6536\u96c6\u5927\u89c4\u6a21\u7684\u771f\u5b9e\u64cd\u63a7\u6570\u636e\u975e\u5e38\u56f0\u96be\u3002</li>\n    <li>\u8fd1\u671f\u7814\u7a76\u4f7f\u7528\u6587\u672c\u63d0\u793a\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6765\u589e\u5f3a\u64cd\u63a7\u6570\u636e\uff0c\u4f46\u5e38\u5e38\u5ffd\u89c6\u4e86\u591a\u89c6\u89d2\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u89c2\u5bdf\u7684\u9700\u6c42\u3002</li>\n    <li>\u4ec5\u9760\u6587\u672c\u63d0\u793a\u65e0\u6cd5\u51c6\u786e\u6307\u5b9a\u573a\u666f\u8bbe\u7f6e\uff0c\u56e0\u6b64\u5f15\u5165\u4e86\u89c6\u89c9\u8eab\u4efd\u63d0\u793a\uff0c\u901a\u8fc7\u793a\u4f8b\u56fe\u50cf\u5f15\u5bfc\u751f\u6210\u6240\u9700\u7684\u573a\u666f\u8bbe\u7f6e\u3002</li>\n    <li>\u4f7f\u7528\u589e\u5f3a\u7684\u64cd\u63a7\u6570\u636e\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u548c\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6a21\u578b\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5747\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Collecting real-world manipulation data for robots is challenging due to hardware limitations and diverse environments.</li>\n    <li>Recent methods use image diffusion models to enhance manipulation data by changing backgrounds and objects but often miss the need for multi-view and time-consistent observations.</li>\n    <li>Text prompts alone are not enough to clearly define the scene setup for robots.</li>\n    <li>The study introduces visual identity prompting, which uses example images to better guide the scene generation.</li>\n    <li>The new approach improves robot performance in both simulated and real-world tasks by using the enhanced manipulation data.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:22.000Z", "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png", "numComments": 2, "submittedBy": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "fullname": "Boyang Wang", "name": "HikariDawn", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05167", "authors": [{"_id": "69606c325b7998385e639481", "user": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "name": "Chengsong Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:21.851Z", "hidden": false}, {"_id": "69606c325b7998385e639482", "name": "Tong Zheng", "hidden": false}, {"_id": "69606c325b7998385e639483", "user": {"_id": "65e02d89574e5aa0e9ce3efa", "avatarUrl": "/avatars/2ab152a10b21d81fb1defc726b8e951a.svg", "isPro": false, "fullname": "Langlin Huang", "user": "shrango", "type": "user"}, "name": "Langlin Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:53:24.796Z", "hidden": false}, {"_id": "69606c325b7998385e639484", "name": "Jinyuan Li", "hidden": false}, {"_id": "69606c325b7998385e639485", "name": "Haolin Liu", "hidden": false}, {"_id": "69606c325b7998385e639486", "name": "Jiaxin Huang", "hidden": false}], "publishedAt": "2026-01-08T17:56:16.000Z", "submittedOnDailyAt": "2026-01-09T00:17:45.320Z", "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding", "submittedOnDailyBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.", "upvotes": 19, "discussionId": "69606c325b7998385e639487", "githubRepo": "https://github.com/Chengsong-Huang/RelayLLM", "githubRepoAddedBy": "user", "ai_summary": "RelayLLM enables efficient collaborative reasoning between small and large language models through token-level dynamic invocation, achieving high accuracy with minimal computational overhead.", "ai_keywords": ["Large Language Models", "Small Language Models", "collaborative decoding", "token-level collaboration", "Group Relative Policy Optimization", "policy optimization", "dynamic invocation", "computational efficiency", "reasoning capacity"], "githubStars": 6, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\u3002</li>\n    <li>\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u867d\u7136\u8d44\u6e90\u9ad8\u6548\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002</li>\n    <li>\u73b0\u6709\u7684\u534f\u540c\u65b9\u6cd5\u5982\u7ea7\u8054\u6216\u8def\u7531\uff0c\u6548\u7387\u4f4e\uff0c\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RelayLLM\uff0c\u4e00\u4e2a\u901a\u8fc7\u4ee4\u724c\u7ea7\u534f\u540c\u89e3\u7801\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\u3002</li>\n    <li>RelayLLM\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa49.52%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4e14\u53ea\u8c03\u7528\u4e861.07%\u7684LLM\u751f\u6210\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e8698.2%\u7684\u6210\u672c\u964d\u4f4e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are expensive and slow for complex reasoning tasks.</li>\n    <li>Small Language Models (SLMs) are more efficient but struggle with reasoning.</li>\n    <li>The proposed RelayLLM framework allows SLMs to control when to use LLMs for only important tokens.</li>\n    <li>RelayLLM includes a training method to help the model know when to ask for help.</li>\n    <li>It shows improved accuracy while significantly reducing costs by using LLMs only for a small percentage of tokens.</li>\n</ul>"}, "publishedAt": "2026-01-08T12:56:16.000Z", "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding", "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05167.png", "numComments": 1, "submittedBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "fullname": "Chengsong Huang", "name": "ChengsongHuang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04767", "authors": [{"_id": "69609fe35b7998385e6395ed", "user": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "isPro": false, "fullname": "zongzefang", "user": "zzfoutofspace", "type": "user"}, "name": "Zefang Zong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:07.260Z", "hidden": false}, {"_id": "69609fe35b7998385e6395ee", "user": {"_id": "6462271493f702673bf99c0b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg", "isPro": false, "fullname": "Dingwei Chen", "user": "CuSO4-Chen", "type": "user"}, "name": "Dingwei Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:13.098Z", "hidden": false}, {"_id": "69609fe35b7998385e6395ef", "name": "Yang Li", "hidden": false}, {"_id": "69609fe35b7998385e6395f0", "name": "Qi Yi", "hidden": false}, {"_id": "69609fe35b7998385e6395f1", "name": "Bo Zhou", "hidden": false}, {"_id": "69609fe35b7998385e6395f2", "user": {"_id": "65d5f457d032b44853ae79e4", "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg", "isPro": false, "fullname": "chengming li", "user": "daming8000", "type": "user"}, "name": "Chengming Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:28.358Z", "hidden": false}, {"_id": "69609fe35b7998385e6395f3", "name": "Bo Qian", "hidden": false}, {"_id": "69609fe35b7998385e6395f4", "user": {"_id": "60799b15921db717010c7c8e", "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg", "isPro": false, "fullname": "Peng Chen", "user": "pengchen", "type": "user"}, "name": "Peng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:22.694Z", "hidden": false}, {"_id": "69609fe35b7998385e6395f5", "name": "Jie Jiang", "hidden": false}], "publishedAt": "2026-01-08T09:35:49.000Z", "submittedOnDailyAt": "2026-01-09T04:20:12.513Z", "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search", "submittedOnDailyBy": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "isPro": false, "fullname": "zongzefang", "user": "zzfoutofspace", "type": "user"}, "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "upvotes": 18, "discussionId": "69609fe35b7998385e6395f6", "githubRepo": "https://github.com/zzfoutofspace/ATPO", "githubRepoAddedBy": "user", "ai_summary": "AT\u00b2PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.", "ai_keywords": ["Agentic Reinforcement Learning", "tree search", "Entropy-Guided Tree Expansion", "Turn-wise Credit Assignment", "Agentic Turn-based Policy Optimization", "multi-turn tasks", "policy optimization", "reward propagation", "turn-level learning objective"], "githubStars": 2, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>LLM \u4ee3\u7406\u662f\u5904\u7406\u591a\u8f6e\u4efb\u52a1\u7684\u5f3a\u5927\u7cfb\u7edf\uff0c\u7ed3\u5408\u5185\u90e8\u63a8\u7406\u548c\u5916\u90e8\u5de5\u5177\u4e92\u52a8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86 AT^2PO \u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u591a\u8f6e\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\u3002</li>\n    <li>AT^2PO \u901a\u8fc7\u6811\u7ed3\u6784\u5b9e\u73b0\u6218\u7565\u63a2\u7d22\u548c\u7ec6\u81f4\u7684\u5956\u52b1\u4f20\u64ad\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684\u5b66\u4e60\u76ee\u6807\u4e0e\u4ee3\u7406\u4e92\u52a8\u7684\u81ea\u7136\u51b3\u7b56\u7c92\u5ea6\u76f8\u4e00\u81f4\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u6027\u80fd\u63d0\u9ad8\u4e86\u5e73\u5747 1.84 \u4e2a\u767e\u5206\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are effective at handling complex tasks by combining reasoning and using external tools.</li>\n    <li>The paper introduces AT\u00b2PO, a new framework for improving multi-turn agentic reinforcement learning (RL).</li>\n    <li>AT\u00b2PO tackles issues like limited exploration, sparse rewards, and policy alignment in RL.</li>\n    <li>It uses a tree structure for better exploration and reward assignment, making learning more effective.</li>\n    <li>Experiments show that AT\u00b2PO outperforms existing methods by up to 1.84 percentage points on various benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-08T04:35:49.000Z", "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search", "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png", "numComments": 1, "submittedBy": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "fullname": "zongzefang", "name": "zzfoutofspace", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21815", "authors": [{"_id": "6960e5825b7998385e6396da", "name": "Mengqi He", "hidden": false}, {"_id": "6960e5825b7998385e6396db", "name": "Xinyu Tian", "hidden": false}, {"_id": "6960e5825b7998385e6396dc", "name": "Xin Shen", "hidden": false}, {"_id": "6960e5825b7998385e6396dd", "user": {"_id": "64c71a5647418a0a59e5c7cb", "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg", "isPro": false, "fullname": "Jinhong Ni", "user": "mcleanie", "type": "user"}, "name": "Jinhong Ni", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:14.555Z", "hidden": false}, {"_id": "6960e5825b7998385e6396de", "name": "Shu Zou", "hidden": false}, {"_id": "6960e5825b7998385e6396df", "user": {"_id": "6364187d969bdae89e12b209", "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg", "isPro": false, "fullname": "zhaoyuan yang", "user": "zhaoyuan", "type": "user"}, "name": "Zhaoyuan Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:08.195Z", "hidden": false}, {"_id": "6960e5825b7998385e6396e0", "name": "Jing Zhang", "hidden": false}], "publishedAt": "2025-12-26T01:01:25.000Z", "submittedOnDailyAt": "2026-01-09T09:01:04.074Z", "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models", "submittedOnDailyBy": {"_id": "68a3f14e6dd0e4c74c014853", "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg", "isPro": true, "fullname": "Hubert", "user": "ANUHW", "type": "user"}, "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.", "upvotes": 16, "discussionId": "6960e5835b7998385e6396e1", "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.", "ai_keywords": ["vision-language models", "entropy", "adversarial attacks", "autoregressive generation", "high-entropy tokens", "semantic degradation", "attack success rates", "transferability"], "organization": {"_id": "64ed4ba2453a4b4bef2664c5", "name": "anu-cvml", "fullname": "Australian National University"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u6613\u53d7\u653b\u51fb\u3002</li>\n    <li>\u71b5\u4e0e\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u76f8\u5173\uff0c\u4e4b\u524d\u7684\u653b\u51fb\u5047\u8bbe\u6240\u6709\u8bcd\u6c47\u5bf9\u4e0d\u7a33\u5b9a\u6027\u6709\u76f8\u540c\u5f71\u54cd\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\u53ea\u6709\u7ea620%\u7684\u9ad8\u71b5\u8bcd\u6c47\u5bf9\u751f\u6210\u7ed3\u679c\u5f71\u54cd\u5de8\u5927\u3002</li>\n    <li>\u9488\u5bf9\u8fd9\u4e9b\u5173\u952e\u4f4d\u7f6e\u8fdb\u884c\u653b\u51fb\uff0c\u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u8d44\u6e90\u5b9e\u73b0\u663e\u8457\u7684\u6548\u679c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u71b5\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u9ad8\u4e14\u63ed\u793a\u4e86VLM\u7684\u5b89\u5168\u9690\u60a3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) are effective but can be easily attacked.</li>\n    <li>Entropy indicates how uncertain a model is, and it relates to the model's reliability.</li>\n    <li>Only about 20% of the high-entropy tokens are crucial for generating outputs, not all tokens are equally important.</li>\n    <li>Focusing on these key tokens in attacks can cause significant harm while using fewer resources.</li>\n    <li>Selective attacks can change 35-49% of normal outputs into harmful ones, highlighting a serious safety risk in VLMs.</li>\n</ul>"}, "publishedAt": "2025-12-25T20:01:25.000Z", "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models", "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png", "numComments": 1, "submittedBy": {"_id": "68a3f14e6dd0e4c74c014853", "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg", "fullname": "Hubert", "name": "ANUHW", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "64ed4ba2453a4b4bef2664c5", "name": "anu-cvml", "fullname": "Australian National University"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05175", "authors": [{"_id": "69606c015b7998385e639468", "user": {"_id": "65803d64defc9c0d30db4f88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65803d64defc9c0d30db4f88/gVxJaB8vSUFj4r0vnTVbP.jpeg", "isPro": true, "fullname": "Shuming Liu", "user": "sming256", "type": "user"}, "name": "Shuming Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:23.773Z", "hidden": false}, {"_id": "69606c015b7998385e639469", "user": {"_id": "64403daae44f30a72323e4ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png", "isPro": false, "fullname": "mingchen zhuge", "user": "tjpxiaoming", "type": "user"}, "name": "Mingchen Zhuge", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:42.678Z", "hidden": false}, {"_id": "69606c015b7998385e63946a", "user": {"_id": "64d49ef30f76abaf363b88d6", "avatarUrl": "/avatars/93b5cc51305ac88198cea1dad8104db2.svg", "isPro": false, "fullname": "Changsheng Zhao", "user": "mikezhaocs", "type": "user"}, "name": "Changsheng Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:05.174Z", "hidden": false}, {"_id": "69606c015b7998385e63946b", "name": "Jun Chen", "hidden": false}, {"_id": "69606c015b7998385e63946c", "user": {"_id": "6364acf56fa33d2f59397c59", "avatarUrl": "/avatars/f642a7dcc098f50fef865f30f93a25c3.svg", "isPro": false, "fullname": "Lemeng Wu", "user": "klight", "type": "user"}, "name": "Lemeng Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:12.390Z", "hidden": false}, {"_id": "69606c015b7998385e63946d", "name": "Zechun Liu", "hidden": false}, {"_id": "69606c015b7998385e63946e", "user": {"_id": "66f6e80e7db9927533cbd3e1", "avatarUrl": "/avatars/f169a3cb4f48781e38cc111a35741d2d.svg", "isPro": false, "fullname": "Chenchen Zhu", "user": "zcckernel", "type": "user"}, "name": "Chenchen Zhu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:32.413Z", "hidden": false}, {"_id": "69606c015b7998385e63946f", "name": "Zhipeng Cai", "hidden": false}, {"_id": "69606c015b7998385e639470", "name": "Chong Zhou", "hidden": false}, {"_id": "69606c015b7998385e639471", "name": "Haozhe Liu", "hidden": false}, {"_id": "69606c015b7998385e639472", "name": "Ernie Chang", "hidden": false}, {"_id": "69606c015b7998385e639473", "user": {"_id": "645417e98617183806210cfc", "avatarUrl": "/avatars/7a3ba5b0c38878db51da7091fed15f99.svg", "isPro": false, "fullname": "Saksham Suri", "user": "sakshams", "type": "user"}, "name": "Saksham Suri", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:08.056Z", "hidden": false}, {"_id": "69606c015b7998385e639474", "name": "Hongyu Xu", "hidden": false}, {"_id": "69606c015b7998385e639475", "name": "Qi Qian", "hidden": false}, {"_id": "69606c015b7998385e639476", "name": "Wei Wen", "hidden": false}, {"_id": "69606c015b7998385e639477", "user": {"_id": "6570032714fa8cfccd39c6ad", "avatarUrl": "/avatars/f4b6140e21db5d18241dcc9b2e94ae33.svg", "isPro": false, "fullname": "Balakrishnan Varadarajan", "user": "balakv", "type": "user"}, "name": "Balakrishnan Varadarajan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:15.446Z", "hidden": false}, {"_id": "69606c015b7998385e639478", "name": "Zhuang Liu", "hidden": false}, {"_id": "69606c015b7998385e639479", "name": "Hu Xu", "hidden": false}, {"_id": "69606c015b7998385e63947a", "user": {"_id": "63166336894404e2506b8811", "avatarUrl": "/avatars/ec51a1dd86511127cf83afd4bf7c0f52.svg", "isPro": false, "fullname": "Florian Bordes", "user": "Fbordes", "type": "user"}, "name": "Florian Bordes", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:21.715Z", "hidden": false}, {"_id": "69606c015b7998385e63947b", "name": "Raghuraman Krishnamoorthi", "hidden": false}, {"_id": "69606c015b7998385e63947c", "user": {"_id": "6808bf97ffadd78ec71cb721", "avatarUrl": "/avatars/9adca3142c06b8f69889fcbe85fa374d.svg", "isPro": false, "fullname": "Bernard Ghanem", "user": "bernardghanem", "type": "user"}, "name": "Bernard Ghanem", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:31.147Z", "hidden": false}, {"_id": "69606c015b7998385e63947d", "name": "Vikas Chandra", "hidden": false}, {"_id": "69606c015b7998385e63947e", "user": {"_id": "65304b62e7535baecd85d080", "avatarUrl": "/avatars/6e546c7d1414bd92c5a7c8d8c404de92.svg", "isPro": false, "fullname": "Yunyang Xiong", "user": "yunyangx", "type": "user"}, "name": "Yunyang Xiong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:41.516Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"], "publishedAt": "2026-01-08T18:00:59.000Z", "submittedOnDailyAt": "2026-01-09T00:16:39.681Z", "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "upvotes": 15, "discussionId": "69606c015b7998385e63947f", "projectPage": "https://ivul-kaust.github.io/projects/videoauto-r1/", "githubRepo": "https://github.com/IVUL-KAUST/VideoAuto-R1/", "githubRepoAddedBy": "user", "ai_summary": "VideoAuto-R1 framework employs a reason-when-necessary strategy for video understanding, using a Thinking Once, Answering Twice training paradigm with verifiable rewards and confidence-based reasoning activation during inference.", "ai_keywords": ["Chain-of-thought reasoning", "multimodal large language models", "video understanding", "RL-trained video models", "VideoAuto-R1", "Thinking Once Answering Twice", "verifiable rewards", "confidence score", "perception-oriented tasks", "reasoning-intensive tasks"], "githubStars": 11, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff08CoT\uff09\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5fc5\u8981\u6027\u548c\u4f18\u52bf\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u89c6\u9891\u6a21\u578b\uff0c\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u7684\u6548\u679c\u901a\u5e38\u4e0eCoT\u76f8\u5f53\uff0c\u751a\u81f3\u66f4\u597d\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVideoAuto-R1\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u91c7\u7528\u201c\u5fc5\u8981\u65f6\u63a8\u7406\u201d\u7684\u7b56\u7565\u3002</li>\n    <li>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8be5\u6846\u67b6\u5148\u751f\u6210\u521d\u6b65\u7b54\u6848\uff0c\u518d\u8fdb\u884c\u63a8\u7406\uff0c\u6700\u540e\u8f93\u51fa\u5ba1\u6838\u540e\u7684\u7b54\u6848\uff0c\u63d0\u5347\u4e86\u6548\u7387\u3002</li>\n    <li>VideoAuto-R1\u5728\u89c6\u9891\u95ee\u7b54\u548c\u76ee\u6807\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u65b0\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5e73\u5747\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11\u4e86\u7ea63.3\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Chain-of-thought reasoning (CoT) is useful for video understanding tasks but may not always be better than direct answers.</li>\n    <li>In experiments, direct answering sometimes performed as well as or better than CoT, even though CoT is more complex.</li>\n    <li>The new framework called VideoAuto-R1 uses a \"reason-when-necessary\" strategy for better efficiency.</li>\n    <li>VideoAuto-R1 first generates an answer, then reasons, and finally provides a reviewed answer, leading to a significant reduction in response length.</li>\n    <li>It shows that reasoning is helpful for complex tasks, but not needed for simpler ones, indicating a balance is possible.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:00:59.000Z", "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05175.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05138", "authors": [{"_id": "69606cd45b7998385e639489", "user": {"_id": "6528fe394513680346a500ed", "avatarUrl": "/avatars/3021a3be7d900e3928cdc375e2a2bf09.svg", "isPro": false, "fullname": "Sixiao Zheng (SII)", "user": "sxzheng", "type": "user"}, "name": "Sixiao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:19.760Z", "hidden": false}, {"_id": "69606cd45b7998385e63948a", "name": "Minghao Yin", "hidden": false}, {"_id": "69606cd45b7998385e63948b", "name": "Wenbo Hu", "hidden": false}, {"_id": "69606cd45b7998385e63948c", "name": "Xiaoyu Li", "hidden": false}, {"_id": "69606cd45b7998385e63948d", "name": "Ying Shan", "hidden": false}, {"_id": "69606cd45b7998385e63948e", "user": {"_id": "6409fcc8f3dabf93824c84c6", "avatarUrl": "/avatars/dd8fd579630e50ba3058c3829604478e.svg", "isPro": false, "fullname": "YANWEI", "user": "yanweifuture", "type": "user"}, "name": "Yanwei Fu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:43.763Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"], "publishedAt": "2026-01-08T17:28:52.000Z", "submittedOnDailyAt": "2026-01-09T00:21:14.757Z", "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.", "upvotes": 11, "discussionId": "69606cd45b7998385e63948f", "ai_summary": "VerseCrafter is a 4D-aware video world model that enables unified control over camera and object dynamics through 4D geometric control representation and video diffusion models.", "ai_keywords": ["video world models", "4D geometric control", "point cloud", "3D Gaussian trajectories", "video diffusion model", "view-consistent videos", "automatic data engine", "in-the-wild videos"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u4e16\u754c\u6a21\u578b\u65e8\u5728\u6a21\u62df\u771f\u5b9e\u73af\u5883\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u63a7\u5236\u6444\u50cf\u673a\u548c\u591a\u7269\u4f53\u8fd0\u52a8\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86VerseCrafter\uff0c\u8fd9\u662f\u4e00\u4e2a\u652f\u63014D\u7684\u89c6\u542c\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u63a7\u5236\u6444\u50cf\u673a\u548c\u7269\u4f53\u7684\u52a8\u6001\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u65b0\u76844D\u51e0\u4f55\u63a7\u5236\u8868\u793a\uff0c\u7ed3\u5408\u9759\u6001\u80cc\u666f\u70b9\u4e91\u548c\u6bcf\u4e2a\u7269\u4f53\u76843D\u9ad8\u65af\u8f68\u8ff9\u3002</li>\n    <li>\u6b64\u8868\u793a\u65b9\u6cd5\u7075\u6d3b\uff0c\u80fd\u591f\u6355\u6349\u7269\u4f53\u7684\u8fd0\u52a8\u8def\u5f84\u548c\u5176\u968f\u65f6\u95f4\u53d8\u5316\u7684\u4e09\u7ef4\u5360\u7528\u6982\u7387\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u6570\u636e\u5f15\u64ce\uff0c\u4ece\u5b9e\u9645\u89c6\u9891\u4e2d\u63d0\u53d6\u6240\u9700\u76844D\u63a7\u5236\uff0c\u4ee5\u8bad\u7ec3\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VerseCrafter is a new video world model that improves how we control camera and object movements in videos.</li>\n    <li>It uses a unique 4D representation that shows both the background and the movement of objects over time.</li>\n    <li>This method captures an object's path and its possible positions instead of just using simple boxes.</li>\n    <li>VerseCrafter helps create high-quality videos that match the desired movements and perspectives.</li>\n    <li>To tackle the problem of limited training data, it has a system that automatically gathers the needed 4D information from existing videos.</li>\n</ul>"}, "publishedAt": "2026-01-08T12:28:52.000Z", "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control", "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05138.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u5e76\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u5f3a\u5316\u5b66\u4e60(RL)\u5f00\u59cb\u91c7\u7528\u591a\u79cd\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u5b9e\u73b0\u8fd9\u4e9b\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u4f1a\u5bfc\u81f4\u4e0d\u540c\u5956\u52b1\u7ec4\u5408\u7684\u6548\u679c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316(GDPO)\uff0c\u65e8\u5728\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u7684\u5f52\u4e00\u5316\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0cGDPO\u5728\u51c6\u786e\u6027\u548c\u9075\u5faa\u7ea6\u675f\u65b9\u9762\u90fd\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate responses and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to help models learn these preferred behaviors.</li>\n    <li>Current methods like Group Relative Policy Optimization (GRPO) can lead to problems, making the training less effective.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves training by keeping the differences between rewards clearer.</li>\n    <li>GDPO shows better performance than GRPO in three tasks: tool calling, math reasoning, and coding reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03252", "authors": [{"_id": "695dc956c03d6d81e4399ea4", "name": "Hao Yu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea5", "user": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "name": "Haotong Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:04.783Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea6", "name": "Jiawei Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea7", "name": "Jiaxin Li", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea8", "name": "Yida Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea9", "user": {"_id": "6791a6c19ce382eae861ed61", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg", "isPro": false, "fullname": "Xueyang Zhang", "user": "zhangxueyang001", "type": "user"}, "name": "Xueyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:39.946Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399eaa", "name": "Yue Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399eab", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "695dc956c03d6d81e4399eac", "name": "Ruizhen Hu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ead", "user": {"_id": "62986ca2b58e71e2ac9b8f01", "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg", "isPro": false, "fullname": "Sida Peng", "user": "pengsida", "type": "user"}, "name": "Sida Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:12.074Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "publishedAt": "2026-01-06T18:57:06.000Z", "submittedOnDailyAt": "2026-01-07T00:26:37.060Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "submittedOnDailyBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "upvotes": 71, "discussionId": "695dc956c03d6d81e4399eae", "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.", "ai_keywords": ["neural implicit fields", "local implicit decoder", "continuous 2D coordinates", "arbitrary-resolution depth estimation", "synthetic benchmark", "4K synthetic benchmark", "novel view synthesis", "viewpoint shifts"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u5728\u79bb\u6563\u7684\u56fe\u50cf\u7f51\u683c\u4e0a\u9884\u6d4b\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u6062\u590d\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86InfiniDepth\uff0c\u5b83\u5c06\u6df1\u5ea6\u8868\u793a\u4e3a\u795e\u7ecf\u9690\u5f0f\u573a\uff0c\u901a\u8fc7\u5c40\u90e8\u9690\u5f0f\u89e3\u7801\u5668\u53ef\u4ee5\u5728\u8fde\u7eed\u76842D\u5750\u6807\u4e0a\u67e5\u8be2\u6df1\u5ea6\u3002</li>\n    <li>InfiniDepth\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u548c\u7ec6\u81f4\u6df1\u5ea6\u4f30\u8ba1\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u5176\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76844K\u5408\u6210\u57fa\u51c6\uff0c\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u7684\u6e38\u620f\uff0c\u573a\u666f\u591a\u6837\u4e14\u7ec6\u8282\u4e30\u5bcc\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cInfiniDepth\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u8282\u4e30\u5bcc\u7684\u533a\u57df\u8868\u73b0\u7a81\u51fa\u3002\u5b83\u8fd8\u63d0\u9ad8\u4e86\u5728\u5927\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u65b0\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current depth estimation methods can only predict depth on fixed image grids, limiting their flexibility and detail.</li>\n    <li>InfiniDepth uses neural implicit fields to represent depth, allowing depth to be queried at any point for better resolution and detail.</li>\n    <li>The authors created a high-quality 4K benchmark using scenes from five different games to test their method.</li>\n    <li>InfiniDepth outperforms other methods in both synthetic and real-world tests, especially in areas with fine details.</li>\n    <li>It also improves the quality of new views from different angles, resulting in fewer errors and artifacts.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:57:06.000Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png", "numComments": 8, "submittedBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "fullname": "Haotong Lin", "name": "haotongl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02151", "authors": [{"_id": "695f2d8a5fa3847525c41f8d", "user": {"_id": "6768c97367e4b4606a3c9cec", "avatarUrl": "/avatars/5ddafe7a05828366f66c79072556f370.svg", "isPro": false, "fullname": "diaomuxi", "user": "diaomuxi", "type": "user"}, "name": "Muxi Diao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:56.507Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8e", "user": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "name": "Lele Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:51.246Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8f", "user": {"_id": "64c0f972d76592ba899c2c9c", "avatarUrl": "/avatars/d6940beb135f99241c6fb2cf0e8ccdbe.svg", "isPro": false, "fullname": "GongWuxuan", "user": "Wuxuan-Gong", "type": "user"}, "name": "Wuxuan Gong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:19.470Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f90", "name": "Yutong Zhang", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f91", "user": {"_id": "64fbd4e69a62bb2791b3a665", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fbd4e69a62bb2791b3a665/ZEMtU8O0z98ryeRCG3l_K.jpeg", "isPro": false, "fullname": "Zhonghao Yan", "user": "zzzyzh", "type": "user"}, "name": "Zhonghao Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:53.904Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f92", "name": "Yufei Han", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f93", "user": {"_id": "67f4a56928cbc4f2f75c008d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qoX8HT0JsjqW2OQoENSvg.png", "isPro": false, "fullname": "Kongming Liang", "user": "KongmingLiang", "type": "user"}, "name": "Kongming Liang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:38.812Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f94", "name": "Weiran Xu", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f95", "name": "Zhanyu Ma", "hidden": false}], "publishedAt": "2026-01-05T14:28:17.000Z", "submittedOnDailyAt": "2026-01-08T01:42:04.476Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "submittedOnDailyBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "upvotes": 64, "discussionId": "695f2d8b5fa3847525c41f96", "projectPage": "https://ymxyll.github.io/EAFT/", "githubRepo": "https://github.com/PRIS-CV/EAFT", "githubRepoAddedBy": "user", "ai_summary": "Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.", "ai_keywords": ["supervised fine-tuning", "catastrophic forgetting", "on-policy reinforcement learning", "distributional gap", "Confident Conflicts", "token-level entropy", "epistemic uncertainty", "knowledge conflict", "gradient updates", "downstream performance"], "githubStars": 18, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "summary_zh": "<ul>\n    <li>\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5e38\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u6709\u6548\u4fdd\u7559\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0cSFT\u548cRL\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5916\u90e8\u76d1\u7763\u548c\u5185\u90e8\u4fe1\u5ff5\u4e4b\u95f4\u4e0d\u5339\u914d\u3002</li>\n    <li>\u8fd9\u79cd\u4e0d\u5339\u914d\u4f1a\u4ea7\u751f\u201c\u81ea\u4fe1\u51b2\u7a81\u201d\u4ee4\u724c\uff0c\u6a21\u578b\u5bf9\u81ea\u5df1\u9884\u6d4b\u975e\u5e38\u81ea\u4fe1\uff0c\u4f46\u5374\u88ab\u8feb\u5b66\u4e60\u4e0d\u540c\u7684\u771f\u76f8\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u71b5\u81ea\u9002\u5e94\u5fae\u8c03\uff08EAFT\uff09\uff0c\u5b83\u5229\u7528\u4ee4\u724c\u7ea7\u71b5\u6765\u533a\u5206\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u533b\u5b66\u548c\u667a\u80fd\u4ee3\u7406\u7b49\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cEAFT\u5728\u4fdd\u6301\u6807\u51c6SFT\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u7528\u80fd\u529b\u7684\u9000\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Supervised Fine-Tuning (SFT) can lead to \"catastrophic forgetting\" when adapting to new domains.</li>\n    <li>On-policy Reinforcement Learning (RL) helps maintain the model's general abilities better than SFT.</li>\n    <li>The difference arises because RL aligns with the model's beliefs, while SFT forces it to fit external instructions, causing \"Confident Conflicts.\"</li>\n    <li>These conflicts occur when the model is sure of its prediction but has to learn a different truth, leading to harmful updates.</li>\n    <li>Entropy-Adaptive Fine-Tuning (EAFT) is proposed to help the model learn from uncertain data and avoid conflicts, and it shows strong performance in various domains without losing general capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-05T09:28:17.000Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02151.png", "numComments": 6, "submittedBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "fullname": "\u6768\u4e50\u4e50", "name": "ssl-asuka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03509", "authors": [{"_id": "695fbc7d5b7998385e639349", "name": "Haochen Shi", "hidden": false}, {"_id": "695fbc7d5b7998385e63934a", "name": "Xingdi Yuan", "hidden": false}, {"_id": "695fbc7d5b7998385e63934b", "name": "Bang Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "publishedAt": "2026-01-07T01:43:25.000Z", "submittedOnDailyAt": "2026-01-08T11:50:05.687Z", "title": "Evolving Programmatic Skill Networks", "submittedOnDailyBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "isPro": false, "fullname": "Bang Liu", "user": "Bang-UdeM-Mila", "type": "user"}, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "upvotes": 54, "discussionId": "695fbc7e5b7998385e63934c", "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.", "ai_keywords": ["Programmatic Skill Network", "executable symbolic programs", "skill composition", "structured fault localization", "progressive optimization", "maturity-aware update gating", "canonical structural refactoring", "rollback validation", "neural network training", "skill reuse", "rapid adaptation", "generalization"], "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u6301\u7eed\u5b66\u4e60\u6280\u80fd\u7684\u8fc7\u7a0b\uff0c\u4ee3\u7406\u9700\u8981\u5efa\u7acb\u548c\u4f18\u5316\u53ef\u6267\u884c\u6280\u80fd\u5e93\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u7a0b\u5e8f\u5316\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\u6846\u67b6\uff0c\u4f7f\u5f97\u6280\u80fd\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u6267\u884c\u7684\u7b26\u53f7\u7a0b\u5e8f\u5f62\u6210\u4e00\u4e2a\u4e0d\u65ad\u53d1\u5c55\u7684\u7f51\u7edc\u3002</li>\n    <li>PSN\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a\u7ed3\u6784\u5316\u6545\u969c\u5b9a\u4f4d\u3001\u6210\u719f\u5ea6\u610f\u8bc6\u7684\u4f18\u5316\u66f4\u65b0\u3001\u4ee5\u53ca\u5728\u56de\u6eda\u9a8c\u8bc1\u4e0b\u7684\u7ed3\u6784\u91cd\u6784\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cPSN\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6280\u80fd\u91cd\u7528\u3001\u5feb\u901f\u9002\u5e94\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u8ba1\u5212\u5f00\u6e90\u8be5\u4ee3\u7801\uff0c\u4ee5\u4fbf\u5176\u4ed6\u4eba\u4f7f\u7528\u548c\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>We explore how agents can continuously learn and improve skills in changing environments.</li>\n    <li>We created a system called Programmatic Skill Network (PSN) that allows skills to be organized and reused like a library of programs.</li>\n    <li>PSN uses three key features: finding faults in skills, optimizing skills based on their reliability, and restructuring skills to keep the system efficient.</li>\n    <li>Our research shows that PSN's learning process is similar to how neural networks are trained.</li>\n    <li>Tests show that PSN can effectively reuse skills, adapt quickly, and perform well across various tasks.</li>\n</ul>"}, "publishedAt": "2026-01-06T20:43:25.000Z", "title": "Evolving Programmatic Skill Networks", "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png", "numComments": 1, "submittedBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "fullname": "Bang Liu", "name": "Bang-UdeM-Mila", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01554", "authors": [{"_id": "695dcda5c03d6d81e4399eb8", "name": "MOSI. AI", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eb9", "name": "Donghua Yu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eba", "name": "Zhengyuan Lin", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebb", "user": {"_id": "660c345da15ab85523ad00d1", "avatarUrl": "/avatars/b0bfdee89a6c62ff12140b9e85de499a.svg", "isPro": false, "fullname": "Chen Yang", "user": "kiiic", "type": "user"}, "name": "Chen Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:58.894Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebc", "name": "Yiyang Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebd", "name": "Hanfu Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebe", "name": "Jingqi Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebf", "name": "Ke Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec0", "name": "Liwei Fan", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec1", "name": "Yi Jiang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec2", "name": "Jie Zhu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec3", "name": "Muchen Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec4", "name": "Wenxuan Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec5", "name": "Yang Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec6", "user": {"_id": "6443f7bf1bc692d87b25e234", "avatarUrl": "/avatars/fa9e62d96d0691a9a48e3db499a61557.svg", "isPro": false, "fullname": "Xu Zhe", "user": "Phospheneser", "type": "user"}, "name": "Zhe Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:55.950Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec7", "name": "Yitian Gong", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec8", "name": "Yuqian Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec9", "name": "Wenbo Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eca", "user": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "name": "Zhaoye Fei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:10.124Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecb", "user": {"_id": "695757e4fd9dc6e9bac27935", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_uZEu4oOlKJVYqrG763Z-.jpeg", "isPro": false, "fullname": "aa", "user": "qinyuancheng", "type": "user"}, "name": "Qinyuan Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:03.749Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecc", "name": "Shimin Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecd", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:50.004Z", "hidden": false}], "publishedAt": "2026-01-04T15:01:10.000Z", "submittedOnDailyAt": "2026-01-07T00:52:16.123Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "submittedOnDailyBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "upvotes": 45, "discussionId": "695dcda6c03d6d81e4399ece", "projectPage": "https://mosi.cn/models/moss-transcribe-diarize", "ai_summary": "A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.", "ai_keywords": ["multimodal large language model", "end-to-end paradigm", "speaker diarization", "time-stamped transcription", "context window", "robust generalization"], "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>SATS\uff08\u53d1\u8a00\u4eba\u6807\u6ce8\u65f6\u95f4\u6233\u8f6c\u5f55\uff09\u65e8\u5728\u51c6\u786e\u8f6c\u5f55\u53d1\u8a00\u5185\u5bb9\u5e76\u786e\u5b9a\u6bcf\u4f4d\u53d1\u8a00\u8005\u7684\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u4f1a\u8bae\u8f6c\u5f55\u3002</li>\n    <li>\u73b0\u6709\u7684SATS\u7cfb\u7edf\u901a\u5e38\u4e0d\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\uff0c\u5e76\u4e14\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u957f\u7a0b\u53d1\u8a00\u8005\u8bb0\u5fc6\u548c\u4e0d\u80fd\u8f93\u51fa\u65f6\u95f4\u6233\u7b49\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MOSS Transcribe Diarize\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5730\u6267\u884c\u53d1\u8a00\u4eba\u6807\u6ce8\u65f6\u95f4\u6233\u8f6c\u5f55\u3002</li>\n    <li>MOSS Transcribe Diarize\u7ecf\u8fc7\u5927\u91cf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u652f\u6301\u6700\u591a90\u5206\u949f\u7684\u8f93\u5165\uff0c\u5177\u6709128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002</li>\n    <li>\u5728\u5404\u7c7b\u8bc4\u4f30\u4e2d\uff0c\u5b83\u5728\u591a\u4e2a\u516c\u5171\u548c\u5185\u90e8\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5546\u4e1a\u7cfb\u7edf\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>MOSS Transcribe Diarize is a new system for transcribing speech that identifies who is speaking and when they are speaking.</li>\n    <li>It addresses limitations of existing systems, such as short context windows and poor memory for long discussions.</li>\n    <li>The system can handle long audio inputs (up to 90 minutes) and provides accurate timestamps for each speaker.</li>\n    <li>It has been trained on a large amount of real-world data, making it effective and adaptable.</li>\n    <li>MOSS Transcribe Diarize has been shown to perform better than leading commercial transcription systems in various tests.</li>\n</ul>"}, "publishedAt": "2026-01-04T10:01:10.000Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01554.png", "numComments": 2, "submittedBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "fullname": "Zhaoye Fei", "name": "ngc7293", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02204", "authors": [{"_id": "695c7d0d6aa73bc11f091433", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091434", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:57.686Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091435", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091436", "name": "Hang Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091437", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091438", "name": "Yongsheng Dong", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091439", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143a", "name": "Xian Li", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143b", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143c", "user": {"_id": "6344dcb1cd37e44d9ed46508", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg", "isPro": false, "fullname": "Yi Jiang", "user": "JiangYi", "type": "user"}, "name": "Yi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:55.158Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143d", "name": "Hu Ye", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143e", "name": "Bo Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143f", "name": "Yiming Gao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091440", "name": "Peng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091441", "name": "Akide Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091442", "name": "Zhipeng Yang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091443", "name": "Qili Deng", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091444", "name": "Linjie Xing", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091445", "name": "Jiyang Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091446", "name": "Zhao Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091447", "name": "Yang Zhou", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091448", "name": "Mingcong Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091449", "name": "Yi Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144a", "name": "Qian He", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144b", "name": "Xiwei Hu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144c", "name": "Zhongqi Qi", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144d", "name": "Jie Shao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144e", "name": "Zhiye Fu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144f", "name": "Shuai Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091450", "name": "Fangmin Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091451", "name": "Xuezhi Chai", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091452", "name": "Zhihua Wu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091453", "name": "Yitong Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091454", "name": "Zehuan Yuan", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091455", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091456", "name": "Xinglong Wu", "hidden": false}], "publishedAt": "2026-01-05T15:27:04.000Z", "submittedOnDailyAt": "2026-01-06T00:52:35.953Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "upvotes": 45, "discussionId": "695c7d0d6aa73bc11f091457", "githubRepo": "https://github.com/ByteVisionLab/NextFlow", "githubRepoAddedBy": "user", "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.", "ai_keywords": ["decoder-only autoregressive transformer", "interleaved text-image discrete tokens", "unified vision representation", "multimodal understanding", "multimodal generation", "next-token prediction", "next-scale prediction", "raster-scan methods", "visual generation", "prefix-tuning strategy", "reinforcement learning", "diffusion baselines"], "githubStars": 60, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>NextFlow \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u8bad\u7ec3\u4e86 6 \u4e07\u4ebf\u4e2a\u6587\u672c-\u56fe\u50cf\u79bb\u6563\u6807\u8bb0\u3002</li>\n    <li>\u5b83\u80fd\u591f\u8fdb\u884c\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u5305\u62ec\u56fe\u50cf\u7f16\u8f91\u3001\u4ea4\u9519\u5185\u5bb9\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5bf9\u4e8e\u6587\u672c\uff0c\u7ee7\u7eed\u4f7f\u7528\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff1b\u5bf9\u4e8e\u89c6\u89c9\u751f\u6210\uff0c\u91c7\u7528\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u9884\u6d4b\uff0c\u8fd9\u6837\u53ef\u4ee5\u5feb\u901f\u751f\u6210 1024x1024 \u50cf\u7d20\u7684\u56fe\u50cf\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u8bad\u7ec3\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u5c3a\u5ea6\u751f\u6210\u7684\u4e0d\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cNextFlow \u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u89c6\u89c9\u8d28\u91cf\u63a5\u8fd1\u4e13\u4e1a\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NextFlow is a new AI model that can understand and generate both text and images using a single system.</li>\n    <li>It was trained on 6 trillion combined text and image tokens, allowing it to perform tasks like image editing and video generation.</li>\n    <li>The model uses a unique method for generating images, producing high-quality 1024x1024 images in just 5 seconds.</li>\n    <li>NextFlow improves stability in generating images at multiple scales and includes a new training method.</li>\n    <li>It outperforms other unified models and competes well with specialized models in terms of visual quality.</li>\n</ul>"}, "publishedAt": "2026-01-05T10:27:04.000Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01739", "authors": [{"_id": "695c72346aa73bc11f0913bf", "user": {"_id": "6044fd39e6aa3e130cb92867", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg", "isPro": false, "fullname": "Eunbi Choi", "user": "unbiarirang", "type": "user"}, "name": "Eunbi Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:17.472Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c0", "user": {"_id": "64d31ca9465b6039259838df", "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg", "isPro": false, "fullname": "kibong choi", "user": "bongchoi", "type": "user"}, "name": "Kibong Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:11.322Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c1", "name": "Seokhee Hong", "hidden": false}, {"_id": "695c72346aa73bc11f0913c2", "user": {"_id": "63c50e590c24c8b53958f75e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png", "isPro": false, "fullname": "Junwon Hwang", "user": "nuxlear", "type": "user"}, "name": "Junwon Hwang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:09.333Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c3", "name": "Hyojin Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913c4", "user": {"_id": "66a9e066a203add977948988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg", "isPro": false, "fullname": "hyunjik.jo", "user": "switiz87", "type": "user"}, "name": "Hyunjik Jo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:13.551Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c5", "name": "Joonkee Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c6", "name": "Seonghwan Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c7", "name": "Soyeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c8", "name": "Sunkyoung Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c9", "name": "Yireun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ca", "name": "Yongil Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913cb", "name": "Haeju Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cc", "name": "Jinsik Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cd", "name": "Kyungmin Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ce", "name": "Sangha Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913cf", "name": "Heuiyeen Yeen", "hidden": false}, {"_id": "695c72346aa73bc11f0913d0", "name": "Hwan Chang", "hidden": false}, {"_id": "695c72346aa73bc11f0913d1", "name": "Stanley Jungkyu Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d2", "name": "Yejin Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d3", "name": "Jiwon Ham", "hidden": false}, {"_id": "695c72346aa73bc11f0913d4", "name": "Kijeong Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913d5", "name": "Geunyeong Jeong", "hidden": false}, {"_id": "695c72346aa73bc11f0913d6", "name": "Gerrard Jeongwon Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d7", "name": "Yonghwan Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d8", "name": "Jiyeon Jung", "hidden": false}, {"_id": "695c72346aa73bc11f0913d9", "name": "Naeun Kang", "hidden": false}, {"_id": "695c72346aa73bc11f0913da", "name": "Dohoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913db", "name": "Euisoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dc", "name": "Hayeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dd", "name": "Hyosang Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913de", "name": "Hyunseo Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913df", "name": "Jieun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e0", "name": "Minu Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e1", "name": "Myoungshin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e2", "name": "Unsol Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e3", "name": "Youchul Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e4", "name": "YoungJin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e5", "name": "Chaeeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e6", "name": "Chaeyoon Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e7", "user": {"_id": "6399ab9e92e12136b99ef60e", "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg", "isPro": false, "fullname": "Changhun Lee", "user": "xvyaward", "type": "user"}, "name": "Changhun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:15.420Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913e8", "name": "Dahm Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e9", "name": "Edward Hwayoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ea", "name": "Honglak Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913eb", "name": "Jinsang Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ec", "name": "Jiyoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ed", "name": "Sangeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ee", "name": "Seungwon Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ef", "name": "Solji Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f0", "name": "Woohyung Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f1", "name": "Chanwoo Moon", "hidden": false}, {"_id": "695c72346aa73bc11f0913f2", "name": "Jaewoo Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f3", "name": "Jinho Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f4", "name": "Yongmin Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f5", "name": "Hyerin Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f6", "name": "Wooseok Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f7", "name": "Yongwoo Song", "hidden": false}, {"_id": "695c72346aa73bc11f0913f8", "name": "Sejong Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913f9", "name": "Sihoon Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913fa", "name": "Chang En Yea", "hidden": false}, {"_id": "695c72346aa73bc11f0913fb", "name": "Sihyuk Yi", "hidden": false}, {"_id": "695c72346aa73bc11f0913fc", "name": "Chansik Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fd", "name": "Dongkeun Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fe", "name": "Sangyeon Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913ff", "name": "Hyeongu Yun", "hidden": false}], "publishedAt": "2026-01-05T02:30:59.000Z", "submittedOnDailyAt": "2026-01-06T01:03:14.011Z", "title": "K-EXAONE Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "upvotes": 44, "discussionId": "695c72356aa73bc11f091400", "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE", "githubRepoAddedBy": "user", "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.", "ai_keywords": ["Mixture-of-Experts", "256K-token context window", "multilingual language model", "parameter-efficient fine-tuning"], "githubStars": 39, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "summary_zh": "<ul>\n    <li>K-EXAONE\u662fLG AI Research\u5f00\u53d1\u7684\u5927\u578b\u591a\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u6a21\u578b\u57fa\u4e8eMixture-of-Experts\u67b6\u6784\uff0c\u603b\u53c2\u6570\u91cf\u4e3a2360\u4ebf\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\u3002</li>\n    <li>\u652f\u6301256K\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u6db5\u76d6\u516d\u79cd\u8bed\u8a00\uff1a\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u3002</li>\n    <li>\u5728\u63a8\u7406\u548c\u591a\u8bed\u8a00\u7b49\u80fd\u529b\u7684\u8bc4\u4f30\u4e2d\uff0c\u8868\u73b0\u4e0e\u76f8\u4f3c\u5927\u5c0f\u7684\u5f00\u653e\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>K-EXAONE\u65e8\u5728\u63a8\u52a8AI\u7684\u53d1\u5c55\uff0c\u4ee5\u6539\u5584\u751f\u6d3b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u4e1a\u548c\u7814\u7a76\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>K-EXAONE is a large multilingual language model created by LG AI Research.</li>\n    <li>It has 236 billion total parameters and uses 23 billion of them during operation.</li>\n    <li>The model can understand six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.</li>\n    <li>K-EXAONE is tested on various abilities and performs well compared to similar models that are open-source.</li>\n    <li>It aims to improve AI technology for better life and is suitable for industrial and research uses.</li>\n</ul>"}, "publishedAt": "2026-01-04T21:30:59.000Z", "title": "K-EXAONE Technical Report", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03233", "authors": [{"_id": "695dc6d9c03d6d81e4399e85", "user": {"_id": "6303cc5e0547362a22a51af0", "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg", "isPro": false, "fullname": "Yoav HaCohen", "user": "yoavhacohen", "type": "user"}, "name": "Yoav HaCohen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:29.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e86", "user": {"_id": "6489c487b9e9258ba065418f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png", "isPro": false, "fullname": "Benny Brazowski", "user": "benibraz", "type": "user"}, "name": "Benny Brazowski", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:35.981Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e87", "user": {"_id": "62dd30a8d43078cd49ac8ad8", "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg", "isPro": false, "fullname": "Nisan Chiprut", "user": "nisan", "type": "user"}, "name": "Nisan Chiprut", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:41.634Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e88", "user": {"_id": "64a7adc087cbd4dc7301fdd6", "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg", "isPro": false, "fullname": "Yaki Bitterman", "user": "jacobitterman", "type": "user"}, "name": "Yaki Bitterman", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:46.749Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e89", "user": {"_id": "65897258509bcae23fa162c9", "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg", "isPro": false, "fullname": "Andrew Kvochko", "user": "kvochko", "type": "user"}, "name": "Andrew Kvochko", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:51.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8a", "name": "Avishai Berkowitz", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8b", "name": "Daniel Shalem", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8c", "user": {"_id": "681af83e2f4aaa88639e703d", "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg", "isPro": false, "fullname": "Daphna Lifschitz", "user": "Daphnal", "type": "user"}, "name": "Daphna Lifschitz", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:04.180Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8d", "user": {"_id": "636b97a57631fe5e86fe1fa2", "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg", "isPro": false, "fullname": "Dudu Moshe", "user": "dudumoshe", "type": "user"}, "name": "Dudu Moshe", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:13.512Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8e", "name": "Eitan Porat", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8f", "user": {"_id": "677a422979d3c32a5dd87a0a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png", "isPro": false, "fullname": "Eitan Richardson", "user": "eitanrich", "type": "user"}, "name": "Eitan Richardson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:22.677Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e90", "user": {"_id": "673f6911d83832a6ce15e7bf", "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg", "isPro": false, "fullname": "Guy Shiran", "user": "guysrn", "type": "user"}, "name": "Guy Shiran", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:28.250Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e91", "user": {"_id": "65744a2fe09de6aa74026d80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg", "isPro": false, "fullname": "Itay Chachy", "user": "ItayChachy", "type": "user"}, "name": "Itay Chachy", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:36.781Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e92", "name": "Jonathan Chetboun", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e93", "user": {"_id": "6678365ac411b340b32d6148", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg", "isPro": false, "fullname": "Michael Finkelson", "user": "MichaelFinkelson", "type": "user"}, "name": "Michael Finkelson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:53.574Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e94", "user": {"_id": "6318aa43cb4ca740c4c55651", "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg", "isPro": false, "fullname": "michael kupchick", "user": "michaellightricks", "type": "user"}, "name": "Michael Kupchick", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:59.945Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e95", "user": {"_id": "673f29b568595672b8d3e90e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png", "isPro": false, "fullname": "Nir Zabari", "user": "NirZabariLTX", "type": "user"}, "name": "Nir Zabari", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:07.297Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e96", "user": {"_id": "64ae89c043dda9449a1eb1ba", "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg", "isPro": false, "fullname": "Nitzan Guetta", "user": "nitzanguetta", "type": "user"}, "name": "Nitzan Guetta", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:14.712Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e97", "name": "Noa Kotler", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e98", "user": {"_id": "631f58935ba8c026340b377c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg", "isPro": false, "fullname": "Ofir Bibi", "user": "ofirbibi", "type": "user"}, "name": "Ofir Bibi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:27.196Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e99", "user": {"_id": "674348b46215a2c0878e219b", "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg", "isPro": false, "fullname": "Ori Gordon", "user": "origordon", "type": "user"}, "name": "Ori Gordon", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:34.878Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9a", "name": "Poriya Panet", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9b", "name": "Roi Benita", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9c", "name": "Shahar Armon", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9d", "name": "Victor Kulikov", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9e", "name": "Yaron Inger", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9f", "name": "Yonatan Shiftan", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea0", "name": "Zeev Melumian", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea1", "name": "Zeev Farbman", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "publishedAt": "2026-01-06T18:24:41.000Z", "submittedOnDailyAt": "2026-01-07T00:07:29.528Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "upvotes": 40, "discussionId": "695dc6d9c03d6d81e4399ea2", "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v", "githubRepo": "https://github.com/Lightricks/LTX-2", "githubRepoAddedBy": "user", "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.", "ai_keywords": ["text-to-video diffusion models", "audiovisual content", "dual-stream transformer", "cross-attention layers", "temporal positional embeddings", "AdaLN", "classifier-free guidance", "modality-aware classifier-free guidance", "multilingual text encoder", "diffusion models"], "githubStars": 922, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aLTX-2\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u97f3\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>LTX-2\u4f7f\u7528\u4e86\u4e00\u4e2a\u4e0d\u5bf9\u79f0\u7684\u53cc\u6d41\u53d8\u6362\u5668\uff0c\u89c6\u9891\u6d41\u6709140\u4ebf\u53c2\u6570\uff0c\u97f3\u9891\u6d41\u670950\u4ebf\u53c2\u6570\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u97f3\u89c6\u9891\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u548c\u65f6\u95f4\u4f4d\u7f6e\u5d4c\u5165\u5b9e\u73b0\u97f3\u89c6\u9891\u7684\u540c\u6b65\u751f\u6210\u3002</li>\n    <li>LTX-2\u80fd\u591f\u751f\u6210\u4e0e\u573a\u666f\u89d2\u8272\u3001\u73af\u5883\u3001\u98ce\u683c\u548c\u60c5\u611f\u76f8\u7b26\u7684\u97f3\u9891\u8f68\u9053\uff0c\u5305\u62ec\u81ea\u7136\u80cc\u666f\u97f3\u548c\u97f3\u6548\u3002</li>\n    <li>\u6a21\u578b\u5728\u97f3\u89c6\u9891\u8d28\u91cf\u548c\u63d0\u793a\u9075\u5faa\u65b9\u9762\u8fbe\u5230\u4e86\u5f00\u6e90\u7cfb\u7edf\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u8fdc\u4f4e\u4e8e\u4e13\u6709\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LTX-2 is a new open-source model that creates both video and audio together, improving how they match each other.</li>\n    <li>It uses a special design with two parts: one for video (14 billion parameters) and one for audio (5 billion parameters), allowing for more focus on video creation.</li>\n    <li>The model understands multiple languages and has a system for better aligning audio and video to make them more controllable.</li>\n    <li>LTX-2 not only generates speech but also creates fitting background sounds and effects that match the video scenes.</li>\n    <li>It offers high-quality audiovisual results that are competitive with top models while being more efficient and cost-effective.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:24:41.000Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01425", "authors": [{"_id": "695c765d6aa73bc11f091402", "name": "Xu Guo", "hidden": false}, {"_id": "695c765d6aa73bc11f091403", "name": "Fulong Ye", "hidden": false}, {"_id": "695c765d6aa73bc11f091404", "name": "Xinghui Li", "hidden": false}, {"_id": "695c765d6aa73bc11f091405", "name": "Pengqi Tu", "hidden": false}, {"_id": "695c765d6aa73bc11f091406", "name": "Pengze Zhang", "hidden": false}, {"_id": "695c765d6aa73bc11f091407", "user": {"_id": "674566cb79d6f3a9da7be0de", "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg", "isPro": false, "fullname": "Qichao Sun", "user": "Simons212", "type": "user"}, "name": "Qichao Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:07.336Z", "hidden": false}, {"_id": "695c765d6aa73bc11f091408", "name": "Songtao Zhao", "hidden": false}, {"_id": "695c765d6aa73bc11f091409", "name": "Xiangwang Hou", "hidden": false}, {"_id": "695c765d6aa73bc11f09140a", "name": "Qian He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "publishedAt": "2026-01-04T08:07:11.000Z", "submittedOnDailyAt": "2026-01-06T00:30:06.666Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "submittedOnDailyBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "upvotes": 33, "discussionId": "695c765d6aa73bc11f09140b", "projectPage": "https://guoxu1233.github.io/DreamID-V/", "githubRepo": "https://github.com/bytedance/DreamID-V", "githubRepoAddedBy": "user", "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.", "ai_keywords": ["Video Face Swapping", "Image Face Swapping", "diffusion transformer", "Modality-Aware Conditioning", "Synthetic-to-Real Curriculum", "Identity-Coherence Reinforcement Learning", "IDBench-V", "Identity-Anchored Video Synthesizer", "bidirectional ID quadruplets", "multi-model conditions"], "githubStars": 86, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u6280\u672f\u9700\u8981\u5c06\u4e00\u4e2a\u4eba\u7684\u8eab\u4efd\u65e0\u7f1d\u6ce8\u5165\u5230\u76ee\u6807\u89c6\u9891\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7684\u59ff\u52bf\u3001\u8868\u60c5\u3001\u5149\u7167\u3001\u80cc\u666f\u548c\u52a8\u6001\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u6362\u8138\u6280\u672f\u7684\u4f18\u52bf\u8f6c\u79fb\u5230\u89c6\u9891\u9886\u57df\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86SyncID-Pipe\u6570\u636e\u7ba1\u9053\u548cDreamID-V\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86IDBench-V\u57fa\u51c6\uff0c\u5305\u542b\u591a\u79cd\u573a\u666f\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDreamID-V\u5728\u5404\u79cd\u6362\u8138\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video Face Swapping (VFS) aims to replace a person's face in a video while keeping the original video's look and feel.</li>\n    <li>Current methods struggle with keeping the replaced face similar in identity and maintaining a consistent look throughout the video.</li>\n    <li>We propose a new framework called DreamID-V that improves face swapping in videos by using advanced techniques from Image Face Swapping (IFS).</li>\n    <li>Our approach includes a special data pipeline and a unique training strategy to enhance the quality and consistency of the swapped faces.</li>\n    <li>We also created IDBench-V, a new benchmark for testing face swapping methods, showing that DreamID-V outperforms existing solutions.</li>\n</ul>"}, "publishedAt": "2026-01-04T03:07:11.000Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01425.png", "numComments": 2, "submittedBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "fullname": "xuguo", "name": "XuGuo699", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03872", "authors": [{"_id": "695f1a475fa3847525c41d06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:32:36.055Z", "hidden": false}, {"_id": "695f1a475fa3847525c41d07", "name": "Guocheng Zhai", "hidden": false}, {"_id": "695f1a475fa3847525c41d08", "name": "Ruihan Jin", "hidden": false}, {"_id": "695f1a475fa3847525c41d09", "name": "Jiahao Yuan", "hidden": false}, {"_id": "695f1a475fa3847525c41d0a", "name": "Yuhao Shen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0b", "name": "Shuai Zhang", "hidden": false}, {"_id": "695f1a475fa3847525c41d0c", "name": "Zhengqi Wen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0d", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-07T12:38:33.000Z", "submittedOnDailyAt": "2026-01-08T04:50:03.287Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "upvotes": 30, "discussionId": "695f1a475fa3847525c41d0e", "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.", "ai_keywords": ["large language models", "external tools", "model-tool combination", "high-dimensional optimization", "dual-path framework", "training-free cluster-based routing", "RL-based multi-step routing", "cross-domain complex reasoning", "domain-specific alignment", "out-of-distribution generalization"], "summary_zh": "<ul>\n    <li>\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u5916\u90e8\u5de5\u5177\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u9009\u62e9\u6700\u4f73\u7684\u6a21\u578b\u548c\u5de5\u5177\u7ec4\u5408\u53d8\u5f97\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u6216\u56fa\u5b9a\u5de5\u5177\u903b\u8f91\u3002</li>\n    <li>\u63d0\u51fa\u4e86ATLAS\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u8def\u5f84\u65b9\u6cd5\u4f18\u5316\u5de5\u5177\u7684\u52a8\u6001\u4f7f\u7528\u3002</li>\n    <li>ATLAS\u901a\u8fc7\u96c6\u7fa4\u8def\u7531\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6b65\u8def\u7531\uff0c\u63d0\u5347\u8de8\u9886\u57df\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cATLAS\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) can do more when they work with external tools, but choosing the best combinations is complicated.</li>\n    <li>Current methods usually use one model or a fixed way to call tools, missing out on better options.</li>\n    <li>The paper introduces ATLAS, a framework that smartly uses tools for complex reasoning in different areas.</li>\n    <li>ATLAS uses two main strategies: one that aligns tools with specific domains and another that learns how to use tools in new situations.</li>\n    <li>Tests show ATLAS performs better than models like GPT-4o on various tasks, especially in visual reasoning with specialized tools.</li>\n</ul>"}, "publishedAt": "2026-01-07T07:38:33.000Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u7684\u6570\u636e\u51c6\u5907\u65b9\u5f0f\u5b58\u5728\u4e0d\u5c11\u95ee\u9898\uff0c\u5982\u811a\u672c\u4e0d\u89c4\u8303\u3001\u96be\u4ee5\u91cd\u73b0\u7b49\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u4e86\u63a5\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\uff0c\u5e76\u6db5\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u591a\u4e2a\u9886\u57df\u7684\u7ba1\u9053\u3002</li>\n    <li>\u901a\u8fc7DataFlow-Agent\uff0c\u7528\u6237\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\uff0c\u63d0\u9ad8\u4e86\u53ef\u7528\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4f7f\u7528\u6848\u4f8b\u4e2d\uff0cDataFlow\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u5408\u6210\u57fa\u7ebf\u6570\u636e\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current data preparation methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create modular and reusable data preparation pipelines for LLMs.</li>\n    <li>It provides a user-friendly API for building dataflows and includes nearly 200 reusable operators for various tasks.</li>\n    <li>DataFlow-Agent can convert natural language instructions into executable data pipelines, making it easier to use.</li>\n    <li>The framework has shown to improve LLM performance in various tests, outperforming existing datasets and methods significantly.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u5b8c\u6574\u7684\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5206\u79bb\u6d41\u7a0b\u7684\u5c40\u9650\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>Kling-Omni\u4e0d\u4ec5\u662f\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\uff0c\u8fd8\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u3001\u751f\u6210\u548c\u4e0e\u590d\u6742\u52a8\u6001\u4e16\u754c\u4e92\u52a8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual and language inputs.</li>\n    <li>It combines various tasks like video generation, editing, and reasoning into one complete system, rather than using separate steps.</li>\n    <li>The framework can handle inputs such as text, images, and videos to create cohesive video content.</li>\n    <li>It uses a strong data system and advanced training methods to improve its performance and efficiency.</li>\n    <li>Kling-Omni shows great skills in generating videos, editing based on reasoning, and following various instructions, moving towards a more interactive video creation tool.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 96, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 80, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5168\u9762\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u7684\u4efb\u52a1\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\uff0c\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u548c\u6574\u4f53\u7406\u89e3\u80fd\u529b\u5f31\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5c06\u8bb0\u5fc6\u6269\u5c55\u4e3a\u52a8\u6001\u548c\u8868\u8fbe\u4e30\u5bcc\u7684\u7ed3\u6784\uff0c\u652f\u6301\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u8fde\u63a5\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u5f3a\u7684\u63a8\u7406\u652f\u6301\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5168\u7403\u7406\u89e3\u7684\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\uff0c\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) improve reasoning and comprehension.</li>\n    <li>Current memory designs in RAG mainly store facts passively, missing important connections between them.</li>\n    <li>This static memory limits effective reasoning and understanding in complex tasks.</li>\n    <li>HGMem is a new memory system that uses a hypergraph to create dynamic and interconnected knowledge structures.</li>\n    <li>Tests show HGMem improves performance in multi-step RAG and outperforms existing methods across various tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u5e76\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u5f3a\u5316\u5b66\u4e60(RL)\u5f00\u59cb\u91c7\u7528\u591a\u79cd\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u5b9e\u73b0\u8fd9\u4e9b\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u4f1a\u5bfc\u81f4\u4e0d\u540c\u5956\u52b1\u7ec4\u5408\u7684\u6548\u679c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316(GDPO)\uff0c\u65e8\u5728\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u7684\u5f52\u4e00\u5316\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0cGDPO\u5728\u51c6\u786e\u6027\u548c\u9075\u5faa\u7ea6\u675f\u65b9\u9762\u90fd\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate responses and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to help models learn these preferred behaviors.</li>\n    <li>Current methods like Group Relative Policy Optimization (GRPO) can lead to problems, making the training less effective.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves training by keeping the differences between rewards clearer.</li>\n    <li>GDPO shows better performance than GRPO in three tasks: tool calling, math reasoning, and coding reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u7075\u6d3b\u76844D\u4e16\u754c\u6a21\u578b\uff0c\u652f\u63014D\u91cd\u5efa\u548c\u65b0\u8f68\u8ff9\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u8bc6\u522b\u4e86\u5f53\u524d4D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u5171\u540c\u9650\u5236\u3002</li>\n    <li>NeoVerse\u80fd\u591f\u5904\u7406\u5404\u79cd\u5355\u76ee\u89c6\u9891\uff0c\u5177\u6709\u65e0\u9700\u59ff\u6001\u7684\u524d\u99884D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u76ee\u9000\u5316\u6a21\u5f0f\u6a21\u62df\u3002</li>\n    <li>NeoVerse\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002</li>\n    <li>\u9879\u76ee\u9875\u9762\u53ef\u8bbf\u95ee\uff1ahttps://neoverse-4d.github.io</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions and generate videos from new perspectives.</li>\n    <li>It addresses the common issue of scalability found in current 4D modeling methods.</li>\n    <li>NeoVerse can work with regular monocular videos without needing complex training processes.</li>\n    <li>It includes features like pose-free reconstruction and online simulation of video quality degradation.</li>\n    <li>NeoVerse performs at a high level in various benchmark tests for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u80fd\u529b\u9759\u6001\u7684\u95ee\u9898\u3002</li>\n    <li>Youtu-Agent\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u53d1\u5c55\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5177\u6709\u7075\u6d3b\u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u3001\u63d0\u793a\u548c\u914d\u7f6e\u3002</li>\n    <li>Youtu-Agent\u5305\u542b\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\uff1a\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u5de5\u5177\u5408\u6210\u6210\u529f\u7387\u548c\u6027\u80fd\u63d0\u5347\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Existing LLM frameworks have high setup costs and cannot easily adapt to changing environments.</li>\n    <li>Youtu-Agent is a new system that automates the creation and improvement of LLM agents.</li>\n    <li>It uses a modular design that separates different parts, allowing for easier updates and reuse.</li>\n    <li>Youtu-Agent has two modes for generating agents: one for standard tasks and another for complex ones.</li>\n    <li>It improves agent performance through experience-based learning and faster training methods, achieving better results in various benchmarks.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u80fd\u591f\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u4e14\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u9075\u5faa\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u9650\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982FVD\uff09\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u63a8\u7406\u9519\u8bef\uff0c\u4f8b\u5982\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u6cd5\u5219\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\u6846\u67b6\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\uff08\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\uff09\u8fdb\u884c\u8bc4\u4f30\u3002</li>\n    <li>MMGR\u8bc4\u4f30\u6db5\u76d6\u4e09\u4e2a\u9886\u57df\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u5e76\u91c7\u7528\u7ec6\u81f4\u7684\u6307\u6807\u6765\u8bc4\u4f30\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u51c6\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u9886\u5148\u7684\u89c6\u9891\u548c\u56fe\u50cf\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u5728\u4e0d\u540c\u9886\u57df\u7684\u8868\u73b0\u5dee\u8ddd\u660e\u663e\uff0c\u5c24\u5176\u5728\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic content, but their reliability depends on understanding physical and logical rules.</li>\n    <li>Current evaluation methods focus too much on visual quality and miss reasoning issues like causality and consistency.</li>\n    <li>MMGR is a new evaluation framework that measures reasoning abilities in five areas: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>Benchmarking shows that while models perform well in some areas, they struggle significantly with abstract reasoning and long-term planning.</li>\n    <li>Key problems with current models include too much focus on visual data and not enough on logical correctness.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd(SGI)\u4ecd\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c3d\u7ba1\u79d1\u5b66AI\u5df2\u6709\u8fdb\u5c55\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u7528\u8be2\u95ee\u6a21\u578b(PIM)\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u7814\u7a76\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u548c\u5b9e\u9a8c\u65b9\u9762\u5b58\u5728\u591a\u9879\u4e0d\u8db3\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60(TTRL)\uff0c\u4ee5\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\u548c\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current AI lacks a clear framework for Scientific General Intelligence (SGI), which is the ability to think and work like a scientist across different fields.</li>\n    <li>We define SGI using the Practical Inquiry Model (PIM) and suggest four tasks: deep research, idea generation, experiments, and reasoning about experiments.</li>\n    <li>SGI-Bench includes over 1,000 examples to test AI performance on scientific questions, revealing significant gaps in their capabilities.</li>\n    <li>Results show low accuracy in deep research and experiments, with AI struggling to generate feasible ideas and maintain accuracy in executing tasks.</li>\n    <li>We propose Test-Time Reinforcement Learning (TTRL) to improve AI's creativity and ability to generate new hypotheses during evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f7f\u7528VAE\u7a7a\u95f4\u6765\u5b66\u4e60\u89c6\u9891\u7684\u5206\u5e03\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SemanticGen\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u5b9a\u4e49\u89c6\u9891\u7684\u5168\u5c40\u5e03\u5c40\uff1b\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u7a7a\u95f4\u4e2d\u751f\u6210\u66f4\u5feb\uff0c\u4e14\u5728\u957f\u89c6\u9891\u751f\u6210\u65f6\u4e5f\u66f4\u6709\u6548\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new method for generating videos that addresses slow generation and high computational costs of traditional models.</li>\n    <li>It starts by creating a basic structure of the video in a simpler, high-level semantic space before adding detailed features.</li>\n    <li>The process includes two stages: first, it generates semantic video features for the overall layout, and second, it creates detailed video elements based on these features.</li>\n    <li>This approach allows for faster video generation and is more efficient for creating longer videos.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and outperforms existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u65b0\u6280\u672f\u4f7f\u5f97\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u81ea\u52a8\u5316\u53d8\u5f97\u66f4\u52a0\u9ad8\u6548\uff0c\u4f46\u83b7\u5f97\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u53d8\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5229\u7528\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u6807\u6ce8\u51c6\u786e\u7387\uff0c\u6210\u672c\u964d\u4f4e10-100\u500d\u3002</li>\n    <li>\u63a8\u51fa\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u5c55\u793a\u4e86\u5728GUI\u6027\u80fd\u4e0a\u7684\u9886\u5148\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u652f\u6301\u9ad8\u9690\u79c1\u6267\u884c\u3002</li>\n    <li>\u5f15\u5165\u4e86AndroidDaily\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\uff0c\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u7684\u5b9e\u9645\u4f7f\u7528\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods for training GUI automation models have been developed to efficiently gather high-quality data while ensuring accuracy.</li>\n    <li>A self-evolving training system called the Calibrated Step Reward System allows for reliable training signals from model-generated actions at a lower cost.</li>\n    <li>The Step-GUI models demonstrate top performance in GUI tasks and maintain strong general capabilities.</li>\n    <li>To ensure user privacy and standardize interfaces, a new Model Context Protocol (GUI-MCP) has been introduced, which allows sensitive data to remain on devices during execution.</li>\n    <li>A new benchmark, AndroidDaily, tests model performance based on real mobile usage, showing strong results in both static actions and complete tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 11, 2026";
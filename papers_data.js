window.trendingPapers = {
    "today": [{"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u7684\u201c\u751f\u6210-\u9a8c\u8bc1-\u6539\u8fdb\u201d\u5468\u671f\u6765\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u65e0\u6cd5\u5728\u6709\u9650\u9884\u7b97\u5185\u53d1\u73b0\u66f4\u590d\u6742\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u63d0\u51fa\u7684\u63a7\u5236\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\u65b9\u6cd5\u89e3\u51b3\u4e86\u521d\u59cb\u5316\u504f\u5dee\u548c\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u95ee\u9898\u3002</li>\n    <li>CSE\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u57fa\u4e8e\u53cd\u9988\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5c42\u6b21\u5316\u8fdb\u5316\u8bb0\u5fc6\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCSE\u5728\u4e0d\u540c\u7684LLM\u57fa\u7840\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\uff0c\u4e14\u5728\u65e9\u671f\u751f\u6210\u4e2d\u6548\u7387\u66f4\u9ad8\uff0c\u6301\u7eed\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods improve code generation but often struggle to find better solutions efficiently.</li>\n    <li>The inefficiency is caused by poor starting points, random operations without guidance, and not using past experiences effectively.</li>\n    <li>Controlled Self-Evolution (CSE) aims to fix these issues with three main features: diverse planning for better initial strategies, feedback-guided evolution methods, and a memory system for learning from past successes and failures.</li>\n    <li>Tests show that CSE works better than previous methods across different large language models.</li>\n    <li>CSE shows improved efficiency early on and continues to get better as it evolves.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09688", "authors": [{"_id": "696864c90ac10a06522f6a4a", "name": "Yibo Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4b", "name": "Lei Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4c", "name": "Yue Deng", "hidden": false}, {"_id": "696864c90ac10a06522f6a4d", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:02:22.232Z", "hidden": false}, {"_id": "696864c90ac10a06522f6a4e", "name": "Yao Xiao", "hidden": false}, {"_id": "696864c90ac10a06522f6a4f", "name": "Huanjin Yao", "hidden": false}, {"_id": "696864c90ac10a06522f6a50", "name": "Liwei Kang", "hidden": false}, {"_id": "696864c90ac10a06522f6a51", "name": "Hai Ye", "hidden": false}, {"_id": "696864c90ac10a06522f6a52", "name": "Yongcheng Jing", "hidden": false}, {"_id": "696864c90ac10a06522f6a53", "name": "Lidong Bing", "hidden": false}], "publishedAt": "2026-01-14T18:38:31.000Z", "submittedOnDailyAt": "2026-01-15T01:33:59.520Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "submittedOnDailyBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "upvotes": 90, "discussionId": "696864c90ac10a06522f6a54", "githubRepo": "https://github.com/Infinity-AILab/DeepResearchEval", "githubRepoAddedBy": "user", "ai_summary": "DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.", "ai_keywords": ["automated framework", "deep research task construction", "agentic evaluation", "persona-driven pipeline", "task qualification", "search necessity", "adaptive point-wise quality evaluation", "active fact-checking", "web search", "multi-source evidence integration"], "githubStars": 67, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7528\u4e8e\u591a\u6b65\u9aa4\u7684\u7f51\u7edc\u7814\u7a76\u548c\u5206\u6790\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u4ecd\u7136\u5f88\u56f0\u96be\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6ce8\u91ca\uff0c\u4f9d\u8d56\u9759\u6001\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6216\u8005\u5728\u7f3a\u5c11\u5f15\u7528\u65f6\u65e0\u6cd5\u53ef\u9760\u5730\u9a8c\u8bc1\u4e8b\u5b9e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DeepResearchEval\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u6784\u5efa\u548c\u8bc4\u4f30\u3002</li>\n    <li>\u4efb\u52a1\u6784\u5efa\u4f7f\u7528\u4e2a\u6027\u5316\u7684\u6d41\u7a0b\u751f\u6210\u590d\u6742\u7684\u7814\u7a76\u4efb\u52a1\uff0c\u5e76\u7b5b\u9009\u9700\u8981\u591a\u6e90\u8bc1\u636e\u6574\u5408\u7684\u4efb\u52a1\u3002</li>\n    <li>\u8bc4\u4f30\u91c7\u7528\u52a8\u6001\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u4e3b\u52a8\u4e8b\u5b9e\u68c0\u67e5\uff0c\u80fd\u591f\u5728\u7f3a\u5c11\u5f15\u7528\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u548c\u9a8c\u8bc1\u62a5\u544a\u5185\u5bb9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep research systems help with complex web research and analysis, but evaluating them is difficult.</li>\n    <li>Current evaluation methods need a lot of manual work, use fixed standards, or struggle to check facts without citations.</li>\n    <li>DeepResearchEval is a new automated framework for creating and evaluating research tasks.</li>\n    <li>It uses a persona-driven approach to create realistic research tasks that need information from multiple sources.</li>\n    <li>The evaluation includes a method that adapts to each task and automatically checks facts through web searches.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:38:31.000Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png", "numComments": 1, "submittedBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "fullname": "Keming Wu", "name": "wukeming11", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09259", "authors": [{"_id": "696856230ac10a06522f69dd", "name": "Jian Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69de", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:12.229Z", "hidden": false}, {"_id": "696856230ac10a06522f69df", "name": "Zhangqi Wang", "hidden": false}, {"_id": "696856230ac10a06522f69e0", "name": "Yu He", "hidden": false}, {"_id": "696856230ac10a06522f69e1", "name": "Haoran Luo", "hidden": false}, {"_id": "696856230ac10a06522f69e2", "name": "li yuan", "hidden": false}, {"_id": "696856230ac10a06522f69e3", "name": "Lingling Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69e4", "name": "Rui Mao", "hidden": false}, {"_id": "696856230ac10a06522f69e5", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:14.086Z", "hidden": false}, {"_id": "696856230ac10a06522f69e6", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T07:48:00.000Z", "submittedOnDailyAt": "2026-01-15T00:22:01.292Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "upvotes": 81, "discussionId": "696856230ac10a06522f69e7", "githubRepo": "https://github.com/exoskeletonzj/MAXS", "githubRepoAddedBy": "user", "ai_summary": "MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.", "ai_keywords": ["LLM agents", "tool execution", "reasoning planning", "lookahead strategy", "advantage value", "step consistency variance", "inter-step trend slopes", "trajectory convergence", "multi-tool reasoning", "inference efficiency"], "githubStars": 5, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u901a\u8fc7\u591a\u79cd\u5de5\u5177\u7684\u534f\u4f5c\u5c55\u73b0\u51fa\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002</li>\n    <li>\u5f53\u524d\u65b9\u6cd5\u9762\u4e34\u5c40\u90e8\u76ee\u5149\u77ed\u6d45\u548c\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u504f\u79bb\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMAXS\u7684\u5143\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u591f\u7075\u6d3b\u6574\u5408\u5de5\u5177\u6267\u884c\u548c\u63a8\u7406\u89c4\u5212\u3002</li>\n    <li>MAXS\u91c7\u7528\u524d\u77bb\u7b56\u7565\uff0c\u63d0\u524d\u51e0\u6b65\u6269\u5c55\u63a8\u7406\u8def\u5f84\uff0c\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u7684\u4f18\u52bf\u503c\u3002</li>\n    <li>\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0cMAXS\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u524d\u77bb\u7b56\u7565\u548c\u5de5\u5177\u4f7f\u7528\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model (LLM) Agents can reason effectively by using multiple tools together.</li>\n    <li>Current methods face problems like short-sighted decisions and errors that lead to bigger mistakes.</li>\n    <li>To solve these issues, a new approach called meta-adaptive exploration with LLM agents (MAXS) is proposed.</li>\n    <li>MAXS uses a lookahead strategy to improve reasoning by predicting tool effectiveness and ensuring stable reasoning steps.</li>\n    <li>Tests show that MAXS works better and faster than previous methods across various models and datasets.</li>\n</ul>"}, "publishedAt": "2026-01-14T02:48:00.000Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09259.png", "numComments": 3, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09274", "authors": [{"_id": "6968568f0ac10a06522f69e9", "name": "Jian Zhang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ea", "name": "Yu He", "hidden": false}, {"_id": "6968568f0ac10a06522f69eb", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:02.764Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69ec", "name": "Zhangqi Wang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ed", "name": "Kai He", "hidden": false}, {"_id": "6968568f0ac10a06522f69ee", "name": "Fangzhi Xu", "hidden": false}, {"_id": "6968568f0ac10a06522f69ef", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:05.035Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69f0", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T08:17:41.000Z", "submittedOnDailyAt": "2026-01-15T00:23:45.077Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "upvotes": 74, "discussionId": "6968568f0ac10a06522f69f1", "projectPage": "https://a3-bench.github.io/", "githubRepo": "https://github.com/exoskeletonzj/A3-Bench", "githubRepoAddedBy": "user", "githubStars": 0, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u63a8\u7406\u4e0d\u4ec5\u4f9d\u8d56\u903b\u8f91\u63a8\u7406\uff0c\u8fd8\u9700\u8981\u6fc0\u6d3b\u5df2\u6709\u77e5\u8bc6\u548c\u7ecf\u9a8c\u7ed3\u6784\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6216\u6b65\u9aa4\u4e00\u81f4\u6027\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u9a71\u52a8\u673a\u5236\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86A^3-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u7684\u65b0\u57fa\u51c6\uff0c\u57fa\u4e8e\u951a\u70b9\u548c\u5438\u5f15\u70b9\u7684\u6fc0\u6d3b\u673a\u5236\u3002</li>\n    <li>\u8be5\u57fa\u51c6\u5305\u542b2198\u4e2a\u79d1\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u53cc\u5c3a\u5ea6\u8bb0\u5fc6\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u91cf\u8bb0\u5fc6\u6fc0\u6d3b\u7387\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1A^3-Bench\uff0c\u5206\u6790\u8bb0\u5fc6\u6fc0\u6d3b\u5bf9\u63a8\u7406\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u8bb0\u5fc6\u9a71\u52a8\u79d1\u5b66\u63a8\u7406\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Scientific reasoning uses both logic and prior knowledge to enhance understanding.</li>\n    <li>Current tests mainly focus on answers rather than the memory processes that support reasoning.</li>\n    <li>A^3-Bench is a new benchmark created to evaluate scientific reasoning using memory-driven techniques.</li>\n    <li>It includes 2,198 annotated science reasoning problems and measures how memory aids reasoning through a new framework.</li>\n    <li>The study shows how memory activation influences reasoning skills and offers insights into this process.</li>\n</ul>"}, "publishedAt": "2026-01-14T03:17:41.000Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09274.png", "numComments": 2, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09088", "authors": [{"_id": "69688bbc0ac10a06522f6aeb", "user": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "name": "Shaotian Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:27.639Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aec", "name": "Kaiyuan Liu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aed", "user": {"_id": "64b73e3830a0b8ff60145a29", "avatarUrl": "/avatars/297469812b57b2ddf7d52b9391d80bde.svg", "isPro": false, "fullname": "Chen Shen", "user": "zjushenchen", "type": "user"}, "name": "Chen Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:19.260Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aee", "user": {"_id": "6225b0d87f5fba1007d62fae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6225b0d87f5fba1007d62fae/clONu5C-lkoSswcJjcG0u.jpeg", "isPro": false, "fullname": "Bing Wang", "user": "wangbing1416", "type": "user"}, "name": "Bing Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:17.091Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aef", "user": {"_id": "694a226980a37f293a4ce7c0", "avatarUrl": "/avatars/7a48f4eeb80a1b5688cbfb10a59765a0.svg", "isPro": false, "fullname": "Sinan Fan", "user": "sinan25", "type": "user"}, "name": "Sinan Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:14.748Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af0", "name": "Jun Zhang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af1", "name": "Yue Wu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af2", "name": "Zheng Wang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af3", "name": "Jieping Ye", "hidden": false}], "publishedAt": "2026-01-14T02:43:17.000Z", "submittedOnDailyAt": "2026-01-15T07:24:58.461Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "submittedOnDailyBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "upvotes": 43, "discussionId": "69688bbc0ac10a06522f6af4", "projectPage": "https://github.com/D2I-ai/dasd-thinking", "githubRepo": "https://github.com/D2I-ai/dasd-thinking", "githubRepoAddedBy": "user", "ai_summary": "A lightweight open-source reasoning model achieves state-of-the-art performance through enhanced sequence-level distillation that addresses limitations in current teacher-student knowledge transfer methods.", "ai_keywords": ["sequence-level distillation", "teacher-student distillation", "SFT", "heuristic rules", "output distribution", "generalization capability", "exposure bias", "teacher-forced training", "autoregressive inference"], "githubStars": 16, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86DASD-4B-Thinking\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u6570\u5b66\u3001\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u66f4\u5927\u7684\u6a21\u578b\u3002</li>\n    <li>\u62a5\u544a\u6279\u5224\u6027\u5730\u91cd\u65b0\u5ba1\u89c6\u4e86\u5f53\u524d\u6d41\u884c\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u5b9e\u8df5\u4e2d\u7684\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b9\u6cd5\u521b\u65b0\uff0c\u5f62\u6210\u4e86\u6539\u8fdb\u7684\u5e8f\u5217\u7ea7\u84b8\u998f\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>DASD-4B-Thinking\u4ec5\u4f7f\u7528448K\u8bad\u7ec3\u6837\u672c\u5c31\u53d6\u5f97\u4e86\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u6a21\u578b\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u4ee5\u652f\u6301\u793e\u533a\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DASD-4B-Thinking is a new open-source reasoning model that performs very well in tasks like math, science, and code generation.</li>\n    <li>The report critiques current methods of training models, specifically focusing on how they learn from teacher models.</li>\n    <li>It identifies three main problems in existing training methods: poor representation of teacher outputs, misalignment in learning, and exposure bias.</li>\n    <li>To improve model training, the authors suggest new methods that enhance the interaction between teacher and student models.</li>\n    <li>DASD-4B-Thinking achieves strong results using significantly fewer training samples than most other models and the authors share their models and datasets for community use.</li>\n</ul>"}, "publishedAt": "2026-01-13T21:43:17.000Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09088.png", "numComments": 4, "submittedBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "fullname": "Shaotian", "name": "ystluffy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09708", "authors": [{"_id": "69684f740ac10a06522f69ba", "user": {"_id": "64705d224be5cf1f3348d6bc", "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg", "isPro": false, "fullname": "Chi-Pin Huang", "user": "jasper0314-huang", "type": "user"}, "name": "Chi-Pin Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:32.512Z", "hidden": false}, {"_id": "69684f740ac10a06522f69bb", "name": "Yunze Man", "hidden": false}, {"_id": "69684f740ac10a06522f69bc", "name": "Zhiding Yu", "hidden": false}, {"_id": "69684f740ac10a06522f69bd", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:27.724Z", "hidden": false}, {"_id": "69684f740ac10a06522f69be", "name": "Jan Kautz", "hidden": false}, {"_id": "69684f740ac10a06522f69bf", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69684f740ac10a06522f69c0", "user": {"_id": "6312cab05beb528b5c1500e3", "avatarUrl": "/avatars/a328e8cc99fb031b2d5c911c4b577e7e.svg", "isPro": false, "fullname": "Fu-En Yang", "user": "FuEnYang", "type": "user"}, "name": "Fu-En Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:30.478Z", "hidden": false}], "publishedAt": "2026-01-14T18:59:59.000Z", "submittedOnDailyAt": "2026-01-15T00:10:27.528Z", "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning", "submittedOnDailyBy": {"_id": "64705d224be5cf1f3348d6bc", "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg", "isPro": false, "fullname": "Chi-Pin Huang", "user": "jasper0314-huang", "type": "user"}, "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.", "upvotes": 36, "discussionId": "69684f740ac10a06522f69c1", "projectPage": "https://jasper0314-huang.github.io/fast-thinkact/", "ai_summary": "Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.", "ai_keywords": ["chain-of-thought", "latent reasoning", "preference-guided objective", "embodied control", "policy learning", "inference latency", "vision-language-action"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u4efb\u52a1\u9700\u8981\u5bf9\u590d\u6742\u7684\u89c6\u89c9\u573a\u666f\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u9002\u5e94\u6027\u52a8\u4f5c\u3002</li>\n    <li>\u867d\u7136\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u53ef\u4ee5\u63d0\u9ad8\u63a8\u7406\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u8f83\u957f\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Fast-ThinkAct\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u53e3\u8ff0\u7684\u6f5c\u5728\u63a8\u7406\u5b9e\u73b0\u7d27\u51d1\u800c\u6709\u6548\u7684\u89c4\u5212\u3002</li>\n    <li>Fast-ThinkAct\u901a\u8fc7\u4ece\u6559\u5e08\u4e2d\u63d0\u70bc\u77e5\u8bc6\uff0c\u5b66\u4e60\u9ad8\u6548\u63a8\u7406\uff0c\u5e76\u7ed3\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u89c4\u5212\u80fd\u529b\u8fdb\u884c\u63a7\u5236\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFast-ThinkAct\u5728\u63a8\u7406\u5ef6\u8fdf\u4e0a\u6bd4\u73b0\u6709\u6280\u672f\u51cf\u5c11\u4e8689.3%\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u6548\u7684\u957f\u671f\u89c4\u5212\u548c\u5feb\u901f\u9002\u5e94\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) tasks involve understanding visual scenes and taking actions in changing environments.</li>\n    <li>Current reasoning methods for VLAs can take a long time to produce results because they rely on long reasoning processes.</li>\n    <li>Fast-ThinkAct is a new framework that improves efficiency by using shorter reasoning methods while still being effective.</li>\n    <li>This framework learns from a teacher model and focuses on aligning language and visual planning for better action control.</li>\n    <li>Fast-ThinkAct shows significant improvements, reducing the time needed for reasoning by up to 89.3%, while still performing well in various tasks.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:59:59.000Z", "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning", "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09708.png", "numComments": 1, "submittedBy": {"_id": "64705d224be5cf1f3348d6bc", "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg", "fullname": "Chi-Pin Huang", "name": "jasper0314-huang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09136", "authors": [{"_id": "6968555c0ac10a06522f69c3", "name": "Lijun Liu", "hidden": false}, {"_id": "6968555c0ac10a06522f69c4", "name": "Linwei Chen", "hidden": false}, {"_id": "6968555c0ac10a06522f69c5", "name": "Zhishou Zhang", "hidden": false}, {"_id": "6968555c0ac10a06522f69c6", "name": "Meng Tian", "hidden": false}, {"_id": "6968555c0ac10a06522f69c7", "name": "Hengfu Cui", "hidden": false}, {"_id": "6968555c0ac10a06522f69c8", "name": "Ruiyang Li", "hidden": false}, {"_id": "6968555c0ac10a06522f69c9", "name": "Zhaocheng Liu", "hidden": false}, {"_id": "6968555c0ac10a06522f69ca", "user": {"_id": "62dcdb86d36b2070f928a51e", "avatarUrl": "/avatars/a341e4305217f8abd14cff97201a24aa.svg", "isPro": false, "fullname": "sdujq", "user": "sdujq", "type": "user"}, "name": "Qiang Ju", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:25.438Z", "hidden": false}, {"_id": "6968555c0ac10a06522f69cb", "name": "Qianxi Li", "hidden": false}, {"_id": "6968555c0ac10a06522f69cc", "name": "Hong-Yu Zhou", "hidden": false}], "publishedAt": "2026-01-14T04:21:07.000Z", "submittedOnDailyAt": "2026-01-15T00:59:44.722Z", "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL", "submittedOnDailyBy": {"_id": "642438eaa3adbc7142c3ca0f", "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg", "isPro": false, "fullname": "CharlesChen", "user": "CharlesChen2023", "type": "user"}, "summary": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.", "upvotes": 36, "discussionId": "6968555c0ac10a06522f69cd", "ai_summary": "SkinFlow introduces a novel framework for dermatological vision-language modeling that improves diagnostic accuracy through optimized visual information transmission efficiency rather than parameter scaling alone.", "ai_keywords": ["Large Vision-Language Models", "diffuse attention", "Virtual-Width Dynamic Vision Encoder", "reinforcement learning", "visual information transmission efficiency", "diagnostic reasoning", "Fitzpatrick17k benchmark", "Top-1 accuracy", "Top-6 accuracy"], "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6SkinFlow\uff0c\u7528\u4e8e\u63d0\u5347\u76ae\u80a4\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>SkinFlow\u901a\u8fc7\u4f18\u5316\u89c6\u89c9\u4fe1\u606f\u4f20\u8f93\u6548\u7387\uff0c\u800c\u4e0d\u662f\u4ec5\u4f9d\u8d56\u53c2\u6570\u6269\u5927\uff0c\u6765\u89e3\u51b3\"\u6269\u6563\u6ce8\u610f\u529b\"\u7684\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e00\u79cd\u865a\u62df\u5bbd\u5ea6\u52a8\u6001\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u6765\u5904\u7406\u590d\u6742\u7684\u75c5\u7406\u7279\u5f81\u3002</li>\n    <li>\u6211\u4eec\u5236\u5b9a\u4e86\u4e00\u4e2a\u4e34\u5e8a\u8bc4\u4f30\u534f\u8bae\uff0c\u5f3a\u8c03\u8bca\u65ad\u5b89\u5168\u6027\u548c\u5c42\u6b21\u76f8\u5173\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u76847B\u6a21\u578b\u5728Fitzpatrick17k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8d8a\u4e86\u8bb8\u591a\u5927\u578b\u901a\u7528\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>General-purpose Large Vision-Language Models struggle with dermatology due to \"diffuse attention,\" making it hard to distinguish skin issues from background noise.</li>\n    <li>SkinFlow is a new framework that improves diagnosis without just adding more parameters, focusing instead on how efficiently visual information is transmitted.</li>\n    <li>The framework uses a special Dynamic Vision Encoder to analyze complex skin conditions and employs a two-step Reinforcement Learning process for better accuracy.</li>\n    <li>A new evaluation method is introduced that emphasizes safety and relevance in diagnosis rather than just matching labels.</li>\n    <li>The 7B model from this research outperformed larger models, showing significant improvements in accuracy on the Fitzpatrick17k benchmark.</li>\n</ul>"}, "publishedAt": "2026-01-13T23:21:07.000Z", "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL", "summary": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09136.png", "numComments": 4, "submittedBy": {"_id": "642438eaa3adbc7142c3ca0f", "avatarUrl": "/avatars/8deff70e0c93d259a42ee47f00a31e3e.svg", "fullname": "CharlesChen", "name": "CharlesChen2023", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09575", "authors": [{"_id": "696869800ac10a06522f6a92", "name": "Sheng-Yu Huang", "hidden": false}, {"_id": "696869800ac10a06522f6a93", "name": "Jaesung Choe", "hidden": false}, {"_id": "696869800ac10a06522f6a94", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "696869800ac10a06522f6a95", "name": "Cheng Sun", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Mf0UQtcxycDL3jlgJpk6d.jpeg"], "publishedAt": "2026-01-14T15:45:57.000Z", "submittedOnDailyAt": "2026-01-15T01:44:30.828Z", "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.", "upvotes": 22, "discussionId": "696869810ac10a06522f6a96", "projectPage": "https://peterjohnsonhuang.github.io/openvoxel-pages/", "ai_summary": "OpenVoxel enables open-vocabulary 3D scene understanding through training-free grouping and captioning of sparse voxels using Vision Language Models and Multi-modal Large Language Models.", "ai_keywords": ["open-vocabulary 3D scene understanding", "sparse voxels", "sparse voxel rasterization", "Vision Language Models", "Multi-modal Large Language Models", "open-vocabulary segmentation", "referring expression segmentation", "training-free algorithm"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOpenVoxel\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5bf9\u7a00\u758f\u4f53\u7d20\u8fdb\u884c\u5206\u7ec4\u548c\u63cf\u8ff0\uff0c\u9002\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u76843D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u3002</li>\n    <li>OpenVoxel\u80fd\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u83b7\u5f97\u7684\u7a00\u758f\u4f53\u7d20\u6805\u683c\u6a21\u578b\u4e2d\uff0c\u751f\u6210\u63cf\u8ff0\u573a\u666f\u4e2d\u4e0d\u540c\u7269\u4f53\u7684\u6709\u610f\u4e49\u7684\u5206\u7ec4\u3002</li>\n    <li>\u8be5\u7b97\u6cd5\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u4fe1\u606f\u4e30\u5bcc\u7684\u573a\u666f\u56fe\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5206\u7ec4\u6dfb\u52a0\u63cf\u8ff0\u3002</li>\n    <li>\u4e0e\u4ee5\u5f80\u65b9\u6cd5\u4e0d\u540c\uff0cOpenVoxel\u4e0d\u9700\u8981\u8bad\u7ec3\uff0c\u4e5f\u4e0d\u4f7f\u7528CLIP/BERT\u6587\u672c\u7f16\u7801\u5668\uff0c\u800c\u662f\u76f4\u63a5\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u641c\u7d22\u3002</li>\n    <li>\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cOpenVoxel\u5728\u590d\u6742\u7684\u53c2\u7167\u8868\u8fbe\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u8fd1\u7684\u7814\u7a76\uff0c\u4ee3\u7801\u5c06\u4f1a\u516c\u5f00\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>OpenVoxel is a new algorithm that groups and describes sparse voxels for understanding 3D scenes without needing training.</li>\n    <li>It uses a model created from images of a scene to identify and group different objects effectively.</li>\n    <li>OpenVoxel leverages advanced language models to create informative maps by labeling each group of objects.</li>\n    <li>Unlike earlier methods, it does not require embedding from other models and uses a direct text-to-text search approach.</li>\n    <li>Our experiments show that OpenVoxel outperforms other recent methods, especially in complex tasks like referring expression segmentation.</li>\n</ul>"}, "publishedAt": "2026-01-14T10:45:57.000Z", "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding", "summary": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Mf0UQtcxycDL3jlgJpk6d.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09575.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 209, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09028", "authors": [{"_id": "69686b880ac10a06522f6aae", "name": "Fengran Mo", "hidden": false}, {"_id": "69686b880ac10a06522f6aaf", "name": "Zhan Su", "hidden": false}, {"_id": "69686b880ac10a06522f6ab0", "user": {"_id": "63164fe6550b00d37db957b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63164fe6550b00d37db957b6/D_SH8nQ2wCOcN-i7tLZ9R.jpeg", "isPro": false, "fullname": "Yuchen Hui", "user": "Meranti", "type": "user"}, "name": "Yuchen Hui", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:41.214Z", "hidden": false}, {"_id": "69686b880ac10a06522f6ab1", "name": "Jinghan Zhang", "hidden": false}, {"_id": "69686b880ac10a06522f6ab2", "name": "Jia Ao Sun", "hidden": false}, {"_id": "69686b880ac10a06522f6ab3", "name": "Zheyuan Liu", "hidden": false}, {"_id": "69686b880ac10a06522f6ab4", "name": "Chao Zhang", "hidden": false}, {"_id": "69686b880ac10a06522f6ab5", "name": "Tetsuya Sakai", "hidden": false}, {"_id": "69686b880ac10a06522f6ab6", "name": "Jian-Yun Nie", "hidden": false}], "publishedAt": "2026-01-13T23:26:30.000Z", "submittedOnDailyAt": "2026-01-15T18:11:42.429Z", "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG", "submittedOnDailyBy": {"_id": "6570cc04c4993b8fb975a2e3", "avatarUrl": "/avatars/64d8e0580ffa8d704933f94985bf7d5c.svg", "isPro": false, "fullname": "Fengran Mo", "user": "fengran", "type": "user"}, "summary": "The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.", "upvotes": 17, "discussionId": "69686b890ac10a06522f6ab7", "ai_summary": "OpenDecoder enhances retrieval-augmented generation by explicitly evaluating retrieved information quality through relevance, ranking, and query performance prediction scores, improving robustness to noisy context.", "ai_keywords": ["large language models", "retrieval-augmented generation", "QPP", "query performance prediction", "relevance score", "ranking score", "post-training"], "organization": {"_id": "6886fa1c363ab9ef0bb63d0a", "name": "UniversitedeMontreal", "fullname": "Universit\u00e9 de Montr\u00e9al", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6886f890bea3309b70e5950c/OZiMpTqoDfbwxXr10n8B4.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u662f\u5728\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u9762\u3002</li>\n    <li>\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u53d6\u51b3\u4e8e\u68c0\u7d22\u4fe1\u606f\u7684\u76f8\u5173\u6027\u548cLLMs\u5904\u7406\u4fe1\u606f\u7684\u80fd\u529b\u3002</li>\n    <li>\u68c0\u7d22\u4fe1\u606f\u7684\u76f8\u5173\u6027\u53ef\u80fd\u4f1a\u56e0\u95ee\u9898\u548c\u6587\u6863\u96c6\u7684\u4e0d\u540c\u800c\u6709\u6240\u53d8\u5316\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5OpenDecoder\uff0c\u901a\u8fc7\u660e\u786e\u8bc4\u4f30\u68c0\u7d22\u4fe1\u606f\u7684\u8d28\u91cf\u6765\u589e\u5f3a\u751f\u6210\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOpenDecoder\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) perform well in tasks like retrieval-augmented generation (RAG).</li>\n    <li>The quality of answers depends on how useful the retrieved information is and how well LLMs process it.</li>\n    <li>The relevance of the retrieved information can vary based on the question and source material.</li>\n    <li>OpenDecoder is a new method that uses evaluation scores of retrieved information to improve answer generation.</li>\n    <li>Tests show that OpenDecoder is more effective and robust than other methods and can be adapted for various uses.</li>\n</ul>"}, "publishedAt": "2026-01-13T18:26:30.000Z", "title": "OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG", "summary": "The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09028.png", "numComments": 1, "submittedBy": {"_id": "6570cc04c4993b8fb975a2e3", "avatarUrl": "/avatars/64d8e0580ffa8d704933f94985bf7d5c.svg", "fullname": "Fengran Mo", "name": "fengran", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6886fa1c363ab9ef0bb63d0a", "name": "UniversitedeMontreal", "fullname": "Universit\u00e9 de Montr\u00e9al", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6886f890bea3309b70e5950c/OZiMpTqoDfbwxXr10n8B4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08605", "authors": [{"_id": "69670413c5e371f6b235d0ea", "name": "Wenyuan Zhang", "hidden": false}, {"_id": "69670413c5e371f6b235d0eb", "name": "Xinghua Zhang", "hidden": false}, {"_id": "69670413c5e371f6b235d0ec", "name": "Haiyang Yu", "hidden": false}, {"_id": "69670413c5e371f6b235d0ed", "name": "Shuaiyi Nie", "hidden": false}, {"_id": "69670413c5e371f6b235d0ee", "name": "Bingli Wu", "hidden": false}, {"_id": "69670413c5e371f6b235d0ef", "name": "Juwei Yue", "hidden": false}, {"_id": "69670413c5e371f6b235d0f0", "name": "Tingwen Liu", "hidden": false}, {"_id": "69670413c5e371f6b235d0f1", "name": "Yongbin Li", "hidden": false}], "publishedAt": "2026-01-13T14:48:34.000Z", "submittedOnDailyAt": "2026-01-15T02:32:45.902Z", "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents", "submittedOnDailyBy": {"_id": "6617c98901ad3a0642a2a08f", "avatarUrl": "/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg", "isPro": false, "fullname": "Wenyuan Zhang", "user": "WYRipple", "type": "user"}, "summary": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.", "upvotes": 15, "discussionId": "69670413c5e371f6b235d0f2", "ai_summary": "ExpSeek enables web agents to proactively seek experience during interaction by using entropy-based timing and tailored content, achieving significant performance improvements across multiple benchmarks.", "ai_keywords": ["experience intervention", "web agents", "step-level proactive seeking", "entropy thresholds", "intrinsic signals", "experience content design", "Qwen3-8B", "Qwen3-32B", "web agent benchmarks"], "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n  <li>\u7ecf\u9a8c\u5e72\u9884\u5728\u7f51\u7edc\u4ee3\u7406\u4e2d\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u4ee3\u7406\u7684\u4e92\u52a8\u80fd\u529b\u3002</li>\n  <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u662f\u5728\u4efb\u52a1\u6267\u884c\u524d\u88ab\u52a8\u6ce8\u5165\u7ecf\u9a8c\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002</li>\n  <li>\u6211\u4eec\u63d0\u51fa\u4e86ExpSeek\uff0c\u91c7\u7528\u4e3b\u52a8\u5bfb\u6c42\u7ecf\u9a8c\u7684\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u4e2d\u8fdb\u884c\u5e72\u9884\u3002</li>\n  <li>\u5b9e\u9a8c\u8868\u660e\uff0cExpSeek\u5728Qwen3-8B\u548c32B\u6a21\u578b\u4e0a\u5206\u522b\u63d0\u9ad8\u4e869.3%\u548c7.5%\u7684\u6027\u80fd\u3002</li>\n  <li>\u7ed3\u679c\u9a8c\u8bc1\u4e86\u71b5\u4f5c\u4e3a\u81ea\u89e6\u53d1\u4fe1\u53f7\u7684\u53ef\u884c\u6027\uff0c\u5373\u4f7f\u662f\u5c0f\u89c4\u6a21\u7684\u7ecf\u9a8c\u6a21\u578b\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u6a21\u578b\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Experience intervention helps web agents interact better by using insights from past experiences.</li>\n    <li>Current methods only use past experiences before tasks, which doesn't adapt well to changing situations.</li>\n    <li>We introduce ExpSeek, which actively seeks relevant experiences during each step of the task.</li>\n    <li>ExpSeek uses specific signals to decide when to intervene and designs tailored experiences for each step.</li>\n    <li>Tests show ExpSeek improves performance by 9.3% and 7.5% on two different models, proving its effectiveness.</li>\n</ul>"}, "publishedAt": "2026-01-13T09:48:34.000Z", "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents", "summary": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08605.png", "numComments": 1, "submittedBy": {"_id": "6617c98901ad3a0642a2a08f", "avatarUrl": "/avatars/cf52fb511f2f31de7940f9c13d19b8e7.svg", "fullname": "Wenyuan Zhang", "name": "WYRipple", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u53ea\u63d0\u4f9b\u6709\u9650\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u7b54\u6848\u5219\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u57df\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u9700\u8981\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u7ebf\u7d22\u3001\u8fdb\u884c\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0Agentic\u65b9\u6cd5\u7684\u6548\u679c\u4f9d\u8d56\u4e8e\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u7ebf\u7d22\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u672a\u6765\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u4e3b\u8981\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often relies on visual clues from videos and information from the web.</li>\n    <li>To address this, the researchers created VideoDR, a new benchmark for video-based question answering.</li>\n    <li>VideoDR includes tasks like extracting visual information from videos, retrieving information from the web, and reasoning using multiple pieces of evidence.</li>\n    <li>The study tested various large language models and found that their performance varies based on how well they keep track of video clues during retrieval.</li>\n    <li>VideoDR helps identify challenges for improving future video question answering systems.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u8868\u73b0\u8584\u5f31\uff0c\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u4e0d\u8db3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u59823\u5c81\u513f\u7ae5\u3002</li>\n    <li>\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5f97\u5206\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0cGemini3-Pro-Preview\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u843d\u540e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u5e74\u4eba\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u662f\u671d\u7740\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u5e76\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u5305\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop basic visual skills much earlier than they learn to speak, but modern Multimodal LLMs (MLLMs) still depend on language to understand visuals.</li>\n    <li>Research shows that top MLLMs struggle with simple visual tasks that even very young children can do easily.</li>\n    <li>BabyVision is a new benchmark created to test MLLMs' visual skills without relying on language, containing 388 tasks in 22 categories.</li>\n    <li>Results indicate that leading MLLMs, like Gemini3-Pro-Preview, score much lower than both 6-year-old children and average adults in visual tasks.</li>\n    <li>BabyVision aims to improve visual perception and reasoning in MLLMs, and resources for this research are available online.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u7684\u201c\u751f\u6210-\u9a8c\u8bc1-\u6539\u8fdb\u201d\u5468\u671f\u6765\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u65e0\u6cd5\u5728\u6709\u9650\u9884\u7b97\u5185\u53d1\u73b0\u66f4\u590d\u6742\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u63d0\u51fa\u7684\u63a7\u5236\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\u65b9\u6cd5\u89e3\u51b3\u4e86\u521d\u59cb\u5316\u504f\u5dee\u548c\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u95ee\u9898\u3002</li>\n    <li>CSE\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u57fa\u4e8e\u53cd\u9988\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5c42\u6b21\u5316\u8fdb\u5316\u8bb0\u5fc6\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCSE\u5728\u4e0d\u540c\u7684LLM\u57fa\u7840\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\uff0c\u4e14\u5728\u65e9\u671f\u751f\u6210\u4e2d\u6548\u7387\u66f4\u9ad8\uff0c\u6301\u7eed\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods improve code generation but often struggle to find better solutions efficiently.</li>\n    <li>The inefficiency is caused by poor starting points, random operations without guidance, and not using past experiences effectively.</li>\n    <li>Controlled Self-Evolution (CSE) aims to fix these issues with three main features: diverse planning for better initial strategies, feedback-guided evolution methods, and a memory system for learning from past successes and failures.</li>\n    <li>Tests show that CSE works better than previous methods across different large language models.</li>\n    <li>CSE shows improved efficiency early on and continues to get better as it evolves.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09688", "authors": [{"_id": "696864c90ac10a06522f6a4a", "name": "Yibo Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4b", "name": "Lei Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4c", "name": "Yue Deng", "hidden": false}, {"_id": "696864c90ac10a06522f6a4d", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:02:22.232Z", "hidden": false}, {"_id": "696864c90ac10a06522f6a4e", "name": "Yao Xiao", "hidden": false}, {"_id": "696864c90ac10a06522f6a4f", "name": "Huanjin Yao", "hidden": false}, {"_id": "696864c90ac10a06522f6a50", "name": "Liwei Kang", "hidden": false}, {"_id": "696864c90ac10a06522f6a51", "name": "Hai Ye", "hidden": false}, {"_id": "696864c90ac10a06522f6a52", "name": "Yongcheng Jing", "hidden": false}, {"_id": "696864c90ac10a06522f6a53", "name": "Lidong Bing", "hidden": false}], "publishedAt": "2026-01-14T18:38:31.000Z", "submittedOnDailyAt": "2026-01-15T01:33:59.520Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "submittedOnDailyBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "upvotes": 90, "discussionId": "696864c90ac10a06522f6a54", "githubRepo": "https://github.com/Infinity-AILab/DeepResearchEval", "githubRepoAddedBy": "user", "ai_summary": "DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.", "ai_keywords": ["automated framework", "deep research task construction", "agentic evaluation", "persona-driven pipeline", "task qualification", "search necessity", "adaptive point-wise quality evaluation", "active fact-checking", "web search", "multi-source evidence integration"], "githubStars": 67, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7528\u4e8e\u591a\u6b65\u9aa4\u7684\u7f51\u7edc\u7814\u7a76\u548c\u5206\u6790\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u4ecd\u7136\u5f88\u56f0\u96be\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6ce8\u91ca\uff0c\u4f9d\u8d56\u9759\u6001\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6216\u8005\u5728\u7f3a\u5c11\u5f15\u7528\u65f6\u65e0\u6cd5\u53ef\u9760\u5730\u9a8c\u8bc1\u4e8b\u5b9e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DeepResearchEval\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u6784\u5efa\u548c\u8bc4\u4f30\u3002</li>\n    <li>\u4efb\u52a1\u6784\u5efa\u4f7f\u7528\u4e2a\u6027\u5316\u7684\u6d41\u7a0b\u751f\u6210\u590d\u6742\u7684\u7814\u7a76\u4efb\u52a1\uff0c\u5e76\u7b5b\u9009\u9700\u8981\u591a\u6e90\u8bc1\u636e\u6574\u5408\u7684\u4efb\u52a1\u3002</li>\n    <li>\u8bc4\u4f30\u91c7\u7528\u52a8\u6001\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u4e3b\u52a8\u4e8b\u5b9e\u68c0\u67e5\uff0c\u80fd\u591f\u5728\u7f3a\u5c11\u5f15\u7528\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u548c\u9a8c\u8bc1\u62a5\u544a\u5185\u5bb9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep research systems help with complex web research and analysis, but evaluating them is difficult.</li>\n    <li>Current evaluation methods need a lot of manual work, use fixed standards, or struggle to check facts without citations.</li>\n    <li>DeepResearchEval is a new automated framework for creating and evaluating research tasks.</li>\n    <li>It uses a persona-driven approach to create realistic research tasks that need information from multiple sources.</li>\n    <li>The evaluation includes a method that adapts to each task and automatically checks facts through web searches.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:38:31.000Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png", "numComments": 1, "submittedBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "fullname": "Keming Wu", "name": "wukeming11", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09259", "authors": [{"_id": "696856230ac10a06522f69dd", "name": "Jian Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69de", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:12.229Z", "hidden": false}, {"_id": "696856230ac10a06522f69df", "name": "Zhangqi Wang", "hidden": false}, {"_id": "696856230ac10a06522f69e0", "name": "Yu He", "hidden": false}, {"_id": "696856230ac10a06522f69e1", "name": "Haoran Luo", "hidden": false}, {"_id": "696856230ac10a06522f69e2", "name": "li yuan", "hidden": false}, {"_id": "696856230ac10a06522f69e3", "name": "Lingling Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69e4", "name": "Rui Mao", "hidden": false}, {"_id": "696856230ac10a06522f69e5", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:14.086Z", "hidden": false}, {"_id": "696856230ac10a06522f69e6", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T07:48:00.000Z", "submittedOnDailyAt": "2026-01-15T00:22:01.292Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "upvotes": 81, "discussionId": "696856230ac10a06522f69e7", "githubRepo": "https://github.com/exoskeletonzj/MAXS", "githubRepoAddedBy": "user", "ai_summary": "MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.", "ai_keywords": ["LLM agents", "tool execution", "reasoning planning", "lookahead strategy", "advantage value", "step consistency variance", "inter-step trend slopes", "trajectory convergence", "multi-tool reasoning", "inference efficiency"], "githubStars": 5, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u901a\u8fc7\u591a\u79cd\u5de5\u5177\u7684\u534f\u4f5c\u5c55\u73b0\u51fa\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002</li>\n    <li>\u5f53\u524d\u65b9\u6cd5\u9762\u4e34\u5c40\u90e8\u76ee\u5149\u77ed\u6d45\u548c\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u504f\u79bb\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMAXS\u7684\u5143\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u591f\u7075\u6d3b\u6574\u5408\u5de5\u5177\u6267\u884c\u548c\u63a8\u7406\u89c4\u5212\u3002</li>\n    <li>MAXS\u91c7\u7528\u524d\u77bb\u7b56\u7565\uff0c\u63d0\u524d\u51e0\u6b65\u6269\u5c55\u63a8\u7406\u8def\u5f84\uff0c\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u7684\u4f18\u52bf\u503c\u3002</li>\n    <li>\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0cMAXS\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u524d\u77bb\u7b56\u7565\u548c\u5de5\u5177\u4f7f\u7528\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model (LLM) Agents can reason effectively by using multiple tools together.</li>\n    <li>Current methods face problems like short-sighted decisions and errors that lead to bigger mistakes.</li>\n    <li>To solve these issues, a new approach called meta-adaptive exploration with LLM agents (MAXS) is proposed.</li>\n    <li>MAXS uses a lookahead strategy to improve reasoning by predicting tool effectiveness and ensuring stable reasoning steps.</li>\n    <li>Tests show that MAXS works better and faster than previous methods across various models and datasets.</li>\n</ul>"}, "publishedAt": "2026-01-14T02:48:00.000Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09259.png", "numComments": 3, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09274", "authors": [{"_id": "6968568f0ac10a06522f69e9", "name": "Jian Zhang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ea", "name": "Yu He", "hidden": false}, {"_id": "6968568f0ac10a06522f69eb", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:02.764Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69ec", "name": "Zhangqi Wang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ed", "name": "Kai He", "hidden": false}, {"_id": "6968568f0ac10a06522f69ee", "name": "Fangzhi Xu", "hidden": false}, {"_id": "6968568f0ac10a06522f69ef", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:05.035Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69f0", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T08:17:41.000Z", "submittedOnDailyAt": "2026-01-15T00:23:45.077Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "upvotes": 74, "discussionId": "6968568f0ac10a06522f69f1", "projectPage": "https://a3-bench.github.io/", "githubRepo": "https://github.com/exoskeletonzj/A3-Bench", "githubRepoAddedBy": "user", "githubStars": 0, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u63a8\u7406\u4e0d\u4ec5\u4f9d\u8d56\u903b\u8f91\u63a8\u7406\uff0c\u8fd8\u9700\u8981\u6fc0\u6d3b\u5df2\u6709\u77e5\u8bc6\u548c\u7ecf\u9a8c\u7ed3\u6784\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6216\u6b65\u9aa4\u4e00\u81f4\u6027\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u9a71\u52a8\u673a\u5236\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86A^3-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u7684\u65b0\u57fa\u51c6\uff0c\u57fa\u4e8e\u951a\u70b9\u548c\u5438\u5f15\u70b9\u7684\u6fc0\u6d3b\u673a\u5236\u3002</li>\n    <li>\u8be5\u57fa\u51c6\u5305\u542b2198\u4e2a\u79d1\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u53cc\u5c3a\u5ea6\u8bb0\u5fc6\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u91cf\u8bb0\u5fc6\u6fc0\u6d3b\u7387\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1A^3-Bench\uff0c\u5206\u6790\u8bb0\u5fc6\u6fc0\u6d3b\u5bf9\u63a8\u7406\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u8bb0\u5fc6\u9a71\u52a8\u79d1\u5b66\u63a8\u7406\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Scientific reasoning uses both logic and prior knowledge to enhance understanding.</li>\n    <li>Current tests mainly focus on answers rather than the memory processes that support reasoning.</li>\n    <li>A^3-Bench is a new benchmark created to evaluate scientific reasoning using memory-driven techniques.</li>\n    <li>It includes 2,198 annotated science reasoning problems and measures how memory aids reasoning through a new framework.</li>\n    <li>The study shows how memory activation influences reasoning skills and offers insights into this process.</li>\n</ul>"}, "publishedAt": "2026-01-14T03:17:41.000Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09274.png", "numComments": 2, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05593", "authors": [{"_id": "6965b990fc8c4ecc02c7f8df", "name": "Jingcheng Hu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e0", "name": "Yinmin Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e1", "name": "Shijie Shang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e2", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e3", "name": "Yue Peng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e4", "name": "Zhewei Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e5", "name": "Hebin Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e6", "name": "Xin Wu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e7", "name": "Jie Cheng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e8", "name": "Fanqi Wan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e9", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ea", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8eb", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ec", "name": "Ailin Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ed", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ee", "name": "Qi Han", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ef", "name": "Zheng Ge", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f0", "name": "Daxin Jiang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f1", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f2", "name": "Heung-Yeung Shum", "hidden": false}], "publishedAt": "2026-01-09T07:24:43.000Z", "submittedOnDailyAt": "2026-01-13T00:51:45.124Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "upvotes": 62, "discussionId": "6965b990fc8c4ecc02c7f8f3", "githubRepo": "https://github.com/stepfun-ai/PaCoRe", "githubRepoAddedBy": "user", "ai_summary": "Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.", "ai_keywords": ["test-time compute", "sequential reasoning", "parallel exploration", "message-passing architecture", "reinforcement learning", "multi-million-token", "HMMT 2025", "GPT-5"], "githubStars": 261, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u79f0\u4e3a\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u80fd\u529b\u7684\u9650\u5236\u3002</li>\n    <li>PaCoRe \u91c7\u7528\u5e76\u884c\u63a2\u7d22\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u6d88\u606f\u4f20\u9012\u67b6\u6784\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u987a\u5e8f\u63a8\u7406\u3002</li>\n    <li>\u6bcf\u4e00\u8f6e\u90fd\u542f\u52a8\u591a\u4e2a\u5e76\u884c\u63a8\u7406\u8def\u5f84\uff0c\u5c06\u7ed3\u679c\u538b\u7f29\u6210\u6709\u9650\u4e0a\u4e0b\u6587\u7684\u6d88\u606f\uff0c\u5e76\u5408\u6210\u8fd9\u4e9b\u6d88\u606f\u4ee5\u6307\u5bfc\u4e0b\u4e00\u8f6e\u63a8\u7406\u3002</li>\n    <li>\u8be5\u6a21\u578b\u7ecf\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u591f\u5728\u4e0d\u8d85\u8fc7\u4e0a\u4e0b\u6587\u9650\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u5904\u7406\u6570\u767e\u4e07\u4e2a\u4ee4\u724c\u7684\u6709\u6548\u8ba1\u7b97\u3002</li>\n    <li>PaCoRe \u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u4e0a\u8d85\u8d8a\u4e86\u524d\u6cbf\u7cfb\u7edf\uff0c\u5f00\u653e\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>PaCoRe is a new framework that improves how language models handle reasoning tasks by allowing them to work in parallel instead of sequentially.</li>\n    <li>It uses a message-passing system to coordinate multiple reasoning processes at the same time, which helps manage large amounts of information.</li>\n    <li>The model is trained using reinforcement learning to enhance its ability to synthesize information effectively.</li>\n    <li>PaCoRe significantly improves performance in various areas, especially in mathematics, outperforming other models like GPT-5.</li>\n    <li>The developers are making their model and resources available to the public to encourage further research and development.</li>\n</ul>"}, "publishedAt": "2026-01-09T02:24:43.000Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05593.png", "numComments": 1, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06789", "authors": [{"_id": "69671036c5e371f6b235d143", "user": {"_id": "692881094c3f4293dfe29e3d", "avatarUrl": "/avatars/bddfaae8041a45498d46ef65ba17c920.svg", "isPro": false, "fullname": "qihao wang", "user": "jimson991", "type": "user"}, "name": "Qihao Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:07.079Z", "hidden": false}, {"_id": "69671036c5e371f6b235d144", "user": {"_id": "64b74fca17570fdff9b2aded", "avatarUrl": "/avatars/8b3519a7011af52dadc87ffef700c77c.svg", "isPro": false, "fullname": "Ziming Cheng", "user": "cadche", "type": "user"}, "name": "Ziming Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:13.370Z", "hidden": false}, {"_id": "69671036c5e371f6b235d145", "user": {"_id": "6513ee3c9af40a65586b43f5", "avatarUrl": "/avatars/815ed3876cefa12b25bf955edcbf71a3.svg", "isPro": false, "fullname": "shuo zhang", "user": "shuozhang", "type": "user"}, "name": "Shuo Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:18.640Z", "hidden": false}, {"_id": "69671036c5e371f6b235d146", "name": "Fan Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d147", "name": "Rui Xu", "hidden": false}, {"_id": "69671036c5e371f6b235d148", "name": "Heng Lian", "hidden": false}, {"_id": "69671036c5e371f6b235d149", "user": {"_id": "65bb3c545a5dbabc818e9044", "avatarUrl": "/avatars/4c239557bd5e33179cbf4f3a440bbf33.svg", "isPro": false, "fullname": "Kunyi Wang", "user": "KunyiWang", "type": "user"}, "name": "Kunyi Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:25.411Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14a", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaoming Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:47.145Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14b", "name": "Jianghao Yin", "hidden": false}, {"_id": "69671036c5e371f6b235d14c", "name": "Sen Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14d", "name": "Yue Hu", "hidden": false}, {"_id": "69671036c5e371f6b235d14e", "user": {"_id": "64803e5dc57f629056c601f1", "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg", "isPro": false, "fullname": "Shaolei Zhang", "user": "zhangshaolei", "type": "user"}, "name": "Shaolei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:13.491Z", "hidden": false}, {"_id": "69671036c5e371f6b235d14f", "name": "Yanbing Liu", "hidden": false}, {"_id": "69671036c5e371f6b235d150", "user": {"_id": "6874f7f0f8e67e9b5714adf2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png", "isPro": false, "fullname": "RongHao Chen", "user": "SuPA4ki", "type": "user"}, "name": "Ronghao Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:49:02.800Z", "hidden": false}, {"_id": "69671036c5e371f6b235d151", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:48:57.426Z", "hidden": false}], "publishedAt": "2026-01-11T06:41:26.000Z", "submittedOnDailyAt": "2026-01-14T01:40:39.607Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "upvotes": 59, "discussionId": "69671036c5e371f6b235d152", "githubRepo": "https://github.com/QuantaAlpha/MemGovern", "githubRepoAddedBy": "user", "ai_summary": "MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.", "ai_keywords": ["autonomous software engineering", "SWE agents", "closed-world limitation", "open-world experience", "GitHub", "experience governance", "experience cards", "agentic experience search", "SWE-bench Verified"], "githubStars": 19, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4ee3\u7406\u9762\u4e34\u201c\u5c01\u95ed\u4e16\u754c\u201d\u9650\u5236\uff0c\u53ea\u80fd\u4f7f\u7528\u5c40\u90e8\u4e0a\u4e0b\u6587\u4fee\u590dbug\uff0c\u5ffd\u7565\u4e86\u4e30\u5bcc\u7684\u5386\u53f2\u4eba\u7c7b\u7ecf\u9a8c\u3002</li>\n    <li>\u5f53\u524d\u7684\u95ee\u9898\u8ffd\u8e2a\u6570\u636e\u7ed3\u6784\u677e\u6563\uff0c\u96be\u4ee5\u8bbf\u95eeGitHub\u7b49\u5e73\u53f0\u4e0a\u7684\u5f00\u653e\u4e16\u754c\u7ecf\u9a8c\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86MemGovern\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u539f\u59cbGitHub\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u4f9b\u4ee3\u7406\u4f7f\u7528\u7684\u7ecf\u9a8c\u8bb0\u5fc6\u3002</li>\n    <li>MemGovern\u901a\u8fc7\u7ecf\u9a8c\u6cbb\u7406\u751f\u6210\u4ee3\u7406\u53cb\u597d\u7684\u7ecf\u9a8c\u5361\u7247\uff0c\u5e76\u5f15\u5165\u903b\u8f91\u9a71\u52a8\u7684\u7ecf\u9a8c\u641c\u7d22\u7b56\u7565\u3002</li>\n    <li>MemGovern\u751f\u6210\u4e86135K\u4e2a\u7ecf\u9a8c\u5361\u7247\uff0c\u63d0\u5347\u4e86SWE-bench Verified\u7684\u89e3\u51b3\u73874.65%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autonomous software engineering agents are changing how programming is done but have a limitation in using only local context to fix bugs.</li>\n    <li>They miss out on valuable historical data from platforms like GitHub due to the chaotic nature of issue-tracking data.</li>\n    <li>MemGovern is a new framework that organizes raw GitHub data into useful experience cards for these agents.</li>\n    <li>It helps agents find and use human expertise effectively, improving their performance significantly.</li>\n    <li>MemGovern has created 135,000 experience cards and improved bug resolution rates by 4.65% on a specific benchmark.</li>\n</ul>"}, "publishedAt": "2026-01-11T01:41:26.000Z", "title": "MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences", "summary": "While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06789.png", "numComments": 1, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07022", "authors": [{"_id": "6966474587c71000b5a910d2", "user": {"_id": "65446c938737c799e9ad6f83", "avatarUrl": "/avatars/6ade251e01442b14cbf8cd7888358fd1.svg", "isPro": false, "fullname": "Sungrae Park", "user": "sungrae-park", "type": "user"}, "name": "Sungrae Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-14T12:41:08.385Z", "hidden": false}, {"_id": "6966474587c71000b5a910d3", "name": "Sanghoon Kim", "hidden": false}, {"_id": "6966474587c71000b5a910d4", "name": "Jungho Cho", "hidden": false}, {"_id": "6966474587c71000b5a910d5", "name": "Gyoungjin Gim", "hidden": false}, {"_id": "6966474587c71000b5a910d6", "name": "Dawoon Jung", "hidden": false}, {"_id": "6966474587c71000b5a910d7", "name": "Mikyoung Cha", "hidden": false}, {"_id": "6966474587c71000b5a910d8", "name": "Eunhae Choo", "hidden": false}, {"_id": "6966474587c71000b5a910d9", "name": "Taekgyu Hong", "hidden": false}, {"_id": "6966474587c71000b5a910da", "name": "Minbyul Jeong", "hidden": false}, {"_id": "6966474587c71000b5a910db", "name": "SeHwan Joo", "hidden": false}, {"_id": "6966474587c71000b5a910dc", "name": "Minsoo Khang", "hidden": false}, {"_id": "6966474587c71000b5a910dd", "name": "Eunwon Kim", "hidden": false}, {"_id": "6966474587c71000b5a910de", "name": "Minjeong Kim", "hidden": false}, {"_id": "6966474587c71000b5a910df", "name": "Sujeong Kim", "hidden": false}, {"_id": "6966474587c71000b5a910e0", "name": "Yunsu Kim", "hidden": false}, {"_id": "6966474587c71000b5a910e1", "name": "Hyeonju Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e2", "name": "Seunghyun Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e3", "name": "Sukyung Lee", "hidden": false}, {"_id": "6966474587c71000b5a910e4", "name": "Siyoung Park", "hidden": false}, {"_id": "6966474587c71000b5a910e5", "name": "Gyungin Shin", "hidden": false}, {"_id": "6966474587c71000b5a910e6", "user": {"_id": "64f04fa29a957782e2224dea", "avatarUrl": "/avatars/db853c30ceb59ddabc9a83dc25845690.svg", "isPro": false, "fullname": "Inseo Song", "user": "SSON9", "type": "user"}, "name": "Inseo Song", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T09:52:53.501Z", "hidden": false}, {"_id": "6966474587c71000b5a910e7", "name": "Wonho Song", "hidden": false}, {"_id": "6966474587c71000b5a910e8", "name": "Seonghoon Yang", "hidden": false}, {"_id": "6966474587c71000b5a910e9", "user": {"_id": "66e0d4bf290df82f137de44c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66e0d4bf290df82f137de44c/RJ43RxY56_OvZmauF88Tw.jpeg", "isPro": false, "fullname": "Kyle Yi", "user": "younatics", "type": "user"}, "name": "Seungyoun Yi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-14T12:42:43.424Z", "hidden": false}, {"_id": "6966474587c71000b5a910ea", "name": "Sanghoon Yoon", "hidden": false}, {"_id": "6966474587c71000b5a910eb", "name": "Jeonghyun Ko", "hidden": false}, {"_id": "6966474587c71000b5a910ec", "name": "Seyoung Song", "hidden": false}, {"_id": "6966474587c71000b5a910ed", "name": "Keunwoo Choi", "hidden": false}, {"_id": "6966474587c71000b5a910ee", "name": "Hwalsuk Lee", "hidden": false}, {"_id": "6966474587c71000b5a910ef", "name": "Sunghun Kim", "hidden": false}, {"_id": "6966474587c71000b5a910f0", "name": "Du-Seong Chang", "hidden": false}, {"_id": "6966474587c71000b5a910f1", "name": "Kyunghyun Cho", "hidden": false}, {"_id": "6966474587c71000b5a910f2", "name": "Junsuk Choe", "hidden": false}, {"_id": "6966474587c71000b5a910f3", "name": "Hwaran Lee", "hidden": false}, {"_id": "6966474587c71000b5a910f4", "name": "Jae-Gil Lee", "hidden": false}, {"_id": "6966474587c71000b5a910f5", "name": "KyungTae Lim", "hidden": false}, {"_id": "6966474587c71000b5a910f6", "name": "Alice Oh", "hidden": false}], "publishedAt": "2026-01-11T18:33:09.000Z", "submittedOnDailyAt": "2026-01-14T02:22:03.363Z", "title": "Solar Open Technical Report", "submittedOnDailyBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "isPro": false, "fullname": "Minbyul Jeong", "user": "Minbyul", "type": "user"}, "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.", "upvotes": 50, "discussionId": "6966474587c71000b5a910f7", "ai_summary": "Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.", "ai_keywords": ["Mixture-of-Experts", "language model", "underserved languages", "data synthesis", "progressive curriculum", "reinforcement learning", "SnapPO", "domain-specific data", "quality thresholds", "composition optimization"], "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "summary_zh": "<ul>\n    <li>Solar Open \u662f\u4e00\u4e2a bilingual Mixture-of-Experts \u8bed\u8a00\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u4e3a1020\u4ebf\uff0c\u4e13\u4e3a\u670d\u52a1\u4e0d\u8db3\u7684\u8bed\u8a00\u8bbe\u8ba1\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u5408\u62104.5\u4e07\u4ebf\u4e2a\u9ad8\u8d28\u91cf\u7684\u9886\u57df\u7279\u5b9a\u6570\u636e\u6765\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u534f\u8c03\u6570\u636e\uff0c\u9010\u6b65\u4f18\u5316\u7ec4\u5408\u3001\u8d28\u91cf\u548c\u9886\u57df\u8986\u76d6\uff0c\u5171\u4f7f\u7528\u4e8620\u4e07\u4ebf\u4e2a\u6807\u8bb0\u3002</li>\n    <li>\u4f7f\u7528SnapPO\u6846\u67b6\u8fdb\u884c\u9ad8\u6548\u4f18\u5316\uff0c\u4ee5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSolar Open \u5c55\u73b0\u4e86\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Solar Open is a large language model designed to help languages that don't have much data available.</li>\n    <li>It was trained using 4.5 trillion tokens of high-quality data to overcome data scarcity.</li>\n    <li>The training process involved a structured approach to improve data quality and coverage.</li>\n    <li>It uses a new framework called SnapPO to enhance reasoning abilities through efficient optimization.</li>\n    <li>Tests show that Solar Open performs well, especially in English and Korean, proving its effectiveness for developing AI for underserved languages.</li>\n</ul>"}, "publishedAt": "2026-01-11T13:33:09.000Z", "title": "Solar Open Technical Report", "summary": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07022.png", "numComments": 1, "submittedBy": {"_id": "64587be872b60ae7a3817858", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png", "fullname": "Minbyul Jeong", "name": "Minbyul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "62940d125d1c94a62e838db2", "name": "upstage", "fullname": "upstage", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09088", "authors": [{"_id": "69688bbc0ac10a06522f6aeb", "user": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "name": "Shaotian Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:27.639Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aec", "name": "Kaiyuan Liu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aed", "user": {"_id": "64b73e3830a0b8ff60145a29", "avatarUrl": "/avatars/297469812b57b2ddf7d52b9391d80bde.svg", "isPro": false, "fullname": "Chen Shen", "user": "zjushenchen", "type": "user"}, "name": "Chen Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:19.260Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aee", "user": {"_id": "6225b0d87f5fba1007d62fae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6225b0d87f5fba1007d62fae/clONu5C-lkoSswcJjcG0u.jpeg", "isPro": false, "fullname": "Bing Wang", "user": "wangbing1416", "type": "user"}, "name": "Bing Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:17.091Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aef", "user": {"_id": "694a226980a37f293a4ce7c0", "avatarUrl": "/avatars/7a48f4eeb80a1b5688cbfb10a59765a0.svg", "isPro": false, "fullname": "Sinan Fan", "user": "sinan25", "type": "user"}, "name": "Sinan Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:14.748Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af0", "name": "Jun Zhang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af1", "name": "Yue Wu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af2", "name": "Zheng Wang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af3", "name": "Jieping Ye", "hidden": false}], "publishedAt": "2026-01-14T02:43:17.000Z", "submittedOnDailyAt": "2026-01-15T07:24:58.461Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "submittedOnDailyBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "upvotes": 43, "discussionId": "69688bbc0ac10a06522f6af4", "projectPage": "https://github.com/D2I-ai/dasd-thinking", "githubRepo": "https://github.com/D2I-ai/dasd-thinking", "githubRepoAddedBy": "user", "ai_summary": "A lightweight open-source reasoning model achieves state-of-the-art performance through enhanced sequence-level distillation that addresses limitations in current teacher-student knowledge transfer methods.", "ai_keywords": ["sequence-level distillation", "teacher-student distillation", "SFT", "heuristic rules", "output distribution", "generalization capability", "exposure bias", "teacher-forced training", "autoregressive inference"], "githubStars": 16, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86DASD-4B-Thinking\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u6570\u5b66\u3001\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u66f4\u5927\u7684\u6a21\u578b\u3002</li>\n    <li>\u62a5\u544a\u6279\u5224\u6027\u5730\u91cd\u65b0\u5ba1\u89c6\u4e86\u5f53\u524d\u6d41\u884c\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u5b9e\u8df5\u4e2d\u7684\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b9\u6cd5\u521b\u65b0\uff0c\u5f62\u6210\u4e86\u6539\u8fdb\u7684\u5e8f\u5217\u7ea7\u84b8\u998f\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>DASD-4B-Thinking\u4ec5\u4f7f\u7528448K\u8bad\u7ec3\u6837\u672c\u5c31\u53d6\u5f97\u4e86\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u6a21\u578b\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u4ee5\u652f\u6301\u793e\u533a\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DASD-4B-Thinking is a new open-source reasoning model that performs very well in tasks like math, science, and code generation.</li>\n    <li>The report critiques current methods of training models, specifically focusing on how they learn from teacher models.</li>\n    <li>It identifies three main problems in existing training methods: poor representation of teacher outputs, misalignment in learning, and exposure bias.</li>\n    <li>To improve model training, the authors suggest new methods that enhance the interaction between teacher and student models.</li>\n    <li>DASD-4B-Thinking achieves strong results using significantly fewer training samples than most other models and the authors share their models and datasets for community use.</li>\n</ul>"}, "publishedAt": "2026-01-13T21:43:17.000Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09088.png", "numComments": 4, "submittedBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "fullname": "Shaotian", "name": "ystluffy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u53ea\u63d0\u4f9b\u6709\u9650\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u7b54\u6848\u5219\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u57df\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u9700\u8981\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u7ebf\u7d22\u3001\u8fdb\u884c\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0Agentic\u65b9\u6cd5\u7684\u6548\u679c\u4f9d\u8d56\u4e8e\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u7ebf\u7d22\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u672a\u6765\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u4e3b\u8981\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often relies on visual clues from videos and information from the web.</li>\n    <li>To address this, the researchers created VideoDR, a new benchmark for video-based question answering.</li>\n    <li>VideoDR includes tasks like extracting visual information from videos, retrieving information from the web, and reasoning using multiple pieces of evidence.</li>\n    <li>The study tested various large language models and found that their performance varies based on how well they keep track of video clues during retrieval.</li>\n    <li>VideoDR helps identify challenges for improving future video question answering systems.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u8feb\u5207\u9700\u8981\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u62bd\u8c61\uff0c\u5bfc\u81f4\u53ef\u91cd\u590d\u6027\u5dee\u3002</li>\n    <li>\u63d0\u51fa\u4e86DataFlow\u6846\u67b6\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\uff0c\u652f\u6301\u8c03\u8bd5\u548c\u4f18\u5316\u7684\u7ba1\u9053\u6784\u5efa\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\uff0c\u5e76\u652f\u6301\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u516d\u4e2a\u9886\u57df\u7684\u7ba1\u9053\u3002</li>\n    <li>DataFlow\u5728\u516d\u4e2a\u4f7f\u7528\u6848\u4f8b\u4e2d\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u4e0e\u5176\u4ed6\u6570\u636e\u96c6\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There's a growing need for better data preparation for Large Language Models (LLMs), but current methods are often unorganized and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create high-quality data preparation pipelines in a more efficient and structured way.</li>\n    <li>It offers nearly 200 reusable tools and various pipelines for different tasks like text processing, code handling, and knowledge extraction.</li>\n    <li>DataFlow-Agent can automatically create executable data pipelines from natural language instructions, making it easier to use.</li>\n    <li>In tests, DataFlow improved the performance of LLMs significantly compared to traditional datasets, proving its effectiveness for AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u8868\u73b0\u8584\u5f31\uff0c\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u4e0d\u8db3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u59823\u5c81\u513f\u7ae5\u3002</li>\n    <li>\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5f97\u5206\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0cGemini3-Pro-Preview\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u843d\u540e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u5e74\u4eba\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u662f\u671d\u7740\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u5e76\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u5305\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop basic visual skills much earlier than they learn to speak, but modern Multimodal LLMs (MLLMs) still depend on language to understand visuals.</li>\n    <li>Research shows that top MLLMs struggle with simple visual tasks that even very young children can do easily.</li>\n    <li>BabyVision is a new benchmark created to test MLLMs' visual skills without relying on language, containing 388 tasks in 22 categories.</li>\n    <li>Results indicate that leading MLLMs, like Gemini3-Pro-Preview, score much lower than both 6-year-old children and average adults in visual tasks.</li>\n    <li>BabyVision aims to improve visual perception and reasoning in MLLMs, and resources for this research are available online.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u4f7f\u7528\u89c6\u89c9\u7ebf\u7d22\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u5f62\u6210\u4e00\u4e2a\u57fa\u4e8e\u5730\u56fe\u7684\u667a\u80fd\u4f53\u5faa\u73af\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7edd\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728500\u7c73\u51c6\u786e\u7387\u4e0a\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to determine where a photo was taken using visual cues.</li>\n    <li>Current models often ignore the human strategy of using maps, which this work aims to incorporate.</li>\n    <li>The authors introduce a method called Thinking with Map, using an agent-in-the-map loop for better location prediction.</li>\n    <li>They use a two-step process that includes reinforcement learning to enhance the model's decision-making and parallel testing for exploring multiple options.</li>\n    <li>The new method, tested on real-world images using the MAPBench benchmark, shows significant improvement in accuracy over existing models.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u5206\u5f00\u7684\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u589e\u5f3a\u6027\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u63a8\u7406\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u5411\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual language inputs like text and images.</li>\n    <li>It combines video creation, editing, and reasoning tasks into one efficient system, rather than using separate methods.</li>\n    <li>The framework can process various user inputs into a single format to produce impressive video content.</li>\n    <li>It is built on a strong data system and uses advanced training methods for better performance.</li>\n    <li>Kling-Omni shows excellent skills in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 101, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 88, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u4e0d\u80fd\u6709\u6548\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u52a8\u6001\u8868\u8fbe\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u5c06\u8bb0\u5fc6\u5355\u4f4d\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4ea4\u4e92\uff0c\u4fc3\u8fdb\u6df1\u5c42\u63a8\u7406\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models for complex tasks requiring deep reasoning.</li>\n    <li>Current memory systems in RAG are mainly passive, storing isolated facts without utilizing their connections for better reasoning.</li>\n    <li>This limitation leads to fragmented reasoning and weak understanding in complex situations.</li>\n    <li>HGMem is a new memory system that uses a hypergraph structure to enhance memory representation and enable dynamic reasoning.</li>\n    <li>Tests show that HGMem significantly improves the performance of multi-step RAG on various challenging tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u6a21\u578b\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u51c6\u786e\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5f00\u59cb\u5f15\u5165\u591a\u79cd\u5956\u52b1\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u671d\u5411\u671f\u671b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u7684\u591a\u5956\u52b1\u4f18\u5316\u65b9\u6cd5\uff08GRPO\uff09\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u4fe1\u53f7\u5206\u8fa8\u7387\u964d\u4f4e\uff0c\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002</li>\n    <li>\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08GDPO\uff09\uff0c\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u7684\u5f52\u4e00\u5316\uff0c\u4fdd\u7559\u5956\u52b1\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u63d0\u9ad8\u4f18\u5316\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u666e\u904d\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>As language models improve, users want them to not only be accurate but also align with different human preferences.</li>\n    <li>Reinforcement learning (RL) now uses multiple rewards to guide models, but applying Group Relative Policy Optimization (GRPO) can cause issues.</li>\n    <li>Using GRPO can lead to the collapse of reward differences, making training less effective and sometimes causing early failures.</li>\n    <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) to better handle multiple rewards.</li>\n    <li>GDPO shows better performance than GRPO in tasks like tool calling, math reasoning, and coding reasoning, improving both accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u7684\u201c\u751f\u6210-\u9a8c\u8bc1-\u6539\u8fdb\u201d\u5468\u671f\u6765\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u65e0\u6cd5\u5728\u6709\u9650\u9884\u7b97\u5185\u53d1\u73b0\u66f4\u590d\u6742\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u63d0\u51fa\u7684\u63a7\u5236\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\u65b9\u6cd5\u89e3\u51b3\u4e86\u521d\u59cb\u5316\u504f\u5dee\u548c\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u95ee\u9898\u3002</li>\n    <li>CSE\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u57fa\u4e8e\u53cd\u9988\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5c42\u6b21\u5316\u8fdb\u5316\u8bb0\u5fc6\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCSE\u5728\u4e0d\u540c\u7684LLM\u57fa\u7840\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\uff0c\u4e14\u5728\u65e9\u671f\u751f\u6210\u4e2d\u6548\u7387\u66f4\u9ad8\uff0c\u6301\u7eed\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods improve code generation but often struggle to find better solutions efficiently.</li>\n    <li>The inefficiency is caused by poor starting points, random operations without guidance, and not using past experiences effectively.</li>\n    <li>Controlled Self-Evolution (CSE) aims to fix these issues with three main features: diverse planning for better initial strategies, feedback-guided evolution methods, and a memory system for learning from past successes and failures.</li>\n    <li>Tests show that CSE works better than previous methods across different large language models.</li>\n    <li>CSE shows improved efficiency early on and continues to get better as it evolves.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>\u81ea\u52a8\u5f62\u5f0f\u5316\u6280\u672f\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u7ffb\u8bd1\u4e3a\u5f62\u5f0f\u5316\u8bed\u53e5\uff0c\u5e2e\u52a9\u673a\u5668\u63a8\u7406\uff0c\u4f46\u9762\u4e34\u591a\u6a21\u6001\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMFormalizer\uff0c\u901a\u8fc7\u5c06\u771f\u5b9e\u4e16\u754c\u7684\u6570\u5b66\u548c\u7269\u7406\u5b9e\u4f53\u4e0e\u9002\u5e94\u6027\u57fa\u7840\u7ed3\u5408\uff0c\u6269\u5c55\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u3002</li>\n    <li>MMFormalizer\u901a\u8fc7\u9012\u5f52\u57fa\u7840\u548c\u516c\u7406\u7ec4\u5408\uff0c\u4ece\u611f\u77e5\u57fa\u7840\u6784\u5efa\u6b63\u5f0f\u547d\u9898\uff0c\u786e\u4fdd\u6bcf\u4e2a\u62bd\u8c61\u90fd\u6709\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5PhyX-AF\u4e0a\u8bc4\u4f30MMFormalizer\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u81ea\u52a8\u5f62\u5f0f\u5316\u4efb\u52a1\uff0c\u663e\u793a\u524d\u6cbf\u6a21\u578b\u5982GPT-5\u5728\u7269\u7406\u63a8\u7406\u4e0a\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>MMFormalizer\u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization helps convert natural language math into formal statements for machine reasoning, but struggles with physical concepts that require understanding visual elements.</li>\n    <li>MMFormalizer is a new method that combines real-world math and physics with visual evidence to create formal propositions using a process called recursive grounding.</li>\n    <li>It has been tested on a new benchmark called PhyX-AF, which includes a variety of multimodal autoformalization tasks.</li>\n    <li>Results show that advanced models like GPT-5 perform well in physical reasoning, while geometry tasks are still quite difficult.</li>\n    <li>MMFormalizer is the first method that can effectively handle complex topics in classical mechanics, relativity, quantum mechanics, and thermodynamics.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09688", "authors": [{"_id": "696864c90ac10a06522f6a4a", "name": "Yibo Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4b", "name": "Lei Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4c", "name": "Yue Deng", "hidden": false}, {"_id": "696864c90ac10a06522f6a4d", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:02:22.232Z", "hidden": false}, {"_id": "696864c90ac10a06522f6a4e", "name": "Yao Xiao", "hidden": false}, {"_id": "696864c90ac10a06522f6a4f", "name": "Huanjin Yao", "hidden": false}, {"_id": "696864c90ac10a06522f6a50", "name": "Liwei Kang", "hidden": false}, {"_id": "696864c90ac10a06522f6a51", "name": "Hai Ye", "hidden": false}, {"_id": "696864c90ac10a06522f6a52", "name": "Yongcheng Jing", "hidden": false}, {"_id": "696864c90ac10a06522f6a53", "name": "Lidong Bing", "hidden": false}], "publishedAt": "2026-01-14T18:38:31.000Z", "submittedOnDailyAt": "2026-01-15T01:33:59.520Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "submittedOnDailyBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "upvotes": 90, "discussionId": "696864c90ac10a06522f6a54", "githubRepo": "https://github.com/Infinity-AILab/DeepResearchEval", "githubRepoAddedBy": "user", "ai_summary": "DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.", "ai_keywords": ["automated framework", "deep research task construction", "agentic evaluation", "persona-driven pipeline", "task qualification", "search necessity", "adaptive point-wise quality evaluation", "active fact-checking", "web search", "multi-source evidence integration"], "githubStars": 67, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7528\u4e8e\u591a\u6b65\u9aa4\u7684\u7f51\u7edc\u7814\u7a76\u548c\u5206\u6790\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u4ecd\u7136\u5f88\u56f0\u96be\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6ce8\u91ca\uff0c\u4f9d\u8d56\u9759\u6001\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6216\u8005\u5728\u7f3a\u5c11\u5f15\u7528\u65f6\u65e0\u6cd5\u53ef\u9760\u5730\u9a8c\u8bc1\u4e8b\u5b9e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DeepResearchEval\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u6784\u5efa\u548c\u8bc4\u4f30\u3002</li>\n    <li>\u4efb\u52a1\u6784\u5efa\u4f7f\u7528\u4e2a\u6027\u5316\u7684\u6d41\u7a0b\u751f\u6210\u590d\u6742\u7684\u7814\u7a76\u4efb\u52a1\uff0c\u5e76\u7b5b\u9009\u9700\u8981\u591a\u6e90\u8bc1\u636e\u6574\u5408\u7684\u4efb\u52a1\u3002</li>\n    <li>\u8bc4\u4f30\u91c7\u7528\u52a8\u6001\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u4e3b\u52a8\u4e8b\u5b9e\u68c0\u67e5\uff0c\u80fd\u591f\u5728\u7f3a\u5c11\u5f15\u7528\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u548c\u9a8c\u8bc1\u62a5\u544a\u5185\u5bb9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep research systems help with complex web research and analysis, but evaluating them is difficult.</li>\n    <li>Current evaluation methods need a lot of manual work, use fixed standards, or struggle to check facts without citations.</li>\n    <li>DeepResearchEval is a new automated framework for creating and evaluating research tasks.</li>\n    <li>It uses a persona-driven approach to create realistic research tasks that need information from multiple sources.</li>\n    <li>The evaluation includes a method that adapts to each task and automatically checks facts through web searches.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:38:31.000Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png", "numComments": 1, "submittedBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "fullname": "Keming Wu", "name": "wukeming11", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 16, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53eb\u505a\u201c\u591a\u6837\u6027\u8d85\u8fde\u63a5\u201d\uff08HC\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u6b8b\u5dee\u8fde\u63a5\u3002</li>\n    <li>\u8fd9\u79cd\u65b0\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u524a\u5f31\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u8eab\u4efd\u6620\u5c04\u7279\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6269\u5c55\u6027\u5dee\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u201d\uff08mHC\uff09\uff0c\u53ef\u4ee5\u6062\u590d\u8eab\u4efd\u6620\u5c04\u7279\u6027\uff0c\u5e76\u4f18\u5316\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cmHC\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u6709\u6548\uff0c\u6027\u80fd\u548c\u6269\u5c55\u6027\u90fd\u6709\u660e\u663e\u63d0\u5347\u3002</li>\n    <li>mHC\u9884\u8ba1\u5c06\u5e2e\u52a9\u66f4\u597d\u5730\u7406\u89e3\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u65b0\u7684\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods like Hyper-Connections (HC) improve neural network performance by changing how connections work.</li>\n    <li>However, these changes can make training unstable and increase memory use.</li>\n    <li>To solve these issues, we created Manifold-Constrained Hyper-Connections (mHC) that keeps the benefits of HC while restoring stability.</li>\n    <li>mHC has been shown to work well for larger training tasks, improving both performance and scalability.</li>\n    <li>This approach may help us better design advanced neural networks and improve foundational models in the future.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86Youtu-LLM\uff0c\u4e00\u4e2a\u8f7b\u91cf\u5374\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u667a\u80fd\u3002</li>\n    <li>Youtu-LLM\uff081.96B\uff09\u662f\u4ece\u96f6\u5f00\u59cb\u9884\u8bad\u7ec3\u7684\uff0c\u65e8\u5728\u57f9\u517b\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u91c7\u7528\u7d27\u51d1\u67b6\u6784\u548c\u957f\u4e0a\u4e0b\u6587\u652f\u6301\uff0c\u80fd\u591f\u5904\u7406128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9002\u5408\u957f\u65f6\u95f4\u7684\u63a8\u7406\u548c\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u591a\u9636\u6bb5\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u6a21\u578b\u4ece\u4e00\u822c\u5e38\u8bc6\u9010\u6b65\u5b66\u4e60\u590d\u6742\u7684STEM\u548c\u81ea\u4e3b\u4efb\u52a1\u3002</li>\n    <li>\u7ecf\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0cYoutu-LLM\u5728\u5c0f\u4e8e2B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u81ea\u4e3b\u667a\u80fd\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a powerful language model that is efficient and designed for reasoning and planning, built from scratch with 1.96 billion parameters.</li>\n    <li>It features a compact architecture that allows for long-context support of up to 128,000 tokens, making it suitable for complex tasks without using too much memory.</li>\n    <li>The model is trained using a carefully designed curriculum that starts with general knowledge and progresses to complex STEM and agent-related tasks, fostering deeper cognitive skills.</li>\n    <li>For mid-training, it uses diverse and high-quality data to help the model learn effective planning and reflection skills in areas like math and coding.</li>\n    <li>Youtu-LLM outperforms other lightweight models and competes well with larger models, showing that smaller models can still have strong problem-solving abilities.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "name": "Chunkang Zhang", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 38, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 9, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5185\u5b58\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u5185\u5b58\u7684\u52a8\u6001\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\u8bb0\u5fc6\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9ad8\u9636\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models (LLMs) for tasks requiring comprehensive understanding and reasoning.</li>\n    <li>Current memory systems mainly serve as passive storage, collecting isolated facts but lacking in meaningful connections between them.</li>\n    <li>This static approach limits reasoning abilities and can lead to fragmented understanding in complex tasks.</li>\n    <li>HGMem is a new memory system that uses a hypergraph to create dynamic connections between facts, enhancing reasoning and understanding.</li>\n    <li>Tests show that HGMem significantly improves multi-step RAG performance and outperforms existing systems on various challenging tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 2, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Agentic Learning Ecosystem (ALE)\uff0c\u4e00\u4e2a\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5f00\u53d1\u7684\u57fa\u7840\u8bbe\u65bd\u3002</li>\n    <li>ALE\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1aROLL\uff08\u6743\u91cd\u4f18\u5316\u6846\u67b6\uff09\u3001ROCK\uff08\u73af\u5883\u7ba1\u7406\u5668\uff09\u548ciFlow CLI\uff08\u4ee3\u7406\u6846\u67b6\uff09\u3002</li>\n    <li>\u53d1\u5e03\u4e86ROME\uff0c\u4e00\u4e2a\u57fa\u4e8eALE\u7684\u5f00\u6e90\u4ee3\u7406\u6a21\u578b\uff0c\u8bad\u7ec3\u4e86\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u8f68\u8ff9\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653f\u7b56\u4f18\u5316\u7b97\u6cd5\uff0cInteraction-based Policy Alignment (IPA)\uff0c\u63d0\u9ad8\u4e86\u957f\u65f6\u95f4\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u901a\u8fc7\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\uff0cROME\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86ALE\u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting allows LLMs (Large Language Models) to act and learn in real-world situations through multiple interactions.</li>\n    <li>The open-source community currently lacks a complete system to support the development of these agent LLMs.</li>\n    <li>The Agentic Learning Ecosystem (ALE) is introduced to enhance the development process, comprising three main components: ROLL (for weight optimization), ROCK (for managing environments), and iFlow CLI (for context engineering).</li>\n    <li>ROME is an open-source agent model created using ALE, trained with over one million interaction examples.</li>\n    <li>ROME shows strong performance in various tests, demonstrating the effectiveness of the ALE infrastructure for developing agent LLMs.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24617", "authors": [{"_id": "69573165832867f253525871", "user": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "name": "Xingwei Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:07.662Z", "hidden": false}, {"_id": "69573165832867f253525872", "name": "Shaowen Wang", "hidden": false}, {"_id": "69573165832867f253525873", "name": "Zihao Huang", "hidden": false}, {"_id": "69573165832867f253525874", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:53.467Z", "hidden": false}, {"_id": "69573165832867f253525875", "name": "Fan Yin", "hidden": false}, {"_id": "69573165832867f253525876", "name": "Rui-Jie Zhu", "hidden": false}, {"_id": "69573165832867f253525877", "name": "Jundong Zhou", "hidden": false}, {"_id": "69573165832867f253525878", "name": "Qiyang Min", "hidden": false}, {"_id": "69573165832867f253525879", "name": "Zihao Wang", "hidden": false}, {"_id": "69573165832867f25352587a", "name": "Yizhi Li", "hidden": false}, {"_id": "69573165832867f25352587b", "name": "Tianyu Zhang", "hidden": false}, {"_id": "69573165832867f25352587c", "name": "He Xing", "hidden": false}, {"_id": "69573165832867f25352587d", "name": "Zheng Zhang", "hidden": false}, {"_id": "69573165832867f25352587e", "name": "Yuxuan Song", "hidden": false}, {"_id": "69573165832867f25352587f", "name": "Tianyu Zheng", "hidden": false}, {"_id": "69573165832867f253525880", "name": "Zhiyuan Zeng", "hidden": false}, {"_id": "69573165832867f253525881", "name": "Chenghua Lin", "hidden": false}, {"_id": "69573165832867f253525882", "name": "Ge Zhang", "hidden": false}, {"_id": "69573165832867f253525883", "name": "Wenhao Huang", "hidden": false}], "publishedAt": "2025-12-31T04:19:33.000Z", "submittedOnDailyAt": "2026-01-02T00:17:55.450Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "submittedOnDailyBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "upvotes": 24, "discussionId": "69573165832867f253525884", "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u6240\u6709\u8bcd\u5143\u4f7f\u7528\u7edf\u4e00\u8ba1\u7b97\uff0c\u5bfc\u81f4\u4fe1\u606f\u5904\u7406\u6548\u7387\u4f4e\u4e0b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u52a8\u6001\u5927\u6982\u5ff5\u6a21\u578b\uff08DLCM\uff09\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u8bed\u4e49\u8fb9\u754c\u6765\u4f18\u5316\u8ba1\u7b97\uff0c\u5c06\u8ba1\u7b97\u4ece\u8bcd\u5143\u8f6c\u79fb\u5230\u66f4\u6709\u6548\u7684\u6982\u5ff5\u7a7a\u95f4\u3002</li>\n    <li>DLCM\u53ef\u4ee5\u7aef\u5230\u7aef\u5730\u53d1\u73b0\u53ef\u53d8\u957f\u5ea6\u7684\u6982\u5ff5\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u8bed\u8a00\u5355\u4f4d\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u538b\u7f29\u611f\u77e5\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u533a\u5206\u4e86\u8bcd\u5143\u7ea7\u548c\u6982\u5ff5\u7ea7\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u4f18\u5316\u8ba1\u7b97\u5206\u914d\u3002</li>\n    <li>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cDLCM\u63d0\u9ad8\u4e86\u7ea62.69%\u7684\u6027\u80fd\uff0c\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) treat all parts of language the same, which isn't efficient because some parts are more important than others.</li>\n    <li>Dynamic Large Concept Models (DLCM) change this by learning important language boundaries and focusing computation on key concepts instead of individual tokens.</li>\n    <li>DLCM finds flexible concepts without needing set language rules and changes how scaling works for language models.</li>\n    <li>It introduces a new way to think about scaling that separates different types of computational needs, allowing for better resource use.</li>\n    <li>In practical tests, DLCM improved performance by about 2.69% on various tasks while keeping the same amount of computation power used.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:19:33.000Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24617.png", "numComments": 2, "submittedBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "fullname": "Qu", "name": "ScottQu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24165", "authors": [{"_id": "69571b38832867f25352584d", "user": {"_id": "67247adb73d1eb17b6bfd27c", "avatarUrl": "/avatars/57bdbb7362f9854c87dd0a71ae071652.svg", "isPro": false, "fullname": "Zefeng He", "user": "yhx12", "type": "user"}, "name": "Zefeng He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:20.236Z", "hidden": false}, {"_id": "69571b38832867f25352584e", "name": "Xiaoye Qu", "hidden": false}, {"_id": "69571b38832867f25352584f", "name": "Yafu Li", "hidden": false}, {"_id": "69571b38832867f253525850", "user": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "isPro": false, "fullname": "Tong Zhu", "user": "Spico", "type": "user"}, "name": "Tong Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:16.903Z", "hidden": false}, {"_id": "69571b38832867f253525851", "name": "Siyuan Huang", "hidden": false}, {"_id": "69571b38832867f253525852", "name": "Yu Cheng", "hidden": false}], "publishedAt": "2025-12-30T11:51:18.000Z", "submittedOnDailyAt": "2026-01-02T03:34:41.232Z", "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models", "submittedOnDailyBy": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "isPro": false, "fullname": "Tong Zhu", "user": "Spico", "type": "user"}, "summary": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.", "upvotes": 17, "discussionId": "69571b38832867f253525853", "projectPage": "https://diffthinker-project.github.io/", "githubRepo": "https://github.com/lcqysl/DiffThinker", "githubRepoAddedBy": "user", "githubStars": 13, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u5e76\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u7406\u6846\u67b6DiffThinker\u3002</li>\n    <li>DiffThinker\u5c06\u591a\u6a21\u6001\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u4efb\u52a1\uff0c\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u7cbe\u786e\u5ea6\u3002</li>\n    <li>\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83DiffThinker\u548cMLLMs\uff0c\u63ed\u793a\u4e86\u5176\u56db\u4e2a\u6838\u5fc3\u7279\u6027\uff1a\u9ad8\u6548\u6027\u3001\u53ef\u63a7\u6027\u3001\u539f\u751f\u5e76\u884c\u6027\u548c\u534f\u4f5c\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cDiffThinker\u660e\u663e\u8d85\u8d8a\u4e86\u5176\u4ed6\u9886\u5148\u6a21\u578b\uff0c\u663e\u793a\u51fa\u751f\u6210\u591a\u6a21\u6001\u63a8\u7406\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u826f\u597d\u524d\u666f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent models for combining text and images struggle with complex visual tasks due to their focus on text.</li>\n    <li>This paper introduces DiffThinker, a new framework that improves multimodal reasoning by treating it as a generative image-to-image task.</li>\n    <li>DiffThinker shows better logical consistency and spatial precision in visual tasks compared to existing models.</li>\n    <li>The study identifies four key features of DiffThinker: efficiency, controllability, native parallelism, and collaboration.</li>\n    <li>Experiments show DiffThinker outperforms top models like GPT-5 and Gemini-3-Flash by a significant margin in various domains.</li>\n</ul>"}, "publishedAt": "2025-12-30T06:51:18.000Z", "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models", "summary": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24165.png", "numComments": 3, "submittedBy": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "fullname": "Tong Zhu", "name": "Spico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19}, "isAuthorParticipating": true}, {"paper": {"id": "2512.22630", "authors": [{"_id": "6957329e832867f253525886", "name": "Ziqi Jin", "hidden": false}, {"_id": "6957329e832867f253525887", "name": "Bin Wang", "hidden": false}, {"_id": "6957329e832867f253525888", "name": "Xiang Lin", "hidden": false}, {"_id": "6957329e832867f253525889", "name": "Lidong Bing", "hidden": false}, {"_id": "6957329e832867f25352588a", "name": "Aixin Sun", "hidden": false}], "publishedAt": "2025-12-27T16:03:08.000Z", "submittedOnDailyAt": "2026-01-02T00:23:07.951Z", "title": "On the Role of Discreteness in Diffusion LLMs", "submittedOnDailyBy": {"_id": "625921d05f80a3c1aad0bae3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625921d05f80a3c1aad0bae3/ElN3-6V5nGId2fzI3Dqlr.jpeg", "isPro": true, "fullname": "Phi", "user": "Xalphinions", "type": "user"}, "summary": "Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.", "upvotes": 9, "discussionId": "6957329e832867f25352588b", "organization": {"_id": "682c435aa186ba2f1fdde607", "name": "miromind-ai", "fullname": "MiroMind AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682c41fb2f8a52030ec93ce0/Cna52_IapEXuNBsyI3lvR.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u6a21\u578b\u5728\u8bed\u8a00\u751f\u6210\u4e2d\u6709\u5438\u5f15\u529b\u7684\u7279\u6027\uff0c\u5982\u5e76\u884c\u89e3\u7801\u548c\u8fed\u4ee3\u6539\u8fdb\uff0c\u4f46\u6587\u672c\u7684\u79bb\u6563\u548c\u9ad8\u5ea6\u7ed3\u6784\u5316\u7279\u6027\u4f7f\u5176\u76f4\u63a5\u5e94\u7528\u53d8\u5f97\u56f0\u96be\u3002</li>\n    <li>\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u6269\u6563\u8bed\u8a00\u5efa\u6a21\uff0c\u6307\u51fa\u4e86\u4e94\u4e2a\u5c06\u6269\u6563\u673a\u5236\u4e0e\u8bed\u8a00\u7279\u5b9a\u9700\u6c42\u533a\u5206\u5f00\u7684\u7279\u6027\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u5d4c\u5165\u7a7a\u95f4\u7684\u8fde\u7eed\u6269\u6563\u548c\u57fa\u4e8e\u7b26\u53f7\u7684\u79bb\u6563\u6269\u6563\uff0c\u4f46\u5b83\u4eec\u53ea\u80fd\u6ee1\u8db3\u5176\u4e2d\u90e8\u5206\u7279\u6027\uff0c\u5b58\u5728\u7ed3\u6784\u4e0a\u7684\u6743\u8861\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u8fd1\u671f\u5927\u578b\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5206\u6790\uff0c\u53d1\u73b0\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5747\u5300\u7834\u574f\u4e0d\u7b26\u5408\u4fe1\u606f\u5728\u4f4d\u7f6e\u4e0a\u7684\u5206\u5e03\uff0c\u4e14\u57fa\u4e8e\u7b26\u53f7\u7684\u8fb9\u9645\u8bad\u7ec3\u65e0\u6cd5\u6355\u6349\u591a\u7b26\u53f7\u4f9d\u8d56\u5173\u7cfb\u3002</li>\n    <li>\u8fd9\u4e9b\u89c2\u5bdf\u4fc3\u4f7f\u6211\u4eec\u63a2\u7d22\u66f4\u7b26\u5408\u6587\u672c\u7ed3\u6784\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u9f13\u52b1\u672a\u6765\u7814\u7a76\u66f4\u4e00\u81f4\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion models are useful for generating language due to their ability to process information in parallel and improve outputs iteratively.</li>\n    <li>However, applying these models directly to text is challenging because text is discrete and structured.</li>\n    <li>The authors identify five important properties that distinguish diffusion methods from the unique needs of language modeling.</li>\n    <li>Current approaches to diffusion in language can be categorized into two types: continuous diffusion in the embedding space and discrete diffusion over individual tokens.</li>\n    <li>Two main problems were found: one is that uniform corruption of text does not consider how information is arranged, and the other is that training on single tokens fails to understand relationships between multiple tokens.</li>\n</ul>"}, "publishedAt": "2025-12-27T11:03:08.000Z", "title": "On the Role of Discreteness in Diffusion LLMs", "summary": "Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22630.png", "numComments": 2, "submittedBy": {"_id": "625921d05f80a3c1aad0bae3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625921d05f80a3c1aad0bae3/ElN3-6V5nGId2fzI3Dqlr.jpeg", "fullname": "Phi", "name": "Xalphinions", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "682c435aa186ba2f1fdde607", "name": "miromind-ai", "fullname": "MiroMind AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682c41fb2f8a52030ec93ce0/Cna52_IapEXuNBsyI3lvR.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24724", "authors": [{"_id": "6957d82d832867f25352598d", "name": "Jibin Song", "hidden": false}, {"_id": "6957d82d832867f25352598e", "name": "Mingi Kwon", "hidden": false}, {"_id": "6957d82d832867f25352598f", "name": "Jaeseok Jeong", "hidden": false}, {"_id": "6957d82d832867f253525990", "name": "Youngjung Uh", "hidden": false}], "publishedAt": "2025-12-31T08:41:27.000Z", "submittedOnDailyAt": "2026-01-02T12:08:10.653Z", "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.", "upvotes": 2, "discussionId": "6957d82d832867f253525991", "summary_zh": "<ul>\n    <li>\u6a21\u578b\u7684\u80fd\u529b\u5bf9\u4e0d\u540c\u65f6\u95f4\u9636\u6bb5\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u65e9\u671f\u548c\u665a\u671f\u9636\u6bb5\u5f88\u91cd\u8981\uff0c\u4f46\u4e2d\u95f4\u9636\u6bb5\u5f71\u54cd\u4e0d\u5927\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlowBlending\u7684\u591a\u6a21\u578b\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u548c\u5c0f\u6a21\u578b\u4ee5\u5e94\u5bf9\u4e0d\u540c\u9636\u6bb5\u7684\u9700\u6c42\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u9009\u62e9\u9636\u6bb5\u8fb9\u754c\u7684\u7b80\u5355\u6807\u51c6\uff0c\u5e76\u901a\u8fc7\u901f\u5ea6-\u5dee\u5f02\u5206\u6790\u8bc6\u522b\u9700\u8981\u5173\u6ce8\u7684\u9636\u6bb5\u3002</li>\n    <li>\u5728LTX-Video\u548cWAN 2.1\u6570\u636e\u96c6\u4e0a\uff0cFlowBlending\u5b9e\u73b0\u4e86\u6700\u9ad81.65\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u548c57.35%\u7684FLOPs\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002</li>\n    <li>FlowBlending\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u52a0\u901f\u6280\u672f\u517c\u5bb9\uff0c\u8fdb\u4e00\u6b65\u5b9e\u73b0\u6700\u9ad82\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The effectiveness of model capacity changes at different stages of processing: it matters a lot early and late but not much in between.</li>\n    <li>We introduced a method called FlowBlending that uses a large model at critical stages and a smaller model during less important stages.</li>\n    <li>Simple guidelines were created to determine when to switch between these stages, using velocity-divergence analysis to find important areas.</li>\n    <li>FlowBlending can speed up processing by up to 1.65 times and reduce resource use by 57.35% while still keeping high quality in results.</li>\n    <li>The method works well with other speed-up techniques, potentially doubling the speed improvement.</li>\n</ul>"}, "publishedAt": "2025-12-31T03:41:27.000Z", "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation", "summary": "In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24724.png", "numComments": 2, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 9039}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24766", "authors": [{"_id": "6957d6a1832867f253525983", "name": "Karthik Dharmarajan", "hidden": false}, {"_id": "6957d6a1832867f253525984", "name": "Wenlong Huang", "hidden": false}, {"_id": "6957d6a1832867f253525985", "name": "Jiajun Wu", "hidden": false}, {"_id": "6957d6a1832867f253525986", "name": "Li Fei-Fei", "hidden": false}, {"_id": "6957d6a1832867f253525987", "name": "Ruohan Zhang", "hidden": false}], "publishedAt": "2025-12-31T10:25:24.000Z", "submittedOnDailyAt": "2026-01-02T12:01:38.691Z", "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.", "upvotes": 1, "discussionId": "6957d6a2832867f253525988", "summary_zh": "<ul>\n    <li>\u751f\u6210\u89c6\u9891\u5efa\u6a21\u662f\u4e00\u79cd\u65b0\u5174\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u673a\u5668\u4eba\u7406\u89e3\u548c\u6267\u884c\u7269\u7406\u4ea4\u4e92\u4efb\u52a1\u3002</li>\n    <li>Dream2Flow\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u80fd\u5c06\u89c6\u9891\u751f\u6210\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u7ed3\u5408\u8d77\u6765\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc73D\u7269\u4f53\u8fd0\u52a8\u91cd\u5efa\u6765\u8ddf\u8e2a\u7269\u4f53\u8f68\u8ff9\uff0c\u5b9e\u73b0\u5bf9\u7269\u4f53\u7684\u64cd\u63a7\u3002</li>\n    <li>Dream2Flow\u80fd\u591f\u5904\u7406\u5404\u79cd\u7c7b\u578b\u7684\u7269\u4f53\uff0c\u5305\u62ec\u521a\u6027\u3001\u5173\u8282\u3001\u53ef\u53d8\u5f62\u548c\u9897\u7c92\u72b6\u7269\u4f53\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u5728\u6ca1\u6709\u7279\u5b9a\u4efb\u52a1\u6f14\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u5c063D\u7269\u4f53\u8fd0\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u4f4e\u7ea7\u6307\u4ee4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Generative video modeling helps understand how objects interact in physical space for robotic tasks.</li>\n    <li>Dream2Flow is a new framework that connects video generation to robotic control using 3D object motion.</li>\n    <li>It reconstructs 3D motions from videos and guides robots to track these movements.</li>\n    <li>Dream2Flow allows robots to manipulate various object types without needing specific training for each task.</li>\n    <li>Tests show that 3D object flow is a flexible and effective method for robotic manipulation in real-world settings.</li>\n</ul>"}, "publishedAt": "2025-12-31T05:25:24.000Z", "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow", "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24766.png", "numComments": 2, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 9039}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24007", "authors": [{"_id": "6957e7af832867f2535259cb", "name": "Bulent Soykan", "hidden": false}, {"_id": "6957e7af832867f2535259cc", "name": "Sean Mondesire", "hidden": false}, {"_id": "6957e7af832867f2535259cd", "name": "Ghaith Rabadi", "hidden": false}], "publishedAt": "2025-12-30T06:03:37.000Z", "submittedOnDailyAt": "2026-01-02T13:14:57.263Z", "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems", "submittedOnDailyBy": {"_id": "626273fbcbebf7e1ac2820ab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626273fbcbebf7e1ac2820ab/VPc5BVvdSel6ox7yzrcvb.jpeg", "isPro": false, "fullname": "Bulent Soykan", "user": "bulentsoykan", "type": "user"}, "summary": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.", "upvotes": 1, "discussionId": "6957e7af832867f2535259ce", "githubRepo": "https://github.com/bulentsoykan/TESO", "githubRepoAddedBy": "user", "githubStars": 1, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5143\u542f\u53d1\u5f0f\u6846\u67b6\uff0c\u79f0\u4e3a\u201c\u7981\u5fcc\u589e\u5f3a\u6a21\u62df\u4f18\u5316\u201d\uff08TESO\uff09\u3002</li>\n    <li>TESO\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u641c\u7d22\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u566a\u58f0\u8bc4\u4f30\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u3002</li>\n    <li>\u4f7f\u7528\u77ed\u671f\u7981\u5fcc\u5217\u8868\u9632\u6b62\u5faa\u73af\u5e76\u4fc3\u8fdb\u591a\u6837\u5316\uff0c\u957f\u671f\u7cbe\u82f1\u8bb0\u5fc6\u5f15\u5bfc\u89e3\u51b3\u65b9\u6848\u5f3a\u5316\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u6270\u52a8\uff0cTESO\u5728\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u52a8\u6001\u5e73\u8861\u3002</li>\n    <li>\u5728\u961f\u5217\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cTESO\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u5176\u8bb0\u5fc6\u7ec4\u4ef6\u7684\u8d21\u732e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper presents a new method called Tabu-Enhanced Simulation Optimization (TESO) to improve optimization in complex situations.</li>\n    <li>TESO combines adaptive search techniques with memory strategies to avoid repeating searches and to explore new options.</li>\n    <li>It uses a short-term memory list to prevent cycling through the same solutions and a long-term memory to refine the best solutions.</li>\n    <li>An aspiration criterion allows TESO to consider exceptional solutions even if they break the usual rules.</li>\n    <li>The method was tested on a queue optimization problem and showed better results than existing methods, with its memory features proving beneficial.</li>\n</ul>"}, "publishedAt": "2025-12-30T01:03:37.000Z", "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems", "summary": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24007.png", "numComments": 1, "submittedBy": {"_id": "626273fbcbebf7e1ac2820ab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626273fbcbebf7e1ac2820ab/VPc5BVvdSel6ox7yzrcvb.jpeg", "fullname": "Bulent Soykan", "name": "bulentsoykan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u7ea6\u675f\uff0c\u5bfc\u81f4\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\uff08ERC\uff09\u635f\u5931\uff0c\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u8f85\u52a9\u635f\u5931\uff0c\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u5f3a\u5236\u8981\u6c42\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u5176\u4ee3\u7406\u6807\u8bb0\u7684\u6fc0\u6d3b\u503c\u9ad8\u4e8e\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406\u6807\u8bb0\uff0c\u5e76\u4e14\u6bcf\u4e2a\u4ee3\u7406\u6807\u8bb0\u5fc5\u987b\u5f15\u53d1\u5176\u5bf9\u5e94\u4e13\u5bb6\u66f4\u5f3a\u7684\u6fc0\u6d3b\u3002</li>\n    <li>ERC\u635f\u5931\u5728\u8ba1\u7b97\u4e0a\u9ad8\u6548\uff0c\u4ec5\u5bf9n\u00b2\u4e2a\u6fc0\u6d3b\u8fdb\u884c\u64cd\u4f5c\uff0c\u5176\u4e2dn\u4e3a\u4e13\u5bb6\u6570\u91cf\uff0c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf93B\u523015B\u53c2\u6570\u7684MoE-LLM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6211\u4eec\u5c55\u793a\u4e86ERC\u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u4e13\u5bb6\u4e13\u4e1a\u5316\u7a0b\u5ea6\u7684\u7075\u6d3b\u63a7\u5236\u548c\u5b9a\u91cf\u8ddf\u8e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models can struggle because the router's choices don't always match the abilities of the experts.</li>\n    <li>We introduce a new method called expert-router coupling (ERC) loss, which connects the router's decisions more closely with what each expert can do.</li>\n    <li>The ERC loss uses a proxy token for each expert to help guide how well the expert performs with the tokens it receives.</li>\n    <li>It sets two main rules: experts should activate more strongly for their own tokens, and each token should get a better response from its matching expert.</li>\n    <li>This method is efficient and allows for better tracking of how well each expert is specializing during training, showing strong results in tests with large models and datasets.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53eb\u505a\u201c\u591a\u6837\u6027\u8d85\u8fde\u63a5\u201d\uff08HC\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u6b8b\u5dee\u8fde\u63a5\u3002</li>\n    <li>\u8fd9\u79cd\u65b0\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u524a\u5f31\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u8eab\u4efd\u6620\u5c04\u7279\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6269\u5c55\u6027\u5dee\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u201d\uff08mHC\uff09\uff0c\u53ef\u4ee5\u6062\u590d\u8eab\u4efd\u6620\u5c04\u7279\u6027\uff0c\u5e76\u4f18\u5316\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cmHC\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u6709\u6548\uff0c\u6027\u80fd\u548c\u6269\u5c55\u6027\u90fd\u6709\u660e\u663e\u63d0\u5347\u3002</li>\n    <li>mHC\u9884\u8ba1\u5c06\u5e2e\u52a9\u66f4\u597d\u5730\u7406\u89e3\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u65b0\u7684\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods like Hyper-Connections (HC) improve neural network performance by changing how connections work.</li>\n    <li>However, these changes can make training unstable and increase memory use.</li>\n    <li>To solve these issues, we created Manifold-Constrained Hyper-Connections (mHC) that keeps the benefits of HC while restoring stability.</li>\n    <li>mHC has been shown to work well for larger training tasks, improving both performance and scalability.</li>\n    <li>This approach may help us better design advanced neural networks and improve foundational models in the future.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23576", "authors": [{"_id": "69534f1e89916ff627aa3fe3", "name": "Ethan Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe4", "name": "Zhulin Hu", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe5", "name": "Bohao Tang", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe6", "name": "Jiadi Su", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe7", "name": "Steffi Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe8", "name": "Zhijie Deng", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe9", "name": "Pengfei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "publishedAt": "2025-12-29T16:17:36.000Z", "submittedOnDailyAt": "2025-12-30T02:36:23.479Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "submittedOnDailyBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "isPro": false, "fullname": "Ethan Chern", "user": "ethanchern", "type": "user"}, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "upvotes": 51, "discussionId": "69534f1e89916ff627aa3fea", "githubRepo": "https://github.com/GAIR-NLP/LiveTalk", "githubRepoAddedBy": "user", "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.", "ai_keywords": ["diffusion models", "bidirectional attention", "distillation methods", "on-policy distillation", "Self Forcing", "audio language models", "Anchor-Heavy Identity Sinks", "multimodal conditioning", "autoregressive", "on-policy optimization"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u5bf9\u4e8e\u6784\u5efa\u901a\u7528\u591a\u6a21\u6001\u4e92\u52a8AI\u7cfb\u7edf\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u53bb\u566a\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u5b58\u5728\u89c6\u89c9\u4f2a\u5f71\u548c\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002</li>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51cf\u5c11\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u3002</li>\n    <li>\u901a\u8fc7\u4e0e\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLiveTalk\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u4e92\u52a8\u5934\u50cf\u7cfb\u7edf\u3002</li>\n    <li>LiveTalk\u5728\u591a\u8f6e\u4e92\u52a8\u4e2d\u7684\u89c6\u9891\u8fde\u8d2f\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\uff0c\u4e14\u54cd\u5e94\u65f6\u95f4\u5927\u5e45\u7f29\u77ed\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper focuses on creating a real-time video generation system using diffusion models for better human-AI interactions.</li>\n    <li>Current methods struggle with video quality and efficiency, especially when combining text, image, and audio inputs.</li>\n    <li>The authors propose an improved method for distillation that enhances video quality and reduces processing time.</li>\n    <li>Tests show their model achieves high visual quality with significantly lower costs and faster response times compared to traditional models.</li>\n    <li>The new system, called LiveTalk, outperforms existing models in generating coherent and high-quality video interactions in real time.</li>\n</ul>"}, "publishedAt": "2025-12-29T11:17:36.000Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png", "numComments": 1, "submittedBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "fullname": "Ethan Chern", "name": "ethanchern", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86Youtu-LLM\uff0c\u4e00\u4e2a\u8f7b\u91cf\u5374\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u667a\u80fd\u3002</li>\n    <li>Youtu-LLM\uff081.96B\uff09\u662f\u4ece\u96f6\u5f00\u59cb\u9884\u8bad\u7ec3\u7684\uff0c\u65e8\u5728\u57f9\u517b\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u91c7\u7528\u7d27\u51d1\u67b6\u6784\u548c\u957f\u4e0a\u4e0b\u6587\u652f\u6301\uff0c\u80fd\u591f\u5904\u7406128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9002\u5408\u957f\u65f6\u95f4\u7684\u63a8\u7406\u548c\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u591a\u9636\u6bb5\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u6a21\u578b\u4ece\u4e00\u822c\u5e38\u8bc6\u9010\u6b65\u5b66\u4e60\u590d\u6742\u7684STEM\u548c\u81ea\u4e3b\u4efb\u52a1\u3002</li>\n    <li>\u7ecf\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0cYoutu-LLM\u5728\u5c0f\u4e8e2B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u81ea\u4e3b\u667a\u80fd\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a powerful language model that is efficient and designed for reasoning and planning, built from scratch with 1.96 billion parameters.</li>\n    <li>It features a compact architecture that allows for long-context support of up to 128,000 tokens, making it suitable for complex tasks without using too much memory.</li>\n    <li>The model is trained using a carefully designed curriculum that starts with general knowledge and progresses to complex STEM and agent-related tasks, fostering deeper cognitive skills.</li>\n    <li>For mid-training, it uses diverse and high-quality data to help the model learn effective planning and reflection skills in areas like math and coding.</li>\n    <li>Youtu-LLM outperforms other lightweight models and competes well with larger models, showing that smaller models can still have strong problem-solving abilities.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "name": "Chunkang Zhang", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 38, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 9, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5185\u5b58\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u5185\u5b58\u7684\u52a8\u6001\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\u8bb0\u5fc6\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9ad8\u9636\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models (LLMs) for tasks requiring comprehensive understanding and reasoning.</li>\n    <li>Current memory systems mainly serve as passive storage, collecting isolated facts but lacking in meaningful connections between them.</li>\n    <li>This static approach limits reasoning abilities and can lead to fragmented understanding in complex tasks.</li>\n    <li>HGMem is a new memory system that uses a hypergraph to create dynamic connections between facts, enhancing reasoning and understanding.</li>\n    <li>Tests show that HGMem significantly improves multi-step RAG performance and outperforms existing systems on various challenging tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 2, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Agentic Learning Ecosystem (ALE)\uff0c\u4e00\u4e2a\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5f00\u53d1\u7684\u57fa\u7840\u8bbe\u65bd\u3002</li>\n    <li>ALE\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1aROLL\uff08\u6743\u91cd\u4f18\u5316\u6846\u67b6\uff09\u3001ROCK\uff08\u73af\u5883\u7ba1\u7406\u5668\uff09\u548ciFlow CLI\uff08\u4ee3\u7406\u6846\u67b6\uff09\u3002</li>\n    <li>\u53d1\u5e03\u4e86ROME\uff0c\u4e00\u4e2a\u57fa\u4e8eALE\u7684\u5f00\u6e90\u4ee3\u7406\u6a21\u578b\uff0c\u8bad\u7ec3\u4e86\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u8f68\u8ff9\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653f\u7b56\u4f18\u5316\u7b97\u6cd5\uff0cInteraction-based Policy Alignment (IPA)\uff0c\u63d0\u9ad8\u4e86\u957f\u65f6\u95f4\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u901a\u8fc7\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\uff0cROME\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86ALE\u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting allows LLMs (Large Language Models) to act and learn in real-world situations through multiple interactions.</li>\n    <li>The open-source community currently lacks a complete system to support the development of these agent LLMs.</li>\n    <li>The Agentic Learning Ecosystem (ALE) is introduced to enhance the development process, comprising three main components: ROLL (for weight optimization), ROCK (for managing environments), and iFlow CLI (for context engineering).</li>\n    <li>ROME is an open-source agent model created using ALE, trained with over one million interaction examples.</li>\n    <li>ROME shows strong performance in various tests, demonstrating the effectiveness of the ALE infrastructure for developing agent LLMs.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23705", "authors": [{"_id": "6953546989916ff627aa4002", "name": "Shaocong Xu", "hidden": false}, {"_id": "6953546989916ff627aa4003", "name": "Songlin Wei", "hidden": false}, {"_id": "6953546989916ff627aa4004", "name": "Qizhe Wei", "hidden": false}, {"_id": "6953546989916ff627aa4005", "name": "Zheng Geng", "hidden": false}, {"_id": "6953546989916ff627aa4006", "name": "Hong Li", "hidden": false}, {"_id": "6953546989916ff627aa4007", "name": "Licheng Shen", "hidden": false}, {"_id": "6953546989916ff627aa4008", "name": "Qianpu Sun", "hidden": false}, {"_id": "6953546989916ff627aa4009", "name": "Shu Han", "hidden": false}, {"_id": "6953546989916ff627aa400a", "name": "Bin Ma", "hidden": false}, {"_id": "6953546989916ff627aa400b", "name": "Bohan Li", "hidden": false}, {"_id": "6953546989916ff627aa400c", "name": "Chongjie Ye", "hidden": false}, {"_id": "6953546989916ff627aa400d", "name": "Yuhang Zheng", "hidden": false}, {"_id": "6953546989916ff627aa400e", "name": "Nan Wang", "hidden": false}, {"_id": "6953546989916ff627aa400f", "name": "Saining Zhang", "hidden": false}, {"_id": "6953546989916ff627aa4010", "name": "Hao Zhao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "publishedAt": "2025-12-29T18:59:24.000Z", "submittedOnDailyAt": "2025-12-30T01:56:18.708Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "submittedOnDailyBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "isPro": true, "fullname": "Shaocong.Xu", "user": "Daniellesry", "type": "user"}, "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "upvotes": 32, "discussionId": "6953546a89916ff627aa4011", "projectPage": "https://daniellli.github.io/projects/DKT/", "githubRepo": "https://github.com/Daniellli/DKT", "githubRepoAddedBy": "user", "githubStars": 94, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u900f\u660e\u7269\u4f53\u5bf9\u611f\u77e5\u7cfb\u7edf\u800c\u8a00\u5f88\u96be\u5904\u7406\uff0c\u56e0\u4e3a\u6298\u5c04\u3001\u53cd\u5c04\u548c\u900f\u5c04\u4f1a\u5bfc\u81f4\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u7a33\u5b9a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u4ee3\u89c6\u9891\u6269\u6563\u6a21\u578b\u80fd\u591f\u5408\u6210\u771f\u5b9e\u7684\u900f\u660e\u73b0\u8c61\uff0c\u8868\u660e\u5b83\u4eec\u7406\u89e3\u4e86\u5149\u5b66\u89c4\u5219\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86TransPhy3D\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u900f\u660e\u548c\u53cd\u5c04\u573a\u666f\u7684\u5408\u6210\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5171\u670911,000\u4e2a\u5e8f\u5217\u3002</li>\n    <li>\u5f00\u53d1\u7684\u6a21\u578bDKT\u5728\u900f\u660e\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>DKT\u6a21\u578b\u5728\u5904\u7406\u900f\u660e\u3001\u53cd\u5c04\u548c\u6f2b\u53cd\u5c04\u8868\u9762\u65f6\uff0c\u6210\u529f\u7387\u8d85\u8fc7\u4e4b\u524d\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Transparent objects are difficult for perception systems to process due to issues like refraction and reflection.</li>\n    <li>The researchers created TransPhy3D, a video dataset of transparent and reflective scenes using Blender/Cycles, containing 11,000 video sequences.</li>\n    <li>They developed a model called DKT that learns to translate video to depth and normals while maintaining consistency across video frames.</li>\n    <li>DKT outperforms other models in estimating depth for videos with transparency, showing better accuracy and temporal consistency.</li>\n    <li>This work suggests that generative video models can be adapted for effective perception in real-world tasks involving challenging materials.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:24.000Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23705.png", "numComments": 1, "submittedBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "fullname": "Shaocong.Xu", "name": "Daniellesry", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23709", "authors": [{"_id": "69537f4189916ff627aa40c0", "name": "Hau-Shiang Shiu", "hidden": false}, {"_id": "69537f4189916ff627aa40c1", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "69537f4189916ff627aa40c2", "name": "Zhixiang Wang", "hidden": false}, {"_id": "69537f4189916ff627aa40c3", "name": "Chi-Wei Hsiao", "hidden": false}, {"_id": "69537f4189916ff627aa40c4", "name": "Po-Fan Yu", "hidden": false}, {"_id": "69537f4189916ff627aa40c5", "name": "Yu-Chih Chen", "hidden": false}, {"_id": "69537f4189916ff627aa40c6", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "publishedAt": "2025-12-29T18:59:57.000Z", "submittedOnDailyAt": "2025-12-30T05:04:09.292Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "upvotes": 29, "discussionId": "69537f4289916ff627aa40c7", "projectPage": "https://jamichss.github.io/stream-diffvsr-project-page/", "summary_zh": "<ul>\n    <li>Stream-DiffVSR\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u5728\u7ebf\u5b9e\u65f6\u5904\u7406\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u53ea\u4f7f\u7528\u8fc7\u53bb\u7684\u5e27\uff0c\u907f\u514d\u4e86\u5bf9\u672a\u6765\u5e27\u7684\u4f9d\u8d56\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002</li>\n    <li>\u5b83\u7ed3\u5408\u4e86\u5feb\u901f\u63a8\u7406\u7684\u56db\u6b65\u53bb\u566a\u5668\u548c\u65f6\u95f4\u5f15\u5bfc\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u7ec6\u8282\u3002</li>\n    <li>\u5728RTX4090 GPU\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5904\u7406720p\u5e27\u7684\u65f6\u95f4\u4e3a0.328\u79d2\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6280\u672f\u3002</li>\n    <li>Stream-DiffVSR\u662f\u9996\u4e2a\u9002\u5408\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u90e8\u7f72\u7684\u6269\u6563\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Stream-DiffVSR is a new video super-resolution method that works quickly and efficiently for real-time use.</li>\n    <li>It only uses past video frames, avoiding reliance on future frames, which helps reduce delays.</li>\n    <li>The method includes a four-step denoiser and a special module for better motion alignment during processing.</li>\n    <li>Stream-DiffVSR can process 720p videos in just 0.328 seconds, significantly faster than previous methods.</li>\n    <li>It improves video quality and has the lowest latency for diffusion-based video super-resolution available.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:57.000Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23709.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22615", "authors": [{"_id": "6953489889916ff627aa3f25", "name": "Jiacheng Ye", "hidden": false}, {"_id": "6953489889916ff627aa3f26", "name": "Shansan Gong", "hidden": false}, {"_id": "6953489889916ff627aa3f27", "name": "Jiahui Gao", "hidden": false}, {"_id": "6953489889916ff627aa3f28", "name": "Junming Fan", "hidden": false}, {"_id": "6953489889916ff627aa3f29", "name": "Shuang Wu", "hidden": false}, {"_id": "6953489889916ff627aa3f2a", "name": "Wei Bi", "hidden": false}, {"_id": "6953489889916ff627aa3f2b", "name": "Haoli Bai", "hidden": false}, {"_id": "6953489889916ff627aa3f2c", "name": "Lifeng Shang", "hidden": false}, {"_id": "6953489889916ff627aa3f2d", "name": "Lingpeng Kong", "hidden": false}], "publishedAt": "2025-12-27T14:46:24.000Z", "submittedOnDailyAt": "2025-12-30T03:42:33.237Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "submittedOnDailyBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "isPro": false, "fullname": "Shansan Gong", "user": "Sansa", "type": "user"}, "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "upvotes": 27, "discussionId": "6953489889916ff627aa3f2e", "projectPage": "https://hkunlp.github.io/blog/2025/dream-vlx/", "githubRepo": "https://github.com/DreamLM/Dream-VLX", "githubRepoAddedBy": "user", "ai_summary": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines.", "ai_keywords": ["diffusion-based large language models (dLLMs)", "Vision-Language Models (VLMs)", "Dream-VL", "Vision-Language-Action model (dVLA)", "Dream-VLA", "action chunking", "parallel generation", "LIBERO", "SimplerEnv-Bridge", "SimplerEnv-Fractal", "continuous pre-training"], "githubStars": 40, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u89c4\u5212\u548c\u52a8\u6001\u673a\u5668\u4eba\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002</li>\n    <li>\u7814\u7a76\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u6784\u5efa\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u63a8\u51fa\u4e86Dream-VL\uff0c\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>Dream-VL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u9876\u7ea7\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\u3002</li>\n    <li>\u57fa\u4e8eDream-VL\uff0c\u63a8\u51fa\u4e86Dream-VLA\uff0c\u4e13\u6ce8\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u4efb\u52a1\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002</li>\n    <li>Dream-VLA\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u8d85\u8fc797%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u53d1\u5e03\u4e86\u8fd9\u4e24\u4e2a\u6a21\u578b\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive Large Vision-Language Models (VLMs) have limitations in complex visual planning and robotic control due to their sequential generation.</li>\n    <li>The new model, Dream-VL, is a diffusion-based VLM that outperforms previous models and is effective in visual planning tasks.</li>\n    <li>Dream-VLA is an advanced model based on Dream-VL, designed for Vision-Language-Action tasks, showing faster training and better performance.</li>\n    <li>Dream-VLA achieved high success rates on various benchmarks, outperforming other leading models.</li>\n    <li>Both Dream-VL and Dream-VLA are available for the research community to use and improve upon.</li>\n</ul>"}, "publishedAt": "2025-12-27T09:46:24.000Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22615.png", "numComments": 1, "submittedBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "fullname": "Shansan Gong", "name": "Sansa", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24617", "authors": [{"_id": "69573165832867f253525871", "user": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "name": "Xingwei Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:07.662Z", "hidden": false}, {"_id": "69573165832867f253525872", "name": "Shaowen Wang", "hidden": false}, {"_id": "69573165832867f253525873", "name": "Zihao Huang", "hidden": false}, {"_id": "69573165832867f253525874", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:53.467Z", "hidden": false}, {"_id": "69573165832867f253525875", "name": "Fan Yin", "hidden": false}, {"_id": "69573165832867f253525876", "name": "Rui-Jie Zhu", "hidden": false}, {"_id": "69573165832867f253525877", "name": "Jundong Zhou", "hidden": false}, {"_id": "69573165832867f253525878", "name": "Qiyang Min", "hidden": false}, {"_id": "69573165832867f253525879", "name": "Zihao Wang", "hidden": false}, {"_id": "69573165832867f25352587a", "name": "Yizhi Li", "hidden": false}, {"_id": "69573165832867f25352587b", "name": "Tianyu Zhang", "hidden": false}, {"_id": "69573165832867f25352587c", "name": "He Xing", "hidden": false}, {"_id": "69573165832867f25352587d", "name": "Zheng Zhang", "hidden": false}, {"_id": "69573165832867f25352587e", "name": "Yuxuan Song", "hidden": false}, {"_id": "69573165832867f25352587f", "name": "Tianyu Zheng", "hidden": false}, {"_id": "69573165832867f253525880", "name": "Zhiyuan Zeng", "hidden": false}, {"_id": "69573165832867f253525881", "name": "Chenghua Lin", "hidden": false}, {"_id": "69573165832867f253525882", "name": "Ge Zhang", "hidden": false}, {"_id": "69573165832867f253525883", "name": "Wenhao Huang", "hidden": false}], "publishedAt": "2025-12-31T04:19:33.000Z", "submittedOnDailyAt": "2026-01-02T00:17:55.450Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "submittedOnDailyBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "upvotes": 24, "discussionId": "69573165832867f253525884", "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u6240\u6709\u8bcd\u5143\u4f7f\u7528\u7edf\u4e00\u8ba1\u7b97\uff0c\u5bfc\u81f4\u4fe1\u606f\u5904\u7406\u6548\u7387\u4f4e\u4e0b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u52a8\u6001\u5927\u6982\u5ff5\u6a21\u578b\uff08DLCM\uff09\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u8bed\u4e49\u8fb9\u754c\u6765\u4f18\u5316\u8ba1\u7b97\uff0c\u5c06\u8ba1\u7b97\u4ece\u8bcd\u5143\u8f6c\u79fb\u5230\u66f4\u6709\u6548\u7684\u6982\u5ff5\u7a7a\u95f4\u3002</li>\n    <li>DLCM\u53ef\u4ee5\u7aef\u5230\u7aef\u5730\u53d1\u73b0\u53ef\u53d8\u957f\u5ea6\u7684\u6982\u5ff5\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u8bed\u8a00\u5355\u4f4d\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u538b\u7f29\u611f\u77e5\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u533a\u5206\u4e86\u8bcd\u5143\u7ea7\u548c\u6982\u5ff5\u7ea7\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u4f18\u5316\u8ba1\u7b97\u5206\u914d\u3002</li>\n    <li>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cDLCM\u63d0\u9ad8\u4e86\u7ea62.69%\u7684\u6027\u80fd\uff0c\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) treat all parts of language the same, which isn't efficient because some parts are more important than others.</li>\n    <li>Dynamic Large Concept Models (DLCM) change this by learning important language boundaries and focusing computation on key concepts instead of individual tokens.</li>\n    <li>DLCM finds flexible concepts without needing set language rules and changes how scaling works for language models.</li>\n    <li>It introduces a new way to think about scaling that separates different types of computational needs, allowing for better resource use.</li>\n    <li>In practical tests, DLCM improved performance by about 2.69% on various tasks while keeping the same amount of computation power used.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:19:33.000Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24617.png", "numComments": 2, "submittedBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "fullname": "Qu", "name": "ScottQu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u4e3a\u4e86\u6ee1\u8db3\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDataFlow\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\u3002</li>\n    <li>DataFlow\u5177\u6709\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u529f\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6613\u4e8e\u8c03\u8bd5\u548c\u4f18\u5316\u7684\u7ba1\u9053\u6784\u5efaAPI\u3002</li>\n    <li>\u6846\u67b6\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7ba1\u9053\uff0c\u652f\u6301\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u591a\u79cd\u5e94\u7528\u3002</li>\n    <li>\u901a\u8fc7\u81ea\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u683c\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7ba1\u9053\uff0cDataFlow-Agent\u63d0\u9ad8\u4e86\u7528\u6237\u7684\u6613\u7528\u6027\u3002</li>\n    <li>DataFlow\u5728\u591a\u4e2a\u7528\u4f8b\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38LLM\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u5408\u6210\u57fa\u7ebf\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>The demand for high-quality data in Large Language Models (LLMs) is growing, but current methods are often unorganized and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps prepare data for LLMs using a more structured and modular approach.</li>\n    <li>It includes almost 200 reusable tools and pipelines for various tasks like text processing and mathematical reasoning.</li>\n    <li>DataFlow-Agent can automatically create data preparation workflows from simple language descriptions.</li>\n    <li>Using DataFlow has shown to improve the performance of LLMs significantly across different tasks compared to traditional datasets.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u201d\u6846\u67b6\uff0c\u91c7\u7528\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u548c\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\u6027\uff08TPP\uff09\uff0c\u901a\u8fc7\u591aGPU\u5206\u5e03\u5f0f\u63a8\u7406\u6765\u63d0\u9ad8\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u91c7\u7528\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u52a8\u6001\u8c03\u6574\u5916\u89c2\u4ee5\u4fdd\u6301\u5e8f\u5217\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>\u6211\u4eec\u7684\u7cfb\u7edf\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u9996\u6b21\u5728\u8fd9\u4e00\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u7528\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new method for creating avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>It introduces a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up the process by using multiple GPUs, making it faster and more efficient.</li>\n    <li>The Rolling Sink Frame Mechanism (RSFM) helps keep avatars looking consistent and prevents issues like identity drift and color problems.</li>\n    <li>Live Avatar can generate avatars at 20 frames per second, making it suitable for real-time applications.</li>\n    <li>This work sets a new standard for using advanced video generation models in professional long-form video projects.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u5206\u5f00\u7684\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>Kling-Omni\u4e0d\u4ec5\u662f\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\uff0c\u66f4\u662f\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u3001\u751f\u6210\u548c\u4e0e\u590d\u6742\u4e16\u754c\u4e92\u52a8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various inputs like text, images, and existing videos.</li>\n    <li>It combines different tasks such as video generation, editing, and reasoning into one seamless framework.</li>\n    <li>The system processes various user inputs into a single format for better video content creation.</li>\n    <li>Kling-Omni uses advanced training methods and optimizations to improve performance and efficiency.</li>\n    <li>It shows strong abilities in generating videos, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\u4e14\u6269\u5c55\u6027\u6709\u9650\uff0cWan-Move\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u5bc6\u96c6\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u5141\u8bb8\u5bf9\u573a\u666f\u8fdb\u884c\u7ec6\u81f4\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u751f\u6210\u7684480p\u89c6\u9891\u65f6\u957f\u4e3a5\u79d2\uff0c\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0e\u5546\u4e1a\u8f6f\u4ef6Kling 1.5 Pro\u76f8\u5f53\u3002</li>\n    <li>\u652f\u6301\u5168\u9762\u8bc4\u4f30\u7684MoveBench\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b\u591a\u79cd\u5185\u5bb9\u7c7b\u522b\u548c\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6ce8\u91ca\uff0c\u6570\u636e\u91cf\u5927\u4e14\u89c6\u9891\u65f6\u957f\u957f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over how objects move in videos.</li>\n    <li>The framework uses point trajectories to guide video synthesis without needing extra components like motion encoders.</li>\n    <li>Wan-Move can generate 5-second videos with motion control similar to commercial tools, as shown in user studies.</li>\n    <li>A new benchmark called MoveBench has been created to evaluate motion quality in video generation.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u4ea7\u751f\u903c\u771f\u7684\u5185\u5bb9\uff0c\u4f46\u5176\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u80fd\u6355\u6349\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5982FVD\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86MMGR\u6846\u67b6\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u8bc4\u4f30\u5305\u62ec\u62bd\u8c61\u63a8\u7406\u3001\u5b9e\u4f53\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u4e09\u4e2a\u9886\u57df\uff0c\u4e14\u4f7f\u7528\u7cbe\u7ec6\u7684\u6307\u6807\u6765\u8861\u91cf\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u5728\u957f\u65f6\u95f4\u7a7a\u95f4\u89c4\u5212\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models create realistic content, but their effectiveness as world simulators depends on capturing physical, logical, and spatial rules.</li>\n    <li>Current evaluation methods focus on visual quality but miss important reasoning errors like causality and consistency.</li>\n    <li>MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark) introduces a framework to assess five reasoning abilities across various tasks.</li>\n    <li>Testing shows that while models perform moderately well on physical tasks, they struggle significantly with abstract reasoning and long-term planning.</li>\n    <li>MMGR reveals weaknesses in existing models, highlighting the need for better reasoning capabilities instead of just visual appeal.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u4eba\u5de5\u667a\u80fd\uff08SGI\uff09\u4ecd\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u65e0\u6cd5\u81ea\u4e3b\u8fdb\u884c\u79d1\u5b66\u7814\u7a76\u548c\u63a8\u7406\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u9645\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u9879\u79d1\u5b66\u5bb6\u76f8\u5173\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u548c\u5b9e\u9a8c\u6267\u884c\u7b49\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u4fc3\u8fdb\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which allows AI to think and work independently in science.</li>\n    <li>The authors propose a new definition of SGI based on the Practical Inquiry Model (PIM) and outline four key tasks for scientists: research, idea generation, experiments, and reasoning.</li>\n    <li>They created SGI-Bench, a benchmark with over 1,000 examples to evaluate AI performance in science, inspired by important scientific questions.</li>\n    <li>Results showed that AI struggles with deep research accuracy, idea feasibility, and executing experiments correctly.</li>\n    <li>The authors also introduced a method called Test-Time Reinforcement Learning (TTRL) to improve AI\u2019s ability to generate new hypotheses during evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f7f\u7528VAE\u7a7a\u95f4\u6765\u5b66\u4e60\u89c6\u9891\u7279\u5f81\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002</li>\n    <li>SemanticGen\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u66f4\u5feb\u6536\u655b\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Traditional video generation models use VAE to convert video features into pixels but are slow and costly for long videos.</li>\n    <li>SemanticGen is a new method that generates videos in a simpler, high-level semantic space first, improving efficiency.</li>\n    <li>The process has two stages: first, it creates a basic video layout with a diffusion model, then adds details with another diffusion model.</li>\n    <li>This approach is faster and more effective for generating long videos compared to previous methods.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and outperforms existing techniques.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u6211\u6f14\u53d8\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002</li>\n    <li>\u65b0\u6a21\u578bStep-GUI\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u81ea\u52a8\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u884c\u4e1a\u9886\u5148\u7684\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u652f\u6301\u9ad8\u9690\u79c1\u6267\u884c\u3002</li>\n    <li>\u5f15\u5165\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u624b\u673a\u4f7f\u7528\u6a21\u5f0f\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u9645GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u663e\u793a\u51fa\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5f3a\u5927\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New multimodal models help automate graphical user interfaces (GUIs) but collecting good training data is still a challenge.</li>\n    <li>We developed a training system that uses a new reward method, achieving over 90% accuracy in data annotation at a much lower cost.</li>\n    <li>Our models, called Step-GUI, perform well on different GUI benchmarks and maintain strong general capabilities.</li>\n    <li>We introduced GUI-MCP, a protocol that allows for secure and efficient GUI automation while keeping user data private.</li>\n    <li>We created AndroidDaily, a benchmark that tests agents on real mobile usage patterns, showing promising results for everyday tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u800c\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u867d\u7136\u53ef\u4ee5\u5e76\u884c\u5904\u7406\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u751f\u6210\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion\u662f\u4e00\u79cd\u65b0\u7684\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5e76\u884c\u89e3\u7801\u63d0\u5347\u5230\u66f4\u9ad8\u7684\u69fd\u7ea7\u522b\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u201c\u89c4\u5212\u4e0e\u586b\u5145\u201d\u7684\u8fed\u4ee3\u89e3\u7801\u8fc7\u7a0b\uff0c\u9996\u5148\u8bc6\u522b\u5f31\u4f9d\u8d56\u7684\u69fd\uff0c\u7136\u540e\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u69fd\u3002</li>\n    <li>\u69fd\u7ea7\u8bbe\u8ba1\u4f7f\u5f97\u53ef\u4ee5\u5b8c\u5168\u91cd\u7528\u952e\u503c\u7f13\u5b58\uff0c\u5e76\u964d\u4f4e\u4e86\u5b66\u4e60\u590d\u6742\u6027\u3002</li>\n    <li>\u5728\u4e03\u4e2a\u4e0d\u540c\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReFusion\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4ee5\u524d\u7684MDMs\uff0c\u5e73\u5747\u901f\u5ea6\u63d0\u5347\u8d85\u8fc718\u500d\uff0c\u5e76\u7f29\u5c0f\u4e86\u4e0e\u5f3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e73\u5747\u901f\u5ea6\u4ecd\u63d0\u53472.33\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Autoregressive models (ARMs) are slow at generating results one step at a time.</li>\n  <li>Masked diffusion models (MDMs) can work faster but have issues like high computing costs and confusing results.</li>\n  <li>ReFusion is a new model that improves on MDMs by processing information in larger groups called slots instead of individual tokens.</li>\n  <li>The model uses a two-step process: first planning which slots to fill, and then filling them all at once.</li>\n  <li>ReFusion is much faster and more efficient than previous models, showing significant performance improvements in tests.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u7ea6\u675f\uff0c\u5bfc\u81f4\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\uff08ERC\uff09\u635f\u5931\uff0c\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u8f85\u52a9\u635f\u5931\uff0c\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u5f3a\u5236\u8981\u6c42\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u5176\u4ee3\u7406\u6807\u8bb0\u7684\u6fc0\u6d3b\u503c\u9ad8\u4e8e\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406\u6807\u8bb0\uff0c\u5e76\u4e14\u6bcf\u4e2a\u4ee3\u7406\u6807\u8bb0\u5fc5\u987b\u5f15\u53d1\u5176\u5bf9\u5e94\u4e13\u5bb6\u66f4\u5f3a\u7684\u6fc0\u6d3b\u3002</li>\n    <li>ERC\u635f\u5931\u5728\u8ba1\u7b97\u4e0a\u9ad8\u6548\uff0c\u4ec5\u5bf9n\u00b2\u4e2a\u6fc0\u6d3b\u8fdb\u884c\u64cd\u4f5c\uff0c\u5176\u4e2dn\u4e3a\u4e13\u5bb6\u6570\u91cf\uff0c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf93B\u523015B\u53c2\u6570\u7684MoE-LLM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6211\u4eec\u5c55\u793a\u4e86ERC\u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u4e13\u5bb6\u4e13\u4e1a\u5316\u7a0b\u5ea6\u7684\u7075\u6d3b\u63a7\u5236\u548c\u5b9a\u91cf\u8ddf\u8e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models can struggle because the router's choices don't always match the abilities of the experts.</li>\n    <li>We introduce a new method called expert-router coupling (ERC) loss, which connects the router's decisions more closely with what each expert can do.</li>\n    <li>The ERC loss uses a proxy token for each expert to help guide how well the expert performs with the tokens it receives.</li>\n    <li>It sets two main rules: experts should activate more strongly for their own tokens, and each token should get a better response from its matching expert.</li>\n    <li>This method is efficient and allows for better tracking of how well each expert is specializing during training, showing strong results in tests with large models and datasets.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 03, 2026";
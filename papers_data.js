window.trendingPapers = {
    "today": [{"paper": {"id": "2512.16093", "authors": [{"_id": "6944be16fbf17e708e186002", "user": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "name": "Jintao Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:52:30.866Z", "hidden": false}, {"_id": "6944be16fbf17e708e186003", "name": "Kaiwen Zheng", "hidden": false}, {"_id": "6944be16fbf17e708e186004", "name": "Kai Jiang", "hidden": false}, {"_id": "6944be16fbf17e708e186005", "name": "Haoxu Wang", "hidden": false}, {"_id": "6944be16fbf17e708e186006", "name": "Ion Stoica", "hidden": false}, {"_id": "6944be16fbf17e708e186007", "name": "Joseph E. Gonzalez", "hidden": false}, {"_id": "6944be16fbf17e708e186008", "name": "Jianfei Chen", "hidden": false}, {"_id": "6944be16fbf17e708e186009", "name": "Jun Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "publishedAt": "2025-12-18T02:21:30.000Z", "submittedOnDailyAt": "2025-12-25T00:44:44.469Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "submittedOnDailyBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "upvotes": 48, "discussionId": "6944be16fbf17e708e18600a", "projectPage": "https://github.com/thu-ml/TurboDiffusion", "githubRepo": "https://github.com/thu-ml/TurboDiffusion", "githubRepoAddedBy": "user", "ai_summary": "TurboDiffusion accelerates video generation by 100-200x using attention acceleration, step distillation, and quantization, while maintaining video quality.", "ai_keywords": ["SageAttention", "Sparse-Linear Attention", "rCM", "W8A8 quantization", "diffusion generation", "video generation", "RTX 5090 GPU"], "githubStars": 1958, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "summary_zh": "<ul>\n    <li>TurboDiffusion\u662f\u4e00\u4e2a\u89c6\u9891\u751f\u6210\u52a0\u901f\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u89c6\u9891\u751f\u6210\u901f\u5ea6\u63d0\u9ad8100-200\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\u3002</li>\n    <li>\u4e3b\u8981\u52a0\u901f\u65b9\u6cd5\u5305\u62ec\u4f4e\u4f4d\u6570SageAttention\u548c\u53ef\u8bad\u7ec3\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b(SLA)\u6765\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u3002</li>\n    <li>\u91c7\u7528rCM\u8fdb\u884c\u9ad8\u6548\u7684\u6b65\u9aa4\u84b8\u998f\u3002</li>\n    <li>\u5c06\u6a21\u578b\u53c2\u6570\u548c\u6fc0\u6d3b\u91cf\u5316\u4e3a8\u4f4d\uff0c\u4ee5\u52a0\u901f\u7ebf\u6027\u5c42\u5e76\u538b\u7f29\u6a21\u578b\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u5728\u5355\u4e2aRTX 5090 GPU\u4e0a\uff0cTurboDiffusion\u4e5f\u80fd\u5b9e\u73b0100-200\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u89c6\u9891\u8d28\u91cf\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>TurboDiffusion is a new framework that makes video generation 100-200 times faster while keeping the video quality high.</li>\n    <li>It speeds up the process using special techniques like low-bit SageAttention and Sparse-Linear Attention for faster computations.</li>\n    <li>It also uses a method called step distillation for efficiency and quantizes model data to 8 bits to speed up processing and reduce model size.</li>\n    <li>Tests on various video models show that it works well, even on a single RTX 5090 GPU.</li>\n    <li>You can find the code and model files on their GitHub page: https://github.com/thu-ml/TurboDiffusion.</li>\n</ul>"}, "publishedAt": "2025-12-17T21:21:30.000Z", "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/-RjP9vMsFF9ejLei8FwOh.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/OvRYA9op0fUwGSuuoHIO1.png", "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/0JdwlvuPuNKQepAUD2cYM.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16093.png", "numComments": 2, "submittedBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "fullname": "Jintao Zhang", "name": "jt-zhang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "66b1baeff10262fc4fa61961", "name": "UCBerkeley", "fullname": "University of California, Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f425c3a096536aeab42dea/bxNKEkprdm5JI1wkjmNAL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 45, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6587\u672c\uff0c\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u5f53\u524d\u65b9\u6cd5\u4f7f\u7528\u8f85\u52a9\u56fe\u50cf\u7b49\u6765\u6307\u5bfc\u4e2d\u95f4\u89c6\u89c9\u6b65\u9aa4\uff0c\u4f46\u6709\u5f88\u591a\u9650\u5236\u548c\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u673a\u5236\uff0c\u4f7fLMMs\u5728\u6ca1\u6709\u660e\u786e\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u53d1\u73b0\u548c\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u7f16\u7801\u56fe\u50cf\uff0c\u63d0\u53d6\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6848\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u6574\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text and struggle with visual reasoning tasks.</li>\n    <li>Current methods use helper images and annotations but have limitations like high costs and poor generalization.</li>\n    <li>The proposed solution allows LMMs to learn visual reasoning on their own without needing explicit guidance.</li>\n    <li>This new approach helps LMMs adaptively extract important visual information and improves performance on various vision tasks.</li>\n    <li>The method achieves top results even in complex tasks and works well with multi-task instruction tuning.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 3, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20605", "authors": [{"_id": "694e76e5746a34b55dd545eb", "name": "Seijin Kobayashi", "hidden": false}, {"_id": "694e76e5746a34b55dd545ec", "name": "Yanick Schimpf", "hidden": false}, {"_id": "694e76e5746a34b55dd545ed", "name": "Maximilian Schlegel", "hidden": false}, {"_id": "694e76e5746a34b55dd545ee", "name": "Angelika Steger", "hidden": false}, {"_id": "694e76e5746a34b55dd545ef", "name": "Maciej Wolczyk", "hidden": false}, {"_id": "694e76e5746a34b55dd545f0", "name": "Johannes von Oswald", "hidden": false}, {"_id": "694e76e5746a34b55dd545f1", "name": "Nino Scherrer", "hidden": false}, {"_id": "694e76e5746a34b55dd545f2", "name": "Kaitlin Maile", "hidden": false}, {"_id": "694e76e5746a34b55dd545f3", "name": "Guillaume Lajoie", "hidden": false}, {"_id": "694e76e5746a34b55dd545f4", "name": "Blake A. Richards", "hidden": false}, {"_id": "694e76e5746a34b55dd545f5", "name": "Rif A. Saurous", "hidden": false}, {"_id": "694e76e5746a34b55dd545f6", "name": "James Manyika", "hidden": false}, {"_id": "694e76e5746a34b55dd545f7", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "694e76e5746a34b55dd545f8", "name": "Alexander Meulemans", "hidden": false}, {"_id": "694e76e5746a34b55dd545f9", "name": "Jo\u00e3o Sacramento", "hidden": false}], "publishedAt": "2025-12-23T18:51:50.000Z", "submittedOnDailyAt": "2025-12-26T11:17:05.505Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "submittedOnDailyBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "upvotes": 43, "discussionId": "694e76e5746a34b55dd545fa", "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u81ea\u56de\u5f52\u6a21\u578b\u5728\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9010\u4e2a\u751f\u6210\u6807\u8bb0\u7684\u5b66\u4e60\u6548\u7387\u4f4e\u3002</li>\n    <li>\u901a\u8fc7\u5728\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u8fdb\u884c\u63a2\u7d22\uff0c\u53ef\u4ee5\u514b\u670d\u8fd9\u79cd\u4f4e\u6548\u7387\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u66f4\u9ad8\u9636\u7684\u975e\u56e0\u679c\u5e8f\u5217\u6a21\u578b\uff0c\u80fd\u591f\u63a7\u5236\u57fa\u7840\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6fc0\u6d3b\u6d41\u3002</li>\n    <li>\u5728\u7f51\u683c\u4e16\u754c\u548c\u57fa\u4e8eMuJoCo\u7684\u4efb\u52a1\u4e2d\uff0c\u8be5\u6a21\u578b\u5b66\u4f1a\u5c06\u957f\u65f6\u95f4\u5e8f\u5217\u538b\u7f29\u5230\u5185\u90e8\u63a7\u5236\u5668\u4e0a\uff0c\u6267\u884c\u6709\u610f\u4e49\u7684\u52a8\u4f5c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684\u201c\u5185\u90e8\u5f3a\u5316\u5b66\u4e60\u201d\u4f7f\u5f97\u5728\u7a00\u758f\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u5b66\u4e60\u53d8\u5f97\u53ef\u80fd\uff0c\u663e\u793a\u51fa\u6f5c\u5728\u52a8\u4f5c\u751f\u6210\u548c\u5f3a\u5316\u7684\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large-scale models trained to predict the next token can struggle with learning efficiently when rewards are rare.</li>\n    <li>The authors propose a new method that allows these models to explore actions within their internal structure instead of generating outputs one token at a time.</li>\n    <li>This method uses a higher-order model to manage sequences of actions, enabling the model to learn long-term strategies and behaviors.</li>\n    <li>Tests in grid world and MuJoCo tasks show that this approach improves learning efficiency and helps the model deal with sparse rewards.</li>\n    <li>The study highlights the potential of \"internal reinforcement learning\" to enhance hierarchical learning in advanced language models.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:51:50.000Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png", "numComments": 3, "submittedBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "fullname": "Maximilian Schlegel", "name": "schlegelm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e00\u822c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u4e0a\u4ecd\u7136\u8f83\u5f31\u3002</li>\n    <li>\u4e3a\u4e86\u6539\u5584\u8fd9\u4e00\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDSR Suite\u7684\u65b0\u5de5\u5177\uff0c\u5305\u62ec\u81ea\u52a8\u751f\u6210\u591a\u9879\u9009\u62e9\u9898\u548c\u7b54\u6848\u7684\u6d41\u7a0b\u3002</li>\n    <li>\u8be5\u6d41\u7a0b\u4ece\u5b9e\u9645\u89c6\u9891\u4e2d\u63d0\u53d6\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u6784\u5efa\u7528\u4e8e\u5b66\u4e60\u7684DSR-Train\u548c\u7528\u4e8e\u8bc4\u4f30\u7684DSR-Bench\u3002</li>\n    <li>\u6570\u636e\u5f3a\u8c03\u73b0\u5b9e\u89c6\u9891\u6e90\u30013D\u9700\u6c42\u3001\u89c6\u89d2\u8f6c\u6362\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u7cbe\u7ec6\u7684\u8fc7\u7a0b\u6027\u7b54\u6848\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\uff0c\u6709\u6548\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Dynamic Spatial Reasoning (DSR) is important for understanding how objects and their relationships change in 3D space over time, but current models struggle with this.</li>\n  <li>The DSR Suite includes an automated system that creates question-and-answer pairs from real-world videos to improve DSR.</li>\n  <li>This system gathers detailed geometric and motion information, helping to build a dataset (DSR-Train) for training and a benchmark (DSR-Bench) for evaluation.</li>\n  <li>Key features of the data include using real videos, focusing on 3D object and scene information, handling viewpoint changes, multi-object interactions, and providing detailed answers.</li>\n  <li>A new Geometry Selection Module (GSM) helps VLMs use relevant geometric information efficiently, leading to better DSR performance without losing general understanding accuracy.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aDreaMontage\u7684\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u65e0\u7f1d\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u957f\u65f6\u95f4\u201c\u4e00\u955c\u5230\u5e95\u201d\u89c6\u9891\u3002</li>\n    <li>\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e2d\u4ecb\u6761\u4ef6\u673a\u5236\u548c\u81ea\u9002\u5e94\u8c03\u4f18\u7b56\u7565\uff0c\u589e\u5f3a\u4e86\u5bf9\u4efb\u610f\u5e27\u7684\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>\u521b\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5e76\u5b9e\u65bd\u89c6\u89c9\u8868\u73b0\u7ec6\u5316\u9636\u6bb5\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u7535\u5f71\u8868\u73b0\u529b\u3002</li>\n    <li>\u91c7\u7528\u5b9a\u5236\u7684DPO\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4e3b\u9898\u8fd0\u52a8\u5408\u7406\u6027\u548c\u8fc7\u6e21\u5e73\u6ed1\u6027\u7b49\u95ee\u9898\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u6bb5\u5f0f\u81ea\u56de\u5f52\u63a8\u7406\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u751f\u6210\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" filmmaking technique is unique but often expensive and complicated to achieve.</li>\n    <li>DreaMontage is a new framework that can create smooth and expressive long videos based on user inputs.</li>\n    <li>It uses a special conditioning method to allow for better control over the video frames.</li>\n    <li>The framework improves image quality and smooth transitions by using a carefully curated dataset and specific training methods.</li>\n    <li>It also has a memory-efficient strategy for creating longer video sequences, making it easier for users to create cohesive cinematic experiences.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15716", "authors": [{"_id": "694b65e7746a34b55dd53dbe", "user": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "isPro": false, "fullname": "Jinjing Zhao", "user": "Jinjing713", "type": "user"}, "name": "Jinjing Zhao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:42.906Z", "hidden": false}, {"_id": "694b65e7746a34b55dd53dbf", "name": "Fangyun Wei", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc0", "name": "Zhening Liu", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc1", "name": "Hongyang Zhang", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc2", "name": "Chang Xu", "hidden": false}, {"_id": "694b65e7746a34b55dd53dc3", "name": "Yan Lu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64f8962bce75bb0fb50bdbdb/eC4hVgIfk0MxzPgn6mvGC.mp4"], "publishedAt": "2025-12-17T18:59:59.000Z", "submittedOnDailyAt": "2025-12-26T03:15:40.222Z", "title": "Spatia: Video Generation with Updatable Spatial Memory", "submittedOnDailyBy": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "isPro": false, "fullname": "Jinjing Zhao", "user": "Jinjing713", "type": "user"}, "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.", "upvotes": 20, "discussionId": "694b65e8746a34b55dd53dc4", "projectPage": "https://zhaojingjing713.github.io/Spatia/", "githubRepo": "https://github.com/ZhaoJingjing713/Spatia", "githubRepoAddedBy": "user", "ai_summary": "Spatia, a spatial memory-aware video generation framework, maintains long-term spatial and temporal consistency by preserving and updating a 3D scene point cloud, enabling realistic video generation and interactive editing.", "ai_keywords": ["spatial memory-aware", "video generation framework", "3D scene point cloud", "dynamic-static disentanglement", "visual SLAM", "explicit camera control", "3D-aware interactive editing"], "githubStars": 61, "organization": {"_id": "670621bc820835bbf0d2b499", "name": "Sydney-Uni", "fullname": "The University of Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628dbd0f9ec8275172da853f/YSJ_DfLPaAywMvoIoM2J4.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u957f\u65f6\u95f4\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Spatia\uff0c\u4e00\u4e2a\u7a7a\u95f4\u8bb0\u5fc6\u611f\u77e5\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u4fdd\u5b583D\u573a\u666f\u70b9\u4e91\u4f5c\u4e3a\u6301\u4e45\u7684\u7a7a\u95f4\u8bb0\u5fc6\u3002</li>\n    <li>Spatia\u901a\u8fc7\u89c6\u89c9SLAM\u4e0d\u65ad\u66f4\u65b0\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u8fed\u4ee3\u751f\u6210\u89c6\u9891\u7247\u6bb5\u3002</li>\n    <li>\u8fd9\u4e2a\u8bbe\u8ba1\u589e\u5f3a\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u751f\u6210\u903c\u771f\u52a8\u6001\u5b9e\u4f53\u7684\u80fd\u529b\u3002</li>\n    <li>Spatia\u652f\u6301\u663e\u5f0f\u6444\u50cf\u673a\u63a7\u5236\u548c3D\u4e92\u52a8\u7f16\u8f91\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51e0\u4f55\u7684\u53ef\u6269\u5c55\u89c6\u9891\u751f\u6210\u6846\u67b6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video generation models have trouble keeping consistent visuals and movements over time.</li>\n    <li>Spatia is a new video generation framework that uses a 3D point cloud as a memory for better consistency.</li>\n    <li>It generates video clips using this memory and updates it using visual SLAM technology.</li>\n    <li>This approach improves spatial consistency while still allowing for realistic moving objects.</li>\n    <li>Spatia also supports features like camera control and 3D editing, making it versatile for video generation.</li>\n</ul>"}, "publishedAt": "2025-12-17T13:59:59.000Z", "title": "Spatia: Video Generation with Updatable Spatial Memory", "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64f8962bce75bb0fb50bdbdb/eC4hVgIfk0MxzPgn6mvGC.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15716.png", "numComments": 2, "submittedBy": {"_id": "64f8962bce75bb0fb50bdbdb", "avatarUrl": "/avatars/c85537df848bda7ec92565f56cd32eed.svg", "fullname": "Jinjing Zhao", "name": "Jinjing713", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "670621bc820835bbf0d2b499", "name": "Sydney-Uni", "fullname": "The University of Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628dbd0f9ec8275172da853f/YSJ_DfLPaAywMvoIoM2J4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.19995", "authors": [{"_id": "694e1a8c746a34b55dd54540", "name": "Ming Li", "hidden": false}, {"_id": "694e1a8c746a34b55dd54541", "name": "Chenrui Fan", "hidden": false}, {"_id": "694e1a8c746a34b55dd54542", "name": "Yize Cheng", "hidden": false}, {"_id": "694e1a8c746a34b55dd54543", "name": "Soheil Feizi", "hidden": false}, {"_id": "694e1a8c746a34b55dd54544", "name": "Tianyi Zhou", "hidden": false}], "publishedAt": "2025-12-23T02:44:25.000Z", "submittedOnDailyAt": "2025-12-26T02:51:58.853Z", "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "submittedOnDailyBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "isPro": false, "fullname": "Tianyi Zhou", "user": "zhoutianyi", "type": "user"}, "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "upvotes": 12, "discussionId": "694e1a8d746a34b55dd54545", "githubRepo": "https://github.com/MingLiiii/ThinkARM", "githubRepoAddedBy": "user", "githubStars": 9, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u96be\u4ee5\u8bc6\u522b\u548c\u5206\u6790\uff0c\u6211\u4eec\u5f15\u5165\u4e86ThinkARM\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</li>\n    <li>ThinkARM\u5c06\u63a8\u7406\u8fc7\u7a0b\u62bd\u8c61\u4e3a\u529f\u80fd\u6027\u6b65\u9aa4\uff0c\u4f8b\u5982\u5206\u6790\u3001\u63a2\u7d22\u3001\u5b9e\u65bd\u548c\u9a8c\u8bc1\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u7684\u7ed3\u6784\u5dee\u5f02\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u63a2\u7d22\u662f\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u6548\u7387\u5bfc\u5411\u7684\u65b9\u6cd5\u4f1a\u9009\u62e9\u6027\u5730\u6291\u5236\u53cd\u9988\u6b65\u9aa4\u3002</li>\n    <li>\u672c\u7814\u7a76\u8868\u660e\uff0c\u96c6\u6210\u7684\u60c5\u8282\u7ea7\u8868\u793a\u53ef\u4ee5\u660e\u786e\u63a8\u7406\u6b65\u9aa4\uff0c\u4fc3\u8fdb\u5bf9\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7ed3\u6784\u7684\u7cfb\u7edf\u5206\u6790\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Large language models show reasoning traces, but understanding their thought processes is challenging.</li>\n  <li>The study uses Schoenfeld's Episode Theory to create a framework called ThinkARM, which breaks down reasoning into clear steps.</li>\n  <li>Applying this framework to math problem-solving reveals important patterns in how reasoning models work compared to non-reasoning models.</li>\n  <li>Two case studies highlight that exploring solutions is crucial for getting correct answers and that some methods reduce feedback steps to be more efficient.</li>\n  <li>The findings help clarify how reasoning is organized in language models, allowing for better analysis and understanding.</li>\n</ul>"}, "publishedAt": "2025-12-22T21:44:25.000Z", "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models", "summary": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19995.png", "numComments": 3, "submittedBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "fullname": "Tianyi Zhou", "name": "zhoutianyi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19949", "authors": [{"_id": "694e0a82746a34b55dd54516", "name": "Zixuan Huang", "hidden": false}, {"_id": "694e0a82746a34b55dd54517", "name": "Xiang Li", "hidden": false}, {"_id": "694e0a82746a34b55dd54518", "name": "Zhaoyang Lv", "hidden": false}, {"_id": "694e0a82746a34b55dd54519", "name": "James M. Rehg", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/629fe0dd198a9a5a755f3ce0/4F38iBHO42lBqgkLGfGka.png"], "publishedAt": "2025-12-23T00:38:52.000Z", "submittedOnDailyAt": "2025-12-26T02:44:58.632Z", "title": "How Much 3D Do Video Foundation Models Encode?", "submittedOnDailyBy": {"_id": "629fe0dd198a9a5a755f3ce0", "avatarUrl": "/avatars/e643d43f66c10729f155edca96aef1f8.svg", "isPro": false, "fullname": "Zixuan Huang", "user": "zxhuang1698", "type": "user"}, "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.", "upvotes": 7, "discussionId": "694e0a82746a34b55dd5451a", "projectPage": "https://vidfm-3d-probe.github.io/", "organization": {"_id": "60212a089f64108326fac7c2", "name": "illinois", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u662f3D\u4e16\u754c\u7684\u8fde\u7eed2D\u6295\u5f71\uff0c\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u81ea\u7136\u83b7\u5f97\u5168\u74033D\u7406\u89e3\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u4e0d\u540c\u89c6\u9891\u57fa\u7840\u6a21\u578b\u76843D\u610f\u8bc6\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf93D\u7269\u4f53\u548c\u573a\u666f\u6709\u5f88\u5f3a\u7684\u7406\u89e3\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5c3d\u7ba1\u6ca1\u6709\u63a5\u53d73D\u6570\u636e\u8bad\u7ec3\uff0c\u4f46\u5176\u7406\u89e3\u80fd\u529b\u751a\u81f3\u8d85\u8fc7\u4e86\u4e13\u95e8\u4e3a3D\u4efb\u52a1\u8bad\u7ec3\u7684\u5927\u578b\u4e13\u5bb6\u6a21\u578b\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u76843D\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c2\u5bdf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study explores whether video models can understand 3D scenes after training on large amounts of video data.</li>\n    <li>Researchers created a framework to measure how well different video models (VidFMs) understand 3D properties.</li>\n    <li>Results show that advanced video generation models have a strong grasp of 3D objects and scenes, even without 3D training data.</li>\n    <li>These video models can sometimes outperform specialized models that are designed for 3D tasks.</li>\n    <li>The findings help in developing better and more scalable 3D models in the future.</li>\n</ul>"}, "publishedAt": "2025-12-22T19:38:52.000Z", "title": "How Much 3D Do Video Foundation Models Encode?", "summary": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/629fe0dd198a9a5a755f3ce0/4F38iBHO42lBqgkLGfGka.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19949.png", "numComments": 2, "submittedBy": {"_id": "629fe0dd198a9a5a755f3ce0", "avatarUrl": "/avatars/e643d43f66c10729f155edca96aef1f8.svg", "fullname": "Zixuan Huang", "name": "zxhuang1698", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "60212a089f64108326fac7c2", "name": "illinois", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19680", "authors": [{"_id": "694b4e0c746a34b55dd53c34", "name": "Xinyao Liao", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c35", "name": "Qiyuan He", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c36", "name": "Kai Xu", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c37", "name": "Xiaoye Qu", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c38", "name": "Yicong Li", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c39", "name": "Wei Wei", "hidden": false}, {"_id": "694b4e0c746a34b55dd53c3a", "name": "Angela Yao", "hidden": false}], "publishedAt": "2025-12-22T18:54:30.000Z", "submittedOnDailyAt": "2025-12-26T02:44:27.188Z", "title": "VA-\u03c0: Variational Policy Alignment for Pixel-Aware Autoregressive Generation", "submittedOnDailyBy": {"_id": "64cb54da1af278541d663708", "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg", "isPro": false, "fullname": "Xiaoye Qu", "user": "Xiaoye08", "type": "user"}, "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-\u03c0, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-\u03c0 formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-\u03c0 introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-\u03c0 enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.", "upvotes": 6, "discussionId": "694b4e0c746a34b55dd53c3b", "projectPage": "https://lil-shake.github.io/va-pi.github.io/", "githubRepo": "https://github.com/Lil-Shake/VA-Pi", "githubRepoAddedBy": "user", "ai_summary": "VA-$\\pi$ optimizes autoregressive visual generators using a pixel-space objective to improve image quality and performance without retraining tokenizers or using external rewards.", "ai_keywords": ["autoregressive (AR) visual generation", "tokenizers", "discrete sequences", "evidence lower bound (ELBO)", "reinforcement-based alignment", "policy", "intrinsic reward", "teacher forcing", "distributional consistency", "FID", "IS", "LlamaGen-XXL", "GenEval", "LlamaGen", "Janus-Pro"], "githubStars": 4, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u4f9d\u8d56\u4e8e\u6807\u8bb0\u5668\u5c06\u56fe\u50cf\u4e0e\u79bb\u6563\u5e8f\u5217\u76f8\u4e92\u6620\u5c04\uff0c\u4f46\u73b0\u6709\u6807\u8bb0\u5668\u8bad\u7ec3\u65f6\u4e0e\u81ea\u56de\u5f52\u751f\u6210\u5668\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4f4e\u3002</li>\n    <li>\u63d0\u51fa\u4e86VA-\u03c0\u6846\u67b6\uff0c\u76f4\u63a5\u4f18\u5316\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7ed3\u5408\u50cf\u7d20\u91cd\u5efa\u4e0e\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u3002</li>\n    <li>VA-\u03c0\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u81ea\u56de\u5f52\u751f\u6210\u5668\u89c6\u4e3a\u7b56\u7565\uff0c\u5229\u7528\u50cf\u7d20\u91cd\u5efa\u8d28\u91cf\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u6307\u5bfc\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u73b0\u6709\u81ea\u56de\u5f52\u751f\u6210\u5668\u8fdb\u884c\u5feb\u901f\u9002\u5e94\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6807\u8bb0\u5668\u6216\u4f7f\u7528\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u56fe\u50cf\u751f\u6210\u6548\u679c\u3002</li>\n    <li>\u5728\u4f7f\u75281%\u7684ImageNet-1K\u6570\u636e\u548c25\u5206\u949f\u8c03\u4f18\u540e\uff0c\u663e\u8457\u964d\u4f4eFID\u548c\u63d0\u9ad8IS\uff0c\u540c\u65f6\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e0a\u4e5f\u53d6\u5f97\u4e86\u660e\u663e\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Autoregressive visual generation uses tokenizers to translate images into sequences, but this can result in low-quality images due to misalignment in training.</li>\n  <li>The proposed solution, VA-\u03c0, improves AR models by directly focusing on image quality during training using a pixel-space objective.</li>\n  <li>VA-\u03c0 uses a reinforcement learning approach to align the generator and tokenizer, rewarding the model based on how well it reconstructs images from token sequences.</li>\n  <li>It can quickly adapt existing AR generators without needing to retrain tokenizers or use external reward models.</li>\n  <li>In tests, VA-\u03c0 significantly improved image quality metrics and performance on text-to-image tasks with minimal data and tuning time.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:54:30.000Z", "title": "VA-\u03c0: Variational Policy Alignment for Pixel-Aware Autoregressive Generation", "summary": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-\u03c0, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-\u03c0 formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-\u03c0 introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-\u03c0 enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19680.png", "numComments": 2, "submittedBy": {"_id": "64cb54da1af278541d663708", "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg", "fullname": "Xiaoye Qu", "name": "Xiaoye08", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13043", "authors": [{"_id": "694df96e746a34b55dd544df", "name": "Tong Wei", "hidden": false}, {"_id": "694df96e746a34b55dd544e0", "name": "Yijun Yang", "hidden": false}, {"_id": "694df96e746a34b55dd544e1", "name": "Changhao Zhang", "hidden": false}, {"_id": "694df96e746a34b55dd544e2", "name": "Junliang Xing", "hidden": false}, {"_id": "694df96e746a34b55dd544e3", "name": "Yuanchun Shi", "hidden": false}, {"_id": "694df96e746a34b55dd544e4", "name": "Zongqing Lu", "hidden": false}, {"_id": "694df96e746a34b55dd544e5", "name": "Deheng Ye", "hidden": false}], "publishedAt": "2025-12-15T07:11:56.000Z", "submittedOnDailyAt": "2025-12-26T00:32:37.057Z", "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "submittedOnDailyBy": {"_id": "66c2e8b4a03b764ca9057e65", "avatarUrl": "/avatars/ce17d68f02a08c148ebb9df3170812c8.svg", "isPro": false, "fullname": "Tong Wei", "user": "weit123", "type": "user"}, "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "upvotes": 3, "discussionId": "694df96e746a34b55dd544e6", "summary_zh": "<ul>\n    <li>\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u53d7\u5230\u7a00\u758f\u5956\u52b1\u548c\u957f\u671f\u4fe1\u7528\u5206\u914d\u7684\u6311\u6218\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u901a\u8fc7\u67e5\u8be2\u8001\u5e08\u6a21\u578b\u6765\u589e\u52a0\u5956\u52b1\u5bc6\u5ea6\uff0c\u4f46\u4f9d\u8d56\u6602\u8d35\u7684\u8001\u5e08\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u590d\u5236\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GTR-Turbo\uff0c\u8fd9\u662f\u5bf9GTR\u7684\u9ad8\u6548\u5347\u7ea7\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u67e5\u8be2\u6602\u8d35\u7684\u8001\u5e08\u6a21\u578b\u3002</li>\n    <li>GTR-Turbo\u901a\u8fc7\u5408\u5e76\u6b63\u5728\u8fdb\u884c\u7684RL\u8bad\u7ec3\u4e2d\u7684\u68c0\u67e5\u70b9\u6743\u91cd\uff0c\u4f7f\u7528\u5408\u5e76\u6a21\u578b\u4f5c\u4e3a\u201c\u514d\u8d39\u201d\u8001\u5e08\u6765\u6307\u5bfc\u540e\u7eed\u7684RL\u3002</li>\n    <li>GTR-Turbo\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u4f7f\u57fa\u51c6\u6a21\u578b\u7684\u51c6\u786e\u6027\u63d0\u9ad810-30%\uff0c\u540c\u65f6\u51cf\u5c11\u4e8650%\u7684\u8bad\u7ec3\u65f6\u95f4\u548c60%\u7684\u8ba1\u7b97\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Multi-turn reinforcement learning for agents using vision-language models faces challenges like sparse rewards and long-term feedback issues.</li>\n  <li>Recent methods that improve rewards depend on expensive teacher models, making them hard to use and replicate.</li>\n  <li>GTR-Turbo is a new method that enhances the performance of an existing approach (GTR) without needing these costly teacher models.</li>\n  <li>It combines model weights from training to create a \"free\" teacher, which helps guide the learning process effectively.</li>\n  <li>GTR-Turbo boosts accuracy by 10-30% and significantly cuts down training time and costs compared to GTR.</li>\n</ul>"}, "publishedAt": "2025-12-15T02:11:56.000Z", "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13043.png", "numComments": 2, "submittedBy": {"_id": "66c2e8b4a03b764ca9057e65", "avatarUrl": "/avatars/ce17d68f02a08c148ebb9df3170812c8.svg", "fullname": "Tong Wei", "name": "weit123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SemanticGen\uff0c\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u7684\u6162\u6536\u655b\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u9996\u5148\u5728\u7d27\u51d1\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u7279\u5f81\uff0c\u7136\u540e\u518d\u6dfb\u52a0\u9ad8\u9891\u7ec6\u8282\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5904\u7406\u4f4e\u7ea7\u89c6\u9891\u4fe1\u606f\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u751f\u6210\u66f4\u5feb\uff0c\u4e14\u5728\u957f\u89c6\u9891\u751f\u6210\u65f6\u66f4\u6709\u6548\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0cSemanticGen\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u5e76\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new method for generating videos that addresses slow and costly processes in existing models.</li>\n    <li>It starts by creating a basic outline of the video in a compact, high-level semantic space before adding finer details.</li>\n    <li>The generation process happens in two stages using diffusion models: first for semantic features, then for detailed video output.</li>\n    <li>This approach leads to faster results and is more efficient for generating long videos.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and outperforms current leading methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19693", "authors": [{"_id": "694a0ffa335742716e93227d", "name": "Weichen Fan", "hidden": false}, {"_id": "694a0ffa335742716e93227e", "name": "Haiwen Diao", "hidden": false}, {"_id": "694a0ffa335742716e93227f", "name": "Quan Wang", "hidden": false}, {"_id": "694a0ffa335742716e932280", "name": "Dahua Lin", "hidden": false}, {"_id": "694a0ffa335742716e932281", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-22T18:59:57.000Z", "submittedOnDailyAt": "2025-12-23T01:15:14.379Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "upvotes": 51, "discussionId": "694a0ffa335742716e932282", "githubRepo": "https://github.com/WeichenFan/UAE", "githubRepoAddedBy": "user", "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.", "ai_keywords": ["spectral characteristics", "semantic encoders", "pixel encoders", "feature spectrum", "low-frequency components", "high-frequency information", "Prism Hypothesis", "Unified Autoencoding", "frequency-band modulator", "ImageNet", "MS-COCO", "latent space"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4e86\u4e0d\u540c\u8bed\u4e49\u548c\u50cf\u7d20\u7f16\u7801\u5668\u7684\u5149\u8c31\u7279\u5f81\u3002</li>\n    <li>\u53d1\u73b0\u8bed\u4e49\u7f16\u7801\u5668\u4e3b\u8981\u6355\u6349\u4f4e\u9891\u6210\u5206\uff0c\u4f20\u8fbe\u62bd\u8c61\u610f\u4e49\uff0c\u800c\u50cf\u7d20\u7f16\u7801\u5668\u4fdd\u7559\u9ad8\u9891\u4fe1\u606f\uff0c\u4f20\u9012\u7ec6\u8282\u3002</li>\n    <li>\u63d0\u51fa\u201c\u68f1\u955c\u5047\u8bf4\u201d\uff0c\u8ba4\u4e3a\u4e0d\u540c\u6570\u636e\u6a21\u6001\u53ef\u4ee5\u770b\u4f5c\u81ea\u7136\u4e16\u754c\u5728\u5171\u4eab\u7279\u5f81\u5149\u8c31\u4e0a\u7684\u6295\u5f71\u3002</li>\n    <li>\u63d0\u51fa\u7edf\u4e00\u81ea\u7f16\u7801\u6a21\u578b\uff08UAE\uff09\uff0c\u901a\u8fc7\u9891\u7387\u8c03\u5236\u5668\u7ed3\u5408\u8bed\u4e49\u7ed3\u6784\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n    <li>\u5728ImageNet\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUAE\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u6210\u529f\u7edf\u4e00\u4e86\u8bed\u4e49\u62bd\u8c61\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper analyzes how different types of encoders (semantic and pixel) capture information from data.</li>\n    <li>It discovers that semantic encoders focus on low-frequency data for abstract meanings, while pixel encoders include high-frequency data for detailed information.</li>\n    <li>This relationship is called the Prism Hypothesis, suggesting that different data types project onto a shared feature spectrum.</li>\n    <li>The authors introduce a model called Unified Autoencoding (UAE) that combines these two types of information using a frequency-band modulator.</li>\n    <li>Tests on datasets like ImageNet and MS-COCO show that UAE effectively merges abstract and detailed information, achieving top performance.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:59:57.000Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19673", "authors": [{"_id": "694ac3ad746a34b55dd53b6c", "name": "Yuqiao Tan", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6d", "name": "Minzheng Wang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6e", "name": "Shizhu He", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6f", "name": "Huanxuan Liao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b70", "name": "Chengfeng Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b71", "name": "Qiunan Lu", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b72", "name": "Tian Liang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b73", "name": "Jun Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b74", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-22T18:51:48.000Z", "submittedOnDailyAt": "2025-12-24T00:28:45.252Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "submittedOnDailyBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "isPro": false, "fullname": "mz.w", "user": "iiiiwis", "type": "user"}, "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "upvotes": 49, "discussionId": "694ac3ad746a34b55dd53b75", "githubRepo": "https://github.com/Trae1ounG/BuPO", "githubRepoAddedBy": "user", "ai_summary": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.", "ai_keywords": ["reinforcement learning", "large language models", "Transformer residual stream", "unembedding matrix", "Internal Layer Policies", "Internal Modular Policies", "self-attention", "feed-forward network", "entropy", "Bottom-up Policy Optimization"], "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u5ffd\u89c6\u4e86\u5176\u5185\u90e8\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u5206\u6790Transformer\u7684\u6b8b\u5dee\u6d41\u548c\u9690\u85cf\u72b6\u6001\uff0c\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7b56\u7565\uff0c\u63ed\u793a\u5185\u90e8\u5c42\u7b56\u7565\u548c\u6a21\u5757\u7b56\u7565\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u65e9\u671f\u5c42\u4fdd\u6301\u9ad8\u71b5\u4ee5\u8fdb\u884c\u63a2\u7d22\uff0c\u800c\u9876\u5c42\u5219\u8d8b\u5411\u4e8e\u96f6\u71b5\u4ee5\u8fdb\u884c\u4f18\u5316\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u6536\u655b\u6a21\u5f0f\u4e0d\u540c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u2014\u2014\u81ea\u4e0b\u800c\u4e0a\u7684\u7b56\u7565\u4f18\u5316\uff08BuPO\uff09\uff0c\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u5c42\u7b56\u7565\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBuPO\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u91cd\u5efa\u4e86\u57fa\u7840\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper studies how large language models (LLMs) work internally, breaking down their decision-making processes.</li>\n    <li>It introduces the concept of Internal Layer Policies and Internal Modular Policies to understand contributions from different parts of the model.</li>\n    <li>Findings show that early layers of the model focus on exploration, while later layers focus on refining predictions, with different models showing unique patterns.</li>\n    <li>The authors propose a new approach called Bottom-up Policy Optimization (BuPO), which improves training by optimizing the internal policies from the start.</li>\n    <li>Experiments show that BuPO leads to better performance on complex reasoning tasks, and the code is available online.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:51:48.000Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19673.png", "numComments": 4, "submittedBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "fullname": "mz.w", "name": "iiiiwis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 45, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6587\u672c\uff0c\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u5f53\u524d\u65b9\u6cd5\u4f7f\u7528\u8f85\u52a9\u56fe\u50cf\u7b49\u6765\u6307\u5bfc\u4e2d\u95f4\u89c6\u89c9\u6b65\u9aa4\uff0c\u4f46\u6709\u5f88\u591a\u9650\u5236\u548c\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u673a\u5236\uff0c\u4f7fLMMs\u5728\u6ca1\u6709\u660e\u786e\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u53d1\u73b0\u548c\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u7f16\u7801\u56fe\u50cf\uff0c\u63d0\u53d6\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6848\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u6574\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text and struggle with visual reasoning tasks.</li>\n    <li>Current methods use helper images and annotations but have limitations like high costs and poor generalization.</li>\n    <li>The proposed solution allows LMMs to learn visual reasoning on their own without needing explicit guidance.</li>\n    <li>This new approach helps LMMs adaptively extract important visual information and improves performance on various vision tasks.</li>\n    <li>The method achieves top results even in complex tasks and works well with multi-task instruction tuning.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 3, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20605", "authors": [{"_id": "694e76e5746a34b55dd545eb", "name": "Seijin Kobayashi", "hidden": false}, {"_id": "694e76e5746a34b55dd545ec", "name": "Yanick Schimpf", "hidden": false}, {"_id": "694e76e5746a34b55dd545ed", "name": "Maximilian Schlegel", "hidden": false}, {"_id": "694e76e5746a34b55dd545ee", "name": "Angelika Steger", "hidden": false}, {"_id": "694e76e5746a34b55dd545ef", "name": "Maciej Wolczyk", "hidden": false}, {"_id": "694e76e5746a34b55dd545f0", "name": "Johannes von Oswald", "hidden": false}, {"_id": "694e76e5746a34b55dd545f1", "name": "Nino Scherrer", "hidden": false}, {"_id": "694e76e5746a34b55dd545f2", "name": "Kaitlin Maile", "hidden": false}, {"_id": "694e76e5746a34b55dd545f3", "name": "Guillaume Lajoie", "hidden": false}, {"_id": "694e76e5746a34b55dd545f4", "name": "Blake A. Richards", "hidden": false}, {"_id": "694e76e5746a34b55dd545f5", "name": "Rif A. Saurous", "hidden": false}, {"_id": "694e76e5746a34b55dd545f6", "name": "James Manyika", "hidden": false}, {"_id": "694e76e5746a34b55dd545f7", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "694e76e5746a34b55dd545f8", "name": "Alexander Meulemans", "hidden": false}, {"_id": "694e76e5746a34b55dd545f9", "name": "Jo\u00e3o Sacramento", "hidden": false}], "publishedAt": "2025-12-23T18:51:50.000Z", "submittedOnDailyAt": "2025-12-26T11:17:05.505Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "submittedOnDailyBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "isPro": false, "fullname": "Maximilian Schlegel", "user": "schlegelm", "type": "user"}, "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "upvotes": 43, "discussionId": "694e76e5746a34b55dd545fa", "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u81ea\u56de\u5f52\u6a21\u578b\u5728\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9010\u4e2a\u751f\u6210\u6807\u8bb0\u7684\u5b66\u4e60\u6548\u7387\u4f4e\u3002</li>\n    <li>\u901a\u8fc7\u5728\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u8fdb\u884c\u63a2\u7d22\uff0c\u53ef\u4ee5\u514b\u670d\u8fd9\u79cd\u4f4e\u6548\u7387\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u66f4\u9ad8\u9636\u7684\u975e\u56e0\u679c\u5e8f\u5217\u6a21\u578b\uff0c\u80fd\u591f\u63a7\u5236\u57fa\u7840\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6fc0\u6d3b\u6d41\u3002</li>\n    <li>\u5728\u7f51\u683c\u4e16\u754c\u548c\u57fa\u4e8eMuJoCo\u7684\u4efb\u52a1\u4e2d\uff0c\u8be5\u6a21\u578b\u5b66\u4f1a\u5c06\u957f\u65f6\u95f4\u5e8f\u5217\u538b\u7f29\u5230\u5185\u90e8\u63a7\u5236\u5668\u4e0a\uff0c\u6267\u884c\u6709\u610f\u4e49\u7684\u52a8\u4f5c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684\u201c\u5185\u90e8\u5f3a\u5316\u5b66\u4e60\u201d\u4f7f\u5f97\u5728\u7a00\u758f\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u5b66\u4e60\u53d8\u5f97\u53ef\u80fd\uff0c\u663e\u793a\u51fa\u6f5c\u5728\u52a8\u4f5c\u751f\u6210\u548c\u5f3a\u5316\u7684\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large-scale models trained to predict the next token can struggle with learning efficiently when rewards are rare.</li>\n    <li>The authors propose a new method that allows these models to explore actions within their internal structure instead of generating outputs one token at a time.</li>\n    <li>This method uses a higher-order model to manage sequences of actions, enabling the model to learn long-term strategies and behaviors.</li>\n    <li>Tests in grid world and MuJoCo tasks show that this approach improves learning efficiency and helps the model deal with sparse rewards.</li>\n    <li>The study highlights the potential of \"internal reinforcement learning\" to enhance hierarchical learning in advanced language models.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:51:50.000Z", "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning", "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20605.png", "numComments": 3, "submittedBy": {"_id": "67f3a5638a2b2e738c7aec2b", "avatarUrl": "/avatars/f268121ab33da4c28789880d1086e7a5.svg", "fullname": "Maximilian Schlegel", "name": "schlegelm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20557", "authors": [{"_id": "694b766b746a34b55dd53de6", "user": {"_id": "658d3b74f893598fcaee75f1", "avatarUrl": "/avatars/edb2243ad020bd72a1b305accc2e7034.svg", "isPro": false, "fullname": "Shengchao Zhou", "user": "zhousc", "type": "user"}, "name": "Shengchao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:38.300Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de7", "user": {"_id": "669f3b098c65c172c4d64039", "avatarUrl": "/avatars/d85158964853ab87b9b677fa16df90f8.svg", "isPro": false, "fullname": "Yuxin Chen", "user": "Uasonchen", "type": "user"}, "name": "Yuxin Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:35.585Z", "hidden": false}, {"_id": "694b766b746a34b55dd53de8", "name": "Yuying Ge", "hidden": false}, {"_id": "694b766b746a34b55dd53de9", "user": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "name": "Wei Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:45:32.801Z", "hidden": false}, {"_id": "694b766b746a34b55dd53dea", "name": "Jiehong Lin", "hidden": false}, {"_id": "694b766b746a34b55dd53deb", "name": "Ying Shan", "hidden": false}, {"_id": "694b766b746a34b55dd53dec", "name": "Xiaojuan Qi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "publishedAt": "2025-12-23T17:56:36.000Z", "submittedOnDailyAt": "2025-12-25T00:20:37.914Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "submittedOnDailyBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "isPro": false, "fullname": "Wei Huang", "user": "AaronHuangWei", "type": "user"}, "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "upvotes": 41, "discussionId": "694b766b746a34b55dd53ded", "githubRepo": "https://github.com/TencentARC/DSR_Suite", "githubRepoAddedBy": "user", "ai_summary": "DSR Suite enhances vision-language models with dynamic spatial reasoning through automated data generation and a geometry selection module that integrates geometric priors.", "ai_keywords": ["vision-language models", "dynamic spatial reasoning", "4D-aware training", "automated pipeline", "multiple-choice question-answer pairs", "vision foundation models", "camera poses", "local point clouds", "object masks", "orientations", "3D trajectories", "DSR-Train", "DSR-Bench", "Geometry Selection Module", "geometry tokens", "Qwen2.5-VL-7B", "video understanding benchmarks"], "githubStars": 28, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e00\u822c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\uff08DSR\uff09\u4e0a\u4ecd\u7136\u8f83\u5f31\u3002</li>\n    <li>\u4e3a\u4e86\u6539\u5584\u8fd9\u4e00\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDSR Suite\u7684\u65b0\u5de5\u5177\uff0c\u5305\u62ec\u81ea\u52a8\u751f\u6210\u591a\u9879\u9009\u62e9\u9898\u548c\u7b54\u6848\u7684\u6d41\u7a0b\u3002</li>\n    <li>\u8be5\u6d41\u7a0b\u4ece\u5b9e\u9645\u89c6\u9891\u4e2d\u63d0\u53d6\u51e0\u4f55\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u6784\u5efa\u7528\u4e8e\u5b66\u4e60\u7684DSR-Train\u548c\u7528\u4e8e\u8bc4\u4f30\u7684DSR-Bench\u3002</li>\n    <li>\u6570\u636e\u5f3a\u8c03\u73b0\u5b9e\u89c6\u9891\u6e90\u30013D\u9700\u6c42\u3001\u89c6\u89d2\u8f6c\u6362\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u7cbe\u7ec6\u7684\u8fc7\u7a0b\u6027\u7b54\u6848\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u9009\u62e9\u6a21\u5757\uff08GSM\uff09\uff0c\u6709\u6548\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Dynamic Spatial Reasoning (DSR) is important for understanding how objects and their relationships change in 3D space over time, but current models struggle with this.</li>\n  <li>The DSR Suite includes an automated system that creates question-and-answer pairs from real-world videos to improve DSR.</li>\n  <li>This system gathers detailed geometric and motion information, helping to build a dataset (DSR-Train) for training and a benchmark (DSR-Bench) for evaluation.</li>\n  <li>Key features of the data include using real videos, focusing on 3D object and scene information, handling viewpoint changes, multi-object interactions, and providing detailed answers.</li>\n  <li>A new Geometry Selection Module (GSM) helps VLMs use relevant geometric information efficiently, leading to better DSR performance without losing general understanding accuracy.</li>\n</ul>"}, "publishedAt": "2025-12-23T12:56:36.000Z", "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models", "summary": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/656db3f53dc1d277e5a64410/Wgimy4m8ERFK9NwbdFrt8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20557.png", "numComments": 2, "submittedBy": {"_id": "656db3f53dc1d277e5a64410", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png", "fullname": "Wei Huang", "name": "AaronHuangWei", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20618", "authors": [{"_id": "694ba02a746a34b55dd53e8b", "name": "Runtao Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8c", "name": "Ziyi Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8d", "name": "Jiaqi Tang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8e", "name": "Yue Ma", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8f", "name": "Renjie Pi", "hidden": false}, {"_id": "694ba02a746a34b55dd53e90", "name": "Jipeng Zhang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e91", "name": "Qifeng Chen", "hidden": false}], "publishedAt": "2025-12-23T18:59:49.000Z", "submittedOnDailyAt": "2025-12-24T05:57:23.776Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "submittedOnDailyBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "isPro": true, "fullname": "Jiaqi Tang", "user": "Jiaqi-hkust", "type": "user"}, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "upvotes": 38, "discussionId": "694ba02a746a34b55dd53e92", "ai_summary": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.", "ai_keywords": ["multimodal LLMs", "long-video QA", "multi-agent framework", "grounding agent", "vision agent", "reinforcement learning", "temporal grounding", "LongTVQA", "LongTVQA+"], "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u95ee\u7b54\uff0c\u80fd\u66f4\u597d\u5730\u5904\u7406\u957f\u8fbe\u4e00\u5c0f\u65f6\u7684\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u534f\u8c03\u7684\u4e3b\u667a\u80fd\u4f53\u3001\u4e00\u4e2a\u5b9a\u4f4d\u76f8\u5173\u7247\u6bb5\u7684\u57fa\u7840\u667a\u80fd\u4f53\u548c\u4e00\u4e2a\u63d0\u53d6\u6587\u672c\u89c2\u5bdf\u7684\u89c6\u89c9\u667a\u80fd\u4f53\u3002</li>\n    <li>\u4e3b\u667a\u80fd\u4f53\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5408\u4f5c\u6548\u7387\u548c\u6b63\u786e\u6027\u3002</li>\n    <li>\u5728\u6211\u4eec\u63d0\u51fa\u7684LongTVQA\u548cLongTVQA+\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7cfb\u7edf\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods for analyzing long videos have been developed, improving question answering over hour-long episodes.</li>\n    <li>Many existing techniques lose important details by summarizing or using limited tools.</li>\n    <li>A new multi-agent system is proposed where a master language model coordinates with specialized agents to find and understand relevant video segments.</li>\n    <li>This system improves the focus on important clips and adds useful visual details to subtitles.</li>\n    <li>Tests on new datasets show that this method outperforms traditional approaches and that reinforcement learning enhances its effectiveness.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:49.000Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png", "numComments": 1, "submittedBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "fullname": "Jiaqi Tang", "name": "Jiaqi-hkust", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20617", "authors": [{"_id": "694b58e3746a34b55dd53cff", "name": "Yuxi Xiao", "hidden": false}, {"_id": "694b58e3746a34b55dd53d00", "name": "Longfei Li", "hidden": false}, {"_id": "694b58e3746a34b55dd53d01", "name": "Shen Yan", "hidden": false}, {"_id": "694b58e3746a34b55dd53d02", "name": "Xinhang Liu", "hidden": false}, {"_id": "694b58e3746a34b55dd53d03", "name": "Sida Peng", "hidden": false}, {"_id": "694b58e3746a34b55dd53d04", "name": "Yunchao Wei", "hidden": false}, {"_id": "694b58e3746a34b55dd53d05", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "694b58e3746a34b55dd53d06", "name": "Bingyi Kang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "publishedAt": "2025-12-23T18:59:46.000Z", "submittedOnDailyAt": "2025-12-24T00:38:28.003Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "upvotes": 34, "discussionId": "694b58e4746a34b55dd53d07", "projectPage": "https://spatialtree.github.io/", "ai_summary": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.", "ai_keywords": ["SpatialTree", "low-level perception", "mental mapping", "simulation", "agentic competence", "capability-centric hierarchical benchmark", "targeted supervised fine-tuning", "negative transfer", "cross-level transfer", "naive RL", "auto-think strategy"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u7a7a\u95f4\u80fd\u529b\u9010\u6e10\u53d1\u5c55\uff0c\u5305\u62ec\u611f\u77e5\u3001\u63a8\u7406\u548c\u4e92\u52a8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SpatialTree\uff0c\u4e00\u4e2a\u5206\u4e3a\u56db\u4e2a\u5c42\u6b21\u7684\u7a7a\u95f4\u80fd\u529b\u5c42\u7ea7\uff1a\u4f4e\u7ea7\u611f\u77e5\uff08L1\uff09\u3001\u5fc3\u7406\u6620\u5c04\uff08L2\uff09\u3001\u6a21\u62df\uff08L3\uff09\u548c\u81ea\u4e3b\u80fd\u529b\uff08L4\uff09\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u4e3b\u6d41\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u572827\u79cd\u5b50\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4f4e\u7ea7\u6280\u80fd\u4e4b\u95f4\u5173\u7cfb\u8f83\u5f31\uff0c\u800c\u9ad8\u7ea7\u6280\u80fd\u4e4b\u95f4\u5173\u8054\u6027\u5f3a\u3002</li>\n    <li>\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u6211\u4eec\u53d1\u73b0\u4f4e\u7ea7\u80fd\u529b\u4e0e\u9ad8\u7ea7\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5f3a\u5927\u7684\u8f6c\u79fb\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u201c\u81ea\u52a8\u601d\u8003\u201d\u7b56\u7565\u6765\u63d0\u5347\u6574\u4f53\u8868\u73b0\u3002</li>\n    <li>SpatialTree \u4e3a\u7406\u89e3\u548c\u7cfb\u7edf\u6027\u63d0\u5347 MLLMs \u7684\u7a7a\u95f4\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Spatial abilities develop in stages, from basic perception to complex reasoning and interaction.</li>\n    <li>The authors created a framework called SpatialTree that organizes these abilities into four levels: perception, mental mapping, simulation, and agentic competence.</li>\n    <li>The study evaluated popular multimodal LLMs across 27 different spatial abilities, revealing a clear structure in skill interdependencies.</li>\n    <li>They found that while lower-level skills are independent, higher-level skills are interconnected, and fine-tuning can enhance these abilities.</li>\n    <li>A new strategy called auto-think was proposed to improve performance across all ability levels by reducing unnecessary thinking.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:46.000Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19134", "authors": [{"_id": "694a1765335742716e9322b7", "name": "Dehai Min", "hidden": false}, {"_id": "694a1765335742716e9322b8", "name": "Kailin Zhang", "hidden": false}, {"_id": "694a1765335742716e9322b9", "name": "Tongtong Wu", "hidden": false}, {"_id": "694a1765335742716e9322ba", "name": "Lu Cheng", "hidden": false}], "publishedAt": "2025-12-22T08:28:05.000Z", "submittedOnDailyAt": "2025-12-23T01:46:57.477Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "submittedOnDailyBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "isPro": false, "fullname": "Dehai Min", "user": "ZhishanQ", "type": "user"}, "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "upvotes": 25, "discussionId": "694a1765335742716e9322bb", "githubRepo": "https://github.com/ZhishanQ/QuCo-RAG", "githubRepoAddedBy": "user", "ai_summary": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.", "ai_keywords": ["dynamic retrieval-augmented generation", "large language models", "hallucinations", "model-internal signals", "logits", "entropy", "pre-training data", "uncertainty quantification", "low-frequency entities", "entity co-occurrence", "Infini-gram", "multi-hop QA", "EM gains", "OLMo-2", "Llama", "Qwen", "GPT", "biomedical QA", "domain generalization", "corpus-grounded verification"], "githubStars": 6, "summary_zh": "<ul>\n    <li>QuCo-RAG\u662f\u4e00\u79cd\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u3002</li>\n    <li>\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cQuCo-RAG\u4f7f\u7528\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u8ba1\u7b97\u7684\u5ba2\u89c2\u7edf\u8ba1\u6570\u636e\uff0c\u800c\u975e\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff1a\u751f\u6210\u524d\u8bc6\u522b\u4f4e\u9891\u5b9e\u4f53\uff0c\u751f\u6210\u65f6\u9a8c\u8bc1\u5b9e\u4f53\u5728\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u5171\u73b0\u6027\u3002</li>\n    <li>\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQuCo-RAG\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u63d0\u5347\u4e865\u523012\u5206\uff0c\u751a\u81f3\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002</li>\n    <li>\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9886\u57df\u7684\u5e94\u7528\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u7a33\u5065\u6027\uff0c\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QuCo-RAG is a new method that helps large language models (LLMs) avoid making false statements (hallucinations) by deciding when to retrieve information.</li>\n    <li>Unlike previous methods that rely on unreliable internal signals from the model, QuCo-RAG uses objective data from its training to assess uncertainty.</li>\n    <li>The process involves two steps: identifying rare knowledge areas before generating responses and checking if certain entities appear together in training data during generation.</li>\n    <li>QuCo-RAG has shown significant improvements in question-answering tasks, achieving higher accuracy than existing methods with various models.</li>\n    <li>This approach is effective across different types of models and has been validated in specialized fields like biomedical questions.</li>\n</ul>"}, "publishedAt": "2025-12-22T03:28:05.000Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19134.png", "numComments": 2, "submittedBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "fullname": "Dehai Min", "name": "ZhishanQ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aDreaMontage\u7684\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u65e0\u7f1d\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u957f\u65f6\u95f4\u201c\u4e00\u955c\u5230\u5e95\u201d\u89c6\u9891\u3002</li>\n    <li>\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e2d\u4ecb\u6761\u4ef6\u673a\u5236\u548c\u81ea\u9002\u5e94\u8c03\u4f18\u7b56\u7565\uff0c\u589e\u5f3a\u4e86\u5bf9\u4efb\u610f\u5e27\u7684\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>\u521b\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5e76\u5b9e\u65bd\u89c6\u89c9\u8868\u73b0\u7ec6\u5316\u9636\u6bb5\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u7535\u5f71\u8868\u73b0\u529b\u3002</li>\n    <li>\u91c7\u7528\u5b9a\u5236\u7684DPO\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4e3b\u9898\u8fd0\u52a8\u5408\u7406\u6027\u548c\u8fc7\u6e21\u5e73\u6ed1\u6027\u7b49\u95ee\u9898\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u6bb5\u5f0f\u81ea\u56de\u5f52\u63a8\u7406\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u751f\u6210\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" filmmaking technique is unique but often expensive and complicated to achieve.</li>\n    <li>DreaMontage is a new framework that can create smooth and expressive long videos based on user inputs.</li>\n    <li>It uses a special conditioning method to allow for better control over the video frames.</li>\n    <li>The framework improves image quality and smooth transitions by using a carefully curated dataset and specific training methods.</li>\n    <li>It also has a memory-efficient strategy for creating longer video sequences, making it easier for users to create cohesive cinematic experiences.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86DeepSeek-V3.2\uff0c\u7ed3\u5408\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u51fa\u8272\u7684\u63a8\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u534f\u8bae\u548c\u540e\u671f\u8ba1\u7b97\u6269\u5c55\uff0cDeepSeek-V3.2\u7684\u6027\u80fd\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u5b9a\u7248\u672c\u8d85\u8d8a\u4e86GPT-5\u3002</li>\n    <li>\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u91d1\u724c\u6c34\u5e73\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e86\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that is both fast and effective in reasoning tasks.</li>\n    <li>It features a new attention mechanism called DeepSeek Sparse Attention (DSA) that makes it easier to handle long texts without losing performance.</li>\n    <li>The model uses a strong reinforcement learning approach, allowing it to perform as well as GPT-5, and even better with its high-compute version, DeepSeek-V3.2-Speciale.</li>\n    <li>DeepSeek-V3.2-Speciale has shown excellent reasoning skills, winning top awards in major math and informatics competitions.</li>\n    <li>It includes a new system for creating large amounts of training data, which helps improve how well the model follows instructions and generalizes in complex tasks.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u9700\u6c42\u7684\u5feb\u901f\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u7684\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u53d8\u5f97\u66f4\u52a0\u91cd\u8981\u3002</li>\n    <li>\u5f53\u524d\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u62bd\u8c61\uff0c\u96be\u4ee5\u4fdd\u8bc1\u53ef\u91cd\u590d\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7ba1\u9053\uff0c\u80fd\u591f\u5904\u7406\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u591a\u79cd\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7DataFlow\u751f\u6210\u7684\u6570\u636e\u96c6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current methods are messy and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create better data preparation pipelines with reusable and modular components.</li>\n    <li>It includes about 200 reusable tools and covers various areas like text, math, code, and knowledge extraction.</li>\n    <li>DataFlow-Agent can turn natural language instructions into working data pipelines, making it easier to use.</li>\n    <li>DataFlow consistently improves LLM performance, outperforming existing datasets in several benchmarks.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u8ba1\u7b97\u987a\u5e8f\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u9650\u5236\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u6846\u67b6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u4e8614\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6d41\u5a92\u4f53\u751f\u6210\u7684\u6548\u7387\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u6eda\u52a8\u6c47\u805a\u5e27\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u63d0\u9ad8\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u8272\u5f69\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u53f0H800 GPU\u4e0a\u8fbe\u523020\u5e27\u6bcf\u79d2\u7684\u751f\u6210\u901f\u5ea6\uff0c\u6807\u5fd7\u7740\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u65b0\u7a81\u7834\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video generation methods struggle with speed and consistency, making them hard to use for real-time avatar creation.</li>\n    <li>Live Avatar is a new system that allows for quick and high-quality avatar generation using a powerful 14-billion-parameter model.</li>\n    <li>The system uses a technique called Timestep-forcing Pipeline Parallelism to speed up processing by using multiple GPUs together.</li>\n    <li>To improve visual quality and consistency, it employs a method that adjusts the avatar's appearance based on a stored reference image.</li>\n    <li>Live Avatar achieves impressive performance, generating 20 frames per second on powerful GPUs, setting a new standard for real-time avatar generation.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u6a21\u62df\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u5305\u62ec\u8bbe\u8ba1\u548c\u6784\u5efaSQL\u7ba1\u9053\uff0c\u6210\u529f\u7387\u4f4e\u4e8e20%\uff0c\u663e\u793a\u51fa\u6574\u4f53\u7ba1\u9053\u534f\u8c03\u7684\u74f6\u9888\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u6027\u5546\u4e1a\u95ee\u9898\uff0c\u5e73\u5747\u5f97\u5206\u4f4e\u4e8e40%\uff0c\u8868\u660e\u5728\u5f00\u653e\u5f0f\u63a8\u7406\u4e0a\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u660e\u786e\u8bca\u65ad\u5de5\u7a0b\u548c\u5206\u6790\u80fd\u529b\u7684\u5c40\u9650\uff0c\u4e3a\u5f00\u53d1\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n    <li>\u76f8\u5173\u6570\u636e\u548c\u4ee3\u7801\u53ef\u5728https://da-comp.github.io\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark consisting of 210 tasks that reflect real-world data workflows in businesses.</li>\n    <li>It includes data engineering tasks that involve creating and improving SQL pipelines and data analysis tasks that solve open-ended business problems.</li>\n    <li>Tasks are evaluated based on execution performance and by a reliable judge using detailed rubrics.</li>\n    <li>Current advanced AI agents struggle with these tasks, especially in data engineering where success rates are below 20%.</li>\n    <li>DAComp helps identify challenges in developing effective autonomous data agents for business use.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u5206\u5f00\u7684\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u80cc\u666f\uff0c\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u3002</li>\n    <li>Kling-Omni\u5728\u751f\u6210\u3001\u63a8\u7406\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual and language inputs.</li>\n    <li>The framework combines video generation, editing, and reasoning tasks into one integrated system.</li>\n    <li>It can handle various user inputs such as text, images, and videos to produce cinematic-quality content.</li>\n    <li>Kling-Omni uses a strong data system and efficient training methods to enhance its video creation abilities.</li>\n    <li>It shows excellent performance in generating and editing videos based on user instructions and reasoning.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move \u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8d28\u91cf\u548c\u53ef\u7528\u6027\u3002</li>\n    <li>\u5b83\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u4f7f\u5f97\u573a\u666f\u63a7\u5236\u66f4\u52a0\u7ec6\u81f4\u3002</li>\n    <li>Wan-Move \u80fd\u591f\u751f\u6210 5 \u79d2\u3001480p \u7684\u89c6\u9891\uff0c\u5176\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0e\u5546\u4e1a\u8f6f\u4ef6\u76f8\u5f53\u3002</li>\n    <li>\u7814\u7a76\u8fd8\u8bbe\u8ba1\u4e86 MoveBench \u8bc4\u4f30\u57fa\u51c6\uff0c\u652f\u6301\u591a\u6837\u5316\u5185\u5bb9\u548c\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6ce8\u91ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over video motion, addressing issues with existing methods.</li>\n    <li>The framework uses dense point trajectories to guide how each scene element should move.</li>\n    <li>Wan-Move can be integrated into existing video models without changing their architecture.</li>\n    <li>It has been tested with a new benchmark, MoveBench, showing it produces high-quality motion in videos.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01816", "authors": [{"_id": "692e5c0537312eaa83fd87b8", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:43.760Z", "hidden": false}, {"_id": "692e5c0537312eaa83fd87b9", "name": "Siyuan Li", "hidden": false}, {"_id": "692e5c0537312eaa83fd87ba", "name": "Conghui He", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bb", "name": "Lijun Wu", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bc", "user": {"_id": "64be296a46cc3cdfbb057f7e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg", "isPro": false, "fullname": "Cheng Tan", "user": "chengtan9907", "type": "user"}, "name": "Cheng Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:41.755Z", "hidden": false}], "publishedAt": "2025-12-01T15:52:31.000Z", "submittedOnDailyAt": "2025-12-02T01:31:46.625Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "submittedOnDailyBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "upvotes": 87, "discussionId": "692e5c0537312eaa83fd87bd", "projectPage": "https://opendatalab-raiser.github.io/Envision/", "githubRepo": "https://github.com/opendatalab-raiser/Envision", "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.", "ai_keywords": ["multimodal models", "text-to-image (T2I)", "causal event progression", "spatiotemporal causality", "Envision-a", "Envision-Score", "multi-dimensional consistency", "physicality", "aesthetics", "causal narrative coherence", "spatiotemporal consistency", "multi-frame reasoning", "dynamic world modeling"], "githubStars": 27, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u65e8\u5728\u514b\u670d\u5355\u4e00\u6a21\u6001\u8868\u793a\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u4efb\u52a1\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u4f9d\u8d56\u9759\u6001\u5355\u56fe\u50cf\u751f\u6210\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u5e76\u9650\u5236\u4e86\u52a8\u6001\u8fc7\u7a0b\u5efa\u6a21\u7684\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cEnvision\u201d\u57fa\u51c6\uff0c\u7528\u4e8e\u94fe\u5f0f\u6587\u672c\u5230\u591a\u56fe\u50cf\u751f\u6210\uff0c\u5305\u542b1000\u4e2a\u8de8\u8d8a\u516d\u4e2a\u79d1\u5b66\u548c\u4eba\u6587\u5b66\u79d1\u7684\u56db\u9636\u6bb5\u63d0\u793a\u3002</li>\n    <li>\u5f15\u5165\u201cEnvision-Score\u201d\u6307\u6807\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e16\u754c\u77e5\u8bc6\u5e76\u9075\u5faa\u56e0\u679c\u65f6\u5e8f\u7ea6\u675f\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u4e13\u95e8\u7684T2I\u6a21\u578b\u5728\u7f8e\u5b66\u8868\u73b0\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5185\u5728\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u800c\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u4e8b\u8fde\u8d2f\u6027\u4e0a\u4f18\u4e8e\u4e13\u4e1a\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Current models for generating images from text struggle with understanding dynamic processes because they focus too much on single images.</li>\n  <li>The proposed Envision benchmark allows for generating sequences of images based on text prompts, helping to assess how well models understand cause and effect over time.</li>\n  <li>Envision includes 1,000 prompts across various subjects, aiming to improve evaluations of models by looking at sequences instead of single images.</li>\n  <li>Envision-Score is a new metric that measures how well models maintain consistency and adhere to real-world knowledge while generating images.</li>\n  <li>Results show that while specialized models excel at creating visually appealing images, unified models perform better in understanding and telling coherent stories, but all models still struggle with maintaining consistency over time.</li>\n</ul>"}, "publishedAt": "2025-12-01T10:52:31.000Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png", "numComments": 4, "submittedBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "fullname": "Juanxi Tian", "name": "Juanxi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13}, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u5728\u89c6\u89c9\u4e0a\u771f\u5b9e\uff0c\u4f46\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982FVD\uff09\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u63a8\u7406\u9519\u8bef\uff0c\u4f8b\u5982\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u6cd5\u5219\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faMMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u62bd\u8c61\u63a8\u7406\u3001\u4f53\u73b0\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u7b49\u4e09\u4e2a\u9886\u57df\u8bc4\u4f30\u751f\u6210\u63a8\u7406\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u5404\u9886\u57df\u7684\u6027\u80fd\u5dee\u8ddd\u5f88\u5927\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e00\u822c\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u8868\u73b0\u5dee\uff08ARC-AGI\u51c6\u786e\u7387\u4f4e\u4e8e10%\uff09\uff0c\u4e14\u5728\u957f\u65f6\u95f4\u7a7a\u95f4\u89c4\u5212\u4e2d\u9047\u5230\u56f0\u96be\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models create realistic videos but may not always follow real-world rules like physics and logic.</li>\n    <li>Current evaluation methods focus on visual quality and ignore reasoning mistakes, such as breaking causality and consistency.</li>\n    <li>The new MMGR framework assesses reasoning abilities in five areas: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR tests models in various tasks, revealing that they perform well in some areas but poorly in others, particularly Abstract Reasoning.</li>\n    <li>The analysis points out flaws in current models, like relying too much on visual data and not enough on logical consistency, suggesting MMGR can help improve future models.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.01374", "authors": [{"_id": "692e6bf937312eaa83fd8890", "user": {"_id": "610b70452719facd4ea85e28", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg", "isPro": false, "fullname": "Chujie Zheng", "user": "chujiezheng", "type": "user"}, "name": "Chujie Zheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:27.206Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8891", "name": "Kai Dang", "hidden": false}, {"_id": "692e6bf937312eaa83fd8892", "name": "Bowen Yu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8893", "name": "Mingze Li", "hidden": false}, {"_id": "692e6bf937312eaa83fd8894", "user": {"_id": "6278bd42541f3d2dfa77ea70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg", "isPro": false, "fullname": "Huiqiang Jiang", "user": "iofu728", "type": "user"}, "name": "Huiqiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:41.367Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8895", "name": "Junrong Lin", "hidden": false}, {"_id": "692e6bf937312eaa83fd8896", "name": "Yuqiong Liu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8897", "user": {"_id": "62088594a5943c8a8fc94560", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png", "isPro": false, "fullname": "An Yang", "user": "yangapku", "type": "user"}, "name": "An Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:25.208Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8898", "name": "Jingren Zhou", "hidden": false}, {"_id": "692e6bf937312eaa83fd8899", "name": "Junyang Lin", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "publishedAt": "2025-12-01T07:45:39.000Z", "submittedOnDailyAt": "2025-12-02T02:47:49.367Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "submittedOnDailyBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "isPro": false, "fullname": "Bowen Yu", "user": "Tigerph", "type": "user"}, "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "upvotes": 78, "discussionId": "692e6bfa37312eaa83fd889a", "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.", "ai_keywords": ["reinforcement learning", "large language models", "sequence-level reward", "token-level objective", "policy gradient methods", "REINFORCE", "first-order approximation", "training-inference discrepancy", "policy staleness", "importance sampling correction", "clipping", "Routing Replay", "Mixture-of-Experts", "on-policy training", "off-policy updates"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\uff0c\u6211\u4eec\u89e3\u91ca\u4e86\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\uff0c\u771f\u5b9e\u7684\u5e8f\u5217\u7ea7\u5956\u52b1\u53ef\u4ee5\u901a\u8fc7\u4ee3\u7528\u7684\u4ee4\u724c\u7ea7\u76ee\u6807\u6765\u4f18\u5316\u3002</li>\n    <li>\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u5f02\u548c\u7b56\u7565\u8fc7\u65f6\u6027\u8d8a\u5c0f\uff0c\u4ee3\u7528\u76ee\u6807\u7684\u6709\u6548\u6027\u5c31\u8d8a\u9ad8\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u4fee\u6b63\u7684\u57fa\u672c\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u80fd\u5b9e\u73b0\u6700\u9ad8\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u8bad\u7ec3\u7a33\u5b9a\u540e\uff0c\u6301\u7eed\u4f18\u5316\u80fd\u5728\u4e0d\u540c\u7684\u521d\u59cb\u5316\u60c5\u51b5\u4e0b\u83b7\u5f97\u76f8\u4f3c\u7684\u6700\u7ec8\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper discusses a new approach to improve reinforcement learning (RL) using large language models.</li>\n    <li>It explains how to optimize rewards by using a simpler token-level objective, especially when certain conditions are met.</li>\n    <li>The study highlights the importance of techniques like importance sampling correction and clipping for stable training.</li>\n    <li>Experiments with a large model show that using importance sampling leads to the best training stability during on-policy training.</li>\n    <li>When using off-policy methods, combining clipping and Routing Replay is crucial to reduce instability.</li>\n</ul>"}, "publishedAt": "2025-12-01T02:45:39.000Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png", "numComments": 2, "submittedBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "fullname": "Bowen Yu", "name": "Tigerph", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u6307\u7684\u662f\u5728\u79d1\u5b66\u9886\u57df\u81ea\u4e3b\u6784\u601d\u3001\u7814\u7a76\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\uff1a\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72\u5b9e\u9a8c\u548c\u6e7f\u5b9e\u9a8c\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u4e2d\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4e5f\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u4e3aAI\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which means AI lacks the ability to autonomously explore and reason in science.</li>\n    <li>We define SGI using the Practical Inquiry Model (PIM) and create tasks for AI that mimic scientific work, such as research and experiments.</li>\n    <li>SGI-Bench is a tool with over 1,000 curated examples to test AI performance on scientific questions.</li>\n    <li>Results show that AI struggles in deep research, idea feasibility, and accuracy in experiments, highlighting significant gaps in performance.</li>\n    <li>We propose Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate new hypotheses without needing a reference answer.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 28, 2025";
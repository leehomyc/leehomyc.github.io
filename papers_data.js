window.trendingPapers = {
    "today": [{"paper": {"id": "2602.20159", "authors": [{"_id": "699d1e7a4e37ec6dfa1bc5b7", "user": {"_id": "67f87529318a17cc80365190", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p1YkznAT-op1Cg-vBoFw7.png", "isPro": false, "fullname": "Maijunxian Wang", "user": "Mark7121983123", "type": "user"}, "name": "Maijunxian Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:19.409Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b8", "user": {"_id": "65b74305e602b6c2c9125480", "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg", "isPro": false, "fullname": "wang ruisi", "user": "wruisi", "type": "user"}, "name": "Ruisi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:01.779Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b9", "name": "Juyi Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ba", "name": "Ran Ji", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bb", "name": "Thadd\u00e4us Wiedemer", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bc", "name": "Qingying Gao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bd", "name": "Dezhi Luo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5be", "name": "Yaoyao Qian", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bf", "name": "Lianyu Huang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c0", "name": "Zelong Hong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c1", "name": "Jiahui Ge", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c2", "name": "Qianli Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c3", "name": "Hang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c4", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:04.030Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c5", "name": "Lingzi Guo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c6", "name": "Lantao Mei", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c7", "name": "Jiachen Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c8", "user": {"_id": "6532c3018bde2fae19578587", "avatarUrl": "/avatars/7231538f3d682a1e7b80e15ea91b2a97.svg", "isPro": false, "fullname": "Hanwen Xing", "user": "Hudx111", "type": "user"}, "name": "Hanwen Xing", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:09.132Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c9", "name": "Tianqi Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ca", "name": "Fengyuan Yu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cb", "name": "Weihang Xiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cc", "name": "Yizheng Jiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cd", "name": "Jianheng Hou", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ce", "name": "Danyang Zhang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cf", "user": {"_id": "65d854e5d8134b93774b4080", "avatarUrl": "/avatars/0ff1db8c13095f420a856212d64f88ca.svg", "isPro": false, "fullname": "Pengcheng Xu", "user": "explcre", "type": "user"}, "name": "Pengcheng Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:21.447Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d0", "name": "Boyang Zhong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d1", "name": "Zehong Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d2", "name": "Gaoyun Fang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d3", "name": "John Kitaoka", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d4", "name": "Yile Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d5", "name": "Hua Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d6", "name": "Kenton Blacutt", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d7", "name": "Tin Nguyen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d8", "name": "Siyuan Song", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d9", "name": "Haoran Sun", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5da", "name": "Shaoyue Wen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5db", "name": "Linyang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dc", "name": "Runming Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dd", "name": "Yanzhi Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5de", "name": "Mengyue Yang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5df", "name": "Ziqiao Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e0", "name": "Rapha\u00ebl Milli\u00e8re", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e1", "name": "Freda Shi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e2", "name": "Nuno Vasconcelos", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e3", "name": "Daniel Khashabi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e4", "name": "Alan Yuille", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e5", "name": "Yilun Du", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e6", "name": "Ziming Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e7", "name": "Bo Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e8", "name": "Dahua Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e9", "name": "Ziwei Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ea", "name": "Vikash Kumar", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5eb", "name": "Yijiang Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ec", "user": {"_id": "6626a471430a124253f197c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png", "isPro": false, "fullname": "yl-1993", "user": "yl-1993", "type": "user"}, "name": "Lei Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:17.400Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ed", "user": {"_id": "652d06833b5997ed71ce5c46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg", "isPro": false, "fullname": "Zhongang Cai", "user": "caizhongang", "type": "user"}, "name": "Zhongang Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:06.679Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ee", "user": {"_id": "6793f65033629a5fa8ae47b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/h11aIy3Yuw0kLCil5yeOt.png", "isPro": false, "fullname": "Hokin Deng", "user": "Hokin", "type": "user"}, "name": "Hokin Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:58.948Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "publishedAt": "2026-02-23T18:59:41.000Z", "submittedOnDailyAt": "2026-02-24T01:14:31.428Z", "title": "A Very Big Video Reasoning Suite", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "upvotes": 302, "discussionId": "699d1e7b4e37ec6dfa1bc5ef", "projectPage": "https://video-reason.com/", "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.", "ai_keywords": ["video reasoning", "spatiotemporal consistency", "emergent generalization", "video reasoning benchmark", "video reasoning dataset"], "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:41.000Z", "title": "A Very Big Video Reasoning Suite", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.18532", "authors": [{"_id": "699d29bf4e37ec6dfa1bc66d", "user": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "name": "Xiao-Ming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:14.717Z", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66e", "name": "Bin Fan", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66f", "name": "Kang Liao", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc670", "name": "Jian-Jian Jiang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc671", "name": "Runze Yang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc672", "name": "Yihang Luo", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc673", "name": "Zhonghua Wu", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc674", "name": "Wei-Shi Zheng", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc675", "name": "Chen Change Loy", "hidden": false}], "publishedAt": "2026-02-20T09:26:17.000Z", "submittedOnDailyAt": "2026-02-24T07:48:05.528Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "submittedOnDailyBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "upvotes": 39, "discussionId": "699d29c04e37ec6dfa1bc676", "projectPage": "https://dravenalg.github.io/VLANeXt/", "githubRepo": "https://github.com/DravenALG/VLANeXt", "githubRepoAddedBy": "user", "ai_summary": "Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization.", "ai_keywords": ["Vision-Language-Action models", "foundation models", "policy learning", "VLA design space", "RT-2", "OpenVLA", "perception essentials", "action modelling perspectives", "LIBERO", "LIBERO-plus", "VLANeXt"], "githubStars": 43, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-20T04:26:17.000Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18532.png", "numComments": 1, "submittedBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "fullname": "Xiao-Ming Wu", "name": "DravenALG", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19672", "authors": [{"_id": "699d407c4e37ec6dfa1bc6a7", "user": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "name": "Jiayu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:47.846Z", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a8", "name": "Yifei Ming", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a9", "name": "Zixuan Ke", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6aa", "name": "Shafiq Joty", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ab", "name": "Aws Albarghouthi", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ac", "name": "Frederic Sala", "hidden": false}], "publishedAt": "2026-02-23T10:17:25.000Z", "submittedOnDailyAt": "2026-02-24T03:44:00.879Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "submittedOnDailyBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "upvotes": 30, "discussionId": "699d407c4e37ec6dfa1bc6ad", "ai_summary": "SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods.", "ai_keywords": ["compound AI systems", "orchestration", "routing policy", "reinforcement learning", "skill modeling", "agent-specific competence", "performance-cost trade-off", "multi-turn scenarios", "routing collapse", "end-to-end learning"], "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T05:17:25.000Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19672.png", "numComments": 1, "submittedBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "fullname": "Jiayu (Mila) Wang", "name": "MilaWang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19313", "authors": [{"_id": "699d1f754e37ec6dfa1bc5ff", "name": "Shirui Chen", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc600", "name": "Cole Harrison", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc601", "name": "Ying-Chun Lee", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc602", "name": "Angela Jin Yang", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc603", "name": "Zhongzheng Ren", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc604", "name": "Lillian J. Ratliff", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc605", "name": "Jiafei Duan", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc606", "name": "Dieter Fox", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc607", "name": "Ranjay Krishna", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "publishedAt": "2026-02-22T19:25:48.000Z", "submittedOnDailyAt": "2026-02-24T01:19:53.486Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "submittedOnDailyBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "isPro": false, "fullname": "Duan", "user": "Jiafei1224", "type": "user"}, "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "upvotes": 21, "discussionId": "699d1f754e37ec6dfa1bc608", "projectPage": "https://topreward.github.io/webpage/", "githubRepo": "https://github.com/TOPReward/TOPReward", "githubRepoAddedBy": "user", "ai_summary": "TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.", "ai_keywords": ["Vision-Language-Action models", "Reinforcement Learning", "temporal value functions", "Vision-Language Models", "token logits", "Value-Order Correlation", "reward-aligned behavior cloning"], "githubStars": 1, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-22T14:25:48.000Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19313.png", "numComments": 1, "submittedBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "fullname": "Duan", "name": "Jiafei1224", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.20161", "authors": [{"_id": "699d14864e37ec6dfa1bc503", "name": "Abdelrahman Shaker", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc504", "user": {"_id": "656864e12d73834278a8dea7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg", "isPro": true, "fullname": "Ahmed Heakl", "user": "ahmedheakl", "type": "user"}, "name": "Ahmed Heakl", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:34.835Z", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc505", "name": "Jaseel Muhammad", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc506", "name": "Ritesh Thawkar", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc507", "name": "Omkar Thawakar", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc508", "name": "Senmao Li", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc509", "name": "Hisham Cholakkal", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50a", "name": "Ian Reid", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50b", "name": "Eric P. Xing", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50c", "name": "Salman Khan", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50d", "name": "Fahad Shahbaz Khan", "hidden": false}], "publishedAt": "2026-02-23T18:59:58.000Z", "submittedOnDailyAt": "2026-02-24T00:33:58.107Z", "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device", "submittedOnDailyBy": {"_id": "656864e12d73834278a8dea7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg", "isPro": true, "fullname": "Ahmed Heakl", "user": "ahmedheakl", "type": "user"}, "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/", "upvotes": 18, "discussionId": "699d14864e37ec6dfa1bc50e", "projectPage": "https://amshaker.github.io/Mobile-O/", "githubRepo": "https://github.com/Amshaker/Mobile-O", "githubRepoAddedBy": "user", "ai_summary": "A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology.", "ai_keywords": ["vision-language-diffusion model", "Mobile Conditioning Projector", "depthwise-separable convolutions", "layerwise alignment", "cross-modal conditioning", "diffusion generator", "unified multimodal intelligence", "real-time processing", "edge devices", "visual understanding", "visual generation"], "githubStars": 35, "organization": {"_id": "61fb9e24dc607a42af5f193f", "name": "MBZUAI", "fullname": "Mohamed Bin Zayed University of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:58.000Z", "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device", "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20161.png", "numComments": 2, "submittedBy": {"_id": "656864e12d73834278a8dea7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg", "fullname": "Ahmed Heakl", "name": "ahmedheakl", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 62, "isUserFollowing": false}, "organization": {"_id": "61fb9e24dc607a42af5f193f", "name": "MBZUAI", "fullname": "Mohamed Bin Zayed University of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.20093", "authors": [{"_id": "699d5e5a4e37ec6dfa1bc6ef", "name": "Kun Yang", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f0", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f1", "name": "Yazhe Chen", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f2", "name": "Siyao Zheng", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f3", "name": "Bangyang Hong", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f4", "name": "Kangle Wu", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f5", "name": "Yabo Ni", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f6", "name": "Anxiang Zeng", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f7", "name": "Cong Fu", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f8", "name": "Hui Li", "hidden": false}], "publishedAt": "2026-02-23T18:02:50.000Z", "submittedOnDailyAt": "2026-02-24T05:47:10.522Z", "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation", "submittedOnDailyBy": {"_id": "665e8515045fcbf12b99558a", "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg", "isPro": false, "fullname": "Fu Cong", "user": "fcthebrave", "type": "user"}, "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.", "upvotes": 18, "discussionId": "699d5e5a4e37ec6dfa1bc6f9", "githubRepo": "https://github.com/FuCongResearchSquad/ManCAR", "githubRepoAddedBy": "user", "ai_summary": "ManCAR is a recommendation framework that constrains latent reasoning within a collaborative manifold to prevent implausible trajectories and improve accuracy.", "ai_keywords": ["latent multi-step reasoning", "latent drift", "collaborative manifold", "global interaction graph", "local intent prior", "item simplex", "variational interpretation", "adaptive reasoning", "test-time computation", "NDCG@10"], "githubStars": 2, "organization": {"_id": "693bcc3aca53da32fcf95a61", "name": "PIIR", "fullname": "Personalized&Inclusive Intelligence Research"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:02:50.000Z", "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation", "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20093.png", "numComments": 1, "submittedBy": {"_id": "665e8515045fcbf12b99558a", "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg", "fullname": "Fu Cong", "name": "fcthebrave", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "693bcc3aca53da32fcf95a61", "name": "PIIR", "fullname": "Personalized&Inclusive Intelligence Research"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.20021", "authors": [{"_id": "699d1e434e37ec6dfa1bc58f", "name": "Natalie Shapira", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc590", "name": "Chris Wendler", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc591", "name": "Avery Yen", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc592", "name": "Gabriele Sarti", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc593", "name": "Koyena Pal", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc594", "name": "Olivia Floody", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc595", "name": "Adam Belfki", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc596", "name": "Alex Loftus", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc597", "name": "Aditya Ratan Jannali", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc598", "name": "Nikhil Prakash", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc599", "name": "Jasmine Cui", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc59a", "name": "Giordano Rogers", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc59b", "name": "Jannik Brinkmann", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc59c", "name": "Can Rager", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc59d", "name": "Amir Zur", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc59e", "name": "Michael Ripa", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc59f", "name": "Aruna Sankaranarayanan", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a0", "name": "David Atkinson", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a1", "name": "Rohit Gandikota", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a2", "name": "Jaden Fiotto-Kaufman", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a3", "name": "EunJeong Hwang", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a4", "name": "Hadas Orgad", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a5", "name": "P Sam Sahil", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a6", "name": "Negev Taglicht", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a7", "name": "Tomer Shabtay", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a8", "name": "Atai Ambus", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5a9", "name": "Nitay Alon", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5aa", "name": "Shiri Oron", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5ab", "name": "Ayelet Gordon-Tapiero", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5ac", "name": "Yotam Kaplan", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5ad", "name": "Vered Shwartz", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5ae", "name": "Tamar Rott Shaham", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5af", "name": "Christoph Riedl", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5b0", "name": "Reuth Mirsky", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5b1", "name": "Maarten Sap", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5b2", "name": "David Manheim", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5b3", "name": "Tomer Ullman", "hidden": false}, {"_id": "699d1e434e37ec6dfa1bc5b4", "name": "David Bau", "hidden": false}], "publishedAt": "2026-02-23T16:28:48.000Z", "submittedOnDailyAt": "2026-02-24T01:13:02.595Z", "title": "Agents of Chaos", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.", "upvotes": 14, "discussionId": "699d1e434e37ec6dfa1bc5b5", "projectPage": "https://agentsofchaos.baulab.info/", "ai_summary": "Autonomous language-model-powered agents in a live laboratory environment exhibited numerous security and governance vulnerabilities including unauthorized actions, information disclosure, and system takeovers during a two-week study with twenty researchers.", "ai_keywords": ["red-teaming study", "autonomous language-model-powered agents", "persistent memory", "tool use", "multi-party communication", "security vulnerabilities", "privacy vulnerabilities", "governance vulnerabilities", "accountability", "delegated authority", "responsibility"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T11:28:48.000Z", "title": "Agents of Chaos", "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20021.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.18996", "authors": [{"_id": "699d12104e37ec6dfa1bc4dd", "user": {"_id": "662a1628b3f55ae101e20ca4", "avatarUrl": "/avatars/9a56305a4b0909a429431c3b73e45efb.svg", "isPro": false, "fullname": "Shannan Yan", "user": "ysner", "type": "user"}, "name": "Shannan Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:55.457Z", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4de", "name": "Leqi Zheng", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4df", "name": "Keyu Lv", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4e0", "name": "Jingchen Ni", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4e1", "name": "Hongyang Wei", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4e2", "name": "Jiajun Zhang", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4e3", "name": "Guangting Wang", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4e4", "name": "Jing Lyu", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4e5", "name": "Chun Yuan", "hidden": false}, {"_id": "699d12104e37ec6dfa1bc4e6", "name": "Fengyun Rao", "hidden": false}], "publishedAt": "2026-02-22T00:53:03.000Z", "submittedOnDailyAt": "2026-02-24T07:26:40.827Z", "title": "Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction", "submittedOnDailyBy": {"_id": "662a1628b3f55ae101e20ca4", "avatarUrl": "/avatars/9a56305a4b0909a429431c3b73e45efb.svg", "isPro": false, "fullname": "Shannan Yan", "user": "ysner", "type": "user"}, "summary": "We study the task of establishing object-level visual correspondence across different viewpoints in videos, focusing on the challenging egocentric-to-exocentric and exocentric-to-egocentric scenarios. We propose a simple yet effective framework based on conditional binary segmentation, where an object query mask is encoded into a latent representation to guide the localization of the corresponding object in a target video. To encourage robust, view-invariant representations, we introduce a cycle-consistency training objective: the predicted mask in the target view is projected back to the source view to reconstruct the original query mask. This bidirectional constraint provides a strong self-supervisory signal without requiring ground-truth annotations and enables test-time training (TTT) at inference. Experiments on the Ego-Exo4D and HANDAL-X benchmarks demonstrate the effectiveness of our optimization objective and TTT strategy, achieving state-of-the-art performance. The code is available at https://github.com/shannany0606/CCMP.", "upvotes": 13, "discussionId": "699d12104e37ec6dfa1bc4e7", "githubRepo": "https://github.com/shannany0606/CCMP", "githubRepoAddedBy": "user", "ai_summary": "A conditional binary segmentation framework with cycle-consistency training enables robust object correspondence across egocentric and exocentric viewpoints without ground-truth annotations.", "ai_keywords": ["conditional binary segmentation", "view-invariant representations", "cycle-consistency training", "test-time training", "object correspondence", "visual correspondence"], "githubStars": 3, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-21T19:53:03.000Z", "title": "Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction", "summary": "We study the task of establishing object-level visual correspondence across different viewpoints in videos, focusing on the challenging egocentric-to-exocentric and exocentric-to-egocentric scenarios. We propose a simple yet effective framework based on conditional binary segmentation, where an object query mask is encoded into a latent representation to guide the localization of the corresponding object in a target video. To encourage robust, view-invariant representations, we introduce a cycle-consistency training objective: the predicted mask in the target view is projected back to the source view to reconstruct the original query mask. This bidirectional constraint provides a strong self-supervisory signal without requiring ground-truth annotations and enables test-time training (TTT) at inference. Experiments on the Ego-Exo4D and HANDAL-X benchmarks demonstrate the effectiveness of our optimization objective and TTT strategy, achieving state-of-the-art performance. The code is available at https://github.com/shannany0606/CCMP.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18996.png", "numComments": 1, "submittedBy": {"_id": "662a1628b3f55ae101e20ca4", "avatarUrl": "/avatars/9a56305a4b0909a429431c3b73e45efb.svg", "fullname": "Shannan Yan", "name": "ysner", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.16863", "authors": [{"_id": "699de940dfbcf0b800aec8b2", "name": "Kushal Kedia", "hidden": false}, {"_id": "699de940dfbcf0b800aec8b3", "name": "Tyler Ga Wei Lum", "hidden": false}, {"_id": "699de940dfbcf0b800aec8b4", "name": "Jeannette Bohg", "hidden": false}, {"_id": "699de940dfbcf0b800aec8b5", "name": "C. Karen Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/669093ca3a86663c1e4ae97c/YKjQijmepTsbqqIxmtyzi.mp4"], "publishedAt": "2026-02-18T20:42:39.000Z", "submittedOnDailyAt": "2026-02-24T15:56:08.875Z", "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation", "submittedOnDailyBy": {"_id": "669093ca3a86663c1e4ae97c", "avatarUrl": "/avatars/e3c514c6dbeae3df367c239b80616d0b.svg", "isPro": false, "fullname": "Tyler Lum", "user": "tylerlum", "type": "user"}, "summary": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.", "upvotes": 11, "discussionId": "699de941dfbcf0b800aec8b6", "projectPage": "https://simtoolreal.github.io/", "githubRepo": "https://github.com/tylerlum/simtoolreal", "githubRepoAddedBy": "user", "ai_summary": "SimToolReal enables generalizable robot manipulation of diverse tools through procedural simulation and universal reinforcement learning policies without task-specific training.", "ai_keywords": ["sim-to-real reinforcement learning", "tool manipulation", "dexterous manipulation", "procedural generation", "reinforcement learning policy", "zero-shot performance"], "githubStars": 16, "organization": {"_id": "672c672dcf09d152f4da04c4", "name": "StanfordUniversity", "fullname": "Stanford University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-18T15:42:39.000Z", "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation", "summary": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/669093ca3a86663c1e4ae97c/YKjQijmepTsbqqIxmtyzi.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16863.png", "numComments": 1, "submittedBy": {"_id": "669093ca3a86663c1e4ae97c", "avatarUrl": "/avatars/e3c514c6dbeae3df367c239b80616d0b.svg", "fullname": "Tyler Lum", "name": "tylerlum", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "672c672dcf09d152f4da04c4", "name": "StanfordUniversity", "fullname": "Stanford University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.19895", "authors": [{"_id": "699d1f054e37ec6dfa1bc5f1", "name": "Zhongwei Wan", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f2", "name": "Yun Shen", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f3", "name": "Zhihao Dou", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f4", "name": "Donghao Zhou", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f5", "name": "Yu Zhang", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f6", "name": "Xin Wang", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f7", "name": "Hui Shen", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f8", "name": "Jing Xiong", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5f9", "name": "Chaofan Tao", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5fa", "user": {"_id": "694524196bedf03d6b02cf22", "avatarUrl": "/avatars/733247ed969843cda789e710240477f1.svg", "isPro": false, "fullname": "Zixuan ZHONG", "user": "Zixuan8642", "type": "user"}, "name": "Zixuan Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:11.851Z", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5fb", "name": "Peizhou Huang", "hidden": false}, {"_id": "699d1f054e37ec6dfa1bc5fc", "name": "Mi Zhang", "hidden": false}], "publishedAt": "2026-02-23T14:37:01.000Z", "submittedOnDailyAt": "2026-02-24T01:16:59.044Z", "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning", "submittedOnDailyBy": {"_id": "640feb91b0ee289c8583dd59", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640feb91b0ee289c8583dd59/CA4UZj2mEsJO2UR0NnN5H.jpeg", "isPro": false, "fullname": "Hui Shen", "user": "Cloudriver", "type": "user"}, "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.", "upvotes": 10, "discussionId": "699d1f054e37ec6dfa1bc5fd", "ai_summary": "DSDR is a reinforcement learning framework that enhances large language model reasoning by promoting diversity at both global and local levels through dual-scale regularization techniques.", "ai_keywords": ["reinforcement learning", "large language models", "reasoning", "policy optimization", "entropy regularization", "diversity regularization", "global diversity", "local diversity", "token-level entropy", "trajectory diversity", "group-based optimization", "pass@k"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T09:37:01.000Z", "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning", "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19895.png", "numComments": 1, "submittedBy": {"_id": "640feb91b0ee289c8583dd59", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640feb91b0ee289c8583dd59/CA4UZj2mEsJO2UR0NnN5H.jpeg", "fullname": "Hui Shen", "name": "Cloudriver", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.20159", "authors": [{"_id": "699d1e7a4e37ec6dfa1bc5b7", "user": {"_id": "67f87529318a17cc80365190", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p1YkznAT-op1Cg-vBoFw7.png", "isPro": false, "fullname": "Maijunxian Wang", "user": "Mark7121983123", "type": "user"}, "name": "Maijunxian Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:19.409Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b8", "user": {"_id": "65b74305e602b6c2c9125480", "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg", "isPro": false, "fullname": "wang ruisi", "user": "wruisi", "type": "user"}, "name": "Ruisi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:01.779Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b9", "name": "Juyi Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ba", "name": "Ran Ji", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bb", "name": "Thadd\u00e4us Wiedemer", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bc", "name": "Qingying Gao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bd", "name": "Dezhi Luo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5be", "name": "Yaoyao Qian", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bf", "name": "Lianyu Huang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c0", "name": "Zelong Hong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c1", "name": "Jiahui Ge", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c2", "name": "Qianli Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c3", "name": "Hang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c4", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:04.030Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c5", "name": "Lingzi Guo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c6", "name": "Lantao Mei", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c7", "name": "Jiachen Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c8", "user": {"_id": "6532c3018bde2fae19578587", "avatarUrl": "/avatars/7231538f3d682a1e7b80e15ea91b2a97.svg", "isPro": false, "fullname": "Hanwen Xing", "user": "Hudx111", "type": "user"}, "name": "Hanwen Xing", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:09.132Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c9", "name": "Tianqi Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ca", "name": "Fengyuan Yu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cb", "name": "Weihang Xiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cc", "name": "Yizheng Jiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cd", "name": "Jianheng Hou", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ce", "name": "Danyang Zhang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cf", "user": {"_id": "65d854e5d8134b93774b4080", "avatarUrl": "/avatars/0ff1db8c13095f420a856212d64f88ca.svg", "isPro": false, "fullname": "Pengcheng Xu", "user": "explcre", "type": "user"}, "name": "Pengcheng Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:21.447Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d0", "name": "Boyang Zhong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d1", "name": "Zehong Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d2", "name": "Gaoyun Fang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d3", "name": "John Kitaoka", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d4", "name": "Yile Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d5", "name": "Hua Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d6", "name": "Kenton Blacutt", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d7", "name": "Tin Nguyen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d8", "name": "Siyuan Song", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d9", "name": "Haoran Sun", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5da", "name": "Shaoyue Wen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5db", "name": "Linyang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dc", "name": "Runming Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dd", "name": "Yanzhi Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5de", "name": "Mengyue Yang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5df", "name": "Ziqiao Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e0", "name": "Rapha\u00ebl Milli\u00e8re", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e1", "name": "Freda Shi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e2", "name": "Nuno Vasconcelos", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e3", "name": "Daniel Khashabi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e4", "name": "Alan Yuille", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e5", "name": "Yilun Du", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e6", "name": "Ziming Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e7", "name": "Bo Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e8", "name": "Dahua Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e9", "name": "Ziwei Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ea", "name": "Vikash Kumar", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5eb", "name": "Yijiang Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ec", "user": {"_id": "6626a471430a124253f197c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png", "isPro": false, "fullname": "yl-1993", "user": "yl-1993", "type": "user"}, "name": "Lei Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:17.400Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ed", "user": {"_id": "652d06833b5997ed71ce5c46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg", "isPro": false, "fullname": "Zhongang Cai", "user": "caizhongang", "type": "user"}, "name": "Zhongang Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:06.679Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ee", "user": {"_id": "6793f65033629a5fa8ae47b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/h11aIy3Yuw0kLCil5yeOt.png", "isPro": false, "fullname": "Hokin Deng", "user": "Hokin", "type": "user"}, "name": "Hokin Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:58.948Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "publishedAt": "2026-02-23T18:59:41.000Z", "submittedOnDailyAt": "2026-02-24T01:14:31.428Z", "title": "A Very Big Video Reasoning Suite", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "upvotes": 302, "discussionId": "699d1e7b4e37ec6dfa1bc5ef", "projectPage": "https://video-reason.com/", "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.", "ai_keywords": ["video reasoning", "spatiotemporal consistency", "emergent generalization", "video reasoning benchmark", "video reasoning dataset"], "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:41.000Z", "title": "A Very Big Video Reasoning Suite", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.18532", "authors": [{"_id": "699d29bf4e37ec6dfa1bc66d", "user": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "name": "Xiao-Ming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:14.717Z", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66e", "name": "Bin Fan", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66f", "name": "Kang Liao", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc670", "name": "Jian-Jian Jiang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc671", "name": "Runze Yang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc672", "name": "Yihang Luo", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc673", "name": "Zhonghua Wu", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc674", "name": "Wei-Shi Zheng", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc675", "name": "Chen Change Loy", "hidden": false}], "publishedAt": "2026-02-20T09:26:17.000Z", "submittedOnDailyAt": "2026-02-24T07:48:05.528Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "submittedOnDailyBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "upvotes": 39, "discussionId": "699d29c04e37ec6dfa1bc676", "projectPage": "https://dravenalg.github.io/VLANeXt/", "githubRepo": "https://github.com/DravenALG/VLANeXt", "githubRepoAddedBy": "user", "ai_summary": "Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization.", "ai_keywords": ["Vision-Language-Action models", "foundation models", "policy learning", "VLA design space", "RT-2", "OpenVLA", "perception essentials", "action modelling perspectives", "LIBERO", "LIBERO-plus", "VLANeXt"], "githubStars": 43, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-20T04:26:17.000Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18532.png", "numComments": 1, "submittedBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "fullname": "Xiao-Ming Wu", "name": "DravenALG", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19672", "authors": [{"_id": "699d407c4e37ec6dfa1bc6a7", "user": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "name": "Jiayu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:47.846Z", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a8", "name": "Yifei Ming", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a9", "name": "Zixuan Ke", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6aa", "name": "Shafiq Joty", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ab", "name": "Aws Albarghouthi", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ac", "name": "Frederic Sala", "hidden": false}], "publishedAt": "2026-02-23T10:17:25.000Z", "submittedOnDailyAt": "2026-02-24T03:44:00.879Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "submittedOnDailyBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "upvotes": 30, "discussionId": "699d407c4e37ec6dfa1bc6ad", "ai_summary": "SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods.", "ai_keywords": ["compound AI systems", "orchestration", "routing policy", "reinforcement learning", "skill modeling", "agent-specific competence", "performance-cost trade-off", "multi-turn scenarios", "routing collapse", "end-to-end learning"], "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T05:17:25.000Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19672.png", "numComments": 1, "submittedBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "fullname": "Jiayu (Mila) Wang", "name": "MilaWang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.16705", "authors": [{"_id": "69967a291268a6b79e0d02bb", "user": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "isPro": false, "fullname": "Runpei Dong", "user": "RunpeiDong", "type": "user"}, "name": "Runpei Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:52.994Z", "hidden": false}, {"_id": "69967a291268a6b79e0d02bc", "name": "Ziyan Li", "hidden": false}, {"_id": "69967a291268a6b79e0d02bd", "name": "Xialin He", "hidden": false}, {"_id": "69967a291268a6b79e0d02be", "name": "Saurabh Gupta", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"], "publishedAt": "2026-02-18T18:55:02.000Z", "submittedOnDailyAt": "2026-02-19T00:21:31.776Z", "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation", "submittedOnDailyBy": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "isPro": false, "fullname": "Runpei Dong", "user": "RunpeiDong", "type": "user"}, "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.", "upvotes": 25, "discussionId": "69967a2a1268a6b79e0d02bf", "projectPage": "https://hero-humanoid.github.io/", "ai_summary": "HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.", "ai_keywords": ["end-effector tracking policy", "inverse kinematics", "neural forward model", "goal adjustment", "replanning", "open-vocabulary large vision models", "loco-manipulation", "visual generalization"], "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-18T13:55:02.000Z", "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation", "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16705.png", "numComments": 2, "submittedBy": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "fullname": "Runpei Dong", "name": "RunpeiDong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.17270", "authors": [{"_id": "6997ce527a658569d5a10165", "name": "Jonathan Heek", "hidden": false}, {"_id": "6997ce527a658569d5a10166", "user": {"_id": "671feaa3e269c38584cb065f", "avatarUrl": "/avatars/d7bc35ab98b208ef1d009aa9b629682a.svg", "isPro": false, "fullname": "Emiel Hoogeboom", "user": "Emiel2", "type": "user"}, "name": "Emiel Hoogeboom", "status": "admin_assigned", "statusLastChangedAt": "2026-02-20T09:03:14.323Z", "hidden": false}, {"_id": "6997ce527a658569d5a10167", "user": {"_id": "642ad5bc508b7246f9d27ab9", "avatarUrl": "/avatars/6cdf6f855536896575733e273388d7fb.svg", "isPro": false, "fullname": "Thomas Mensink", "user": "tmensink", "type": "user"}, "name": "Thomas Mensink", "status": "admin_assigned", "statusLastChangedAt": "2026-02-20T09:03:21.125Z", "hidden": false}, {"_id": "6997ce527a658569d5a10168", "name": "Tim Salimans", "hidden": false}], "publishedAt": "2026-02-19T11:18:12.000Z", "submittedOnDailyAt": "2026-02-20T00:30:40.692Z", "title": "Unified Latents (UL): How to train your latents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.", "upvotes": 21, "discussionId": "6997ce527a658569d5a10169", "ai_summary": "Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.", "ai_keywords": ["diffusion prior", "diffusion model", "latent representations", "training objective", "latent bitrate", "FID", "PSNR", "FLOPs", "FVD"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-19T06:18:12.000Z", "title": "Unified Latents (UL): How to train your latents", "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17270.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 235, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.19313", "authors": [{"_id": "699d1f754e37ec6dfa1bc5ff", "name": "Shirui Chen", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc600", "name": "Cole Harrison", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc601", "name": "Ying-Chun Lee", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc602", "name": "Angela Jin Yang", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc603", "name": "Zhongzheng Ren", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc604", "name": "Lillian J. Ratliff", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc605", "name": "Jiafei Duan", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc606", "name": "Dieter Fox", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc607", "name": "Ranjay Krishna", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "publishedAt": "2026-02-22T19:25:48.000Z", "submittedOnDailyAt": "2026-02-24T01:19:53.486Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "submittedOnDailyBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "isPro": false, "fullname": "Duan", "user": "Jiafei1224", "type": "user"}, "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "upvotes": 21, "discussionId": "699d1f754e37ec6dfa1bc608", "projectPage": "https://topreward.github.io/webpage/", "githubRepo": "https://github.com/TOPReward/TOPReward", "githubRepoAddedBy": "user", "ai_summary": "TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.", "ai_keywords": ["Vision-Language-Action models", "Reinforcement Learning", "temporal value functions", "Vision-Language Models", "token logits", "Value-Order Correlation", "reward-aligned behavior cloning"], "githubStars": 1, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-22T14:25:48.000Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19313.png", "numComments": 1, "submittedBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "fullname": "Duan", "name": "Jiafei1224", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.16317", "authors": [{"_id": "6996e36c2b6c6794ff97a40f", "name": "Maksim Elistratov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a410", "name": "Marina Barannikov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a411", "name": "Gregory Ivanov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a412", "name": "Valentin Khrulkov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a413", "name": "Anton Konushin", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a414", "name": "Andrey Kuznetsov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a415", "user": {"_id": "67d5a331eab66ce9cb01bae4", "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg", "isPro": false, "fullname": "DMITRII ZHEMCHUZHNIKOV", "user": "zhemchuzhnikov", "type": "user"}, "name": "Dmitrii Zhemchuzhnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T11:48:52.062Z", "hidden": false}], "publishedAt": "2026-02-18T09:54:57.000Z", "submittedOnDailyAt": "2026-02-19T11:41:42.311Z", "title": "CADEvolve: Creating Realistic CAD via Program Evolution", "submittedOnDailyBy": {"_id": "67d5a331eab66ce9cb01bae4", "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg", "isPro": false, "fullname": "DMITRII ZHEMCHUZHNIKOV", "user": "zhemchuzhnikov", "type": "user"}, "summary": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.", "upvotes": 19, "discussionId": "6996e36d2b6c6794ff97a416", "githubRepo": "https://github.com/zhemdi/CADEvolve", "githubRepoAddedBy": "user", "ai_summary": "CADEvolve presents an evolution-based approach using VLM-guided edits to generate complex CAD programs from simple primitives, creating a large dataset for improved Image2CAD performance.", "ai_keywords": ["CAD", "VLM", "evolution-based pipeline", "parametric generators", "CadQuery", "Image2CAD", "DeepCAD", "Fusion 360", "MCB"], "githubStars": 7, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-18T04:54:57.000Z", "title": "CADEvolve: Creating Realistic CAD via Program Evolution", "summary": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16317.png", "numComments": 2, "submittedBy": {"_id": "67d5a331eab66ce9cb01bae4", "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg", "fullname": "DMITRII ZHEMCHUZHNIKOV", "name": "zhemchuzhnikov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.20161", "authors": [{"_id": "699d14864e37ec6dfa1bc503", "name": "Abdelrahman Shaker", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc504", "user": {"_id": "656864e12d73834278a8dea7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg", "isPro": true, "fullname": "Ahmed Heakl", "user": "ahmedheakl", "type": "user"}, "name": "Ahmed Heakl", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:34.835Z", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc505", "name": "Jaseel Muhammad", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc506", "name": "Ritesh Thawkar", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc507", "name": "Omkar Thawakar", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc508", "name": "Senmao Li", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc509", "name": "Hisham Cholakkal", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50a", "name": "Ian Reid", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50b", "name": "Eric P. Xing", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50c", "name": "Salman Khan", "hidden": false}, {"_id": "699d14864e37ec6dfa1bc50d", "name": "Fahad Shahbaz Khan", "hidden": false}], "publishedAt": "2026-02-23T18:59:58.000Z", "submittedOnDailyAt": "2026-02-24T00:33:58.107Z", "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device", "submittedOnDailyBy": {"_id": "656864e12d73834278a8dea7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg", "isPro": true, "fullname": "Ahmed Heakl", "user": "ahmedheakl", "type": "user"}, "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/", "upvotes": 18, "discussionId": "699d14864e37ec6dfa1bc50e", "projectPage": "https://amshaker.github.io/Mobile-O/", "githubRepo": "https://github.com/Amshaker/Mobile-O", "githubRepoAddedBy": "user", "ai_summary": "A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology.", "ai_keywords": ["vision-language-diffusion model", "Mobile Conditioning Projector", "depthwise-separable convolutions", "layerwise alignment", "cross-modal conditioning", "diffusion generator", "unified multimodal intelligence", "real-time processing", "edge devices", "visual understanding", "visual generation"], "githubStars": 35, "organization": {"_id": "61fb9e24dc607a42af5f193f", "name": "MBZUAI", "fullname": "Mohamed Bin Zayed University of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:58.000Z", "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device", "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20161.png", "numComments": 2, "submittedBy": {"_id": "656864e12d73834278a8dea7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg", "fullname": "Ahmed Heakl", "name": "ahmedheakl", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 62, "isUserFollowing": false}, "organization": {"_id": "61fb9e24dc607a42af5f193f", "name": "MBZUAI", "fullname": "Mohamed Bin Zayed University of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.20093", "authors": [{"_id": "699d5e5a4e37ec6dfa1bc6ef", "name": "Kun Yang", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f0", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f1", "name": "Yazhe Chen", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f2", "name": "Siyao Zheng", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f3", "name": "Bangyang Hong", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f4", "name": "Kangle Wu", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f5", "name": "Yabo Ni", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f6", "name": "Anxiang Zeng", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f7", "name": "Cong Fu", "hidden": false}, {"_id": "699d5e5a4e37ec6dfa1bc6f8", "name": "Hui Li", "hidden": false}], "publishedAt": "2026-02-23T18:02:50.000Z", "submittedOnDailyAt": "2026-02-24T05:47:10.522Z", "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation", "submittedOnDailyBy": {"_id": "665e8515045fcbf12b99558a", "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg", "isPro": false, "fullname": "Fu Cong", "user": "fcthebrave", "type": "user"}, "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.", "upvotes": 18, "discussionId": "699d5e5a4e37ec6dfa1bc6f9", "githubRepo": "https://github.com/FuCongResearchSquad/ManCAR", "githubRepoAddedBy": "user", "ai_summary": "ManCAR is a recommendation framework that constrains latent reasoning within a collaborative manifold to prevent implausible trajectories and improve accuracy.", "ai_keywords": ["latent multi-step reasoning", "latent drift", "collaborative manifold", "global interaction graph", "local intent prior", "item simplex", "variational interpretation", "adaptive reasoning", "test-time computation", "NDCG@10"], "githubStars": 2, "organization": {"_id": "693bcc3aca53da32fcf95a61", "name": "PIIR", "fullname": "Personalized&Inclusive Intelligence Research"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:02:50.000Z", "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation", "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20093.png", "numComments": 1, "submittedBy": {"_id": "665e8515045fcbf12b99558a", "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg", "fullname": "Fu Cong", "name": "fcthebrave", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "693bcc3aca53da32fcf95a61", "name": "PIIR", "fullname": "Personalized&Inclusive Intelligence Research"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.18422", "authors": [{"_id": "699bbf5bf723198bd53a6309", "name": "Linxi Xie", "hidden": false}, {"_id": "699bbf5bf723198bd53a630a", "name": "Lisong C. Sun", "hidden": false}, {"_id": "699bbf5bf723198bd53a630b", "name": "Ashley Neall", "hidden": false}, {"_id": "699bbf5bf723198bd53a630c", "name": "Tong Wu", "hidden": false}, {"_id": "699bbf5bf723198bd53a630d", "name": "Shengqu Cai", "hidden": false}, {"_id": "699bbf5bf723198bd53a630e", "name": "Gordon Wetzstein", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/6_AD51fcx9Fvk7_TGemIO.mp4"], "publishedAt": "2026-02-20T18:45:29.000Z", "submittedOnDailyAt": "2026-02-23T00:17:53.289Z", "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.", "upvotes": 17, "discussionId": "699bbf5cf723198bd53a630f", "projectPage": "https://codeysun.github.io/generated-reality/", "ai_summary": "A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.", "ai_keywords": ["video world models", "diffusion transformer", "3D head pose", "joint-level hand poses", "dexterous hand-object interactions", "bidirectional video diffusion model", "causal interactive system", "egocentric virtual environments"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-20T13:45:29.000Z", "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control", "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/6_AD51fcx9Fvk7_TGemIO.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18422.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 237, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.20159", "authors": [{"_id": "699d1e7a4e37ec6dfa1bc5b7", "user": {"_id": "67f87529318a17cc80365190", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p1YkznAT-op1Cg-vBoFw7.png", "isPro": false, "fullname": "Maijunxian Wang", "user": "Mark7121983123", "type": "user"}, "name": "Maijunxian Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:19.409Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b8", "user": {"_id": "65b74305e602b6c2c9125480", "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg", "isPro": false, "fullname": "wang ruisi", "user": "wruisi", "type": "user"}, "name": "Ruisi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:01.779Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b9", "name": "Juyi Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ba", "name": "Ran Ji", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bb", "name": "Thadd\u00e4us Wiedemer", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bc", "name": "Qingying Gao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bd", "name": "Dezhi Luo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5be", "name": "Yaoyao Qian", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bf", "name": "Lianyu Huang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c0", "name": "Zelong Hong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c1", "name": "Jiahui Ge", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c2", "name": "Qianli Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c3", "name": "Hang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c4", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:04.030Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c5", "name": "Lingzi Guo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c6", "name": "Lantao Mei", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c7", "name": "Jiachen Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c8", "user": {"_id": "6532c3018bde2fae19578587", "avatarUrl": "/avatars/7231538f3d682a1e7b80e15ea91b2a97.svg", "isPro": false, "fullname": "Hanwen Xing", "user": "Hudx111", "type": "user"}, "name": "Hanwen Xing", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:09.132Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c9", "name": "Tianqi Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ca", "name": "Fengyuan Yu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cb", "name": "Weihang Xiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cc", "name": "Yizheng Jiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cd", "name": "Jianheng Hou", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ce", "name": "Danyang Zhang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cf", "user": {"_id": "65d854e5d8134b93774b4080", "avatarUrl": "/avatars/0ff1db8c13095f420a856212d64f88ca.svg", "isPro": false, "fullname": "Pengcheng Xu", "user": "explcre", "type": "user"}, "name": "Pengcheng Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:21.447Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d0", "name": "Boyang Zhong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d1", "name": "Zehong Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d2", "name": "Gaoyun Fang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d3", "name": "John Kitaoka", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d4", "name": "Yile Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d5", "name": "Hua Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d6", "name": "Kenton Blacutt", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d7", "name": "Tin Nguyen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d8", "name": "Siyuan Song", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d9", "name": "Haoran Sun", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5da", "name": "Shaoyue Wen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5db", "name": "Linyang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dc", "name": "Runming Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dd", "name": "Yanzhi Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5de", "name": "Mengyue Yang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5df", "name": "Ziqiao Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e0", "name": "Rapha\u00ebl Milli\u00e8re", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e1", "name": "Freda Shi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e2", "name": "Nuno Vasconcelos", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e3", "name": "Daniel Khashabi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e4", "name": "Alan Yuille", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e5", "name": "Yilun Du", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e6", "name": "Ziming Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e7", "name": "Bo Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e8", "name": "Dahua Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e9", "name": "Ziwei Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ea", "name": "Vikash Kumar", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5eb", "name": "Yijiang Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ec", "user": {"_id": "6626a471430a124253f197c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png", "isPro": false, "fullname": "yl-1993", "user": "yl-1993", "type": "user"}, "name": "Lei Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:17.400Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ed", "user": {"_id": "652d06833b5997ed71ce5c46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg", "isPro": false, "fullname": "Zhongang Cai", "user": "caizhongang", "type": "user"}, "name": "Zhongang Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:06.679Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ee", "user": {"_id": "6793f65033629a5fa8ae47b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/h11aIy3Yuw0kLCil5yeOt.png", "isPro": false, "fullname": "Hokin Deng", "user": "Hokin", "type": "user"}, "name": "Hokin Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:58.948Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "publishedAt": "2026-02-23T18:59:41.000Z", "submittedOnDailyAt": "2026-02-24T01:14:31.428Z", "title": "A Very Big Video Reasoning Suite", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "upvotes": 302, "discussionId": "699d1e7b4e37ec6dfa1bc5ef", "projectPage": "https://video-reason.com/", "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.", "ai_keywords": ["video reasoning", "spatiotemporal consistency", "emergent generalization", "video reasoning benchmark", "video reasoning dataset"], "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:41.000Z", "title": "A Very Big Video Reasoning Suite", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10388", "authors": [{"_id": "698d3bd265c0d15a6d16200e", "user": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "name": "Zhongzhi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:57:05.580Z", "hidden": false}, {"_id": "698d3bd265c0d15a6d16200f", "name": "Xuansheng Wu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162010", "name": "Yijiang Li", "hidden": false}, {"_id": "698d3bd265c0d15a6d162011", "name": "Lijie Hu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162012", "name": "Ninghao Liu", "hidden": false}], "publishedAt": "2026-02-11T00:23:13.000Z", "submittedOnDailyAt": "2026-02-16T02:31:34.708Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "submittedOnDailyBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "upvotes": 200, "discussionId": "698d3bd265c0d15a6d162013", "projectPage": "https://website-sigma-three-35.vercel.app/", "githubRepo": "https://github.com/Zhongzhi660/FAC-Synthesis", "githubRepoAddedBy": "user", "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.", "ai_keywords": ["Feature Activation Coverage", "sparse autoencoder", "data diversity", "downstream performance", "instruction following", "toxicity detection", "reward modeling", "behavior steering", "cross-model knowledge transfer", "data-centric optimization"], "githubStars": 52, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T19:23:13.000Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10388.png", "numComments": 2, "submittedBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "fullname": "ZhongzhiLi", "name": "Zhongzhi1228", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09877", "authors": [{"_id": "698c7abdeb12ea7453916869", "user": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "name": "Chenxu Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-12T16:49:45.534Z", "hidden": false}, {"_id": "698c7abdeb12ea745391686a", "name": "Chaozhuo Li", "hidden": false}, {"_id": "698c7abdeb12ea745391686b", "name": "Songyang Liu", "hidden": false}, {"_id": "698c7abdeb12ea745391686c", "name": "Zejian Chen", "hidden": false}, {"_id": "698c7abdeb12ea745391686d", "name": "Jinyu Hou", "hidden": false}, {"_id": "698c7abdeb12ea745391686e", "name": "Ji Qi", "hidden": false}, {"_id": "698c7abdeb12ea745391686f", "name": "Rui Li", "hidden": false}, {"_id": "698c7abdeb12ea7453916870", "name": "Litian Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916871", "name": "Qiwei Ye", "hidden": false}, {"_id": "698c7abdeb12ea7453916872", "name": "Zheng Liu", "hidden": false}, {"_id": "698c7abdeb12ea7453916873", "name": "Xu Chen", "hidden": false}, {"_id": "698c7abdeb12ea7453916874", "name": "Xi Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916875", "name": "Philip S. Yu", "hidden": false}], "publishedAt": "2026-02-10T15:18:19.000Z", "submittedOnDailyAt": "2026-02-13T00:53:30.377Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "submittedOnDailyBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "upvotes": 169, "discussionId": "698c7abdeb12ea7453916876", "ai_summary": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.", "ai_keywords": ["multi-agent systems", "large language models", "self-evolution", "safety alignment", "information-theoretic framework", "anthropic value distributions", "statistical blind spots", "self-evolving AI societies", "external oversight", "safety-preserving mechanisms"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T10:18:19.000Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png", "numComments": 2, "submittedBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "fullname": "Chenxu Wang", "name": "xunyoyo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10693", "authors": [{"_id": "6992047b50fb2c0be47837f0", "user": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "isPro": false, "fullname": "floyed shen", "user": "floyed", "type": "user"}, "name": "Guobin Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:52:51.206Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f1", "user": {"_id": "63fc5b724c57549ad5e54558", "avatarUrl": "/avatars/1374c1e8969533dd7543959666f16d1a.svg", "isPro": false, "fullname": "Chenxiao Zhao", "user": "ChenShawn", "type": "user"}, "name": "Chenxiao Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-17T17:17:12.583Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f2", "user": {"_id": "655c43d6b426ec8f4b5e7652", "avatarUrl": "/avatars/ddcf9d1ef0e2dc1f564a56ba9153f24f.svg", "isPro": false, "fullname": "Xiang Cheng", "user": "FFFc2", "type": "user"}, "name": "Xiang Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:52:57.697Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f3", "user": {"_id": "61f4c2e981c4d30f58140279", "avatarUrl": "/avatars/c4a69f6563c952354e33682e86045b14.svg", "isPro": false, "fullname": "HuangMeow", "user": "Luckyyy", "type": "user"}, "name": "Lei Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-23T09:46:50.954Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f4", "name": "Xing Yu", "hidden": false}], "publishedAt": "2026-02-11T09:48:08.000Z", "submittedOnDailyAt": "2026-02-23T03:29:14.259Z", "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training", "submittedOnDailyBy": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "isPro": false, "fullname": "floyed shen", "user": "floyed", "type": "user"}, "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO", "upvotes": 158, "discussionId": "6992047c50fb2c0be47837f5", "githubRepo": "https://github.com/FloyedShen/VESPO", "githubRepoAddedBy": "user", "ai_summary": "VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.", "ai_keywords": ["reinforcement learning", "large language models", "policy staleness", "asynchronous training", "importance sampling", "variance reduction", "variational formulation", "proposal distributions", "sequence-level importance weights", "mathematical reasoning benchmarks", "Mixture-of-Experts models"], "githubStars": 14, "organization": {"_id": "68246a0a98117c02df67a547", "name": "rednote-hilab", "fullname": "rednote-hilab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-11T04:48:08.000Z", "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training", "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10693.png", "numComments": 2, "submittedBy": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "fullname": "floyed shen", "name": "floyed", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "68246a0a98117c02df67a547", "name": "rednote-hilab", "fullname": "rednote-hilab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10604", "authors": [{"_id": "698d417065c0d15a6d162026", "name": "Ailin Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162027", "name": "Ang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162028", "name": "Aobo Kong", "hidden": false}, {"_id": "698d417065c0d15a6d162029", "name": "Bin Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202a", "name": "Binxing Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d16202b", "name": "Bo Dong", "hidden": false}, {"_id": "698d417065c0d15a6d16202c", "name": "Bojun Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202d", "name": "Boyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16202e", "name": "Brian Li", "hidden": false}, {"_id": "698d417065c0d15a6d16202f", "name": "Buyun Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162030", "name": "Chang Su", "hidden": false}, {"_id": "698d417065c0d15a6d162031", "name": "Changxin Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162032", "name": "Changyi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162033", "name": "Chao Lou", "hidden": false}, {"_id": "698d417065c0d15a6d162034", "name": "Chen Hu", "hidden": false}, {"_id": "698d417065c0d15a6d162035", "name": "Chen Xu", "hidden": false}, {"_id": "698d417065c0d15a6d162036", "name": "Chenfeng Yu", "hidden": false}, {"_id": "698d417065c0d15a6d162037", "name": "Chengting Feng", "hidden": false}, {"_id": "698d417065c0d15a6d162038", "name": "Chengyuan Yao", "hidden": false}, {"_id": "698d417065c0d15a6d162039", "name": "Chunrui Han", "hidden": false}, {"_id": "698d417065c0d15a6d16203a", "name": "Dan Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203b", "name": "Dapeng Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16203c", "name": "Daxin Jiang", "hidden": false}, {"_id": "698d417065c0d15a6d16203d", "name": "Dehua Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203e", "name": "Deshan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16203f", "name": "Di Qi", "hidden": false}, {"_id": "698d417065c0d15a6d162040", "name": "Enle Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162041", "name": "Fajie Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162042", "name": "Fanqi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162043", "name": "Guanzhe Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162044", "name": "Gulin Yan", "hidden": false}, {"_id": "698d417065c0d15a6d162045", "name": "Guoliang Cao", "hidden": false}, {"_id": "698d417065c0d15a6d162046", "name": "Guopeng Li", "hidden": false}, {"_id": "698d417065c0d15a6d162047", "name": "Han Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162048", "name": "Hangyu Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162049", "user": {"_id": "64b7874b9f5987572ca28461", "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg", "isPro": false, "fullname": "hanshanzhang", "user": "brain-zhang", "type": "user"}, "name": "Hanshan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:52.602Z", "hidden": false}, {"_id": "698d417065c0d15a6d16204a", "name": "Hao Nie", "hidden": false}, {"_id": "698d417065c0d15a6d16204b", "name": "Haonan Jia", "hidden": false}, {"_id": "698d417065c0d15a6d16204c", "name": "Haoran Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204d", "name": "Hebin Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16204e", "name": "Hekun Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204f", "name": "Heng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162050", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "698d417065c0d15a6d162051", "name": "Hongbo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162052", "name": "Hongbo Peng", "hidden": false}, {"_id": "698d417065c0d15a6d162053", "name": "Hongyu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162054", "name": "Hongyuan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162055", "name": "Houyong Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162056", "name": "Huangxi Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d162057", "name": "Huimin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d162058", "name": "Huiyong Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162059", "name": "Jia Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16205a", "name": "Jian Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16205b", "name": "Jianjian Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16205c", "name": "Jiaoren Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16205d", "name": "Jiaran Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16205e", "name": "Jiashu Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16205f", "name": "Jiashuo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162060", "name": "Jiayi Fu", "hidden": false}, {"_id": "698d417065c0d15a6d162061", "name": "Jiayu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162062", "name": "Jie Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162063", "name": "Jie Luo", "hidden": false}, {"_id": "698d417065c0d15a6d162064", "name": "Jie Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162065", "name": "Jie Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162066", "name": "Jieyi Hou", "hidden": false}, {"_id": "698d417065c0d15a6d162067", "name": "Jing Bai", "hidden": false}, {"_id": "698d417065c0d15a6d162068", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:37.335Z", "hidden": false}, {"_id": "698d417065c0d15a6d162069", "name": "Jingjing Xie", "hidden": false}, {"_id": "698d417065c0d15a6d16206a", "name": "Jingwei Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16206b", "name": "Jingyang Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16206c", "name": "Jishi Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16206d", "name": "Junfeng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16206e", "name": "Junzhe Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16206f", "name": "Ka Man Lo", "hidden": false}, {"_id": "698d417065c0d15a6d162070", "name": "Kai Liang", "hidden": false}, {"_id": "698d417065c0d15a6d162071", "name": "Kaibo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162072", "name": "Kaijun Tan", "hidden": false}, {"_id": "698d417065c0d15a6d162073", "user": {"_id": "66668c591964b6188ee310c2", "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg", "isPro": false, "fullname": "Kaiwen Yan", "user": "linrany", "type": "user"}, "name": "Kaiwen Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:58.524Z", "hidden": false}, {"_id": "698d417065c0d15a6d162074", "name": "Kaixiang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162075", "name": "Kang An", "hidden": false}, {"_id": "698d417065c0d15a6d162076", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:56.339Z", "hidden": false}, {"_id": "698d417065c0d15a6d162077", "name": "Lei Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162078", "name": "Liang Lv", "hidden": false}, {"_id": "698d417065c0d15a6d162079", "name": "Liang Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16207a", "name": "Liangyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207b", "name": "Lieyu Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16207c", "name": "Liguo Tan", "hidden": false}, {"_id": "698d417065c0d15a6d16207d", "name": "Lin Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16207e", "name": "Lina Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207f", "name": "Luck Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162080", "name": "Mengqiang Ren", "hidden": false}, {"_id": "698d417065c0d15a6d162081", "name": "Michael Li", "hidden": false}, {"_id": "698d417065c0d15a6d162082", "name": "Ming Li", "hidden": false}, {"_id": "698d417065c0d15a6d162083", "name": "Mingliang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162084", "name": "Mingming Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162085", "name": "Mingrui Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162086", "name": "Mitt Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162087", "name": "Na Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162088", "name": "Peng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162089", "name": "Qi Han", "hidden": false}, {"_id": "698d417065c0d15a6d16208a", "name": "Qian Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16208b", "name": "Qinglin He", "hidden": false}, {"_id": "698d417065c0d15a6d16208c", "name": "Qinxin Du", "hidden": false}, {"_id": "698d417065c0d15a6d16208d", "name": "Qiuping Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16208e", "name": "Quan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16208f", "name": "Rongqiu Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162090", "name": "Ruihang Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162091", "name": "Ruixin Han", "hidden": false}, {"_id": "698d417065c0d15a6d162092", "name": "Ruosi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162093", "name": "Ruyan Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162094", "name": "Shan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162095", "name": "Shaoliang Pang", "hidden": false}, {"_id": "698d417065c0d15a6d162096", "name": "Shaowen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162097", "name": "Shengjie Fan", "hidden": false}, {"_id": "698d417065c0d15a6d162098", "name": "Shijie Shang", "hidden": false}, {"_id": "698d417065c0d15a6d162099", "name": "Shiliang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d16209a", "name": "Shiwei Li", "hidden": false}, {"_id": "698d417065c0d15a6d16209b", "name": "Shuangshuang Tian", "hidden": false}, {"_id": "698d417065c0d15a6d16209c", "name": "Siqi Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16209d", "name": "Siye Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16209e", "name": "Siyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16209f", "name": "Song Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a0", "name": "Tiancheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620a1", "name": "Tianchi Yue", "hidden": false}, {"_id": "698d417065c0d15a6d1620a2", "name": "Tianhao Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d1620a3", "name": "Tianning Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620a4", "name": "Tingdan Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620a5", "name": "Wang You", "hidden": false}, {"_id": "698d417065c0d15a6d1620a6", "name": "Wei Ji", "hidden": false}, {"_id": "698d417065c0d15a6d1620a7", "name": "Wei Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a8", "name": "Wei Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620a9", "name": "Weibo Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620aa", "user": {"_id": "6657620ea496f7fcb67c3871", "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg", "isPro": false, "fullname": "xieweihao", "user": "chalengr", "type": "user"}, "name": "Weihao Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:48.216Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ab", "name": "Wen Sun", "hidden": false}, {"_id": "698d417065c0d15a6d1620ac", "name": "Wenjin Deng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ad", "user": {"_id": "650c04795510464e85b47470", "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg", "isPro": false, "fullname": "wen", "user": "zhengwenzhen", "type": "user"}, "name": "Wenzhen Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:45.930Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ae", "name": "Wuxun Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620af", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b0", "name": "Xiangwen Kong", "hidden": false}, {"_id": "698d417065c0d15a6d1620b1", "name": "Xiangyu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b2", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b3", "name": "Xiaobo Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b4", "name": "Xiaojia Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b5", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620b6", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d1620b7", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620b8", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b9", "name": "Xin Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ba", "name": "Xin Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bb", "name": "Xin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bc", "name": "Xing Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620bd", "name": "Xingping Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620be", "name": "Xinran Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620bf", "name": "Xu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620c0", "user": {"_id": "64ec5b64bfb2aa06a46ff2d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg", "isPro": false, "fullname": "xuan he", "user": "tpa115k31", "type": "user"}, "name": "Xuan He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:36.240Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620c1", "name": "Xuanti Feng", "hidden": false}, {"_id": "698d417065c0d15a6d1620c2", "name": "Xuedan Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c3", "name": "Xuqiang Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620c4", "name": "Yanbo Yu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c5", "name": "Yang Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620c6", "name": "Yang Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c7", "name": "Yanlin Lai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c8", "name": "Yanming Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c9", "name": "Yaoyu Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ca", "name": "Yeqing Shen", "hidden": false}, {"_id": "698d417065c0d15a6d1620cb", "name": "Yibo Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d1620cc", "name": "Yichen Lv", "hidden": false}, {"_id": "698d417065c0d15a6d1620cd", "name": "Yicheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620ce", "name": "Yifeng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620cf", "name": "Yijing Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d0", "name": "Yikun Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d1", "name": "Yin Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d2", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d3", "name": "Yinmin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d4", "name": "Yitong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d5", "name": "Yixuan Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d6", "name": "Yiyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620d7", "name": "Yongchi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d8", "name": "Yongshen Long", "hidden": false}, {"_id": "698d417065c0d15a6d1620d9", "name": "Yongyao Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620da", "name": "Yousong Guan", "hidden": false}, {"_id": "698d417065c0d15a6d1620db", "name": "Yu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620dc", "name": "Yuang Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620dd", "name": "Yuanhao Ding", "hidden": false}, {"_id": "698d417065c0d15a6d1620de", "name": "Yuantao Fan", "hidden": false}, {"_id": "698d417065c0d15a6d1620df", "name": "Yuanzhen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e0", "name": "Yuchu Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620e1", "name": "Yudi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e2", "name": "Yue Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620e3", "name": "Yueqiang Lin", "hidden": false}, {"_id": "698d417065c0d15a6d1620e4", "name": "Yufan Lu", "hidden": false}, {"_id": "698d417065c0d15a6d1620e5", "name": "Yuling Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e6", "name": "Yunzhou Ju", "hidden": false}, {"_id": "698d417065c0d15a6d1620e7", "name": "Yurong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e8", "name": "Yusheng Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620e9", "name": "Yuxiang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ea", "name": "Yuyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620eb", "name": "Yuzhu Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620ec", "name": "Zejia Weng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ed", "name": "Zetao Hong", "hidden": false}, {"_id": "698d417065c0d15a6d1620ee", "name": "Zexi Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ef", "name": "Zhe Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620f0", "name": "Zheng Ge", "hidden": false}, {"_id": "698d417065c0d15a6d1620f1", "name": "Zheng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620f2", "name": "Zheng Zeng", "hidden": false}, {"_id": "698d417065c0d15a6d1620f3", "user": {"_id": "63607ace9ddc44e710e13f0f", "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg", "isPro": false, "fullname": "zy", "user": "lu-vae", "type": "user"}, "name": "Zhenyi Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:50.532Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620f4", "name": "Zhewei Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f5", "name": "Zhichao Chang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f6", "name": "Zhiguo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f7", "name": "Zhiheng Hu", "hidden": false}, {"_id": "698d417065c0d15a6d1620f8", "name": "Zidong Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f9", "name": "Zili Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fa", "name": "Ziqi Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620fb", "name": "Zixin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fc", "name": "Zixuan Wang", "hidden": false}], "publishedAt": "2026-02-11T07:53:51.000Z", "submittedOnDailyAt": "2026-02-12T00:26:49.880Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "upvotes": 150, "discussionId": "698d417165c0d15a6d1620fd", "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash", "githubRepoAddedBy": "user", "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.", "ai_keywords": ["Mixture-of-Experts", "sparse MoE", "foundation model", "active parameters", "interleaved attention", "sliding-window attention", "full attention", "Multi-Token Prediction", "reinforcement learning", "verifiable signals", "preference feedback", "off-policy training", "self-improvement", "IMO-AnswerBench", "LiveCodeBench", "tau2-Bench", "BrowseComp", "Terminal-Bench"], "githubStars": 1245, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-11T02:53:51.000Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 231, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Feb 25, 2026";
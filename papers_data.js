window.trendingPapers = {
    "today": [{"paper": {"id": "2602.06717", "authors": [{"_id": "69898989beecc443208d2741", "name": "Daniil Plyusov", "hidden": false}, {"_id": "69898989beecc443208d2742", "user": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "name": "Alexey Gorbatovski", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:53.815Z", "hidden": false}, {"_id": "69898989beecc443208d2743", "name": "Boris Shaposhnikov", "hidden": false}, {"_id": "69898989beecc443208d2744", "name": "Viacheslav Sinii", "hidden": false}, {"_id": "69898989beecc443208d2745", "user": {"_id": "636e71b2b0ebc04888157b71", "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg", "isPro": false, "fullname": "Alexey Malakhov", "user": "ZeL1k7", "type": "user"}, "name": "Alexey Malakhov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:06:45.653Z", "hidden": false}, {"_id": "69898989beecc443208d2746", "user": {"_id": "62a9c8edc19f92ae443ab37f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9c8edc19f92ae443ab37f/yczqpBOntLco_2Jn4hnT7.jpeg", "isPro": false, "fullname": "Daniil Gavrilov", "user": "kefirski", "type": "user"}, "name": "Daniil Gavrilov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:57.059Z", "hidden": false}], "publishedAt": "2026-02-06T14:07:30.000Z", "submittedOnDailyAt": "2026-02-09T04:48:51.744Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "submittedOnDailyBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "upvotes": 54, "discussionId": "69898989beecc443208d2747", "ai_summary": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.", "ai_keywords": ["reinforcement learning", "verifiable rewards", "group sampling", "advantage estimation", "policy updates", "Focal loss", "GRPO", "DAPO", "CISPO", "pass@k metrics"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u901a\u5e38\u4f9d\u8d56\u4e8e\u7fa4\u4f53\u62bd\u6837\u6765\u4f30\u8ba1\u4f18\u52bf\u548c\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u3002</li>\n    <li>\u5927\u89c4\u6a21\u7fa4\u4f53\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\uff0c\u5bfc\u81f4\u5b66\u4e60\u504f\u5411\u4e8e\u5df2\u77e5\u7684\u8f68\u8ff9\u3002</li>\n    <li>\u5c0f\u89c4\u6a21\u7fa4\u4f53\u53ef\u80fd\u6f0f\u6389\u7a00\u6709\u7684\u6b63\u786e\u8f68\u8ff9\uff0c\u96c6\u4e2d\u6982\u7387\u4e8e\u5e38\u89c1\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96be\u5ea6\u611f\u77e5\u7684\u4f18\u52bf\u7f29\u653e\u7cfb\u6570\uff0c\u65e8\u5728\u964d\u4f4e\u9ad8\u6210\u529f\u7387\u63d0\u793a\u7684\u66f4\u65b0\u6743\u91cd\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u7684RLVR\u7b97\u6cd5\u4e2d\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u4e0d\u53d8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) often uses group sampling to improve learning but faces challenges with group size limitations.</li>\n    <li>Large groups are hard to manage due to computational limits, while small groups can miss important, rare successful outcomes.</li>\n    <li>The study reveals how the probability of missing rare successful paths depends on group size and that updates can unintentionally reduce focus on these paths.</li>\n    <li>A new method is introduced that adjusts how updates are made, favoring less successful prompts to improve learning efficiency.</li>\n    <li>This method shows significant performance improvements on various benchmarks without increasing computational demands or group sizes.</li>\n</ul>"}, "publishedAt": "2026-02-06T09:07:30.000Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06717.png", "numComments": 1, "submittedBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "fullname": "Alexey Gorbatovski", "name": "Myashka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06570", "authors": [{"_id": "69895518beecc443208d2680", "name": "Baichuan-M3 Team", "hidden": false}, {"_id": "69895518beecc443208d2682", "name": "Chengfeng Dou", "hidden": false}, {"_id": "69895518beecc443208d2683", "user": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "isPro": false, "fullname": "FanYang", "user": "fairyang", "type": "user"}, "name": "Fan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T14:32:20.075Z", "hidden": false}, {"_id": "69895518beecc443208d2684", "user": {"_id": "6464dd5234acce85aea186c7", "avatarUrl": "/avatars/3428029b0ae5f12885092c7aea588065.svg", "isPro": false, "fullname": "lifei", "user": "lifei926926", "type": "user"}, "name": "Fei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:07:14.055Z", "hidden": false}, {"_id": "69895518beecc443208d2685", "name": "Jiyuan Jia", "hidden": false}, {"_id": "69895518beecc443208d2686", "name": "Qiang Ju", "hidden": false}, {"_id": "69895518beecc443208d2687", "name": "Shuai Wang", "hidden": false}, {"_id": "69895518beecc443208d2688", "name": "Tianpeng Li", "hidden": false}, {"_id": "69895518beecc443208d2689", "name": "Xiangrong Zeng", "hidden": false}, {"_id": "69895518beecc443208d268a", "name": "Yijie Zhou", "hidden": false}, {"_id": "69895518beecc443208d268b", "name": "Hongda Zhang", "hidden": false}, {"_id": "69895518beecc443208d268c", "name": "Jinyang Tai", "hidden": false}, {"_id": "69895518beecc443208d268d", "name": "Linzhuang Sun", "hidden": false}, {"_id": "69895518beecc443208d268e", "user": {"_id": "6487e2e1eec01aee99cf4c10", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e2e1eec01aee99cf4c10/1U6zZ2OaUOrR1ueD5yraR.jpeg", "isPro": false, "fullname": "Peidong Guo", "user": "GuoPD", "type": "user"}, "name": "Peidong Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:36:44.605Z", "hidden": false}, {"_id": "69895518beecc443208d268f", "name": "Yichuan Mo", "hidden": false}, {"_id": "69895518beecc443208d2690", "name": "Xiaochuan Wang", "hidden": false}, {"_id": "69895518beecc443208d2691", "name": "Hengfu Cui", "hidden": false}, {"_id": "69895518beecc443208d2692", "name": "Zhishou Zhang", "hidden": false}], "publishedAt": "2026-02-06T10:08:59.000Z", "submittedOnDailyAt": "2026-02-09T03:00:35.501Z", "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making", "submittedOnDailyBy": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "isPro": false, "fullname": "FanYang", "user": "fairyang", "type": "user"}, "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.", "upvotes": 54, "discussionId": "69895518beecc443208d2693", "githubRepo": "https://github.com/baichuan-inc/Baichuan-M3-235B", "githubRepoAddedBy": "user", "ai_summary": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.", "ai_keywords": ["large language model", "clinical decision support", "proactive information acquisition", "long-horizon reasoning", "hallucination suppression", "HealthBench", "HealthBench-Hallu", "ScanBench"], "githubStars": 190, "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86Baichuan-M3\uff0c\u4e00\u4e2a\u533b\u5b66\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u4f9b\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002</li>\n    <li>\u8be5\u6a21\u578b\u514b\u670d\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u5f00\u653e\u5f0f\u54a8\u8be2\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6a21\u62df\u533b\u751f\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u4e3b\u8981\u529f\u80fd\u5305\u62ec\u4e3b\u52a8\u83b7\u53d6\u4fe1\u606f\u3001\u957f\u65f6\u95f4\u63a8\u7406\u6574\u5408\u8bc1\u636e\u4ee5\u53ca\u9002\u5e94\u6027\u6291\u5236\u5e7b\u89c9\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cBaichuan-M3\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86GPT-5.2\u3002</li>\n    <li>\u6a21\u578b\u53ef\u5728https://huggingface.co/collections/baichuan-inc/baichuan-m3\u4e0a\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Baichuan-M3 is a new medical-focused language model designed to improve clinical decision-making.</li>\n    <li>It addresses issues in current systems by actively helping doctors during open-ended consultations.</li>\n    <li>The model can gather information, connect different pieces of evidence, and reduce incorrect information.</li>\n    <li>Tests show Baichuan-M3 performs better than GPT-5.2 in medical questions and advice.</li>\n    <li>The model is available for public use online.</li>\n</ul>"}, "publishedAt": "2026-02-06T05:08:59.000Z", "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making", "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06570.png", "numComments": 2, "submittedBy": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "fullname": "FanYang", "name": "fairyang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.05843", "authors": [{"_id": "698567834ad556f294b7ec03", "user": {"_id": "64e6cf78ecce34cb442dc889", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg", "isPro": false, "fullname": "Fangzhi Xu", "user": "xufangzhi", "type": "user"}, "name": "Fangzhi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:51:15.682Z", "hidden": false}, {"_id": "698567834ad556f294b7ec04", "name": "Hang Yan", "hidden": false}, {"_id": "698567834ad556f294b7ec05", "user": {"_id": "6064a0eeb1703ddba0d458b9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png", "isPro": false, "fullname": "Qiushi", "user": "QiushiSun", "type": "user"}, "name": "Qiushi Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:17.132Z", "hidden": false}, {"_id": "698567834ad556f294b7ec06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:19.089Z", "hidden": false}, {"_id": "698567834ad556f294b7ec07", "name": "Zixian Huang", "hidden": false}, {"_id": "698567834ad556f294b7ec08", "user": {"_id": "6628859f1a5c7e6b445868c1", "avatarUrl": "/avatars/a7684d2bd0fd60824c5e810356953243.svg", "isPro": false, "fullname": "Muye Huang", "user": "MuyeHuang", "type": "user"}, "name": "Muye Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:25.705Z", "hidden": false}, {"_id": "698567834ad556f294b7ec09", "name": "Jingyang Gong", "hidden": false}, {"_id": "698567834ad556f294b7ec0a", "user": {"_id": "642b9861bb77f8456634b048", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg", "isPro": false, "fullname": "Zichen Ding", "user": "heroding77", "type": "user"}, "name": "Zichen Ding", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:15.144Z", "hidden": false}, {"_id": "698567834ad556f294b7ec0b", "name": "Kanzhi Cheng", "hidden": false}, {"_id": "698567834ad556f294b7ec0c", "name": "Yian Wang", "hidden": false}, {"_id": "698567834ad556f294b7ec0d", "name": "Xinyu Che", "hidden": false}, {"_id": "698567834ad556f294b7ec0e", "name": "Zeyi Sun", "hidden": false}, {"_id": "698567834ad556f294b7ec0f", "user": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "name": "Jian Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:51:08.819Z", "hidden": false}, {"_id": "698567834ad556f294b7ec10", "name": "Zhangyue Yin", "hidden": false}, {"_id": "698567834ad556f294b7ec11", "name": "Haoran Luo", "hidden": false}, {"_id": "698567834ad556f294b7ec12", "name": "Xuanjing Huang", "hidden": false}, {"_id": "698567834ad556f294b7ec13", "name": "Ben Kao", "hidden": false}, {"_id": "698567834ad556f294b7ec14", "name": "Jun Liu", "hidden": false}, {"_id": "698567834ad556f294b7ec15", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:23.699Z", "hidden": false}], "publishedAt": "2026-02-05T16:31:43.000Z", "submittedOnDailyAt": "2026-02-09T03:14:11.152Z", "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "submittedOnDailyBy": {"_id": "64e6cf78ecce34cb442dc889", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg", "isPro": false, "fullname": "Fangzhi Xu", "user": "xufangzhi", "type": "user"}, "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", "upvotes": 51, "discussionId": "698567834ad556f294b7ec16", "projectPage": "https://yayayacc.github.io/Odyssey-Home/", "githubRepo": "https://github.com/xufangzhi/Odyssey-Arena", "githubRepoAddedBy": "user", "ai_summary": "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.", "ai_keywords": ["Large Language Models", "autonomous agents", "inductive reasoning", "long-horizon planning", "agent evaluation", "transition laws", "OdysseyArena", "OdysseyArena-Lite", "OdysseyArena-Challenge"], "githubStars": 25, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u4fc3\u8fdb\u4e86\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u51fa\u73b0\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6f14\u7ece\u65b9\u6cd5\uff0c\u667a\u80fd\u4f53\u6839\u636e\u660e\u786e\u7684\u89c4\u5219\u548c\u9759\u6001\u76ee\u6807\u6267\u884c\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u81ea\u4e3b\u53d1\u73b0\u6f5c\u5728\u8f6c\u53d8\u6cd5\u5219\u7684\u91cd\u8981\u6027\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86OdysseyArena\uff0c\u91cd\u70b9\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u957f\u65f6\u95f4\u3001\u4e3b\u52a8\u548c\u5f52\u7eb3\u4e92\u52a8\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86OdysseyArena-Lite\uff0c\u63d0\u4f9b120\u4e2a\u4efb\u52a1\u6765\u6d4b\u91cf\u667a\u80fd\u4f53\u7684\u5f52\u7eb3\u6548\u7387\u548c\u957f\u65f6\u95f4\u53d1\u73b0\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5f52\u7eb3\u573a\u666f\u4e2d\u4e5f\u5b58\u5728\u7f3a\u9677\uff0c\u63ed\u793a\u51fa\u81ea\u4e3b\u53d1\u73b0\u7684\u5173\u952e\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are advancing quickly and enabling autonomous agents to operate in complex environments.</li>\n    <li>Current evaluations focus on agents following set rules and goals, which limits their ability to learn and adapt from experience.</li>\n    <li>OdysseyArena is introduced to improve how agents are evaluated by emphasizing long-term interactions and learning from experiences.</li>\n    <li>OdysseyArena-Lite offers 120 tasks to measure how well agents can learn and adapt over time.</li>\n    <li>Tests on over 15 top LLMs show that even the best models struggle with learning in dynamic situations, highlighting a major challenge for future development.</li>\n</ul>"}, "publishedAt": "2026-02-05T11:31:43.000Z", "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05843.png", "numComments": 2, "submittedBy": {"_id": "64e6cf78ecce34cb442dc889", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg", "fullname": "Fangzhi Xu", "name": "xufangzhi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 18, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.05027", "authors": [{"_id": "69871a872d626112378ad69f", "user": {"_id": "660fd34df03515e4ff3f2b64", "avatarUrl": "/avatars/0c2a29b1081ece881234acdd8ef9371a.svg", "isPro": false, "fullname": "Georgii Aparin", "user": "Egorgij21", "type": "user"}, "name": "Georgii Aparin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:34:22.635Z", "hidden": false}, {"_id": "69871a872d626112378ad6a0", "name": "Tasnima Sadekova", "hidden": false}, {"_id": "69871a872d626112378ad6a1", "name": "Alexey Rukhovich", "hidden": false}, {"_id": "69871a872d626112378ad6a2", "name": "Assel Yermekova", "hidden": false}, {"_id": "69871a872d626112378ad6a3", "name": "Laida Kushnareva", "hidden": false}, {"_id": "69871a872d626112378ad6a4", "name": "Vadim Popov", "hidden": false}, {"_id": "69871a872d626112378ad6a5", "name": "Kristian Kuznetsov", "hidden": false}, {"_id": "69871a872d626112378ad6a6", "name": "Irina Piontkovskaya", "hidden": false}], "publishedAt": "2026-02-04T20:29:16.000Z", "submittedOnDailyAt": "2026-02-09T05:28:19.089Z", "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders", "submittedOnDailyBy": {"_id": "636254dc2691058b19d9276a", "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg", "isPro": false, "fullname": "Kushnareva", "user": "Kushnareva", "type": "user"}, "summary": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.", "upvotes": 49, "discussionId": "69871a872d626112378ad6a7", "githubRepo": "https://github.com/audiosae/audiosae_demo", "githubRepoAddedBy": "user", "ai_summary": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.", "ai_keywords": ["Sparse Autoencoders", "encoder layers", "Whisper", "HuBERT", "feature steering", "false speech detections", "WER", "EEG activity", "speech perception"], "githubStars": 7, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728\u97f3\u9891\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002</li>\n    <li>\u6211\u4eec\u5728Whisper\u548cHuBERT\u7684\u6240\u6709\u7f16\u7801\u5668\u5c42\u4e0a\u8bad\u7ec3SAEs\uff0c\u5e76\u8bc4\u4f30\u5176\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002</li>\n    <li>\u8d85\u8fc750%\u7684\u7279\u5f81\u5728\u968f\u673a\u79cd\u5b50\u95f4\u4fdd\u6301\u4e00\u81f4\uff0c\u91cd\u5efa\u8d28\u91cf\u5f97\u4ee5\u4fdd\u6301\u3002</li>\n    <li>SAE\u7279\u5f81\u80fd\u591f\u6709\u6548\u6355\u6349\u4e00\u822c\u58f0\u5b66\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5305\u62ec\u73af\u5883\u566a\u97f3\u548c\u975e\u8bed\u8a00\u58f0\u97f3\uff08\u5982\u7b11\u58f0\u3001\u8033\u8bed\uff09\u3002</li>\n    <li>SAE\u7279\u5f81\u4e0e\u4eba\u7c7b\u5728\u8bed\u8a00\u611f\u77e5\u8fc7\u7a0b\u4e2d\u7684\u8111\u7535\u6d3b\u52a8\u76f8\u5173\u8054\uff0c\u663e\u793a\u51fa\u4e0e\u4eba\u7c7b\u795e\u7ecf\u5904\u7406\u7684\u5bf9\u9f50\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Sparse Autoencoders (SAEs) help understand how neural networks work, but their use in audio is not well-studied.</li>\n    <li>We trained SAEs using the Whisper and HuBERT models and evaluated their stability and usefulness.</li>\n    <li>Over half of the features are consistent even with different initial conditions, and the quality of audio reconstruction is maintained.</li>\n    <li>SAEs can effectively separate different sounds, including environmental noises and sounds like laughter, needing only a small percentage of features to be removed to eliminate a concept.</li>\n    <li>Using SAEs improved the accuracy of speech detection in Whisper by reducing false detections by 70% with only a slight increase in word error rate.</li>\n    <li>SAE features are linked to human brain activity during speech perception, suggesting they align well with how humans process speech.</li>\n</ul>"}, "publishedAt": "2026-02-04T15:29:16.000Z", "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders", "summary": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05027.png", "numComments": 2, "submittedBy": {"_id": "636254dc2691058b19d9276a", "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg", "fullname": "Kushnareva", "name": "Kushnareva", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.03392", "authors": [{"_id": "69858ae34ad556f294b7ec93", "user": {"_id": "652f7bf41ad13fee8c407247", "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg", "isPro": false, "fullname": "Shumin", "user": "Mystery", "type": "user"}, "name": "Shumin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:50:45.887Z", "hidden": false}, {"_id": "69858ae34ad556f294b7ec94", "name": "Yuexiang Xie", "hidden": false}, {"_id": "69858ae34ad556f294b7ec95", "name": "Wenhao Zhang", "hidden": false}, {"_id": "69858ae34ad556f294b7ec96", "user": {"_id": "6541b3d54f939214d3abbfbc", "avatarUrl": "/avatars/37aa9cc51fd98198805456ad04b90023.svg", "isPro": false, "fullname": "yuchang", "user": "hiyuchang", "type": "user"}, "name": "Yuchang Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:09.981Z", "hidden": false}, {"_id": "69858ae34ad556f294b7ec97", "name": "Yanxi Chen", "hidden": false}, {"_id": "69858ae34ad556f294b7ec98", "name": "Yaliang Li", "hidden": false}, {"_id": "69858ae34ad556f294b7ec99", "name": "Yanyong Zhang", "hidden": false}], "publishedAt": "2026-02-03T11:14:58.000Z", "submittedOnDailyAt": "2026-02-09T00:31:59.901Z", "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models", "submittedOnDailyBy": {"_id": "652f7bf41ad13fee8c407247", "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg", "isPro": false, "fullname": "Shumin", "user": "Mystery", "type": "user"}, "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.", "upvotes": 45, "discussionId": "69858ae34ad556f294b7ec9a", "githubRepo": "https://github.com/agentscope-ai/Trinity-RFT", "githubRepoAddedBy": "user", "ai_summary": "The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.", "ai_keywords": ["entropy", "large language models", "reinforcement fine-tuning", "RFT", "logit update", "Group Relative Policy Optimization", "GRPO", "entropy-discriminator clipping", "exploration-exploitation balance"], "githubStars": 521, "summary_zh": "<ul>\n    <li>\u71b5\u662f\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u591a\u6837\u6027\u7684\u5173\u952e\u6307\u6807\uff0c\u6709\u52a9\u4e8e\u4e86\u89e3\u5176\u63a2\u7d22\u80fd\u529b\u3002</li>\n    <li>\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u5173\u6ce8\u71b5\u7684\u76d1\u6d4b\u4e0e\u8c03\u6574\uff0c\u4f46\u5bf9\u5f3a\u5316\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u71b5\u52a8\u6001\u7406\u89e3\u4ecd\u4e0d\u591f\u6df1\u5165\u3002</li>\n    <li>\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u5f3a\u5316\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u71b5\u53d8\u5316\uff0c\u9996\u6b21\u91cf\u5316\u4e86\u5355\u6b21\u66f4\u65b0\u4e0b\u7684\u71b5\u53d8\u5316\u3002</li>\n    <li>\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5f97\u51fa\u7684\u7ed3\u8bba\u6fc0\u53d1\u4e86\u71b5\u63a7\u5236\u65b9\u6cd5\u7684\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u71b5\u76f8\u5173\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e\uff0c\u652f\u6301\u71b5\u5224\u522b\u5668\u526a\u88c1\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e2e\u52a9\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a2\u7d22\u4e0e\u5f00\u53d1\u5e73\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Entropy measures how diverse the outputs of large language models (LLMs) are, helping to understand their ability to explore different options.</li>\n    <li>This paper develops a theoretical framework to analyze how entropy changes during reinforcement fine-tuning (RFT) of LLMs.</li>\n    <li>It introduces a method to quantify how entropy shifts with single logit updates and extends this to a broader optimization approach.</li>\n    <li>The findings help create new methods for controlling entropy and improve understanding of existing methods in research.</li>\n    <li>Empirical evidence is provided to support the study's conclusions and show how the new methods can optimize the balance between exploration and exploitation during LLM training.</li>\n</ul>"}, "publishedAt": "2026-02-03T06:14:58.000Z", "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models", "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03392.png", "numComments": 3, "submittedBy": {"_id": "652f7bf41ad13fee8c407247", "avatarUrl": "/avatars/5c7a74a9edf748025bffeeba97a61505.svg", "fullname": "Shumin", "name": "Mystery", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.01734", "authors": [{"_id": "6987609dbeecc443208d2375", "name": "Lianhai Ren", "hidden": false}, {"_id": "6987609dbeecc443208d2376", "name": "Yucheng Ding", "hidden": false}, {"_id": "6987609dbeecc443208d2377", "user": {"_id": "63fb6e281b4b1bd4e7ffc5be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg", "isPro": false, "fullname": "Xiao Liu", "user": "lx865712528", "type": "user"}, "name": "Xiao Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:31:02.000Z", "hidden": false}, {"_id": "6987609dbeecc443208d2378", "name": "Qianxiao Li", "hidden": false}, {"_id": "6987609dbeecc443208d2379", "name": "Peng Cheng", "hidden": false}, {"_id": "6987609dbeecc443208d237a", "name": "Yeyun Gong", "hidden": false}], "publishedAt": "2026-02-02T07:18:45.000Z", "submittedOnDailyAt": "2026-02-09T02:17:53.967Z", "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration", "submittedOnDailyBy": {"_id": "63fb6e281b4b1bd4e7ffc5be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg", "isPro": false, "fullname": "Xiao Liu", "user": "lx865712528", "type": "user"}, "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via \u03bcP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.", "upvotes": 29, "discussionId": "6987609dbeecc443208d237b", "ai_summary": "Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.", "ai_keywords": ["large language model", "pretraining", "gradient explosions", "weight matrix stable rank", "Frobenius norm", "spectral norm", "Jacobian", "matrix sign operations", "optimizer"], "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u9762\u4e34\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u5e38\u8868\u73b0\u4e3a\u68af\u5ea6\u7206\u70b8\u3002</li>\n    <li>\u6211\u4eec\u7814\u7a76\u4e865M\u53c2\u6570\u7684NanoGPT\u6a21\u578b\uff0c\u53d1\u73b0\u5d29\u6e83\u524d\u6709\u4e24\u4e2a\u5173\u952e\u73b0\u8c61\uff1a\u6743\u91cd\u77e9\u9635\u7a33\u5b9a\u79e9\u8fc5\u901f\u4e0b\u964d\u548c\u76f8\u90bb\u5c42\u96c5\u53ef\u6bd4\u77e9\u9635\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\u589e\u52a0\u3002</li>\n    <li>\u6211\u4eec\u7406\u8bba\u8bc1\u660e\u8fd9\u4e24\u4e2a\u6761\u4ef6\u4f1a\u5bfc\u81f4\u68af\u5ea6\u8303\u6570\u968f\u7f51\u7edc\u6df1\u5ea6\u5448\u6307\u6570\u589e\u957f\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u4e0d\u7a33\u5b9a\u673a\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668MSign\uff0c\u901a\u8fc7\u5b9a\u671f\u5e94\u7528\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u6765\u6062\u590d\u7a33\u5b9a\u79e9\u3002</li>\n    <li>\u57285M\u52303B\u53c2\u6570\u7684\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cMSign\u6709\u6548\u9632\u6b62\u4e86\u8bad\u7ec3\u5931\u8d25\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e7.0%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Training large language models often faces issues like sudden gradient explosions, wasting computing power.</li>\n    <li>In a study of a small NanoGPT model, two main signs of training failure were found: a quick drop in weight matrix stability and increased alignment between layer Jacobians.</li>\n    <li>These issues can lead to rapid growth in gradient norms as the model gets deeper.</li>\n    <li>A new optimizer called MSign was developed to help fix this instability by using matrix sign operations to maintain stable rank.</li>\n    <li>Tests showed that MSign successfully prevents training problems with only a small increase in computational cost.</li>\n</ul>"}, "publishedAt": "2026-02-02T02:18:45.000Z", "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration", "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via \u03bcP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01734.png", "numComments": 2, "submittedBy": {"_id": "63fb6e281b4b1bd4e7ffc5be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fb6e281b4b1bd4e7ffc5be/aiRu_bulgnxvEMrjipGoQ.jpeg", "fullname": "Xiao Liu", "name": "lx865712528", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18415", "authors": [{"_id": "698061616676f93322706708", "user": {"_id": "62b1e0f76a5435fd9a60a8dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655824626110-noauth.png", "isPro": false, "fullname": "Ivan Bondarenko", "user": "bond005", "type": "user"}, "name": "Ivan Bondarenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T15:43:40.523Z", "hidden": false}, {"_id": "698061616676f93322706709", "user": {"_id": "63cb976d80ba2ca4151b67a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675713278440-63cb976d80ba2ca4151b67a2.jpeg", "isPro": false, "fullname": "Daniel Grebenkin", "user": "dangrebenkin", "type": "user"}, "name": "Daniil Grebenkin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T15:43:46.074Z", "hidden": false}, {"_id": "698061616676f9332270670a", "user": {"_id": "61dd9daedb45389905634d3f", "avatarUrl": "/avatars/a0482317b6118d37da294b9dc2bf5a39.svg", "isPro": false, "fullname": "Oleg Sedukhin", "user": "greyzyablik", "type": "user"}, "name": "Oleg Sedukhin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T15:43:42.792Z", "hidden": true}, {"_id": "698061616676f9332270670b", "user": {"_id": "662282de25b14cbf4b0c3f7b", "avatarUrl": "/avatars/78231051b4f29538c83ff9935e54d974.svg", "isPro": false, "fullname": "Klementev Mikhail", "user": "Klemaaaaa", "type": "user"}, "name": "Mikhail Klementev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T15:43:38.368Z", "hidden": false}, {"_id": "698061616676f9332270670c", "user": {"_id": "6415cb01486c7c9a5d1560f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg", "isPro": false, "fullname": "Roman Derunets", "user": "rmndrnts", "type": "user"}, "name": "Roman Derunets", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:51:53.503Z", "hidden": false}, {"_id": "698061616676f9332270670d", "name": "Lyudmila Budneva", "hidden": false}], "publishedAt": "2026-01-26T12:14:51.000Z", "submittedOnDailyAt": "2026-02-09T10:55:09.411Z", "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews", "submittedOnDailyBy": {"_id": "6415cb01486c7c9a5d1560f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg", "isPro": false, "fullname": "Roman Derunets", "user": "rmndrnts", "type": "user"}, "summary": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.", "upvotes": 29, "discussionId": "698061616676f9332270670e", "ai_summary": "A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian speech recognition.", "ai_keywords": ["Wav2Vec2", "Audio Spectrogram Transformer", "Whisper", "curriculum learning", "uncertainty modeling", "speech recognition", "transcription quality"], "organization": {"_id": "62b1e262f4a72794188b6757", "name": "NSU", "fullname": "Novosibirsk State University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1655825086683-62b1e0f76a5435fd9a60a8dc.png"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201cPisets\u201d\u7684\u8bed\u97f3\u8f6c\u6587\u672c\u7cfb\u7edf\uff0c\u4e13\u4e3a\u79d1\u5b66\u5bb6\u548c\u8bb0\u8005\u8bbe\u8ba1\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u91c7\u7528\u4e09\u90e8\u5206\u67b6\u6784\uff0c\u65e8\u5728\u63d0\u9ad8\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u9519\u8bef\u548c\u5e7b\u89c9\u3002</li>\n    <li>\u4e3b\u8981\u67b6\u6784\u5305\u62ec\u4f7f\u7528Wav2Vec2\u8fdb\u884c\u521d\u6b65\u8bc6\u522b\uff0c\u901a\u8fc7\u97f3\u9891\u8c31\u56fe\u53d8\u6362\u5668\uff08AST\uff09\u8fdb\u884c\u865a\u5047\u6b63\u4f8b\u8fc7\u6ee4\uff0c\u6700\u540e\u901a\u8fc7Whisper\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u548c\u591a\u6837\u5316\u7684\u4fc4\u8bed\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u6548\u679c\u3002</li>\n    <li>\u201cPisets\u201d\u7cfb\u7edf\u7684\u6e90\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\uff0c\u94fe\u63a5\u4e3a\uff1ahttps://github.com/bond005/pisets\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"Pisets\" system is a speech-to-text tool designed for scientists and journalists, aimed at better speech recognition.</li>\n    <li>It uses a three-part structure: Wav2Vec2 for initial recognition, Audio Spectrogram Transformer (AST) to filter errors, and Whisper for final recognition.</li>\n    <li>It employs curriculum learning and diverse Russian-language speech data to improve its performance.</li>\n    <li>Advanced techniques for uncertainty modeling have been added, leading to higher transcription quality.</li>\n    <li>The \"Pisets\" system outperforms WhisperX and standard Whisper in transcribing long audio recordings in different sound conditions, and its source code is available on GitHub.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:14:51.000Z", "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews", "summary": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18415.png", "numComments": 2, "submittedBy": {"_id": "6415cb01486c7c9a5d1560f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg", "fullname": "Roman Derunets", "name": "rmndrnts", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "62b1e262f4a72794188b6757", "name": "NSU", "fullname": "Novosibirsk State University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1655825086683-62b1e0f76a5435fd9a60a8dc.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06949", "authors": [{"_id": "69894c74beecc443208d25db", "name": "Shenyuan Gao", "hidden": false}, {"_id": "69894c74beecc443208d25dc", "name": "William Liang", "hidden": false}, {"_id": "69894c74beecc443208d25dd", "name": "Kaiyuan Zheng", "hidden": false}, {"_id": "69894c74beecc443208d25de", "name": "Ayaan Malik", "hidden": false}, {"_id": "69894c74beecc443208d25df", "name": "Seonghyeon Ye", "hidden": false}, {"_id": "69894c74beecc443208d25e0", "name": "Sihyun Yu", "hidden": false}, {"_id": "69894c74beecc443208d25e1", "name": "Wei-Cheng Tseng", "hidden": false}, {"_id": "69894c74beecc443208d25e2", "name": "Yuzhu Dong", "hidden": false}, {"_id": "69894c74beecc443208d25e3", "name": "Kaichun Mo", "hidden": false}, {"_id": "69894c74beecc443208d25e4", "name": "Chen-Hsuan Lin", "hidden": false}, {"_id": "69894c74beecc443208d25e5", "name": "Qianli Ma", "hidden": false}, {"_id": "69894c74beecc443208d25e6", "name": "Seungjun Nah", "hidden": false}, {"_id": "69894c74beecc443208d25e7", "name": "Loic Magne", "hidden": false}, {"_id": "69894c74beecc443208d25e8", "name": "Jiannan Xiang", "hidden": false}, {"_id": "69894c74beecc443208d25e9", "name": "Yuqi Xie", "hidden": false}, {"_id": "69894c74beecc443208d25ea", "name": "Ruijie Zheng", "hidden": false}, {"_id": "69894c74beecc443208d25eb", "name": "Dantong Niu", "hidden": false}, {"_id": "69894c74beecc443208d25ec", "name": "You Liang Tan", "hidden": false}, {"_id": "69894c74beecc443208d25ed", "name": "K. R. Zentner", "hidden": false}, {"_id": "69894c74beecc443208d25ee", "name": "George Kurian", "hidden": false}, {"_id": "69894c74beecc443208d25ef", "name": "Suneel Indupuru", "hidden": false}, {"_id": "69894c74beecc443208d25f0", "name": "Pooya Jannaty", "hidden": false}, {"_id": "69894c74beecc443208d25f1", "name": "Jinwei Gu", "hidden": false}, {"_id": "69894c74beecc443208d25f2", "name": "Jun Zhang", "hidden": false}, {"_id": "69894c74beecc443208d25f3", "name": "Jitendra Malik", "hidden": false}, {"_id": "69894c74beecc443208d25f4", "name": "Pieter Abbeel", "hidden": false}, {"_id": "69894c74beecc443208d25f5", "name": "Ming-Yu Liu", "hidden": false}, {"_id": "69894c74beecc443208d25f6", "name": "Yuke Zhu", "hidden": false}, {"_id": "69894c74beecc443208d25f7", "name": "Joel Jang", "hidden": false}, {"_id": "69894c74beecc443208d25f8", "name": "Linxi \"Jim\" Fan", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"], "publishedAt": "2026-02-06T18:49:43.000Z", "submittedOnDailyAt": "2026-02-09T00:32:34.350Z", "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.", "upvotes": 21, "discussionId": "69894c74beecc443208d25f9", "projectPage": "https://dreamdojo-world.github.io/", "ai_summary": "DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.", "ai_keywords": ["world model", "egocentric videos", "continuous latent actions", "action labels", "distillation pipeline", "real-time speed", "teleoperation", "policy evaluation", "model-based planning", "out-of-distribution benchmarks"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86DreamDojo\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u7840\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4ece4.4\u4e07\u5c0f\u65f6\u7684\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u5404\u79cd\u4e92\u52a8\u548c\u7075\u5de7\u63a7\u5236\u3002</li>\n    <li>DreamDojo\u7684\u6570\u636e\u96c6\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u4e16\u754c\u6a21\u578b\u9884\u8bad\u7ec3\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u65e5\u5e38\u573a\u666f\u548c\u6280\u80fd\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u52a8\u4f5c\u6807\u7b7e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u4f5c\u4e3a\u7edf\u4e00\u7684\u4ee3\u7406\u52a8\u4f5c\uff0c\u4fc3\u8fdb\u4e86\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u4e2d\u77e5\u8bc6\u7684\u8f6c\u79fb\u3002</li>\n    <li>\u7ecf\u8fc7\u5c0f\u89c4\u6a21\u76ee\u6807\u673a\u5668\u4eba\u6570\u636e\u7684\u540e\u8bad\u7ec3\uff0cDreamDojo\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u7269\u7406\u7406\u89e3\u548c\u7cbe\u786e\u7684\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u7684\u5de5\u4f5c\u652f\u6301\u591a\u79cd\u91cd\u8981\u5e94\u7528\uff0c\u5305\u62ec\u5b9e\u65f6\u8fdc\u7a0b\u64cd\u4f5c\u3001\u7b56\u7565\u8bc4\u4f30\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DreamDojo is a new model that helps robots learn how to interact and perform tasks by using a large dataset of human videos.</li>\n    <li>The model was trained on 44,000 hours of video, making it the biggest dataset for this purpose, and includes a variety of daily activities.</li>\n    <li>To overcome the problem of not having enough action labels, DreamDojo uses a method called continuous latent actions to improve learning from unlabeled videos.</li>\n    <li>After training, DreamDojo can understand physics and control actions accurately, and it operates at a speed of 10.81 frames per second.</li>\n    <li>DreamDojo can be used for various applications, such as remote robot operation, evaluating robot behavior, and planning tasks, proving effective in challenging situations.</li>\n</ul>"}, "publishedAt": "2026-02-06T13:49:43.000Z", "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos", "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/MN-A84kxkw1l1lyftyRTR.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06949.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.06130", "authors": [{"_id": "6989cb0bbeecc443208d2864", "name": "Yifu Qiu", "hidden": false}, {"_id": "6989cb0bbeecc443208d2865", "user": {"_id": "64ba8e9d5299e0f164491e45", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ba8e9d5299e0f164491e45/ZK4bUJZuUO8xsj0RZmzMn.jpeg", "isPro": false, "fullname": "Zheng Zhao", "user": "zsquaredz", "type": "user"}, "name": "Zheng Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:06:19.908Z", "hidden": false}, {"_id": "6989cb0bbeecc443208d2866", "name": "Waylon Li", "hidden": false}, {"_id": "6989cb0bbeecc443208d2867", "name": "Yftah Ziser", "hidden": false}, {"_id": "6989cb0bbeecc443208d2868", "name": "Anna Korhonen", "hidden": false}, {"_id": "6989cb0bbeecc443208d2869", "name": "Shay B. Cohen", "hidden": false}, {"_id": "6989cb0bbeecc443208d286a", "name": "Edoardo M. Ponti", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/j_R6vn6-FY-h0yKdFj-3X.gif", "https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/zJRIVeC0mBpuOlMZ19NFx.png"], "publishedAt": "2026-02-05T19:04:41.000Z", "submittedOnDailyAt": "2026-02-09T09:28:38.468Z", "title": "Self-Improving World Modelling with Latent Actions", "submittedOnDailyBy": {"_id": "64686434f43574d9556b1fa6", "avatarUrl": "/avatars/64183d643cfc3b274714a6167c354e39.svg", "isPro": false, "fullname": "Yifu Qiu", "user": "yfqiu-nlp", "type": "user"}, "summary": "Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_\u03b8(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_\u03c6(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.", "upvotes": 19, "discussionId": "6989cb0bbeecc443208d286b", "ai_summary": "SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks.", "ai_keywords": ["Forward World Modelling", "Inverse Dynamics Modelling", "variational information maximisation", "ELBO maximisation", "reinforcement learning", "GRPO", "latent variables", "conditional mutual information", "coordinate ascent", "world models", "state-only sequences"], "summary_zh": "<ul>\n  <li>SWIRL\u662f\u4e00\u79cd\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u4ec5\u5229\u7528\u72b6\u6001\u5e8f\u5217\u6765\u5b66\u4e60\u6a21\u578b\uff0c\u800c\u4e0d\u9700\u8981\u6807\u8bb0\u7684\u52a8\u4f5c\u3002</li>\n  <li>\u5b83\u901a\u8fc7\u524d\u5411\u4e16\u754c\u5efa\u6a21\uff08FWM\uff09\u548c\u9006\u52a8\u6001\u5efa\u6a21\uff08IDM\uff09\u4e24\u4e2a\u9636\u6bb5\u4ea4\u66ff\u8fdb\u884c\u5b66\u4e60\u3002</li>\n  <li>FWM\u65e8\u5728\u751f\u6210\u6700\u5927\u5316\u6761\u4ef6\u4e92\u4fe1\u606f\u7684\u4e0b\u4e00\u72b6\u6001\uff0c\u800cIDM\u5219\u89e3\u91ca\u89c2\u5bdf\u5230\u7684\u72b6\u6001\u8f6c\u53d8\u3002</li>\n  <li>SWIRL\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u5305\u62ec\u89c6\u89c9\u52a8\u6001\u548c\u6587\u672c\u73af\u5883\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002</li>\n  <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWIRL\u5206\u522b\u83b7\u5f97\u4e8616%\u523028%\u7684\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SWIRL is a new framework designed to help LLMs and VLMs learn to predict changes in states without needing expensive action-labeled data.</li>\n    <li>It uses two main processes: Forward World Modelling (FWM) to predict future states and Inverse Dynamics Modelling (IDM) to understand the actions taken.</li>\n    <li>SWIRL improves learning by maximizing information and explaining observed changes in state, using a method called reinforcement learning.</li>\n    <li>The framework has shown significant performance improvements in various tasks, achieving up to 28% better results in some tests.</li>\n    <li>The authors have provided guarantees that their learning methods will work effectively in different environments.</li>\n</ul>"}, "publishedAt": "2026-02-05T14:04:41.000Z", "title": "Self-Improving World Modelling with Latent Actions", "summary": "Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_\u03b8(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_\u03c6(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/j_R6vn6-FY-h0yKdFj-3X.gif", "https://cdn-uploads.huggingface.co/production/uploads/64686434f43574d9556b1fa6/zJRIVeC0mBpuOlMZ19NFx.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06130.png", "numComments": 1, "submittedBy": {"_id": "64686434f43574d9556b1fa6", "avatarUrl": "/avatars/64183d643cfc3b274714a6167c354e39.svg", "fullname": "Yifu Qiu", "name": "yfqiu-nlp", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.06291", "authors": [{"_id": "698964b8beecc443208d26d8", "name": "Guijin Son", "hidden": false}, {"_id": "698964b8beecc443208d26d9", "name": "Donghun Yang", "hidden": false}, {"_id": "698964b8beecc443208d26da", "name": "Hitesh Laxmichand Patel", "hidden": false}, {"_id": "698964b8beecc443208d26db", "name": "Hyunwoo Ko", "hidden": false}, {"_id": "698964b8beecc443208d26dc", "name": "Amit Agarwal", "hidden": false}, {"_id": "698964b8beecc443208d26dd", "name": "Sunghee Ahn", "hidden": false}, {"_id": "698964b8beecc443208d26de", "name": "Kyong-Ha Lee", "hidden": false}, {"_id": "698964b8beecc443208d26df", "name": "Youngjae Yu", "hidden": false}], "publishedAt": "2026-02-06T01:10:28.000Z", "submittedOnDailyAt": "2026-02-09T02:08:27.389Z", "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math", "submittedOnDailyBy": {"_id": "60d3e619b8448e1785bbda2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg", "isPro": true, "fullname": "GUIJIN SON", "user": "amphora", "type": "user"}, "summary": "Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.", "upvotes": 16, "discussionId": "698964b8beecc443208d26e0", "ai_summary": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.", "ai_keywords": ["reasoning models", "research-level mathematics", "verification", "oracle-free evaluator", "in-context exemplar", "reward models", "generative reward models", "LLM judges", "Acc@1", "AUC", "solver-evaluator gap"], "summary_zh": "<ul>\n    <li>\u8fd1\u671f\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u8868\u660e\uff0c\u53ef\u4ee5\u751f\u6210\u7814\u7a76\u7ea7\u6570\u5b66\u7684\u5408\u7406\u89e3\u6cd5\uff0c\u4f46\u9a8c\u8bc1\u8fc7\u7a0b\u4ecd\u7136\u662f\u74f6\u9888\uff0c\u6d88\u8017\u4e86\u4e13\u5bb6\u7684\u65f6\u95f4\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u540e\u679c\u57fa\u7840\u6548\u7528\u201d\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u4e13\u5bb6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u5019\u9009\u89e3\u6cd5\u5728\u89e3\u51b3\u76f8\u5173\u53ef\u9a8c\u8bc1\u95ee\u9898\u4e2d\u7684\u4ef7\u503c\u6765\u8bc4\u5206\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u4e00\u7ec4\u7814\u7a76\u7ea7\u6570\u5b66\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u6709\u4e00\u4e2a\u4e13\u5bb6\u89e3\u548c\u4e5d\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u89e3\u3002</li>\n    <li>\u7ed3\u679c\u663e\u793a\uff0c\u201c\u540e\u679c\u57fa\u7840\u6548\u7528\u201d\u5728\u6392\u540d\u8d28\u91cf\u4e0a\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u7684\u5956\u52b1\u6a21\u578b\u548cLLM\u8bc4\u4f30\u8005\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548c\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUC\uff09\u3002</li>\n    <li>\u4e0eLLM\u8bc4\u4f30\u8005\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6b63\u786e\u4e0e\u9519\u8bef\u7684\u533a\u5206\u4e0a\u8868\u73b0\u66f4\u5f3a\uff0c\u5c24\u5176\u5728\u4e00\u4e9b\u5e95\u5c42\u6c42\u89e3\u5668\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u4e0a\u4e5f\u80fd\u4fdd\u6301\u8f83\u597d\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New reasoning models can create plausible solutions for advanced math, but checking these solutions takes a lot of expert time.</li>\n    <li>We suggest a method called Consequence-Based Utility that evaluates solutions by testing how useful they are for related questions.</li>\n    <li>This method was tested on a set of research-level math problems, comparing expert solutions with those generated by language models.</li>\n    <li>Consequence-Based Utility performed better than other evaluation methods, showing significant improvements in ranking quality.</li>\n    <li>It maintained a clearer distinction between correct and incorrect solutions, even when the solver struggled to find answers.</li>\n</ul>"}, "publishedAt": "2026-02-05T20:10:28.000Z", "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math", "summary": "Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06291.png", "numComments": 1, "submittedBy": {"_id": "60d3e619b8448e1785bbda2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d3e619b8448e1785bbda2a/q2re5u1HNwsCCyIMtid_I.jpeg", "fullname": "GUIJIN SON", "name": "amphora", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 76, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>ERNIE 5.0\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u8d85\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u76ee\u6807\u3002</li>\n    <li>ERNIE 5.0\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5f39\u6027\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f7f\u6a21\u578b\u5728\u4e0d\u540c\u8d44\u6e90\u9650\u5236\u4e0b\u7075\u6d3b\u8c03\u6574\u6027\u80fd\u548c\u5927\u5c0f\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u591a\u6a21\u6001\u6027\u80fd\uff0c\u662f\u7b2c\u4e00\u4e2a\u516c\u5f00\u62ab\u9732\u7684\u4e07\u4ebf\u53c2\u6570\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u3002</li>\n    <li>\u62a5\u544a\u4e2d\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u4e13\u5bb6\u8def\u7531\u53ef\u89c6\u5316\u548c\u5f39\u6027\u8bad\u7ec3\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u6df1\u523b\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new AI model that understands and generates text, images, videos, and audio.</li>\n    <li>It uses a unique training method that allows it to adapt to different resource needs while being efficient.</li>\n    <li>The model can create various smaller versions of itself to balance performance and resource use.</li>\n    <li>ERNIE 5.0 is the first large-scale model with a trillion parameters that works well across different types of media.</li>\n    <li>The report includes helpful visuals and analysis to support further research in this area.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.03152", "authors": [{"_id": "698406a2e34659da7e1f4d65", "name": "Yifei Wang", "hidden": false}, {"_id": "698406a2e34659da7e1f4d66", "name": "Yueqi Wang", "hidden": false}, {"_id": "698406a2e34659da7e1f4d67", "name": "Zhenrui Yue", "hidden": false}, {"_id": "698406a2e34659da7e1f4d68", "name": "Huimin Zeng", "hidden": false}, {"_id": "698406a2e34659da7e1f4d69", "name": "Yong Wang", "hidden": false}, {"_id": "698406a2e34659da7e1f4d6a", "name": "Ismini Lourentzou", "hidden": false}, {"_id": "698406a2e34659da7e1f4d6b", "name": "Zhengzhong Tu", "hidden": false}, {"_id": "698406a2e34659da7e1f4d6c", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698406a2e34659da7e1f4d6d", "name": "Julian McAuley", "hidden": false}], "publishedAt": "2026-02-03T06:09:06.000Z", "submittedOnDailyAt": "2026-02-05T08:51:33.236Z", "title": "FASA: Frequency-aware Sparse Attention", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56times speedup using just 18.9\\% of the cache on AIME24.", "upvotes": 103, "discussionId": "698406a2e34659da7e1f4d6e", "ai_summary": "FASA is a novel framework that uses query-aware token eviction and functional sparsity in RoPE to reduce KV cache memory usage while maintaining high performance in long-context LLM tasks.", "ai_keywords": ["Large Language Models", "Key Value cache", "token pruning", "attention sparsity", "query-dependent token importance", "RoPE", "functional sparsity", "frequency-chunk level", "dominant frequency-chunks", "token eviction", "attention computation", "long-context tasks", "sequence modeling", "CoT reasoning", "token-eviction baselines", "KV cache memory footprint", "computational cost", "LongBench-V1", "AIME24"], "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u957f\u8f93\u5165\u65f6\u9762\u4e34\u5185\u5b58\u74f6\u9888\uff0c\u5c24\u5176\u662f\u952e\u503c\u7f13\u5b58\u7684\u5360\u7528\u3002</li>\n    <li>\u73b0\u6709\u7684\u4ee4\u724c\u4fee\u526a\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u635f\u5931\u98ce\u9669\u6216\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4ee4\u724c\u7684\u91cd\u8981\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86FASA\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9884\u6d4b\u4ee4\u724c\u91cd\u8981\u6027\u6765\u5b9e\u73b0\u67e5\u8be2\u611f\u77e5\u7684\u4ee4\u724c\u5254\u9664\u3002</li>\n    <li>FASA\u5229\u7528RoPE\u7684\u529f\u80fd\u7a00\u758f\u6027\uff0c\u8bc6\u522b\u51fa\u4e00\u5c0f\u90e8\u5206\u201c\u4e3b\u5bfc\u201d\u9891\u7387\u5757\u4f5c\u4e3a\u91cd\u8981\u4ee4\u724c\u7684\u4ee3\u7406\u3002</li>\n    <li>\u5728\u591a\u79cd\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\uff0cFASA\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5e26\u5bbd\u9700\u6c42\u548c\u8ba1\u7b97\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) struggle with long inputs due to high memory usage from the Key Value (KV) cache.</li>\n    <li>Current methods for reducing memory use either lose important information or don\u2019t effectively measure which tokens are important.</li>\n    <li>FASA is a new framework that improves token selection by predicting which tokens matter most based on the query.</li>\n    <li>FASA uses a technique called functional sparsity to identify a small number of key tokens that relate well to the overall context.</li>\n    <li>In tests, FASA outperformed other methods, achieving near-perfect accuracy while using significantly less memory and processing power.</li>\n</ul>"}, "publishedAt": "2026-02-03T01:09:06.000Z", "title": "FASA: Frequency-aware Sparse Attention", "summary": "The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56times speedup using just 18.9\\% of the cache on AIME24.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03152.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.04634", "authors": [{"_id": "69840a42e34659da7e1f4da8", "user": {"_id": "653a5b0f7c01c693a16dd184", "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg", "isPro": false, "fullname": "Zelai Xu", "user": "zelaix", "type": "user"}, "name": "Zelai Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:54:21.312Z", "hidden": false}, {"_id": "69840a42e34659da7e1f4da9", "name": "Zhexuan Xu", "hidden": false}, {"_id": "69840a42e34659da7e1f4daa", "user": {"_id": "683fb41cb1bf6fbcce6bc205", "avatarUrl": "/avatars/544ff46b9ff78f8420981fa507da767e.svg", "isPro": false, "fullname": "Ruize Zhang", "user": "Ruize-Zhang", "type": "user"}, "name": "Ruize Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:54:19.167Z", "hidden": false}, {"_id": "69840a42e34659da7e1f4dab", "name": "Chunyang Zhu", "hidden": false}, {"_id": "69840a42e34659da7e1f4dac", "name": "Shi Yu", "hidden": false}, {"_id": "69840a42e34659da7e1f4dad", "name": "Weilin Liu", "hidden": false}, {"_id": "69840a42e34659da7e1f4dae", "name": "Quanlu Zhang", "hidden": false}, {"_id": "69840a42e34659da7e1f4daf", "name": "Wenbo Ding", "hidden": false}, {"_id": "69840a42e34659da7e1f4db0", "name": "Chao Yu", "hidden": false}, {"_id": "69840a42e34659da7e1f4db1", "name": "Yu Wang", "hidden": false}], "publishedAt": "2026-02-04T15:05:12.000Z", "submittedOnDailyAt": "2026-02-05T02:30:13.468Z", "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning", "submittedOnDailyBy": {"_id": "653a5b0f7c01c693a16dd184", "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg", "isPro": false, "fullname": "Zelai Xu", "user": "zelaix", "type": "user"}, "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.", "upvotes": 71, "discussionId": "69840a43e34659da7e1f4db2", "projectPage": "https://wideseek-r1.github.io/", "githubRepo": "https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1", "githubRepoAddedBy": "user", "ai_summary": "Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.", "ai_keywords": ["Large Language Models", "multi-agent systems", "multi-agent reinforcement learning", "lead-agent-subagent framework", "parallel execution", "information seeking", "WideSearch benchmark", "F1 score"], "githubStars": 2379, "organization": {"_id": "689ea978824b212c988bc8f5", "name": "RLinf", "fullname": "RLinf", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6700\u8fd1\u7684\u8fdb\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u6df1\u5ea6\u6269\u5c55\u4e0a\uff0c\u89e3\u51b3\u957f\u65f6\u95f4\u7684\u95ee\u9898\u3002</li>\n    <li>\u968f\u7740\u4efb\u52a1\u8303\u56f4\u7684\u6269\u5927\uff0c\u5173\u952e\u74f6\u9888\u4ece\u4e2a\u4eba\u80fd\u529b\u8f6c\u5411\u7ec4\u7ec7\u80fd\u529b\u3002</li>\n    <li>\u672c\u6587\u63a2\u7d22\u4f7f\u7528\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u5bbd\u5ea6\u6269\u5c55\uff0c\u4ee5\u5e94\u5bf9\u5e7f\u6cdb\u7684\u4fe1\u606f\u641c\u7d22\u3002</li>\n    <li>\u63d0\u51fa\u4e86WideSeek-R1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u534f\u8c03\u4e0e\u5e76\u884c\u6267\u884c\u3002</li>\n    <li>WideSeek-R1-4B\u5728\u4fe1\u606f\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u968f\u7740\u5e76\u884c\u5b50\u4ee3\u7406\u6570\u91cf\u589e\u52a0\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in Large Language Models (LLMs) focus on solving complex problems through deep reasoning.</li>\n    <li>As tasks become broader, the challenge shifts from individual skill to how well multiple agents can work together.</li>\n    <li>WideSeek-R1 is a new system that uses a lead agent with subagents to work on information-seeking tasks more efficiently.</li>\n    <li>This system is trained using multi-agent reinforcement learning and can handle tasks in parallel, improving productivity.</li>\n    <li>Tests show that WideSeek-R1 performs comparably to other advanced models, with better results as more subagents are added.</li>\n</ul>"}, "publishedAt": "2026-02-04T10:05:12.000Z", "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning", "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04634.png", "numComments": 2, "submittedBy": {"_id": "653a5b0f7c01c693a16dd184", "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg", "fullname": "Zelai Xu", "name": "zelaix", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "689ea978824b212c988bc8f5", "name": "RLinf", "fullname": "RLinf", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04145", "authors": [{"_id": "698412bbe34659da7e1f4e04", "user": {"_id": "64fc20d899123d7698a30e61", "avatarUrl": "/avatars/9231982cf70a0689f50accedf1004702.svg", "isPro": false, "fullname": "Jinyuan Li", "user": "jinyuan222", "type": "user"}, "name": "Jinyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:57.944Z", "hidden": false}, {"_id": "698412bbe34659da7e1f4e05", "name": "Chengsong Huang", "hidden": false}, {"_id": "698412bbe34659da7e1f4e06", "name": "Langlin Huang", "hidden": false}, {"_id": "698412bbe34659da7e1f4e07", "name": "Shaoyang Xu", "hidden": false}, {"_id": "698412bbe34659da7e1f4e08", "name": "Haolin Liu", "hidden": false}, {"_id": "698412bbe34659da7e1f4e09", "name": "Wenxuan Zhang", "hidden": false}, {"_id": "698412bbe34659da7e1f4e0a", "name": "Jiaxin Huang", "hidden": false}], "publishedAt": "2026-02-04T02:27:38.000Z", "submittedOnDailyAt": "2026-02-05T01:21:27.343Z", "title": "Training Data Efficiency in Multimodal Process Reward Models", "submittedOnDailyBy": {"_id": "65e02d89574e5aa0e9ce3efa", "avatarUrl": "/avatars/2ab152a10b21d81fb1defc726b8e951a.svg", "isPro": false, "fullname": "Langlin Huang", "user": "shrango", "type": "user"}, "summary": "Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.", "upvotes": 70, "discussionId": "698412bbe34659da7e1f4e0b", "ai_summary": "Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.", "ai_keywords": ["Multimodal Process Reward Models", "Monte Carlo-annotated corpora", "VisualProcessBench", "Balanced-Information Score", "label mixtures", "label reliability", "gradient updates", "data efficiency"], "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08MPRMs\uff09\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u9010\u6b65\u76d1\u7763\u4e2d\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u8bad\u7ec3MPRMs\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8499\u7279\u5361\u6d1b\uff08MC\uff09\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u968f\u673a\u62bd\u6837\u8bad\u7ec3\u6570\u636e\u65f6\uff0cMPRM\u8bad\u7ec3\u5f88\u5feb\u8fbe\u5230\u9971\u548c\u72b6\u6001\uff0c\u8868\u660e\u73b0\u6709\u6570\u636e\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u5e73\u8861\u4fe1\u606f\u8bc4\u5206\uff08BIS\uff09\uff0c\u4f18\u5148\u8003\u8651\u6807\u7b7e\u6df7\u5408\u548c\u6807\u7b7e\u53ef\u9760\u6027\uff0c\u57fa\u4e8e\u73b0\u6709MC\u4fe1\u53f7\u8fdb\u884c\u9009\u62e9\u3002</li>\n    <li>\u5728\u4e24\u4e2a\u6a21\u578b\u4e0a\u6d4b\u8bd5\u540e\uff0c\u4f7f\u7528BIS\u9009\u62e9\u7684\u6570\u636e\u5b50\u96c6\u4ee5\u4ec510%\u7684\u8bad\u7ec3\u6570\u636e\u8fbe\u5230\u4e86\u5b8c\u6574\u6570\u636e\u7684\u6027\u80fd\uff0c\u63d0\u5347\u4e864.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal Process Reward Models (MPRMs) help with visual reasoning in machine learning language models (MLLMs).</li>\n    <li>Training MPRMs usually needs a lot of data, which can be expensive and time-consuming.</li>\n    <li>Research shows that MPRM training often doesn't need all the training data because there's a lot of redundancy in existing datasets.</li>\n    <li>The study introduces the Balanced-Information Score (BIS) to improve data selection based on important factors without extra costs.</li>\n    <li>Using BIS, the performance of models using only 10% of the data can match that of models using the full dataset, showing a significant improvement over random data selection.</li>\n</ul>"}, "publishedAt": "2026-02-03T21:27:38.000Z", "title": "Training Data Efficiency in Multimodal Process Reward Models", "summary": "Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04145.png", "numComments": 1, "submittedBy": {"_id": "65e02d89574e5aa0e9ce3efa", "avatarUrl": "/avatars/2ab152a10b21d81fb1defc726b8e951a.svg", "fullname": "Langlin Huang", "name": "shrango", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.03786", "authors": [{"_id": "6982c1c69084cb4f0ecb574b", "user": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "isPro": false, "fullname": "jianhao ruan", "user": "Aurorra1123", "type": "user"}, "name": "Jianhao Ruan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:23.320Z", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574c", "name": "Zhihao Xu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574d", "name": "Yiran Peng", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574e", "name": "Fashen Ren", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574f", "name": "Zhaoyang Yu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5750", "name": "Xinbing Liang", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5751", "name": "Jinyu Xiang", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5752", "name": "Bang Liu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5753", "name": "Chenglin Wu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5754", "name": "Yuyu Luo", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5755", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:20.999Z", "hidden": false}], "publishedAt": "2026-02-03T17:46:16.000Z", "submittedOnDailyAt": "2026-02-04T02:34:02.843Z", "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "submittedOnDailyBy": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "isPro": false, "fullname": "jianhao ruan", "user": "Aurorra1123", "type": "user"}, "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "upvotes": 66, "discussionId": "6982c1c69084cb4f0ecb5756", "ai_summary": "AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.", "ai_keywords": ["language agents", "sub-agent-as-tools paradigm", "multi-turn task solving", "agent abstraction", "task automation", "framework-agnostic", "agent orchestration", "automatic agent creation", "Pareto-efficient", "GAIA", "SWE-Bench", "Terminal-Bench"], "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u4ee3\u7406\u5728\u4efb\u52a1\u81ea\u52a8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4ee3\u7406\u62bd\u8c61\u6a21\u578b\uff0c\u5c06\u4ee3\u7406\u89c6\u4e3a\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u6a21\u578b\u7684\u7ec4\u5408\uff0c\u4ee5\u63d0\u9ad8\u9002\u5e94\u6027\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e00\u62bd\u8c61\uff0c\u6211\u4eec\u5f00\u53d1\u4e86AOrchestra\u7cfb\u7edf\uff0c\u80fd\u591f\u52a8\u6001\u9009\u62e9\u5de5\u5177\u548c\u6267\u884c\u6a21\u578b\uff0c\u81ea\u52a8\u521b\u5efa\u4ee3\u7406\u6267\u884c\u4efb\u52a1\u3002</li>\n    <li>AOrchestra\u80fd\u591f\u51cf\u5c11\u4eba\u529b\u5de5\u7a0b\u6295\u5165\uff0c\u5e76\u652f\u6301\u591a\u79cd\u4ee3\u7406\u4f5c\u4e3a\u4efb\u52a1\u6267\u884c\u8005\u3002</li>\n    <li>\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAOrchestra\u5728\u4e0eGemini-3-Flash\u914d\u5408\u65f6\u76f8\u8f83\u4e8e\u6700\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e8616.28%\u7684\u76f8\u5bf9\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Language agents are useful for automating tasks, especially complex ones.</li>\n    <li>Existing systems struggle with flexibility in using sub-agents for different tasks.</li>\n    <li>A new framework called AOrchestra uses a model that defines agents with four key parts: Instruction, Context, Tools, and Model.</li>\n    <li>AOrchestra can create specialized agents on-the-fly, making it easier to manage tasks without heavy human input.</li>\n    <li>In tests, AOrchestra performed better than previous systems, achieving a 16.28% improvement in efficiency.</li>\n</ul>"}, "publishedAt": "2026-02-03T12:46:16.000Z", "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03786.png", "numComments": 1, "submittedBy": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "fullname": "jianhao ruan", "name": "Aurorra1123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.05386", "authors": [{"_id": "698599b14ad556f294b7ecdc", "name": "Zhenxiong Yu", "hidden": false}, {"_id": "698599b14ad556f294b7ecdd", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:50:37.407Z", "hidden": false}, {"_id": "698599b14ad556f294b7ecde", "name": "Zhiheng Jin", "hidden": false}, {"_id": "698599b14ad556f294b7ecdf", "name": "Shuhe Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ece0", "name": "Heng Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ece1", "name": "Yanlin Fei", "hidden": false}, {"_id": "698599b14ad556f294b7ece2", "name": "Lingfeng Zeng", "hidden": false}, {"_id": "698599b14ad556f294b7ece3", "name": "Fangqi Lou", "hidden": false}, {"_id": "698599b14ad556f294b7ece4", "name": "Shuo Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ece5", "name": "Tu Hu", "hidden": false}, {"_id": "698599b14ad556f294b7ece6", "name": "Jingping Liu", "hidden": false}, {"_id": "698599b14ad556f294b7ece7", "name": "Rongze Chen", "hidden": false}, {"_id": "698599b14ad556f294b7ece8", "name": "Xingyu Zhu", "hidden": false}, {"_id": "698599b14ad556f294b7ece9", "name": "Kunyi Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ecea", "name": "Chaofa Yuan", "hidden": false}, {"_id": "698599b14ad556f294b7eceb", "name": "Xin Guo", "hidden": false}, {"_id": "698599b14ad556f294b7ecec", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698599b14ad556f294b7eced", "name": "Feipeng Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ecee", "name": "Jie Huang", "hidden": false}, {"_id": "698599b14ad556f294b7ecef", "name": "Huacan Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ecf0", "name": "Ronghao Chen", "hidden": false}, {"_id": "698599b14ad556f294b7ecf1", "name": "Liwen Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:11:05.000Z", "submittedOnDailyAt": "2026-02-06T05:15:48.526Z", "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.", "upvotes": 56, "discussionId": "698599b14ad556f294b7ecf2", "githubRepo": "https://github.com/aifinlab/Spider-Sense", "githubRepoAddedBy": "user", "ai_summary": "Spider-Sense framework provides intrinsic and selective agent security through event-driven defense with intrinsic risk sensing, achieving low attack success and false positive rates with minimal latency overhead.", "ai_keywords": ["large language models", "autonomous agents", "security challenges", "mandatory checking paradigm", "event-driven defense", "Intrinsic Risk Sensing", "hierarchical defense mechanism", "lightweight similarity matching", "deep internal reasoning", "lifecycle-aware benchmark", "Attack Success Rate", "False Positive Rate"], "githubStars": 9, "organization": {"_id": "696875114bc2a5524dd8fcb7", "name": "AIFin-Lab", "fullname": "AIFin Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6b63\u5728\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u589e\u52a0\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u4ee3\u7406\u9632\u5fa1\u673a\u5236\u901a\u5e38\u5728\u9884\u5b9a\u9636\u6bb5\u5f3a\u5236\u68c0\u67e5\u5b89\u5168\u6027\uff0c\u4f46\u6211\u4eec\u8ba4\u4e3a\u6709\u6548\u7684\u5b89\u5168\u5e94\u8be5\u662f\u5185\u5728\u548c\u9009\u62e9\u6027\u7684\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Spider-Sense\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5185\u5728\u98ce\u9669\u611f\u77e5\u7684\u4e8b\u4ef6\u9a71\u52a8\u9632\u5fa1\u6846\u67b6\uff0c\u80fd\u591f\u5728\u611f\u77e5\u5230\u98ce\u9669\u65f6\u89e6\u53d1\u9632\u5fa1\u3002</li>\n    <li>Spider-Sense\u91c7\u7528\u5206\u5c42\u9632\u5fa1\u673a\u5236\uff0c\u5e73\u8861\u6548\u7387\u548c\u7cbe\u786e\u6027\uff0c\u5feb\u901f\u5904\u7406\u5df2\u77e5\u6a21\u5f0f\uff0c\u540c\u65f6\u5bf9\u6a21\u7cca\u60c5\u51b5\u8fdb\u884c\u6df1\u5165\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7S^2Bench\u57fa\u51c6\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSpider-Sense\u5728\u9632\u5fa1\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u653b\u51fb\u6210\u529f\u7387\u548c\u8bef\u62a5\u7387\u6700\u4f4e\uff0c\u5ef6\u8fdf\u4ec5\u4e3a8.3%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are becoming more independent but face new security challenges.</li>\n    <li>Current security methods often check agents at fixed points, but this approach may not be the best.</li>\n    <li>The proposed Spider-Sense framework uses Intrinsic Risk Sensing (IRS) to detect risks and activate defenses only when needed.</li>\n    <li>Spider-Sense combines quick checks for known problems with deeper analysis for uncertain situations without needing outside models.</li>\n    <li>Testing shows Spider-Sense performs well, with fewer successful attacks and false alarms, while only adding a small delay.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:11:05.000Z", "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening", "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05386.png", "numComments": 3, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "696875114bc2a5524dd8fcb7", "name": "AIFin-Lab", "fullname": "AIFin Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06717", "authors": [{"_id": "69898989beecc443208d2741", "name": "Daniil Plyusov", "hidden": false}, {"_id": "69898989beecc443208d2742", "user": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "name": "Alexey Gorbatovski", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:53.815Z", "hidden": false}, {"_id": "69898989beecc443208d2743", "name": "Boris Shaposhnikov", "hidden": false}, {"_id": "69898989beecc443208d2744", "name": "Viacheslav Sinii", "hidden": false}, {"_id": "69898989beecc443208d2745", "user": {"_id": "636e71b2b0ebc04888157b71", "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg", "isPro": false, "fullname": "Alexey Malakhov", "user": "ZeL1k7", "type": "user"}, "name": "Alexey Malakhov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:06:45.653Z", "hidden": false}, {"_id": "69898989beecc443208d2746", "user": {"_id": "62a9c8edc19f92ae443ab37f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9c8edc19f92ae443ab37f/yczqpBOntLco_2Jn4hnT7.jpeg", "isPro": false, "fullname": "Daniil Gavrilov", "user": "kefirski", "type": "user"}, "name": "Daniil Gavrilov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:57.059Z", "hidden": false}], "publishedAt": "2026-02-06T14:07:30.000Z", "submittedOnDailyAt": "2026-02-09T04:48:51.744Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "submittedOnDailyBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "upvotes": 54, "discussionId": "69898989beecc443208d2747", "ai_summary": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.", "ai_keywords": ["reinforcement learning", "verifiable rewards", "group sampling", "advantage estimation", "policy updates", "Focal loss", "GRPO", "DAPO", "CISPO", "pass@k metrics"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u901a\u5e38\u4f9d\u8d56\u4e8e\u7fa4\u4f53\u62bd\u6837\u6765\u4f30\u8ba1\u4f18\u52bf\u548c\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u3002</li>\n    <li>\u5927\u89c4\u6a21\u7fa4\u4f53\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\uff0c\u5bfc\u81f4\u5b66\u4e60\u504f\u5411\u4e8e\u5df2\u77e5\u7684\u8f68\u8ff9\u3002</li>\n    <li>\u5c0f\u89c4\u6a21\u7fa4\u4f53\u53ef\u80fd\u6f0f\u6389\u7a00\u6709\u7684\u6b63\u786e\u8f68\u8ff9\uff0c\u96c6\u4e2d\u6982\u7387\u4e8e\u5e38\u89c1\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96be\u5ea6\u611f\u77e5\u7684\u4f18\u52bf\u7f29\u653e\u7cfb\u6570\uff0c\u65e8\u5728\u964d\u4f4e\u9ad8\u6210\u529f\u7387\u63d0\u793a\u7684\u66f4\u65b0\u6743\u91cd\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u7684RLVR\u7b97\u6cd5\u4e2d\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u4e0d\u53d8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) often uses group sampling to improve learning but faces challenges with group size limitations.</li>\n    <li>Large groups are hard to manage due to computational limits, while small groups can miss important, rare successful outcomes.</li>\n    <li>The study reveals how the probability of missing rare successful paths depends on group size and that updates can unintentionally reduce focus on these paths.</li>\n    <li>A new method is introduced that adjusts how updates are made, favoring less successful prompts to improve learning efficiency.</li>\n    <li>This method shows significant performance improvements on various benchmarks without increasing computational demands or group sizes.</li>\n</ul>"}, "publishedAt": "2026-02-06T09:07:30.000Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06717.png", "numComments": 1, "submittedBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "fullname": "Alexey Gorbatovski", "name": "Myashka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06570", "authors": [{"_id": "69895518beecc443208d2680", "name": "Baichuan-M3 Team", "hidden": false}, {"_id": "69895518beecc443208d2682", "name": "Chengfeng Dou", "hidden": false}, {"_id": "69895518beecc443208d2683", "user": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "isPro": false, "fullname": "FanYang", "user": "fairyang", "type": "user"}, "name": "Fan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T14:32:20.075Z", "hidden": false}, {"_id": "69895518beecc443208d2684", "user": {"_id": "6464dd5234acce85aea186c7", "avatarUrl": "/avatars/3428029b0ae5f12885092c7aea588065.svg", "isPro": false, "fullname": "lifei", "user": "lifei926926", "type": "user"}, "name": "Fei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:07:14.055Z", "hidden": false}, {"_id": "69895518beecc443208d2685", "name": "Jiyuan Jia", "hidden": false}, {"_id": "69895518beecc443208d2686", "name": "Qiang Ju", "hidden": false}, {"_id": "69895518beecc443208d2687", "name": "Shuai Wang", "hidden": false}, {"_id": "69895518beecc443208d2688", "name": "Tianpeng Li", "hidden": false}, {"_id": "69895518beecc443208d2689", "name": "Xiangrong Zeng", "hidden": false}, {"_id": "69895518beecc443208d268a", "name": "Yijie Zhou", "hidden": false}, {"_id": "69895518beecc443208d268b", "name": "Hongda Zhang", "hidden": false}, {"_id": "69895518beecc443208d268c", "name": "Jinyang Tai", "hidden": false}, {"_id": "69895518beecc443208d268d", "name": "Linzhuang Sun", "hidden": false}, {"_id": "69895518beecc443208d268e", "user": {"_id": "6487e2e1eec01aee99cf4c10", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e2e1eec01aee99cf4c10/1U6zZ2OaUOrR1ueD5yraR.jpeg", "isPro": false, "fullname": "Peidong Guo", "user": "GuoPD", "type": "user"}, "name": "Peidong Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:36:44.605Z", "hidden": false}, {"_id": "69895518beecc443208d268f", "name": "Yichuan Mo", "hidden": false}, {"_id": "69895518beecc443208d2690", "name": "Xiaochuan Wang", "hidden": false}, {"_id": "69895518beecc443208d2691", "name": "Hengfu Cui", "hidden": false}, {"_id": "69895518beecc443208d2692", "name": "Zhishou Zhang", "hidden": false}], "publishedAt": "2026-02-06T10:08:59.000Z", "submittedOnDailyAt": "2026-02-09T03:00:35.501Z", "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making", "submittedOnDailyBy": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "isPro": false, "fullname": "FanYang", "user": "fairyang", "type": "user"}, "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.", "upvotes": 54, "discussionId": "69895518beecc443208d2693", "githubRepo": "https://github.com/baichuan-inc/Baichuan-M3-235B", "githubRepoAddedBy": "user", "ai_summary": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.", "ai_keywords": ["large language model", "clinical decision support", "proactive information acquisition", "long-horizon reasoning", "hallucination suppression", "HealthBench", "HealthBench-Hallu", "ScanBench"], "githubStars": 190, "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86Baichuan-M3\uff0c\u4e00\u4e2a\u533b\u5b66\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u4f9b\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002</li>\n    <li>\u8be5\u6a21\u578b\u514b\u670d\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u5f00\u653e\u5f0f\u54a8\u8be2\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6a21\u62df\u533b\u751f\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u4e3b\u8981\u529f\u80fd\u5305\u62ec\u4e3b\u52a8\u83b7\u53d6\u4fe1\u606f\u3001\u957f\u65f6\u95f4\u63a8\u7406\u6574\u5408\u8bc1\u636e\u4ee5\u53ca\u9002\u5e94\u6027\u6291\u5236\u5e7b\u89c9\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cBaichuan-M3\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86GPT-5.2\u3002</li>\n    <li>\u6a21\u578b\u53ef\u5728https://huggingface.co/collections/baichuan-inc/baichuan-m3\u4e0a\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Baichuan-M3 is a new medical-focused language model designed to improve clinical decision-making.</li>\n    <li>It addresses issues in current systems by actively helping doctors during open-ended consultations.</li>\n    <li>The model can gather information, connect different pieces of evidence, and reduce incorrect information.</li>\n    <li>Tests show Baichuan-M3 performs better than GPT-5.2 in medical questions and advice.</li>\n    <li>The model is available for public use online.</li>\n</ul>"}, "publishedAt": "2026-02-06T05:08:59.000Z", "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making", "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06570.png", "numComments": 2, "submittedBy": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "fullname": "FanYang", "name": "fairyang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.05843", "authors": [{"_id": "698567834ad556f294b7ec03", "user": {"_id": "64e6cf78ecce34cb442dc889", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg", "isPro": false, "fullname": "Fangzhi Xu", "user": "xufangzhi", "type": "user"}, "name": "Fangzhi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:51:15.682Z", "hidden": false}, {"_id": "698567834ad556f294b7ec04", "name": "Hang Yan", "hidden": false}, {"_id": "698567834ad556f294b7ec05", "user": {"_id": "6064a0eeb1703ddba0d458b9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png", "isPro": false, "fullname": "Qiushi", "user": "QiushiSun", "type": "user"}, "name": "Qiushi Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:17.132Z", "hidden": false}, {"_id": "698567834ad556f294b7ec06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:19.089Z", "hidden": false}, {"_id": "698567834ad556f294b7ec07", "name": "Zixian Huang", "hidden": false}, {"_id": "698567834ad556f294b7ec08", "user": {"_id": "6628859f1a5c7e6b445868c1", "avatarUrl": "/avatars/a7684d2bd0fd60824c5e810356953243.svg", "isPro": false, "fullname": "Muye Huang", "user": "MuyeHuang", "type": "user"}, "name": "Muye Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:25.705Z", "hidden": false}, {"_id": "698567834ad556f294b7ec09", "name": "Jingyang Gong", "hidden": false}, {"_id": "698567834ad556f294b7ec0a", "user": {"_id": "642b9861bb77f8456634b048", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg", "isPro": false, "fullname": "Zichen Ding", "user": "heroding77", "type": "user"}, "name": "Zichen Ding", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:15.144Z", "hidden": false}, {"_id": "698567834ad556f294b7ec0b", "name": "Kanzhi Cheng", "hidden": false}, {"_id": "698567834ad556f294b7ec0c", "name": "Yian Wang", "hidden": false}, {"_id": "698567834ad556f294b7ec0d", "name": "Xinyu Che", "hidden": false}, {"_id": "698567834ad556f294b7ec0e", "name": "Zeyi Sun", "hidden": false}, {"_id": "698567834ad556f294b7ec0f", "user": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "name": "Jian Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:51:08.819Z", "hidden": false}, {"_id": "698567834ad556f294b7ec10", "name": "Zhangyue Yin", "hidden": false}, {"_id": "698567834ad556f294b7ec11", "name": "Haoran Luo", "hidden": false}, {"_id": "698567834ad556f294b7ec12", "name": "Xuanjing Huang", "hidden": false}, {"_id": "698567834ad556f294b7ec13", "name": "Ben Kao", "hidden": false}, {"_id": "698567834ad556f294b7ec14", "name": "Jun Liu", "hidden": false}, {"_id": "698567834ad556f294b7ec15", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:35:23.699Z", "hidden": false}], "publishedAt": "2026-02-05T16:31:43.000Z", "submittedOnDailyAt": "2026-02-09T03:14:11.152Z", "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "submittedOnDailyBy": {"_id": "64e6cf78ecce34cb442dc889", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg", "isPro": false, "fullname": "Fangzhi Xu", "user": "xufangzhi", "type": "user"}, "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", "upvotes": 51, "discussionId": "698567834ad556f294b7ec16", "projectPage": "https://yayayacc.github.io/Odyssey-Home/", "githubRepo": "https://github.com/xufangzhi/Odyssey-Arena", "githubRepoAddedBy": "user", "ai_summary": "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.", "ai_keywords": ["Large Language Models", "autonomous agents", "inductive reasoning", "long-horizon planning", "agent evaluation", "transition laws", "OdysseyArena", "OdysseyArena-Lite", "OdysseyArena-Challenge"], "githubStars": 25, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u4fc3\u8fdb\u4e86\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u51fa\u73b0\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6f14\u7ece\u65b9\u6cd5\uff0c\u667a\u80fd\u4f53\u6839\u636e\u660e\u786e\u7684\u89c4\u5219\u548c\u9759\u6001\u76ee\u6807\u6267\u884c\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u81ea\u4e3b\u53d1\u73b0\u6f5c\u5728\u8f6c\u53d8\u6cd5\u5219\u7684\u91cd\u8981\u6027\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86OdysseyArena\uff0c\u91cd\u70b9\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u957f\u65f6\u95f4\u3001\u4e3b\u52a8\u548c\u5f52\u7eb3\u4e92\u52a8\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86OdysseyArena-Lite\uff0c\u63d0\u4f9b120\u4e2a\u4efb\u52a1\u6765\u6d4b\u91cf\u667a\u80fd\u4f53\u7684\u5f52\u7eb3\u6548\u7387\u548c\u957f\u65f6\u95f4\u53d1\u73b0\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5f52\u7eb3\u573a\u666f\u4e2d\u4e5f\u5b58\u5728\u7f3a\u9677\uff0c\u63ed\u793a\u51fa\u81ea\u4e3b\u53d1\u73b0\u7684\u5173\u952e\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are advancing quickly and enabling autonomous agents to operate in complex environments.</li>\n    <li>Current evaluations focus on agents following set rules and goals, which limits their ability to learn and adapt from experience.</li>\n    <li>OdysseyArena is introduced to improve how agents are evaluated by emphasizing long-term interactions and learning from experiences.</li>\n    <li>OdysseyArena-Lite offers 120 tasks to measure how well agents can learn and adapt over time.</li>\n    <li>Tests on over 15 top LLMs show that even the best models struggle with learning in dynamic situations, highlighting a major challenge for future development.</li>\n</ul>"}, "publishedAt": "2026-02-05T11:31:43.000Z", "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05843.png", "numComments": 2, "submittedBy": {"_id": "64e6cf78ecce34cb442dc889", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg", "fullname": "Fangzhi Xu", "name": "xufangzhi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 18, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.05027", "authors": [{"_id": "69871a872d626112378ad69f", "user": {"_id": "660fd34df03515e4ff3f2b64", "avatarUrl": "/avatars/0c2a29b1081ece881234acdd8ef9371a.svg", "isPro": false, "fullname": "Georgii Aparin", "user": "Egorgij21", "type": "user"}, "name": "Georgii Aparin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:34:22.635Z", "hidden": false}, {"_id": "69871a872d626112378ad6a0", "name": "Tasnima Sadekova", "hidden": false}, {"_id": "69871a872d626112378ad6a1", "name": "Alexey Rukhovich", "hidden": false}, {"_id": "69871a872d626112378ad6a2", "name": "Assel Yermekova", "hidden": false}, {"_id": "69871a872d626112378ad6a3", "name": "Laida Kushnareva", "hidden": false}, {"_id": "69871a872d626112378ad6a4", "name": "Vadim Popov", "hidden": false}, {"_id": "69871a872d626112378ad6a5", "name": "Kristian Kuznetsov", "hidden": false}, {"_id": "69871a872d626112378ad6a6", "name": "Irina Piontkovskaya", "hidden": false}], "publishedAt": "2026-02-04T20:29:16.000Z", "submittedOnDailyAt": "2026-02-09T05:28:19.089Z", "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders", "submittedOnDailyBy": {"_id": "636254dc2691058b19d9276a", "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg", "isPro": false, "fullname": "Kushnareva", "user": "Kushnareva", "type": "user"}, "summary": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.", "upvotes": 49, "discussionId": "69871a872d626112378ad6a7", "githubRepo": "https://github.com/audiosae/audiosae_demo", "githubRepoAddedBy": "user", "ai_summary": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.", "ai_keywords": ["Sparse Autoencoders", "encoder layers", "Whisper", "HuBERT", "feature steering", "false speech detections", "WER", "EEG activity", "speech perception"], "githubStars": 7, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728\u97f3\u9891\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u6df1\u5165\u63a2\u7d22\u3002</li>\n    <li>\u6211\u4eec\u5728Whisper\u548cHuBERT\u7684\u6240\u6709\u7f16\u7801\u5668\u5c42\u4e0a\u8bad\u7ec3SAEs\uff0c\u5e76\u8bc4\u4f30\u5176\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002</li>\n    <li>\u8d85\u8fc750%\u7684\u7279\u5f81\u5728\u968f\u673a\u79cd\u5b50\u95f4\u4fdd\u6301\u4e00\u81f4\uff0c\u91cd\u5efa\u8d28\u91cf\u5f97\u4ee5\u4fdd\u6301\u3002</li>\n    <li>SAE\u7279\u5f81\u80fd\u591f\u6709\u6548\u6355\u6349\u4e00\u822c\u58f0\u5b66\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5305\u62ec\u73af\u5883\u566a\u97f3\u548c\u975e\u8bed\u8a00\u58f0\u97f3\uff08\u5982\u7b11\u58f0\u3001\u8033\u8bed\uff09\u3002</li>\n    <li>SAE\u7279\u5f81\u4e0e\u4eba\u7c7b\u5728\u8bed\u8a00\u611f\u77e5\u8fc7\u7a0b\u4e2d\u7684\u8111\u7535\u6d3b\u52a8\u76f8\u5173\u8054\uff0c\u663e\u793a\u51fa\u4e0e\u4eba\u7c7b\u795e\u7ecf\u5904\u7406\u7684\u5bf9\u9f50\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Sparse Autoencoders (SAEs) help understand how neural networks work, but their use in audio is not well-studied.</li>\n    <li>We trained SAEs using the Whisper and HuBERT models and evaluated their stability and usefulness.</li>\n    <li>Over half of the features are consistent even with different initial conditions, and the quality of audio reconstruction is maintained.</li>\n    <li>SAEs can effectively separate different sounds, including environmental noises and sounds like laughter, needing only a small percentage of features to be removed to eliminate a concept.</li>\n    <li>Using SAEs improved the accuracy of speech detection in Whisper by reducing false detections by 70% with only a slight increase in word error rate.</li>\n    <li>SAE features are linked to human brain activity during speech perception, suggesting they align well with how humans process speech.</li>\n</ul>"}, "publishedAt": "2026-02-04T15:29:16.000Z", "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders", "summary": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05027.png", "numComments": 2, "submittedBy": {"_id": "636254dc2691058b19d9276a", "avatarUrl": "/avatars/36eb0e27e0e321fb0ac513f0d4d67c95.svg", "fullname": "Kushnareva", "name": "Kushnareva", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>ERNIE 5.0\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u8d85\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u76ee\u6807\u3002</li>\n    <li>ERNIE 5.0\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5f39\u6027\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f7f\u6a21\u578b\u5728\u4e0d\u540c\u8d44\u6e90\u9650\u5236\u4e0b\u7075\u6d3b\u8c03\u6574\u6027\u80fd\u548c\u5927\u5c0f\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u591a\u6a21\u6001\u6027\u80fd\uff0c\u662f\u7b2c\u4e00\u4e2a\u516c\u5f00\u62ab\u9732\u7684\u4e07\u4ebf\u53c2\u6570\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u3002</li>\n    <li>\u62a5\u544a\u4e2d\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u4e13\u5bb6\u8def\u7531\u53ef\u89c6\u5316\u548c\u5f39\u6027\u8bad\u7ec3\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u6df1\u523b\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new AI model that understands and generates text, images, videos, and audio.</li>\n    <li>It uses a unique training method that allows it to adapt to different resource needs while being efficient.</li>\n    <li>The model can create various smaller versions of itself to balance performance and resource use.</li>\n    <li>ERNIE 5.0 is the first large-scale model with a trillion parameters that works well across different types of media.</li>\n    <li>The report includes helpful visuals and analysis to support further research in this area.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aGreen-VLA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7eff\u8272\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u7684\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u3002</li>\n    <li>Green-VLA\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\u7684\u8bfe\u7a0b\uff0c\u4ece\u57fa\u7840\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5230\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u8c03\u6574\u3002</li>\n    <li>\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u4f7f\u7528\u4e863000\u5c0f\u65f6\u7684\u6f14\u793a\u6570\u636e\uff0c\u786e\u4fdd\u4e86\u6570\u636e\u8d28\u91cf\u548c\u65f6\u95f4\u5bf9\u9f50\u3002</li>\n    <li>VLA\u63a7\u5236\u5668\u5728\u63a8\u7406\u65f6\u589e\u5f3a\u4e86\u5bf9\u60c5\u8282\u8fdb\u5c55\u7684\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u76ee\u6807\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u540e\uff0c\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u65f6\u95f4\u6548\u7387\u90fd\u6709\u663e\u8457\u63d0\u9ad8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a new framework designed for the Green humanoid robot, focusing on real-world tasks and adaptability to different robot types.</li>\n    <li>It uses a five-stage learning process to build its capabilities, starting from basic models to advanced reinforcement learning.</li>\n    <li>The framework processes a large amount of data (3,000 hours of demonstrations) to train the robot effectively.</li>\n    <li>It features a flexible action system that allows one controller to manage various robotic forms like humanoids and mobile manipulators.</li>\n    <li>Tests show that Green-VLA improves the robot's success and efficiency in tasks, making it safer and more reliable in real-world applications.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u79f0\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u9886\u57df\u89c6\u9891\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u8981\u6c42\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u7ebf\u7d22\u3001\u8fdb\u884c\u4e92\u52a8\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u9a8c\u8bc1\u3002</li>\n    <li>\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u4e3a\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6837\u672c\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u95ed\u6e90\u548c\u5f00\u6e90\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u201cAgentic\u201d\u65b9\u6cd5\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u201cWorkflow\u201d\u65b9\u6cd5\u3002</li>\n    <li>\u5206\u6790\u8868\u660e\uff0c\u76ee\u6807\u6f02\u79fb\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u662f\u4e3b\u8981\u74f6\u9888\uff0cVideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often requires models to extract clues from video and retrieve information from the web.</li>\n    <li>The VideoDR benchmark is created to help improve video question answering with a focus on combining video and web information.</li>\n    <li>It includes high-quality samples across six different areas, verified through human annotation.</li>\n    <li>Tests show that different models perform differently depending on their ability to keep track of video details during long searches.</li>\n    <li>VideoDR highlights important challenges for future developments in video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Kimi K2.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u4e24\u79cd\u6a21\u5f0f\u76f8\u4e92\u589e\u5f3a\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5982\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9\u5fae\u8c03\u548c\u8054\u5408\u6587\u672c-\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u5730\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u95ee\u9898\u5e76\u540c\u65f6\u6267\u884c\u3002</li>\n    <li>Kimi K2.5\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u4ee3\u7406\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u7684\u8868\u73b0\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e76\u4e14Agent Swarm\u51cf\u5c11\u4e86\u6700\u591a4.5\u500d\u7684\u5ef6\u8fdf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kimi K2.5 is an open-source model that improves general intelligence by combining text and vision.</li>\n    <li>It uses techniques like joint training of text and vision to make them work better together.</li>\n    <li>K2.5 features Agent Swarm, a system that breaks down complex tasks into smaller tasks and runs them at the same time.</li>\n    <li>The model shows top performance in areas like coding, vision, reasoning, and other intelligent tasks.</li>\n    <li>Kimi K2.5 is released for others to use and develop further in real-world applications.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u51c6\u786e\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u8bb8\u591a\u5e94\u7528\u5f88\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4ecd\u6709\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u540d\u4e3aSocioSeg\u7684\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u62ec\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86SocioReasoner\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces contain many different types of important features, and it's important to identify them from satellite images.</li>\n    <li>Current models can identify physical features like buildings and water, but have difficulty with socially defined features like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg, which includes satellite images, digital maps, and detailed labels for social features.</li>\n    <li>We developed a framework called SocioReasoner that mimics how humans recognize and label social features using advanced reasoning techniques.</li>\n    <li>Our approach shows better performance than existing models and can generalize well to new situations; the dataset and code are available online.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>LongCat-Flash-Thinking-2601 \u662f\u4e00\u4e2a\u5f00\u6e90\u76845600\u4ebf\u53c2\u6570\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u667a\u80fd\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u65b9\u9762\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u5e94\u590d\u6742\u7684\u5de5\u5177\u4e92\u52a8\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u566a\u58f0\u3002</li>\n    <li>\u91c7\u7528 DORA \u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5728\u8d85\u8fc710000\u4e2a\u73af\u5883\u4e2d\u9ad8\u6548\u7a33\u5b9a\u7684\u8bad\u7ec3\u3002</li>\n    <li>\u4e3a\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5f15\u5165\u4e86\u91cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u901a\u8fc7\u5e76\u884c\u601d\u7ef4\u6269\u5c55\u63a8\u7406\u6df1\u5ea6\u548c\u5bbd\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source reasoning model with 560 billion parameters, designed for complex reasoning tasks.</li>\n    <li>It performs very well on various benchmarks, especially in tasks involving searching, using tools, and reasoning with tools.</li>\n    <li>The model is trained using a unique approach that combines expert training and a careful design of data and environments, improving its ability to handle complex tasks.</li>\n    <li>To improve stability and efficiency, the model uses an enhanced reinforcement learning framework for training across many different environments.</li>\n    <li>It also includes a special mode for deeper and broader reasoning that helps it perform better on complicated tasks.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u4f7f\u7528\u4e86 1.2T \u7684\u591a\u6a21\u6001\u6570\u636e\u3002</li>\n    <li>\u5b83\u91c7\u7528\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u6709\u6548\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fdb\u884c\u591a\u6837\u5316\u7684\u89c6\u89c9\u63a8\u7406\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u4ec5\u4e3a 10B\uff0c\u4f46\u5176\u6027\u80fd\u4e0e\u751a\u81f3\u8d85\u8d8a\u4e86 10 \u5230 20 \u500d\u66f4\u5927\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u548c\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>It uses a unique pre-training strategy with a large dataset and combines a language-focused encoder with a powerful decoder.</li>\n    <li>The model includes advanced techniques like Parallel Coordinated Reasoning (PaCoRe) to improve performance during testing.</li>\n    <li>Despite being much smaller than other models, STEP3-VL-10B performs exceptionally well, achieving high scores on various benchmarks.</li>\n    <li>The full model suite is available for the community, aiming to provide an efficient and effective resource for further development.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u6807\u662f\u53bb\u566a\u539f\u59cb\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002</li>\n    <li>\u7531\u4e8e\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4ee5\u53ca\u5f3a\u5927\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u51fa\u73b0\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u8fc5\u901f\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u91cd\u8981\u8d8b\u52bf\u3002</li>\n    <li>\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u4f7f\u7528LLM\u6280\u672f\u8fdb\u884c\u6570\u636e\u51c6\u5907\u7684\u6587\u732e\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u96c6\u6210\u548c\u6570\u636e\u4e30\u5bcc\u4e09\u4e2a\u4efb\u52a1\u3002</li>\n    <li>\u5bf9\u4e8e\u6bcf\u4e2a\u4efb\u52a1\uff0c\u672c\u6587\u8c03\u67e5\u4e86\u4ee3\u8868\u6027\u6280\u672f\uff0c\u5e76\u6307\u51fa\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5982\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6210\u672c\u7b49\u95ee\u9898\u3002</li>\n    <li>\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u7684\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u7684LLM-\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u534f\u8bae\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation helps clean, integrate, and enrich datasets for better use in applications like analytics and decision-making.</li>\n    <li>There is a growing interest in using advanced LLM techniques and flexible infrastructures (like Databricks) to enhance data preparation methods.</li>\n    <li>This paper reviews recent research on how LLM techniques can improve data preparation tasks, shifting from traditional methods to more dynamic and context-aware workflows.</li>\n    <li>It categorizes data preparation into three main tasks: cleaning, integration, and enrichment, while discussing the strengths and weaknesses of different techniques.</li>\n    <li>The paper highlights challenges in the field and suggests future directions for creating more scalable and reliable data systems using LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22060", "authors": [{"_id": "69817968ce18b186280960f0", "user": {"_id": "67dc162ec8c00778e8689f42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png", "isPro": false, "fullname": "Wenxuan Huang", "user": "Osilly", "type": "user"}, "name": "Wenxuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:37.268Z", "hidden": false}, {"_id": "69817968ce18b186280960f1", "user": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "name": "Yu Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:52.098Z", "hidden": false}, {"_id": "69817968ce18b186280960f2", "name": "Qiuchen Wang", "hidden": false}, {"_id": "69817968ce18b186280960f3", "name": "Zhen Fang", "hidden": false}, {"_id": "69817968ce18b186280960f4", "name": "Shaosheng Cao", "hidden": false}, {"_id": "69817968ce18b186280960f5", "name": "Zheng Chu", "hidden": false}, {"_id": "69817968ce18b186280960f6", "name": "Qingyu Yin", "hidden": false}, {"_id": "69817968ce18b186280960f7", "name": "Shuang Chen", "hidden": false}, {"_id": "69817968ce18b186280960f8", "name": "Zhenfei Yin", "hidden": false}, {"_id": "69817968ce18b186280960f9", "user": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "name": "Lin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:49.925Z", "hidden": false}, {"_id": "69817968ce18b186280960fa", "name": "Zehui Chen", "hidden": false}, {"_id": "69817968ce18b186280960fb", "name": "Yao Hu", "hidden": false}, {"_id": "69817968ce18b186280960fc", "name": "Philip Torr", "hidden": false}, {"_id": "69817968ce18b186280960fd", "name": "Feng Zhao", "hidden": false}, {"_id": "69817968ce18b186280960fe", "name": "Wanli Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "publishedAt": "2026-01-29T17:58:40.000Z", "submittedOnDailyAt": "2026-02-03T02:05:47.568Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "submittedOnDailyBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "upvotes": 127, "discussionId": "69817968ce18b186280960ff", "projectPage": "https://osilly.github.io/Vision-DeepResearch/", "githubRepo": "https://github.com/Osilly/Vision-DeepResearch", "githubRepoAddedBy": "user", "ai_summary": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.", "ai_keywords": ["multimodal large language models", "visual and textual search engines", "reasoning-then-tool-call", "multimodal deep-research", "multi-turn search", "multi-entity search", "multi-scale search", "cold-start supervision", "reinforcement learning", "end-to-end multimodal deep-research"], "githubStars": 96, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n    <li>\u4ee5\u5f80\u7684\u65b9\u6cd5\u901a\u8fc7\u201c\u63a8\u7406-\u5de5\u5177\u8c03\u7528\u201d\u6765\u589e\u5f3aMLLMs\uff0c\u5e2e\u52a9\u83b7\u53d6\u5927\u91cf\u4e8b\u5b9e\u4fe1\u606f\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\u3002</li>\n    <li>\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u201cVision-DeepResearch\u201d\u91c7\u7528\u591a\u8f6e\u3001\u591a\u5b9e\u4f53\u548c\u591a\u5c3a\u5ea6\u7684\u89c6\u89c9\u4e0e\u6587\u672c\u641c\u7d22\uff0c\u80fd\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u652f\u6301\u591a\u4e2a\u63a8\u7406\u6b65\u9aa4\u548c\u5927\u91cf\u5f15\u64ce\u4ea4\u4e92\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\u3002</li>\n    <li>\u76f8\u5173\u4ee3\u7801\u5c06\u53d1\u5e03\u5728GitHub\u4e0a\uff0c\u5730\u5740\u4e3ahttps://github.com/Osilly/Vision-DeepResearch\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal large language models (MLLMs) are doing well in visual tasks but have limitations in their knowledge and reasoning abilities.</li>\n    <li>Previous methods used simple queries for searching but struggled with complex real-world scenarios that have a lot of visual noise.</li>\n    <li>Vision-DeepResearch is a new approach that allows for more flexible and thorough searches, using multiple turns and different types of queries.</li>\n    <li>This method supports many reasoning steps and interactions, improving the MLLM's ability to gather and process information.</li>\n    <li>Vision-DeepResearch outperforms existing models and will have its code available on GitHub.</li>\n</ul>"}, "publishedAt": "2026-01-29T12:58:40.000Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22060.png", "numComments": 2, "submittedBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "fullname": "Yu Zeng", "name": "YuZeng260", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n  <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n  <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\u3002</li>\n  <li>\u4ee3\u7406\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n  <li>\u6587\u7ae0\u5c06\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n  <li>\u8ba8\u8bba\u4e86\u4ee3\u7406\u63a8\u7406\u5728\u79d1\u5b66\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning helps with problem-solving, decision-making, and inference.</li>\n    <li>Large language models (LLMs) do well in controlled situations but struggle in open and changing environments.</li>\n    <li>Agentic reasoning changes the way we view LLMs, treating them as independent agents that can learn and adapt through interaction.</li>\n    <li>The survey discusses three layers of agentic reasoning: basic capabilities, self-improvement through feedback, and teamwork among multiple agents.</li>\n    <li>It also highlights real-world applications and future challenges like personalization and effective multi-agent training.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Feb 10, 2026";
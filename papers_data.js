window.trendingPapers = {
    "today": [{"paper": {"id": "2602.01785", "authors": [{"_id": "69818a88ce18b18628096389", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:06.748Z", "hidden": false}, {"_id": "69818a88ce18b1862809638a", "user": {"_id": "68354b3e65397cd063da14e4", "avatarUrl": "/avatars/1b85fc942b41f58dac8e72bd12dd8e55.svg", "isPro": false, "fullname": "Chaoxiang Xie", "user": "bailynlove", "type": "user"}, "name": "Chaoxiang Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:31:45.296Z", "hidden": false}, {"_id": "69818a88ce18b1862809638b", "name": "Zhensu Sun", "hidden": false}, {"_id": "69818a88ce18b1862809638c", "name": "Yeheng Chen", "hidden": false}, {"_id": "69818a88ce18b1862809638d", "name": "Chenxu Zhang", "hidden": false}, {"_id": "69818a88ce18b1862809638e", "name": "Longfei Yun", "hidden": false}, {"_id": "69818a88ce18b1862809638f", "name": "Chengcheng Wan", "hidden": false}, {"_id": "69818a88ce18b18628096390", "name": "Hongyu Zhang", "hidden": false}, {"_id": "69818a88ce18b18628096391", "name": "David Lo", "hidden": false}, {"_id": "69818a88ce18b18628096392", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-02-02T08:10:21.000Z", "submittedOnDailyAt": "2026-02-04T00:15:44.767Z", "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding", "submittedOnDailyBy": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.", "upvotes": 81, "discussionId": "69818a89ce18b18628096393", "ai_summary": "Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.", "ai_keywords": ["Multimodal LLMs", "source code understanding", "token compression", "visual cues", "syntax highlighting", "code completion", "clone detection", "image modality", "visual compression", "computational efficiency"], "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u5f88\u5927\u6210\u529f\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u6210\u4e3a\u4e86\u4e00\u4e2a\u5173\u952e\u74f6\u9888\u3002</li>\n    <li>\u76ee\u524d\u8fd9\u4e9b\u6a21\u578b\u5c06\u6e90\u4ee3\u7801\u89c6\u4e3a\u7ebf\u6027\u5e8f\u5217\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u7ebf\u6027\u589e\u52a0\u3002</li>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63d0\u4f9b\u4e86\u901a\u8fc7\u5c06\u6e90\u4ee3\u7801\u8868\u793a\u4e3a\u56fe\u50cf\u6765\u4f18\u5316\u6548\u7387\u7684\u673a\u4f1a\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0cMLLMs\u5728\u7406\u89e3\u4ee3\u7801\u65f6\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe8\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u5229\u7528\u89c6\u89c9\u63d0\u793a\u6765\u63d0\u9ad8\u4ee3\u7801\u8865\u5168\u6027\u80fd\u3002</li>\n    <li>\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u5bf9\u89c6\u89c9\u538b\u7f29\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u97e7\u6027\uff0c\u67d0\u4e9b\u538b\u7f29\u6bd4\u751a\u81f3\u4f18\u4e8e\u539f\u59cb\u6587\u672c\u8f93\u5165\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are good at understanding code, but they need a lot of computing power as software grows.</li>\n    <li>Currently, LLMs treat code as a sequence of text, which increases costs as more context is added.</li>\n    <li>Multimodal LLMs (MLLMs) can represent code as images, which can be compressed better than text without losing meaning.</li>\n    <li>Our study shows that MLLMs can understand code with up to 8 times less data and benefit from visual features like syntax highlighting.</li>\n    <li>Some code tasks, like detecting similar code, work well even with compressed images, suggesting a new way to improve efficiency in code understanding.</li>\n</ul>"}, "publishedAt": "2026-02-02T03:10:21.000Z", "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding", "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01785.png", "numComments": 2, "submittedBy": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "fullname": "Yuling", "name": "YerbaPage", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 97, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.03786", "authors": [{"_id": "6982c1c69084cb4f0ecb574b", "user": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "isPro": false, "fullname": "jianhao ruan", "user": "Aurorra1123", "type": "user"}, "name": "Jianhao Ruan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:23.320Z", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574c", "name": "Zhihao Xu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574d", "name": "Yiran Peng", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574e", "name": "Fashen Ren", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574f", "name": "Zhaoyang Yu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5750", "name": "Xinbing Liang", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5751", "name": "Jinyu Xiang", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5752", "name": "Bang Liu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5753", "name": "Chenglin Wu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5754", "name": "Yuyu Luo", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5755", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:20.999Z", "hidden": false}], "publishedAt": "2026-02-03T17:46:16.000Z", "submittedOnDailyAt": "2026-02-04T02:34:02.843Z", "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "submittedOnDailyBy": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "isPro": false, "fullname": "jianhao ruan", "user": "Aurorra1123", "type": "user"}, "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "upvotes": 66, "discussionId": "6982c1c69084cb4f0ecb5756", "ai_summary": "AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.", "ai_keywords": ["language agents", "sub-agent-as-tools paradigm", "multi-turn task solving", "agent abstraction", "task automation", "framework-agnostic", "agent orchestration", "automatic agent creation", "Pareto-efficient", "GAIA", "SWE-Bench", "Terminal-Bench"], "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u4ee3\u7406\u5728\u4efb\u52a1\u81ea\u52a8\u5316\u65b9\u9762\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u590d\u6742\u7684\u957f\u671f\u4efb\u52a1\u4ecd\u9762\u4e34\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4ee3\u7406\u62bd\u8c61\u6a21\u578b\uff0c\u5c06\u4efb\u4f55\u4ee3\u7406\u8868\u793a\u4e3a\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u6a21\u578b\u7684\u7ec4\u5408\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e00\u62bd\u8c61\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aAOrchestra\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6bcf\u4e00\u6b65\u52a8\u6001\u9009\u62e9\u7528\u4e8e\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u6a21\u578b\u3002</li>\n    <li>AOrchestra\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u7a0b\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u4e14\u80fd\u591f\u4e0e\u591a\u79cd\u4ee3\u7406\u65e0\u7f1d\u8fde\u63a5\u3002</li>\n    <li>\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAOrchestra\u5728\u6027\u80fd\u4e0a\u76f8\u8f83\u4e8e\u6700\u5f3a\u7684\u57fa\u7ebf\u63d0\u5347\u4e8616.28%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Language agents are useful for automating tasks, especially complex ones that require multiple steps.</li>\n    <li>The current designs of these agents lack flexibility, making them less adaptable to different tasks.</li>\n    <li>This work introduces a new way to think about agents using a model called Instruction, Context, Tools, Model, which helps create specialized agents for specific tasks.</li>\n    <li>The system named AOrchestra uses this model to manage tasks by selecting the right tools and executing them automatically.</li>\n    <li>AOrchestra shows significant improvement in performance on various benchmarks compared to existing methods.</li>\n</ul>"}, "publishedAt": "2026-02-03T12:46:16.000Z", "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03786.png", "numComments": 1, "submittedBy": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "fullname": "jianhao ruan", "name": "Aurorra1123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02103", "authors": [{"_id": "6981ab8dce18b1862809643a", "name": "Liyan Xu", "hidden": false}, {"_id": "6981ab8dce18b1862809643b", "name": "Mo Yu", "hidden": false}, {"_id": "6981ab8dce18b1862809643c", "name": "Fandong Meng", "hidden": false}, {"_id": "6981ab8dce18b1862809643d", "name": "Jie Zhou", "hidden": false}], "publishedAt": "2026-02-02T13:46:56.000Z", "submittedOnDailyAt": "2026-02-04T01:21:43.596Z", "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "submittedOnDailyBy": {"_id": "650f0fac11f3210cf7a8a849", "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg", "isPro": false, "fullname": "Liyan Xu", "user": "lxucs", "type": "user"}, "summary": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "upvotes": 57, "discussionId": "6981ab8dce18b1862809643e", "ai_summary": "Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.", "ai_keywords": ["Chain-of-Thought", "Large Language Models", "latent planning", "hidden states", "Tele-Lens", "multi-step reasoning", "uncertainty estimation", "CoT dynamics"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u53d1\u73b0LLMs\u5728\u591a\u6b65\u9aa4\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u4ecd\u7136\u4f9d\u8d56\u4e8e\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u3002</li>\n    <li>\u901a\u8fc7\"Tele-Lens\"\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86LLMs\u7684\u9690\u85cf\u72b6\u6001\uff0c\u53d1\u73b0\u5b83\u4eec\u4e3b\u8981\u8fdb\u884c\u5c40\u90e8\u89c4\u5212\uff0c\u800c\u975e\u5168\u5c40\u89c4\u5212\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u5047\u8bbe\uff0c\u8ba4\u4e3a\u5c11\u90e8\u5206\u7684CoT\u4f4d\u7f6e\u53ef\u4ee5\u6709\u6548\u4ee3\u8868\u6574\u4e2a\u63a8\u7406\u8def\u5f84\u7684\u4e0d\u786e\u5b9a\u6027\u3002</li>\n    <li>\u5f3a\u8c03\u4e86\u5229\u7528CoT\u52a8\u6001\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u81ea\u52a8\u8bc6\u522bCoT\u7ed5\u8fc7\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This study explores how Large Language Models (LLMs) plan their reasoning, showing that they can think ahead even before using Chain-of-Thought (CoT) methods.</li>\n    <li>CoT is still crucial for tasks that need multi-step reasoning, despite the LLMs' ability to plan internally.</li>\n    <li>The researchers used a method called Tele-Lens to examine LLMs, finding they often make small, short-term decisions instead of long-term plans.</li>\n    <li>They proposed a way to improve how uncertainty is estimated in CoT, showing that just a few CoT steps can represent the overall uncertainty well.</li>\n    <li>The study also highlights the importance of understanding CoT dynamics and shows that it's possible to recognize when CoT isn't needed without losing performance.</li>\n</ul>"}, "publishedAt": "2026-02-02T08:46:56.000Z", "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "summary": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02103.png", "numComments": 1, "submittedBy": {"_id": "650f0fac11f3210cf7a8a849", "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg", "fullname": "Liyan Xu", "name": "lxucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02660", "authors": [{"_id": "6982c09b9084cb4f0ecb5724", "name": "Jiefeng Chen", "hidden": false}, {"_id": "6982c09b9084cb4f0ecb5725", "name": "Bhavana Dalvi Mishra", "hidden": false}, {"_id": "6982c09b9084cb4f0ecb5726", "name": "Jaehyun Nam", "hidden": false}, {"_id": "6982c09b9084cb4f0ecb5727", "name": "Rui Meng", "hidden": false}, {"_id": "6982c09b9084cb4f0ecb5728", "name": "Tomas Pfister", "hidden": false}, {"_id": "6982c09b9084cb4f0ecb5729", "name": "Jinsung Yoon", "hidden": false}], "publishedAt": "2026-02-02T19:00:03.000Z", "submittedOnDailyAt": "2026-02-04T01:14:39.432Z", "title": "MARS: Modular Agent with Reflective Search for Automated AI Research", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.", "upvotes": 47, "discussionId": "6982c09c9084cb4f0ecb572a", "ai_summary": "MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.", "ai_keywords": ["Monte Carlo Tree Search", "MCTS", "modular construction", "comparative reflective memory", "cross-branch transfer"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u52a8\u5316AI\u7814\u7a76\u4e0e\u4e00\u822c\u8f6f\u4ef6\u5de5\u7a0b\u4e0d\u540c\uff0c\u4e3b\u8981\u56e0\u4e3a\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6027\u80fd\u5f52\u56e0\u4e0d\u660e\u786e\u3002</li>\n    <li>\u5f53\u524d\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5e38\u5e38\u751f\u6210\u5ffd\u7565\u6267\u884c\u6210\u672c\u548c\u56e0\u679c\u56e0\u7d20\u7684\u5355\u4e00\u811a\u672c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MARS\uff08\u6a21\u5757\u5316\u53cd\u601d\u641c\u7d22\u4ee3\u7406\uff09\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u81ea\u4e3bAI\u7814\u7a76\u3002</li>\n    <li>MARS\u7684\u4e09\u4e2a\u6838\u5fc3\u8981\u7d20\u5305\u62ec\uff1a\u9884\u7b97\u610f\u8bc6\u89c4\u5212\u3001\u6a21\u5757\u5316\u6784\u5efa\u548c\u6bd4\u8f83\u53cd\u601d\u8bb0\u5fc6\u3002</li>\n    <li>MARS\u5728MLE-Bench\u4e0a\u5c55\u73b0\u51fa\u9886\u5148\u7684\u6027\u80fd\uff0c\u5e76\u4e1463%\u7684\u5b66\u4e60\u6765\u81ea\u8de8\u5206\u652f\u8f6c\u79fb\uff0c\u8868\u660e\u5176\u6709\u6548\u5730\u6982\u62ec\u4e86\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MARS is a new framework for automating AI research that considers the high costs of model training and performance evaluation.</li>\n    <li>It uses a Budget-Aware Planning system to balance performance with execution costs through a method called Monte Carlo Tree Search.</li>\n    <li>MARS organizes complex research tasks using a modular approach, breaking down the process into design, decomposition, and implementation phases.</li>\n    <li>The framework features a Comparative Reflective Memory to improve credit assignment by analyzing differences in solutions for better insights.</li>\n    <li>MARS performs well compared to other leading open-source frameworks and shows that it can learn effectively from previous experiences.</li>\n</ul>"}, "publishedAt": "2026-02-02T14:00:03.000Z", "title": "MARS: Modular Agent with Reflective Search for Automated AI Research", "summary": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02660.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.03796", "authors": [{"_id": "6982ba459084cb4f0ecb56b0", "name": "Zhixue Fang", "hidden": false}, {"_id": "6982ba459084cb4f0ecb56b1", "name": "Xu He", "hidden": false}, {"_id": "6982ba459084cb4f0ecb56b2", "name": "Songlin Tang", "hidden": false}, {"_id": "6982ba459084cb4f0ecb56b3", "name": "Haoxian Zhang", "hidden": false}, {"_id": "6982ba459084cb4f0ecb56b4", "name": "Qingfeng Li", "hidden": false}, {"_id": "6982ba459084cb4f0ecb56b5", "name": "Xiaoqiang Liu", "hidden": false}, {"_id": "6982ba459084cb4f0ecb56b6", "name": "Pengfei Wan", "hidden": false}, {"_id": "6982ba459084cb4f0ecb56b7", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66a356f2f7352f4ffbb4e74a/eYfmS5epN6LuK7w-itRi8.mp4"], "publishedAt": "2026-02-03T17:59:09.000Z", "submittedOnDailyAt": "2026-02-04T01:18:19.937Z", "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation", "submittedOnDailyBy": {"_id": "66a356f2f7352f4ffbb4e74a", "avatarUrl": "/avatars/b905cc8719eb31b29e8a94717219a79b.svg", "isPro": false, "fullname": "He", "user": "Phoebux", "type": "user"}, "summary": "Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.", "upvotes": 43, "discussionId": "6982ba459084cb4f0ecb56b8", "ai_summary": "3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.", "ai_keywords": ["motion encoder", "video generator", "motion tokens", "cross-attention", "view-rich supervision", "geometric supervision", "SMPL", "camera control", "motion fidelity", "visual quality"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e2D\u59ff\u52bf\u6216\u660e\u786e\u76843D\u53c2\u6570\u6a21\u578b\uff08\u5982SMPL\uff09\u3002</li>\n    <li>2D\u59ff\u52bf\u9650\u5236\u4e86\u8fd0\u52a8\u4e0e\u89c6\u89d2\u7684\u7ed1\u5b9a\uff0c\u65e0\u6cd5\u5408\u6210\u65b0\u89c6\u89d2\u3002</li>\n    <li>\u660e\u786e\u76843D\u6a21\u578b\u867d\u7136\u7ed3\u6784\u4fe1\u606f\u4e30\u5bcc\uff0c\u4f46\u5b58\u5728\u6df1\u5ea6\u6a21\u7cca\u548c\u52a8\u6001\u4e0d\u51c6\u786e\u7b49\u95ee\u9898\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e863DiMo\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u8fd0\u52a8\u7f16\u7801\u5668\u548c\u89c6\u9891\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u5bf9\u8fd0\u52a8\u7684\u9690\u5f0f\u3001\u65e0\u89c6\u89d2\u4f9d\u8d56\u8868\u793a\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c3DiMo\u5728\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u7075\u6d3b\u7684\u6587\u672c\u9a71\u52a8\u76f8\u673a\u63a7\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study critiques existing video generation methods that use 2D poses or explicit 3D models for human motion control.</li>\n    <li>2D poses limit the ability to create new viewpoints, while 3D models can be inaccurate and restrict the capabilities of video generators.</li>\n    <li>Introducing 3DiMo, a new approach that uses an implicit, view-agnostic motion representation for better alignment with video generation.</li>\n    <li>This method trains a motion encoder and a video generator together, creating compact motion tokens that work across different camera views.</li>\n    <li>Experiments show that 3DiMo produces high-quality and accurate motion reproduction with more flexible camera control compared to previous methods.</li>\n</ul>"}, "publishedAt": "2026-02-03T12:59:09.000Z", "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation", "summary": "Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66a356f2f7352f4ffbb4e74a/eYfmS5epN6LuK7w-itRi8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03796.png", "numComments": 5, "submittedBy": {"_id": "66a356f2f7352f4ffbb4e74a", "avatarUrl": "/avatars/b905cc8719eb31b29e8a94717219a79b.svg", "fullname": "He", "name": "Phoebux", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02619", "authors": [{"_id": "6982cefe9084cb4f0ecb57a9", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:27:54.679Z", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57aa", "name": "Dayuan Fu", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57ab", "name": "Junhao Shi", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57ac", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62dce08bb2c60f29c3d0a5da/dFRFamdOyPbR1OxQC_qOV.png", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:27:51.423Z", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57ad", "name": "Weiye Si", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57ae", "user": {"_id": "668e476520e499a0786ea56e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668e476520e499a0786ea56e/lnvd1_UWW9o9ddrR6ehwR.png", "isPro": false, "fullname": "Keyu Li (SII)", "user": "weizhihao1", "type": "user"}, "name": "Keyu Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:27:49.277Z", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57af", "name": "Xuefeng Li", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57b0", "name": "Yang Xiao", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57b1", "name": "Wenjie Li", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57b2", "name": "Dequan Wang", "hidden": false}, {"_id": "6982cefe9084cb4f0ecb57b3", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-02-02T13:23:39.000Z", "submittedOnDailyAt": "2026-02-04T02:17:05.727Z", "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...", "upvotes": 43, "discussionId": "6982cefe9084cb4f0ecb57b4", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Agency", "githubRepoAddedBy": "user", "ai_summary": "Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix histories.", "ai_keywords": ["Large Language Models", "long-horizon agentic workflows", "training data", "long-dependency structures", "cross-stage evolutionary dynamics", "data synthesis", "Pull Request sequences", "task decomposition", "functional coherence", "bug-fix histories", "daVinci-Agency", "continuous commits", "unified functional objectives", "verifiable refinement", "causal dependencies", "iterative refinements", "goal-directed behavior", "project-level task modeling", "Toolathlon"], "githubStars": 25, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77ed\u671f\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u957f\u65f6\u95f4\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u9047\u5230\u56f0\u96be\u3002</li>\n    <li>\u4e3b\u8981\u95ee\u9898\u662f\u7f3a\u4e4f\u771f\u5b9e\u7684\u8bad\u7ec3\u6570\u636e\u6765\u6355\u6349\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u548c\u8de8\u9636\u6bb5\u7684\u6f14\u53d8\u52a8\u6001\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u8f6f\u4ef6\u6f14\u53d8\u7684\u89c6\u89d2\u91cd\u65b0\u601d\u8003\u6570\u636e\u5408\u6210\uff0c\u53d1\u73b0\u62c9\u53d6\u8bf7\u6c42\uff08PR\uff09\u5e8f\u5217\u53ef\u4ee5\u63d0\u4f9b\u957f\u671f\u5b66\u4e60\u7684\u76d1\u7763\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86daVinci-Agency\uff0c\u901a\u8fc7\u4e09\u4e2a\u673a\u5236\u7cfb\u7edf\u6027\u5730\u6316\u6398PR\u94fe\u7684\u7ed3\u6784\u5316\u76d1\u7763\u3002</li>\n    <li>daVinci-Agency\u7684\u7ed3\u679c\u663e\u793a\u51fa\u6570\u636e\u7684\u9ad8\u6548\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) struggle with long-term tasks because of a lack of training data that captures complex, evolving processes.</li>\n    <li>Current methods for generating training data often require expensive human input or are limited in their scope.</li>\n    <li>The authors propose a new approach using Pull Request (PR) sequences from software development, which provide valuable insights for long-term learning.</li>\n    <li>The daVinci-Agency system uses PR sequences to create structured training data that reflects real-world task progression and refinement.</li>\n    <li>Fine-tuning models with daVinci-Agency samples shows significant performance improvements, demonstrating its effectiveness for long-term task modeling.</li>\n</ul>"}, "publishedAt": "2026-02-02T08:23:39.000Z", "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently", "summary": "While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02619.png", "numComments": 1, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.01630", "authors": [{"_id": "6982b3dd9084cb4f0ecb564b", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:47.439Z", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb564c", "user": {"_id": "6708920aeae29d1cd41a703b", "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg", "isPro": false, "fullname": "kaixin zhu", "user": "czkk566", "type": "user"}, "name": "Kaixin Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:49.804Z", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb564d", "name": "Daili Hua", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb564e", "name": "Bozhou Li", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb564f", "name": "Chengzhuo Tong", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5650", "user": {"_id": "65e71ef39cf349af2940b317", "avatarUrl": "/avatars/fc1cd8d3510946fc947d67b16b51834b.svg", "isPro": false, "fullname": "Yuran Wang", "user": "Ryann829", "type": "user"}, "name": "Yuran Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:54.435Z", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5651", "name": "Xinyi Huang", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5652", "user": {"_id": "674e77fa59a127e4eacf5dba", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674e77fa59a127e4eacf5dba/W7qr94Buvvaio8zhKrEha.jpeg", "isPro": false, "fullname": "Yifan Dai", "user": "Moonwines", "type": "user"}, "name": "Yifan Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:52.281Z", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5653", "name": "Zixiang Zhang", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5654", "name": "Yifan Yang", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5655", "name": "Zhou Liu", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5656", "name": "Hao Liang", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5657", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5658", "name": "Ruichuan An", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5659", "name": "Tianyi Bai", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb565a", "name": "Hongcheng Gao", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb565b", "name": "Junbo Niu", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb565c", "name": "Yang Shi", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb565d", "name": "Xinlong Chen", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb565e", "name": "Yue Ding", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb565f", "name": "Minglei Shi", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5660", "name": "Kai Zeng", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5661", "name": "Yiwen Tang", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5662", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5663", "name": "Pengfei Wan", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5664", "name": "Xintao Wang", "hidden": false}, {"_id": "6982b3dd9084cb4f0ecb5665", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2026-02-02T04:42:44.000Z", "submittedOnDailyAt": "2026-02-04T00:21:41.173Z", "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.", "upvotes": 41, "discussionId": "6982b3de9084cb4f0ecb5666", "ai_summary": "Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.", "ai_keywords": ["world models", "physical dynamics", "environment interaction", "visual prediction", "3D estimation", "symbol grounding", "unified framework", "normative framework", "spatial representation"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4e16\u754c\u6a21\u578b\u662f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u91cd\u8981\u9886\u57df\uff0c\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u7269\u7406\u52a8\u6001\u548c\u4e16\u754c\u77e5\u8bc6\u6765\u589e\u5f3a\u5927\u578b\u6a21\u578b\u3002</li>\n    <li>\u5f53\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5c06\u4e16\u754c\u77e5\u8bc6\u5e94\u7528\u4e8e\u5355\u4e00\u4efb\u52a1\u4e0a\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5b9a\u4e49\u6216\u6846\u67b6\u3002</li>\n    <li>\u867d\u7136\u4efb\u52a1\u7279\u5b9a\u7684\u6574\u5408\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u6574\u4f53\u7406\u89e3\u6240\u9700\u7684\u7cfb\u7edf\u6027\u4e00\u81f4\u6027\u3002</li>\n    <li>\u672c\u6587\u5206\u6790\u4e86\u8fd9\u4e9b\u96f6\u6563\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e16\u754c\u6a21\u578b\u8bbe\u8ba1\u89c4\u8303\u3002</li>\n    <li>\u4e00\u4e2a\u5f3a\u5927\u7684\u4e16\u754c\u6a21\u578b\u5e94\u6574\u5408\u4e92\u52a8\u3001\u611f\u77e5\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7a7a\u95f4\u8868\u793a\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u80fd\u529b\u96c6\u5408\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>World models are important in AI research to help large models understand and interact with the real world.</li>\n    <li>Current research is scattered, focusing on specific tasks like visual prediction or 3D estimation without a common framework.</li>\n    <li>Task-specific approaches improve performance but lack a comprehensive understanding of the world.</li>\n    <li>The paper critiques these fragmented methods and proposes a unified design for world models.</li>\n    <li>A strong world model should integrate interaction, perception, reasoning, and spatial representation in a cohesive way.</li>\n</ul>"}, "publishedAt": "2026-02-01T23:42:44.000Z", "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "summary": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01630.png", "numComments": 2, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.03048", "authors": [{"_id": "6982c1039084cb4f0ecb572c", "user": {"_id": "6708edcae69f6e30a816af9f", "avatarUrl": "/avatars/c4daa9b0cb2f4bb2a7db0e78b22034cb.svg", "isPro": false, "fullname": "Yao", "user": "distant-yuan", "type": "user"}, "name": "Zhiyuan Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:25.743Z", "hidden": false}, {"_id": "6982c1039084cb4f0ecb572d", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6982c1039084cb4f0ecb572e", "name": "Yuxin Chen", "hidden": false}, {"_id": "6982c1039084cb4f0ecb572f", "name": "Yueqing Sun", "hidden": false}, {"_id": "6982c1039084cb4f0ecb5730", "name": "Zishan Xu", "hidden": false}, {"_id": "6982c1039084cb4f0ecb5731", "name": "Yu Yang", "hidden": false}, {"_id": "6982c1039084cb4f0ecb5732", "name": "Tianhao Hu", "hidden": false}, {"_id": "6982c1039084cb4f0ecb5733", "name": "Qi Gu", "hidden": false}, {"_id": "6982c1039084cb4f0ecb5734", "name": "Hui Su", "hidden": false}, {"_id": "6982c1039084cb4f0ecb5735", "name": "Xunliang Cai", "hidden": false}], "publishedAt": "2026-02-03T03:14:36.000Z", "submittedOnDailyAt": "2026-02-04T01:16:22.296Z", "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.", "upvotes": 32, "discussionId": "6982c1049084cb4f0ecb5736", "ai_summary": "CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "GRPO", "rollout budget", "adaptive methods", "Capability-Oriented Value function", "heap-based greedy strategy", "exploration and exploitation", "LLM post-training efficiency"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5CoBA-RL\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>CoBA-RL\u80fd\u591f\u6839\u636e\u6a21\u578b\u7684\u5b66\u4e60\u72b6\u6001\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u5206\u914d\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u7edf\u4e00\u7684\u9884\u7b97\u3002</li>\n    <li>\u8be5\u7b97\u6cd5\u5229\u7528\u80fd\u529b\u5bfc\u5411\u7684\u4ef7\u503c\u51fd\u6570\u6765\u8bc4\u4f30\u4efb\u52a1\u7684\u6f5c\u5728\u8bad\u7ec3\u6536\u76ca\u3002</li>\n    <li>\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cCoBA-RL\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u6709\u6548\u5e73\u8861\uff0c\u63d0\u9ad8\u4e86\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u6cdb\u5316\u6027\u80fd\u3002</li>\n    <li>\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5316\u6837\u672c\u8bad\u7ec3\u4ef7\u503c\u548c\u4f18\u5316\u9884\u7b97\u5206\u914d\u5bf9\u4e8e\u63d0\u5347LLM\u7684\u540e\u7eed\u8bad\u7ec3\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) improves reasoning in large language models (LLMs).</li>\n    <li>Current methods use a fixed rollout budget, which can waste resources and not adapt to the model's learning needs.</li>\n    <li>CoBA-RL is a new algorithm that adjusts the rollout budget based on how well the model is learning.</li>\n    <li>It uses a special value function to identify tasks that will help the model learn the most and allocates resources accordingly.</li>\n    <li>Experiments show that CoBA-RL balances exploration and exploitation effectively, leading to better performance on various tasks.</li>\n</ul>"}, "publishedAt": "2026-02-02T22:14:36.000Z", "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03048.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.03139", "authors": [{"_id": "6982d8449084cb4f0ecb580d", "name": "Tianhe Wu", "hidden": false}, {"_id": "6982d8449084cb4f0ecb580e", "name": "Ruibin Li", "hidden": false}, {"_id": "6982d8449084cb4f0ecb580f", "name": "Lei Zhang", "hidden": false}, {"_id": "6982d8449084cb4f0ecb5810", "name": "Kede Ma", "hidden": false}], "publishedAt": "2026-02-03T05:45:25.000Z", "submittedOnDailyAt": "2026-02-04T02:57:31.839Z", "title": "Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis", "submittedOnDailyBy": {"_id": "655de51982afda0fc479fb91", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655de51982afda0fc479fb91/-t9RLNEBAESO0niQGHoss.png", "isPro": false, "fullname": "Tianhe Wu", "user": "TianheWu", "type": "user"}, "summary": "Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.", "upvotes": 31, "discussionId": "6982d8449084cb4f0ecb5811", "githubRepo": "https://github.com/Multimedia-Analytics-Laboratory/dpdmd", "githubRepoAddedBy": "user", "ai_summary": "A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.", "ai_keywords": ["distribution matching distillation", "mode collapse", "reverse-KL formulation", "v-prediction", "text-to-image generation", "sample diversity", "quality refinement"], "githubStars": 43, "organization": {"_id": "660f70a49760d0856d246a35", "name": "CityU-HongKong", "fullname": "City University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660f6d7951c7a7d619e75393/BXxWP_bnnPM4OLq6pGpFc.png"}, "summary_zh": "<ul>\n    <li>\u5206\u5e03\u5339\u914d\u84b8\u998f\uff08DMD\uff09\u65e8\u5728\u5c06\u591a\u6b65\u9aa4\u751f\u6210\u5668\u4e0e\u5c11\u6b65\u9aa4\u751f\u6210\u5668\u5bf9\u9f50\uff0c\u4ee5\u964d\u4f4e\u63a8\u7406\u6210\u672c\u5e76\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>DMD\u53ef\u80fd\u4f1a\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\uff0c\u56e0\u4e3a\u5176\u53cd\u5411KL\u516c\u5f0f\u4f1a\u5bfc\u81f4\u6a21\u5f0f\u8ffd\u6c42\u884c\u4e3a\uff0c\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u4e0d\u7a33\u5b9a\u7684\u8bad\u7ec3\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89d2\u8272\u5206\u79bb\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u84b8\u998f\u6b65\u9aa4\u7684\u89d2\u8272\u660e\u786e\u533a\u5206\uff1a\u7b2c\u4e00\u6b65\u4e13\u6ce8\u4e8e\u4fdd\u6301\u6837\u672c\u591a\u6837\u6027\uff0c\u540e\u7eed\u6b65\u9aa4\u5219\u6ce8\u91cd\u8d28\u91cf\u63d0\u5347\u3002</li>\n    <li>\u6211\u4eec\u79f0\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u4fdd\u7559\u591a\u6837\u6027\u7684DMD\uff08DP-DMD\uff09\uff0c\u5b83\u6ca1\u6709\u590d\u6742\u7684\u611f\u77e5\u7f51\u7edc\u3001\u9274\u522b\u5668\u6216\u989d\u5916\u7684\u771f\u5b9e\u56fe\u50cf\u3002</li>\n    <li>DP-DMD\u5728\u5927\u91cf\u6587\u672c\u5230\u56fe\u50cf\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4fdd\u6301\u6837\u672c\u591a\u6837\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study introduces Diversity-Preserved DMD (DP-DMD) to improve the generation quality of images using fewer steps.</li>\n    <li>DP-DMD prevents mode collapse by separating the roles of different steps in the generation process.</li>\n    <li>The first step focuses on keeping sample diversity, while later steps refine the quality of the images.</li>\n    <li>DP-DMD does not require complex systems like perceptual networks or additional ground-truth images, making it simpler and more efficient.</li>\n    <li>In tests, DP-DMD maintains high visual quality comparable to the best current methods while ensuring diverse outputs.</li>\n</ul>"}, "publishedAt": "2026-02-03T00:45:25.000Z", "title": "Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis", "summary": "Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03139.png", "numComments": 2, "submittedBy": {"_id": "655de51982afda0fc479fb91", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655de51982afda0fc479fb91/-t9RLNEBAESO0niQGHoss.png", "fullname": "Tianhe Wu", "name": "TianheWu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "660f70a49760d0856d246a35", "name": "CityU-HongKong", "fullname": "City University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660f6d7951c7a7d619e75393/BXxWP_bnnPM4OLq6pGpFc.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.03419", "authors": [{"_id": "6982d38d9084cb4f0ecb57df", "name": "Shuang Sun", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e0", "name": "Huatong Song", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e1", "name": "Lisheng Huang", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e2", "name": "Jinhao Jiang", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e3", "name": "Ran Le", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e4", "name": "Zhihao Lv", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e5", "name": "Zongchao Chen", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e6", "name": "Yiwen Hu", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e7", "name": "Wenyang Luo", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e8", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57e9", "name": "Yang Song", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57ea", "name": "Hongteng Xu", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57eb", "name": "Tao Zhang", "hidden": false}, {"_id": "6982d38d9084cb4f0ecb57ec", "name": "Ji-Rong Wen", "hidden": false}], "publishedAt": "2026-02-03T11:44:39.000Z", "submittedOnDailyAt": "2026-02-04T02:52:59.734Z", "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments", "submittedOnDailyBy": {"_id": "61b8405b516a20acdf3b85ff", "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg", "isPro": false, "fullname": "Jinhao Jiang", "user": "Boru", "type": "user"}, "summary": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World", "upvotes": 29, "discussionId": "6982d38d9084cb4f0ecb57ed", "ai_summary": "A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.", "ai_keywords": ["large language models", "software engineering agents", "Docker-free framework", "surrogate models", "agent-environment interaction", "test-time scaling", "SWE-bench", "Qwen2.5-Coder-32B"], "organization": {"_id": "6704ef33935b1a7c59795566", "name": "RUC-AIBOX", "fullname": "RUC-AIBOX", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\u4f7f\u5f97\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u4ee3\u7801\u4fee\u6539\u4efb\u52a1\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bb9\u5668\u5316\u73af\u5883\u7684\u6267\u884c\u53cd\u9988\uff0c\u8fd9\u79cd\u65b9\u5f0f\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u96be\u4ee5\u7ef4\u62a4\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SWE-World\uff0c\u4e00\u4e2a\u65e0\u9700Docker\u7684\u6846\u67b6\uff0c\u7528\u5b66\u4e60\u7684\u66ff\u4ee3\u54c1\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u3002</li>\n    <li>SWE-World\u5229\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u578b\u9884\u6d4b\u6267\u884c\u7ed3\u679c\uff0c\u4ece\u800c\u907f\u514d\u4e0e\u771f\u5b9e\u73af\u5883\u7684\u4ea4\u4e92\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSWE-World\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u7684\u5904\u7406\u6027\u80fd\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New technology called SWE-World helps software engineering agents modify code without needing to run programs in physical environments like Docker.</li>\n    <li>SWE-World uses learned models to predict how code changes will perform, reducing the need for resource-heavy setups.</li>\n    <li>This method keeps the usual process of how agents learn while making it easier and cheaper to train and test them.</li>\n    <li>SWE-World allows for selecting the best coding solutions from multiple attempts, improving efficiency in software tasks.</li>\n    <li>Tests showed significant performance improvements for the Qwen2.5-Coder-32B model using SWE-World compared to traditional methods.</li>\n</ul>"}, "publishedAt": "2026-02-03T06:44:39.000Z", "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments", "summary": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03419.png", "numComments": 1, "submittedBy": {"_id": "61b8405b516a20acdf3b85ff", "avatarUrl": "/avatars/3d2eae7c163a80b73260087b05a4230b.svg", "fullname": "Jinhao Jiang", "name": "Boru", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6704ef33935b1a7c59795566", "name": "RUC-AIBOX", "fullname": "RUC-AIBOX", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aGreen-VLA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8ba9\u7eff\u8272\u4eba\u5f62\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6267\u884c\u4efb\u52a1\u3002</li>\n    <li>Green-VLA\u6846\u67b6\u5206\u4e3a\u4e94\u4e2a\u9636\u6bb5\uff0c\u5305\u62ec\u57fa\u7840VLM\u3001\u8de8\u6a21\u6001\u5bf9\u63a5\u3001\u591a\u4f53\u9002\u5e94\u7b49\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u4e863000\u5c0f\u65f6\u7684\u6f14\u793a\u6570\u636e\uff0c\u7ed3\u5408\u65f6\u95f4\u5bf9\u9f50\u548c\u8d28\u91cf\u8fc7\u6ee4\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u5904\u7406\u6548\u7387\u3002</li>\n    <li>\u5728\u63a8\u7406\u9636\u6bb5\uff0cVLA\u63a7\u5236\u5668\u901a\u8fc7\u9884\u6d4b\u8fdb\u5c55\u3001\u68c0\u6d4b\u5f02\u5e38\u548c\u8054\u5408\u9884\u6d4b\u7b49\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u76ee\u6807\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u540e\uff0c\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u671f\u6548\u7387\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a new framework designed for the Green humanoid robot, allowing it to perform tasks in real-world situations.</li>\n    <li>The framework has five stages: starting with basic language models, then moving to understanding multiple types of input, training on different robot types, adapting to specific robots, and finally aligning with reinforcement learning for better decision-making.</li>\n    <li>It uses a large data set of 3,000 hours of demonstrations to improve the robot's performance, ensuring the actions are suitable for various robot types.</li>\n    <li>During operation, the system includes features for predicting progress, detecting unusual situations, and guiding actions to enhance safety and accuracy.</li>\n    <li>Tests on different robots show that the framework improves success rates, robustness, and efficiency in completing tasks.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Kimi K2.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63a8\u52a8\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u8fd9\u4e24\u79cd\u6a21\u5f0f\u76f8\u4e92\u589e\u5f3a\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5982\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9\u5fae\u8c03\u548c\u8054\u5408\u6587\u672c-\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u6211\u5bfc\u5411\u7684\u5e76\u884c\u4ee3\u7406\u534f\u8c03\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u5c06\u590d\u6742\u4efb\u52a1\u62c6\u5206\u4e3a\u5f02\u6784\u5b50\u95ee\u9898\u5e76\u540c\u65f6\u6267\u884c\u3002</li>\n    <li>\u8bc4\u4f30\u8868\u660e\uff0cKimi K2.5\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kimi K2.5 is a new open-source model that combines text and vision to improve intelligent behavior.</li>\n    <li>It uses various techniques to make text and vision work together better, like joint pre-training and reinforcement learning.</li>\n    <li>K2.5 includes a feature called Agent Swarm, which helps break down complex tasks into smaller parts that can be done at the same time.</li>\n    <li>The model has shown excellent performance in areas like coding, vision, reasoning, and intelligent tasks.</li>\n    <li>Kimi K2.5 can work faster, reducing wait times by up to 4.5 times compared to using a single agent.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.22060", "authors": [{"_id": "69817968ce18b186280960f0", "user": {"_id": "67dc162ec8c00778e8689f42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png", "isPro": false, "fullname": "Wenxuan Huang", "user": "Osilly", "type": "user"}, "name": "Wenxuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:37.268Z", "hidden": false}, {"_id": "69817968ce18b186280960f1", "user": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "name": "Yu Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:52.098Z", "hidden": false}, {"_id": "69817968ce18b186280960f2", "name": "Qiuchen Wang", "hidden": false}, {"_id": "69817968ce18b186280960f3", "name": "Zhen Fang", "hidden": false}, {"_id": "69817968ce18b186280960f4", "name": "Shaosheng Cao", "hidden": false}, {"_id": "69817968ce18b186280960f5", "name": "Zheng Chu", "hidden": false}, {"_id": "69817968ce18b186280960f6", "name": "Qingyu Yin", "hidden": false}, {"_id": "69817968ce18b186280960f7", "name": "Shuang Chen", "hidden": false}, {"_id": "69817968ce18b186280960f8", "name": "Zhenfei Yin", "hidden": false}, {"_id": "69817968ce18b186280960f9", "user": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "name": "Lin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:49.925Z", "hidden": false}, {"_id": "69817968ce18b186280960fa", "name": "Zehui Chen", "hidden": false}, {"_id": "69817968ce18b186280960fb", "name": "Yao Hu", "hidden": false}, {"_id": "69817968ce18b186280960fc", "name": "Philip Torr", "hidden": false}, {"_id": "69817968ce18b186280960fd", "name": "Feng Zhao", "hidden": false}, {"_id": "69817968ce18b186280960fe", "name": "Wanli Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "publishedAt": "2026-01-29T17:58:40.000Z", "submittedOnDailyAt": "2026-02-03T02:05:47.568Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "submittedOnDailyBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "upvotes": 127, "discussionId": "69817968ce18b186280960ff", "projectPage": "https://osilly.github.io/Vision-DeepResearch/", "githubRepo": "https://github.com/Osilly/Vision-DeepResearch", "githubRepoAddedBy": "user", "ai_summary": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.", "ai_keywords": ["multimodal large language models", "visual and textual search engines", "reasoning-then-tool-call", "multimodal deep-research", "multi-turn search", "multi-entity search", "multi-scale search", "cold-start supervision", "reinforcement learning", "end-to-end multimodal deep-research"], "githubStars": 96, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n    <li>\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u201c\u63a8\u7406-\u7136\u540e-\u5de5\u5177\u8c03\u7528\u201d\u7684\u7b56\u7565\u6765\u589e\u5f3a\u6a21\u578b\uff0c\u4f46\u5047\u8bbe\u67e5\u8be2\u65b9\u5f0f\u8fc7\u4e8e\u7b80\u5355\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e2d\u7684\u590d\u6742\u60c5\u51b5\u3002</li>\n    <li>\u65b0\u63d0\u51fa\u7684Vision-DeepResearch\u91c7\u7528\u591a\u8f6e\u3001\u591a\u5b9e\u4f53\u548c\u591a\u5c3a\u5ea6\u7684\u641c\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u89c6\u89c9\u566a\u58f0\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u66f4\u591a\u7684\u63a8\u7406\u6b65\u9aa4\u548c\u5f15\u64ce\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u3002</li>\n    <li>Vision-DeepResearch\u5728\u6027\u80fd\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\uff0c\u5e76\u5c06\u5728GitHub\u4e0a\u53d1\u5e03\u4ee3\u7801\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal large language models (MLLMs) are good at vision tasks but need improvements for factual information retrieval.</li>\n    <li>Current methods often assume one simple query is enough, which doesn't work well in real-life situations with a lot of visual noise.</li>\n    <li>The proposed solution, Vision-DeepResearch, allows for multiple searches and reasoning steps to gather information from various sources.</li>\n    <li>This new approach has better performance than existing multimodal models and integrates deep-research capabilities effectively.</li>\n    <li>The code for Vision-DeepResearch will be available for public use on GitHub.</li>\n</ul>"}, "publishedAt": "2026-01-29T12:58:40.000Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22060.png", "numComments": 2, "submittedBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "fullname": "Yu Zeng", "name": "YuZeng260", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02185", "authors": [{"_id": "69817bb2ce18b18628096119", "user": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "name": "Yu Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:45.575Z", "hidden": false}, {"_id": "69817bb2ce18b1862809611a", "user": {"_id": "67dc162ec8c00778e8689f42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png", "isPro": false, "fullname": "Wenxuan Huang", "user": "Osilly", "type": "user"}, "name": "Wenxuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:32.269Z", "hidden": false}, {"_id": "69817bb2ce18b1862809611b", "name": "Zhen Fang", "hidden": false}, {"_id": "69817bb2ce18b1862809611c", "name": "Shuang Chen", "hidden": false}, {"_id": "69817bb2ce18b1862809611d", "name": "Yufan Shen", "hidden": false}, {"_id": "69817bb2ce18b1862809611e", "name": "Yishuo Cai", "hidden": false}, {"_id": "69817bb2ce18b1862809611f", "name": "Xiaoman Wang", "hidden": false}, {"_id": "69817bb2ce18b18628096120", "name": "Zhenfei Yin", "hidden": false}, {"_id": "69817bb2ce18b18628096121", "user": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "name": "Lin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:35.014Z", "hidden": false}, {"_id": "69817bb2ce18b18628096122", "name": "Zehui Chen", "hidden": false}, {"_id": "69817bb2ce18b18628096123", "user": {"_id": "66ae3fbf491b555fef3bac0c", "avatarUrl": "/avatars/47353470d46097ce108d32792dbbf2a2.svg", "isPro": false, "fullname": "Shiting Huang", "user": "chocckaka", "type": "user"}, "name": "Shiting Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:20.349Z", "hidden": false}, {"_id": "69817bb2ce18b18628096124", "user": {"_id": "665ec9ef60c9027be03b4a18", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665ec9ef60c9027be03b4a18/a3iwo6fR2969nL9YyvJM1.jpeg", "isPro": false, "fullname": "Yiming Zhao", "user": "gaotiexinqu", "type": "user"}, "name": "Yiming Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:43.236Z", "hidden": false}, {"_id": "69817bb2ce18b18628096125", "name": "Yao Hu", "hidden": false}, {"_id": "69817bb2ce18b18628096126", "name": "Philip Torr", "hidden": false}, {"_id": "69817bb2ce18b18628096127", "name": "Wanli Ouyang", "hidden": false}, {"_id": "69817bb2ce18b18628096128", "name": "Shaosheng Cao", "hidden": false}], "publishedAt": "2026-02-02T14:53:11.000Z", "submittedOnDailyAt": "2026-02-03T02:09:42.355Z", "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models", "submittedOnDailyBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "summary": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "upvotes": 108, "discussionId": "69817bb3ce18b18628096129", "projectPage": "https://osilly.github.io/Vision-DeepResearch/", "ai_summary": "Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.", "ai_keywords": ["Multimodal Large Language Models", "Vision-DeepResearch", "visual-textual fact-finding", "VQA", "vision search", "text-search", "visual retrieval", "multimodal deep-research systems"], "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u652f\u6301\u4f7f\u7528\u641c\u7d22\u5f15\u64ce\u7684\u89c6\u89c9-\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u7f3a\u4e4f\u4ee5\u89c6\u89c9\u641c\u7d22\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\uff0c\u4ee5\u53ca\u8bc4\u4f30\u573a\u666f\u8fc7\u4e8e\u7406\u60f3\u5316\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVision-DeepResearch\u57fa\u51c6\uff08VDR-Bench\uff09\uff0c\u5305\u542b2000\u4e2aVQA\u5b9e\u4f8b\u3002</li>\n    <li>\u6240\u6709\u95ee\u9898\u7ecf\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u548c\u4e13\u5bb6\u5ba1\u6838\uff0c\u65e8\u5728\u8bc4\u4f30\u89c6\u89c9-\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u88c1\u526a\u641c\u7d22\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u5f53\u524dMLLMs\u5728\u89c6\u89c9\u68c0\u7d22\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5728\u672a\u6765\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal Large Language Models (MLLMs) are now helping with visual and textual fact-finding using search engines.</li>\n    <li>Current evaluation methods for these models have two main problems: they rely too much on text clues and do not challenge the models enough.</li>\n    <li>The new Vision-DeepResearch benchmark (VDR-Bench) consists of 2,000 carefully curated VQA questions to better test model performance in real-world conditions.</li>\n    <li>A new multi-round cropped-search method is introduced to improve the visual search capabilities of MLLMs.</li>\n    <li>This research aims to guide the development of future multimodal deep-research systems, with the code available online.</li>\n</ul>"}, "publishedAt": "2026-02-02T09:53:11.000Z", "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models", "summary": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02185.png", "numComments": 1, "submittedBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "fullname": "Yu Zeng", "name": "YuZeng260", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.01785", "authors": [{"_id": "69818a88ce18b18628096389", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:06.748Z", "hidden": false}, {"_id": "69818a88ce18b1862809638a", "user": {"_id": "68354b3e65397cd063da14e4", "avatarUrl": "/avatars/1b85fc942b41f58dac8e72bd12dd8e55.svg", "isPro": false, "fullname": "Chaoxiang Xie", "user": "bailynlove", "type": "user"}, "name": "Chaoxiang Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:31:45.296Z", "hidden": false}, {"_id": "69818a88ce18b1862809638b", "name": "Zhensu Sun", "hidden": false}, {"_id": "69818a88ce18b1862809638c", "name": "Yeheng Chen", "hidden": false}, {"_id": "69818a88ce18b1862809638d", "name": "Chenxu Zhang", "hidden": false}, {"_id": "69818a88ce18b1862809638e", "name": "Longfei Yun", "hidden": false}, {"_id": "69818a88ce18b1862809638f", "name": "Chengcheng Wan", "hidden": false}, {"_id": "69818a88ce18b18628096390", "name": "Hongyu Zhang", "hidden": false}, {"_id": "69818a88ce18b18628096391", "name": "David Lo", "hidden": false}, {"_id": "69818a88ce18b18628096392", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-02-02T08:10:21.000Z", "submittedOnDailyAt": "2026-02-04T00:15:44.767Z", "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding", "submittedOnDailyBy": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.", "upvotes": 81, "discussionId": "69818a89ce18b18628096393", "ai_summary": "Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.", "ai_keywords": ["Multimodal LLMs", "source code understanding", "token compression", "visual cues", "syntax highlighting", "code completion", "clone detection", "image modality", "visual compression", "computational efficiency"], "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u5f88\u5927\u6210\u529f\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u6210\u4e3a\u4e86\u4e00\u4e2a\u5173\u952e\u74f6\u9888\u3002</li>\n    <li>\u76ee\u524d\u8fd9\u4e9b\u6a21\u578b\u5c06\u6e90\u4ee3\u7801\u89c6\u4e3a\u7ebf\u6027\u5e8f\u5217\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u7ebf\u6027\u589e\u52a0\u3002</li>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63d0\u4f9b\u4e86\u901a\u8fc7\u5c06\u6e90\u4ee3\u7801\u8868\u793a\u4e3a\u56fe\u50cf\u6765\u4f18\u5316\u6548\u7387\u7684\u673a\u4f1a\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0cMLLMs\u5728\u7406\u89e3\u4ee3\u7801\u65f6\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe8\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u5229\u7528\u89c6\u89c9\u63d0\u793a\u6765\u63d0\u9ad8\u4ee3\u7801\u8865\u5168\u6027\u80fd\u3002</li>\n    <li>\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u5bf9\u89c6\u89c9\u538b\u7f29\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u97e7\u6027\uff0c\u67d0\u4e9b\u538b\u7f29\u6bd4\u751a\u81f3\u4f18\u4e8e\u539f\u59cb\u6587\u672c\u8f93\u5165\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are good at understanding code, but they need a lot of computing power as software grows.</li>\n    <li>Currently, LLMs treat code as a sequence of text, which increases costs as more context is added.</li>\n    <li>Multimodal LLMs (MLLMs) can represent code as images, which can be compressed better than text without losing meaning.</li>\n    <li>Our study shows that MLLMs can understand code with up to 8 times less data and benefit from visual features like syntax highlighting.</li>\n    <li>Some code tasks, like detecting similar code, work well even with compressed images, suggesting a new way to improve efficiency in code understanding.</li>\n</ul>"}, "publishedAt": "2026-02-02T03:10:21.000Z", "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding", "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01785.png", "numComments": 2, "submittedBy": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "fullname": "Yuling", "name": "YerbaPage", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 97, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02084", "authors": [{"_id": "6981767dce18b186280960af", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jane Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:07:05.252Z", "hidden": false}, {"_id": "6981767dce18b186280960b0", "user": {"_id": "68088b850a4295fe95b4c798", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/sGikPWke7wL3f7t4LUBwL.png", "isPro": false, "fullname": "ChengYu Yin", "user": "Cipherxzc", "type": "user"}, "name": "Chengyu Yin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:07:03.270Z", "hidden": false}, {"_id": "6981767dce18b186280960b1", "name": "Xin Zhang", "hidden": false}, {"_id": "6981767dce18b186280960b2", "name": "Qingtao Li", "hidden": false}, {"_id": "6981767dce18b186280960b3", "name": "Steven Liu", "hidden": false}, {"_id": "6981767dce18b186280960b4", "name": "Yiming Huang", "hidden": false}, {"_id": "6981767dce18b186280960b5", "name": "Jie Wu", "hidden": false}, {"_id": "6981767dce18b186280960b6", "name": "Hao Liu", "hidden": false}, {"_id": "6981767dce18b186280960b7", "name": "Yangyu Huang", "hidden": false}, {"_id": "6981767dce18b186280960b8", "name": "Yu Kang", "hidden": false}, {"_id": "6981767dce18b186280960b9", "name": "Fangkai Yang", "hidden": false}, {"_id": "6981767dce18b186280960ba", "name": "Ying Xin", "hidden": false}, {"_id": "6981767dce18b186280960bb", "name": "Scarlett Li", "hidden": false}], "publishedAt": "2026-02-02T13:30:00.000Z", "submittedOnDailyAt": "2026-02-03T01:47:04.655Z", "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder", "submittedOnDailyBy": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "summary": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.", "upvotes": 77, "discussionId": "6981767dce18b186280960bc", "projectPage": "https://ayanami2003.github.io/RPG-Encoder/", "githubRepo": "https://github.com/microsoft/RPG-ZeroRepo", "githubRepoAddedBy": "user", "ai_summary": "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.", "ai_keywords": ["Repository Planning Graph", "RPG-Encoder", "code representation", "semantic features", "code dependencies", "incremental topology evolution", "structure-aware navigation", "fine-grained localization", "repository understanding", "codebase reconstruction"], "githubStars": 59, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u4ee3\u7801\u5e93\u4ee3\u7406\u5b58\u5728\u63a8\u7406\u65ad\u88c2\u95ee\u9898\uff0c\u56e0\u5176\u4f9d\u8d56\u5b64\u7acb\u7684API\u6587\u6863\u6216\u7f3a\u4e4f\u8bed\u4e49\u6df1\u5ea6\u7684\u4f9d\u8d56\u56fe\u3002</li>\n    <li>\u6211\u4eec\u8ba4\u4e3a\u4ee3\u7801\u5e93\u7684\u7406\u89e3\u4e0e\u751f\u6210\u662f\u76f8\u4e92\u5173\u8054\u7684\u8fc7\u7a0b\uff1a\u751f\u6210\u662f\u5c06\u610f\u56fe\u6269\u5c55\u4e3a\u5b9e\u73b0\uff0c\u7406\u89e3\u662f\u5c06\u5b9e\u73b0\u538b\u7f29\u56de\u610f\u56fe\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RPG-Encoder\u6846\u67b6\uff0c\u5b83\u5c06\u9759\u6001\u751f\u6210\u84dd\u56fe\u7684\u4ee3\u7801\u5e93\u89c4\u5212\u56fe\uff08RPG\uff09\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u9ad8\u4fdd\u771f\u8868\u793a\u3002</li>\n    <li>RPG-Encoder\u901a\u8fc7\u4e09\u79cd\u673a\u5236\u5f25\u5408\u63a8\u7406\u5faa\u73af\uff1a\u7f16\u7801\u539f\u59cb\u4ee3\u7801\u3001\u9010\u6b65\u6f14\u53d8\u62d3\u6251\u4ee5\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u3001\u4ee5\u53ca\u4f5c\u4e3a\u7ed3\u6784\u611f\u77e5\u5bfc\u822a\u7684\u7edf\u4e00\u63a5\u53e3\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cRPG-Encoder\u5728\u4ee3\u7801\u5e93\u7406\u89e3\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe93.7%\uff0c\u5e76\u5728\u590d\u6742\u4ee3\u7801\u5e93\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7cbe\u7ec6\u5b9a\u4f4d\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current methods for understanding code repositories are limited because they rely on disconnected documentation and lack depth.</li>\n    <li>We introduce RPG-Encoder, a new framework that connects the processes of creating and understanding code.</li>\n    <li>RPG-Encoder uses three key methods: encoding code into a comprehensive graph, improving the structure to lower maintenance costs, and providing a unified way to navigate the code.</li>\n    <li>Our evaluations show that RPG-Encoder outperforms existing methods, achieving high accuracy in understanding complex codebases.</li>\n    <li>It also effectively mirrors the original code, confirming its ability to connect the intent behind the code with its actual implementation.</li>\n</ul>"}, "publishedAt": "2026-02-02T08:30:00.000Z", "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder", "summary": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02084.png", "numComments": 1, "submittedBy": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "fullname": "Jane Luo", "name": "Luo2003", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.21204", "authors": [{"_id": "697c3801a67238fac88cc1b1", "name": "Hong Liu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b2", "name": "Jiaqi Zhang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b3", "name": "Chao Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b4", "name": "Xing Hu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b5", "name": "Linkun Lyu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b6", "name": "Jiaqi Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1b7", "name": "Xurui Yang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b8", "name": "Bo Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b9", "name": "Fengcun Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1ba", "name": "Yulei Qian", "hidden": false}, {"_id": "697c3801a67238fac88cc1bb", "name": "Lingtong Si", "hidden": false}, {"_id": "697c3801a67238fac88cc1bc", "name": "Yerui Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1bd", "name": "Rumei Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1be", "name": "Peng Pei", "hidden": false}, {"_id": "697c3801a67238fac88cc1bf", "name": "Yuchen Xie", "hidden": false}, {"_id": "697c3801a67238fac88cc1c0", "name": "Xunliang Cai", "hidden": false}], "publishedAt": "2026-01-29T03:11:19.000Z", "submittedOnDailyAt": "2026-01-30T02:18:11.112Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "upvotes": 76, "discussionId": "697c3801a67238fac88cc1c1", "ai_summary": "Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.", "ai_keywords": ["Mixture-of-Experts", "sparsity scaling", "embedding scaling", "Pareto frontier", "parameter budgeting", "model width", "model depth", "system optimizations", "speculative decoding", "LongCat-Flash-Lite"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5e38\u7528\uff0c\u4f46\u9762\u4e34\u6536\u76ca\u9012\u51cf\u548c\u7cfb\u7edf\u74f6\u9888\u3002</li>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u5d4c\u5165\u6269\u5c55\uff0c\u8fd9\u662f\u4e00\u79cd\u6709\u6548\u7684\u7a00\u758f\u6027\u6269\u5c55\u65b9\u6cd5\u3002</li>\n    <li>\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5d4c\u5165\u6269\u5c55\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4e13\u5bb6\u6269\u5c55\u3002</li>\n    <li>\u7cfb\u7edf\u4f18\u5316\u548c\u63a8\u6d4b\u89e3\u7801\u5e2e\u52a9\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002</li>\n    <li>\u63a8\u51fa\u7684LongCat-Flash-Lite\u6a21\u578b\u5177\u6709685\u4ebf\u53c2\u6570\uff0c\u8868\u73b0\u4f18\u4e8e\u540c\u89c4\u6a21\u7684\u5176\u4ed6\u6a21\u578b\uff0c\u5c24\u5176\u5728\u667a\u80fd\u4ee3\u7406\u548c\u7f16\u7801\u9886\u57df\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models are commonly used for scaling large language models but have limitations and performance issues.</li>\n    <li>This study investigates using embedding scaling as an alternative way to improve model efficiency.</li>\n    <li>Research findings show that embedding scaling can outperform expert scaling in certain situations.</li>\n    <li>Key architectural factors, like how parameters are allocated and the model's size, affect the success of embedding scaling.</li>\n    <li>The new model, LongCat-Flash-Lite, has 68.5 billion parameters and performs better than similar MoE models, especially in tasks involving reasoning and coding.</li>\n</ul>"}, "publishedAt": "2026-01-28T22:11:19.000Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21204.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02437", "authors": [{"_id": "6981756ace18b18628096056", "name": "Dianyi Wang", "hidden": false}, {"_id": "6981756ace18b18628096057", "name": "Chaofan Ma", "hidden": false}, {"_id": "6981756ace18b18628096058", "name": "Feng Han", "hidden": false}, {"_id": "6981756ace18b18628096059", "name": "Size Wu", "hidden": false}, {"_id": "6981756ace18b1862809605a", "name": "Wei Song", "hidden": false}, {"_id": "6981756ace18b1862809605b", "user": {"_id": "654c6845bac6e6e49895a5b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png", "isPro": false, "fullname": "SII-Yibin Wang", "user": "CodeGoat24", "type": "user"}, "name": "Yibin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:07:21.031Z", "hidden": false}, {"_id": "6981756ace18b1862809605c", "name": "Zhixiong Zhang", "hidden": false}, {"_id": "6981756ace18b1862809605d", "name": "Tianhang Wang", "hidden": false}, {"_id": "6981756ace18b1862809605e", "name": "Siyuan Wang", "hidden": false}, {"_id": "6981756ace18b1862809605f", "name": "Zhongyu Wei", "hidden": false}, {"_id": "6981756ace18b18628096060", "name": "Jiaqi Wang", "hidden": false}], "publishedAt": "2026-02-02T18:34:35.000Z", "submittedOnDailyAt": "2026-02-03T02:26:49.217Z", "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.", "upvotes": 68, "discussionId": "6981756ace18b18628096061", "ai_summary": "UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.", "ai_keywords": ["text-to-image generation", "image editing", "dual reasoning paradigm", "world knowledge-enhanced planning", "visual self-correction", "shared representation", "reasoning-intensive benchmarks", "WISE", "KrisBench", "UniREditBench"], "summary_zh": "<ul>\n    <li>UniReason\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u8fd9\u4e24\u4e2a\u4efb\u52a1\u7ed3\u5408\u8d77\u6765\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u63a8\u7406\u6a21\u5f0f\uff0c\u4f7f\u7528\u4e16\u754c\u77e5\u8bc6\u589e\u5f3a\u89c4\u5212\u548c\u81ea\u6211\u53cd\u601d\u6765\u63d0\u9ad8\u89c6\u89c9\u751f\u6210\u548c\u7f16\u8f91\u7684\u80fd\u529b\u3002</li>\n    <li>UniReason\u4f7f\u7528\u4e00\u4e2a\u5927\u578b\u6570\u636e\u96c6\uff08\u7ea630\u4e07\u4e2a\u6837\u672c\uff09\uff0c\u6db5\u76d6\u4e94\u4e2a\u4e3b\u8981\u77e5\u8bc6\u9886\u57df\u6765\u652f\u6301\u5176\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cUniReason\u5728\u63a8\u7406\u5bc6\u96c6\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u51fa\u8272\u7684\u7efc\u5408\u5408\u6210\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UniReason is a new model that combines text-to-image generation and image editing into one system.</li>\n    <li>This model uses a dual reasoning method that enhances planning with world knowledge and allows for visual corrections.</li>\n    <li>It creates a shared representation for both tasks, similar to how humans plan and refine their ideas.</li>\n    <li>A large dataset with about 300,000 samples from various knowledge areas supports the framework.</li>\n    <li>Tests show that UniReason performs well on complex reasoning tasks while also being good at general synthesis.</li>\n</ul>"}, "publishedAt": "2026-02-02T13:34:35.000Z", "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing", "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02437.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.03786", "authors": [{"_id": "6982c1c69084cb4f0ecb574b", "user": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "isPro": false, "fullname": "jianhao ruan", "user": "Aurorra1123", "type": "user"}, "name": "Jianhao Ruan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:23.320Z", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574c", "name": "Zhihao Xu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574d", "name": "Yiran Peng", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574e", "name": "Fashen Ren", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb574f", "name": "Zhaoyang Yu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5750", "name": "Xinbing Liang", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5751", "name": "Jinyu Xiang", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5752", "name": "Bang Liu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5753", "name": "Chenglin Wu", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5754", "name": "Yuyu Luo", "hidden": false}, {"_id": "6982c1c69084cb4f0ecb5755", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-04T12:28:20.999Z", "hidden": false}], "publishedAt": "2026-02-03T17:46:16.000Z", "submittedOnDailyAt": "2026-02-04T02:34:02.843Z", "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "submittedOnDailyBy": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "isPro": false, "fullname": "jianhao ruan", "user": "Aurorra1123", "type": "user"}, "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "upvotes": 66, "discussionId": "6982c1c69084cb4f0ecb5756", "ai_summary": "AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.", "ai_keywords": ["language agents", "sub-agent-as-tools paradigm", "multi-turn task solving", "agent abstraction", "task automation", "framework-agnostic", "agent orchestration", "automatic agent creation", "Pareto-efficient", "GAIA", "SWE-Bench", "Terminal-Bench"], "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u4ee3\u7406\u5728\u4efb\u52a1\u81ea\u52a8\u5316\u65b9\u9762\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u590d\u6742\u7684\u957f\u671f\u4efb\u52a1\u4ecd\u9762\u4e34\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4ee3\u7406\u62bd\u8c61\u6a21\u578b\uff0c\u5c06\u4efb\u4f55\u4ee3\u7406\u8868\u793a\u4e3a\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u6a21\u578b\u7684\u7ec4\u5408\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e00\u62bd\u8c61\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aAOrchestra\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6bcf\u4e00\u6b65\u52a8\u6001\u9009\u62e9\u7528\u4e8e\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u6a21\u578b\u3002</li>\n    <li>AOrchestra\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u7a0b\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u4e14\u80fd\u591f\u4e0e\u591a\u79cd\u4ee3\u7406\u65e0\u7f1d\u8fde\u63a5\u3002</li>\n    <li>\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAOrchestra\u5728\u6027\u80fd\u4e0a\u76f8\u8f83\u4e8e\u6700\u5f3a\u7684\u57fa\u7ebf\u63d0\u5347\u4e8616.28%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Language agents are useful for automating tasks, especially complex ones that require multiple steps.</li>\n    <li>The current designs of these agents lack flexibility, making them less adaptable to different tasks.</li>\n    <li>This work introduces a new way to think about agents using a model called Instruction, Context, Tools, Model, which helps create specialized agents for specific tasks.</li>\n    <li>The system named AOrchestra uses this model to manage tasks by selecting the right tools and executing them automatically.</li>\n    <li>AOrchestra shows significant improvement in performance on various benchmarks compared to existing methods.</li>\n</ul>"}, "publishedAt": "2026-02-03T12:46:16.000Z", "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03786.png", "numComments": 1, "submittedBy": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "fullname": "jianhao ruan", "name": "Aurorra1123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02103", "authors": [{"_id": "6981ab8dce18b1862809643a", "name": "Liyan Xu", "hidden": false}, {"_id": "6981ab8dce18b1862809643b", "name": "Mo Yu", "hidden": false}, {"_id": "6981ab8dce18b1862809643c", "name": "Fandong Meng", "hidden": false}, {"_id": "6981ab8dce18b1862809643d", "name": "Jie Zhou", "hidden": false}], "publishedAt": "2026-02-02T13:46:56.000Z", "submittedOnDailyAt": "2026-02-04T01:21:43.596Z", "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "submittedOnDailyBy": {"_id": "650f0fac11f3210cf7a8a849", "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg", "isPro": false, "fullname": "Liyan Xu", "user": "lxucs", "type": "user"}, "summary": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "upvotes": 57, "discussionId": "6981ab8dce18b1862809643e", "ai_summary": "Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.", "ai_keywords": ["Chain-of-Thought", "Large Language Models", "latent planning", "hidden states", "Tele-Lens", "multi-step reasoning", "uncertainty estimation", "CoT dynamics"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u53d1\u73b0LLMs\u5728\u591a\u6b65\u9aa4\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u4ecd\u7136\u4f9d\u8d56\u4e8e\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u3002</li>\n    <li>\u901a\u8fc7\"Tele-Lens\"\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86LLMs\u7684\u9690\u85cf\u72b6\u6001\uff0c\u53d1\u73b0\u5b83\u4eec\u4e3b\u8981\u8fdb\u884c\u5c40\u90e8\u89c4\u5212\uff0c\u800c\u975e\u5168\u5c40\u89c4\u5212\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u5047\u8bbe\uff0c\u8ba4\u4e3a\u5c11\u90e8\u5206\u7684CoT\u4f4d\u7f6e\u53ef\u4ee5\u6709\u6548\u4ee3\u8868\u6574\u4e2a\u63a8\u7406\u8def\u5f84\u7684\u4e0d\u786e\u5b9a\u6027\u3002</li>\n    <li>\u5f3a\u8c03\u4e86\u5229\u7528CoT\u52a8\u6001\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u81ea\u52a8\u8bc6\u522bCoT\u7ed5\u8fc7\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This study explores how Large Language Models (LLMs) plan their reasoning, showing that they can think ahead even before using Chain-of-Thought (CoT) methods.</li>\n    <li>CoT is still crucial for tasks that need multi-step reasoning, despite the LLMs' ability to plan internally.</li>\n    <li>The researchers used a method called Tele-Lens to examine LLMs, finding they often make small, short-term decisions instead of long-term plans.</li>\n    <li>They proposed a way to improve how uncertainty is estimated in CoT, showing that just a few CoT steps can represent the overall uncertainty well.</li>\n    <li>The study also highlights the importance of understanding CoT dynamics and shows that it's possible to recognize when CoT isn't needed without losing performance.</li>\n</ul>"}, "publishedAt": "2026-02-02T08:46:56.000Z", "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "summary": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02103.png", "numComments": 1, "submittedBy": {"_id": "650f0fac11f3210cf7a8a849", "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg", "fullname": "Liyan Xu", "name": "lxucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aGreen-VLA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8ba9\u7eff\u8272\u4eba\u5f62\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6267\u884c\u4efb\u52a1\u3002</li>\n    <li>Green-VLA\u6846\u67b6\u5206\u4e3a\u4e94\u4e2a\u9636\u6bb5\uff0c\u5305\u62ec\u57fa\u7840VLM\u3001\u8de8\u6a21\u6001\u5bf9\u63a5\u3001\u591a\u4f53\u9002\u5e94\u7b49\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u4e863000\u5c0f\u65f6\u7684\u6f14\u793a\u6570\u636e\uff0c\u7ed3\u5408\u65f6\u95f4\u5bf9\u9f50\u548c\u8d28\u91cf\u8fc7\u6ee4\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u5904\u7406\u6548\u7387\u3002</li>\n    <li>\u5728\u63a8\u7406\u9636\u6bb5\uff0cVLA\u63a7\u5236\u5668\u901a\u8fc7\u9884\u6d4b\u8fdb\u5c55\u3001\u68c0\u6d4b\u5f02\u5e38\u548c\u8054\u5408\u9884\u6d4b\u7b49\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u76ee\u6807\u9009\u62e9\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u540e\uff0c\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u671f\u6548\u7387\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a new framework designed for the Green humanoid robot, allowing it to perform tasks in real-world situations.</li>\n    <li>The framework has five stages: starting with basic language models, then moving to understanding multiple types of input, training on different robot types, adapting to specific robots, and finally aligning with reinforcement learning for better decision-making.</li>\n    <li>It uses a large data set of 3,000 hours of demonstrations to improve the robot's performance, ensuring the actions are suitable for various robot types.</li>\n    <li>During operation, the system includes features for predicting progress, detecting unusual situations, and guiding actions to enhance safety and accuracy.</li>\n    <li>Tests on different robots show that the framework improves success rates, robustness, and efficiency in completing tasks.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u4ec5\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u7b54\u6848\u901a\u5e38\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u7684\u5f00\u653e\u9886\u57df\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u9700\u8981\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u63d0\u53d6\u3001\u4e92\u52a8\u7f51\u9875\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u9a8c\u8bc1\u3002</li>\n    <li>\u6211\u4eec\u5bf9\u591a\u79cd\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u4f9d\u8d56\u4e8e\u4fdd\u6301\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u89c6\u9891\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u6765\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u9762\u4e34\u7684\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering needs to combine visual clues from videos with information found online.</li>\n    <li>We created VideoDR, a new benchmark for video-based question answering that involves extracting visual information, retrieving online content, and verifying answers.</li>\n    <li>VideoDR includes high-quality samples from six different areas, thanks to careful human annotation.</li>\n    <li>We tested various large language models and found that their performance depends on how well they keep track of video information during retrieval.</li>\n    <li>The study highlights challenges like maintaining focus over long tasks and consistency in reasoning, which are important for future video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Kimi K2.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63a8\u52a8\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u8fd9\u4e24\u79cd\u6a21\u5f0f\u76f8\u4e92\u589e\u5f3a\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5982\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9\u5fae\u8c03\u548c\u8054\u5408\u6587\u672c-\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u6211\u5bfc\u5411\u7684\u5e76\u884c\u4ee3\u7406\u534f\u8c03\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u5c06\u590d\u6742\u4efb\u52a1\u62c6\u5206\u4e3a\u5f02\u6784\u5b50\u95ee\u9898\u5e76\u540c\u65f6\u6267\u884c\u3002</li>\n    <li>\u8bc4\u4f30\u8868\u660e\uff0cKimi K2.5\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kimi K2.5 is a new open-source model that combines text and vision to improve intelligent behavior.</li>\n    <li>It uses various techniques to make text and vision work together better, like joint pre-training and reinforcement learning.</li>\n    <li>K2.5 includes a feature called Agent Swarm, which helps break down complex tasks into smaller parts that can be done at the same time.</li>\n    <li>The model has shown excellent performance in areas like coding, vision, reasoning, and intelligent tasks.</li>\n    <li>Kimi K2.5 can work faster, reducing wait times by up to 4.5 times compared to using a single agent.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u638c\u63e1\u8bed\u8a00\u4e4b\u524d\u5c31\u80fd\u53d1\u5c55\u6838\u5fc3\u89c6\u89c9\u6280\u80fd\uff0c\u4f46\u73b0\u4ee3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u5176\u8106\u5f31\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u5373\u4f7f\u662f3\u5c81\u7684\u5b69\u5b50\u4e5f\u80fd\u8f7b\u677e\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u4e86\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f5c\u8005\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u800c\u4e0d\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u522b\uff0c\u6db5\u76d6\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u9876\u5c16\u7684MLLMs\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\u3002</li>\n    <li>\u4f8b\u5982\uff0cGemini3-Pro-Preview\u5f97\u5206\u4ec5\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u5e74\u4eba\u5e73\u5747\u5206\uff0c\u8fd9\u8868\u660e\u5f53\u524d\u7684MLLMs\u5728\u57fa\u7840\u89c6\u89c9\u80fd\u529b\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills before language, but modern Multimodal LLMs (MLLMs) rely a lot on language for visual understanding.</li>\n    <li>MLLMs struggle with basic visual tasks that even young children can do easily.</li>\n    <li>The BabyVision benchmark was created to test visual abilities of MLLMs without using language knowledge.</li>\n    <li>Results show that top MLLMs score much lower than humans in visual tasks, highlighting their lack of basic visual skills.</li>\n    <li>BabyVision aims to improve MLLMs' visual perception and reasoning, and related tools are available online for further research.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u5b9e\u4f53\u5206\u5272\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u5206\u5272\u6a21\u578b\u5728\u7269\u7406\u5c5e\u6027\uff08\u5982\u5efa\u7b51\u7269\u3001\u6c34\u4f53\uff09\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4e0a\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u672c\u7814\u7a76\u5b9e\u73b0\u4e86\u793e\u4f1a\u8bed\u4e49\u5206\u5272\uff0c\u63d0\u51fa\u4e86\u540d\u4e3aSocioSeg\u7684\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6ce8\u91ca\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many different types of features that can be identified from satellite images.</li>\n    <li>Current models do well with physical features like buildings but struggle with social features like schools and parks.</li>\n    <li>The authors created a new dataset called SocioSeg, which includes satellite images and detailed labels for social features.</li>\n    <li>They developed a framework called SocioReasoner that uses a combination of vision and language to identify social features more accurately.</li>\n    <li>Tests show that their method outperforms existing models and can effectively generalize to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u9879\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u3002</li>\n    <li>\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u5de5\u5177\u4ea4\u4e92\uff0c\u5e76\u5728\u5608\u6742\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u91cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u4ee5\u589e\u5f3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6269\u5c55\u63a8\u7406\u7684\u6df1\u5ea6\u548c\u5bbd\u5ea6\u6765\u63d0\u9ad8\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source model with 560 billion parameters, designed for advanced reasoning tasks.</li>\n    <li>It performs exceptionally well on various benchmarks related to agentic reasoning, such as using tools and solving complex tasks.</li>\n    <li>The model is trained using a unique system that combines expert training and data construction to improve its performance in real-world scenarios.</li>\n    <li>It incorporates methods to handle noisy real-world environments, making it more robust in practical applications.</li>\n    <li>A special \"Heavy Thinking\" mode allows the model to enhance its reasoning abilities during testing by increasing both depth and width of thinking.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u4e24\u79cd\u7b56\u7565\u5b9e\u73b0\uff1a\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u8d85\u8fc71000\u6b21\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\uff0c\u63d0\u5347\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u652f\u6301\u591a\u6837\u7684\u89c6\u89c9\u63a8\u7406\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u8f83\u5c0f\uff08\u4ec510B\u53c2\u6570\uff09\uff0c\u4f46\u5176\u8868\u73b0\u8d85\u8fc7\u4e86\u8bb8\u591a\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u751a\u81f3\u9876\u7ea7\u5546\u4e1a\u4ea7\u54c1\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5df2\u5411\u793e\u533a\u53d1\u5e03\u5b8c\u6574\u6a21\u578b\u5957\u4ef6\uff0c\u4fbf\u4e8e\u91cd\u590d\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new open-source model that balances size and advanced capabilities in understanding both text and images.</li>\n    <li>The model uses a unique training method with a large dataset of 1.2 trillion multimodal tokens to enhance its vision and language understanding.</li>\n    <li>It features advanced reasoning techniques, enabling it to perform well in complex tasks while being smaller than many other models.</li>\n    <li>STEP3-VL-10B achieves high scores on various benchmarks, outperforming much larger models and competing with top proprietary models.</li>\n    <li>The complete model and tools are available for the community to use, promoting efficiency and ease of reproduction in research.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u672a\u5145\u5206\u5229\u7528\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u601d\u7ef4\u4e0e\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e00\u4e2a\u4ee3\u7406-\u5730\u56fe\u5faa\u73af\u8fdb\u884c\u5904\u7406\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4ece8.0%\u63d0\u5347\u81f322.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The image geolocalization task predicts where a photo was taken on Earth using visual clues.</li>\n    <li>Current models don't use maps, which is a common method humans use for this task.</li>\n    <li>This work introduces a new ability called \"Thinking with Map\" and uses a two-stage optimization process.</li>\n    <li>The first stage uses reinforcement learning to improve how the model samples information, and the second stage allows the model to test different paths before making a final decision.</li>\n    <li>Results show that this method performs better than existing models, significantly improving accuracy in locating images.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u539f\u59cb\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002</li>\n    <li>\u7531\u4e8e\u5bf9\u5e94\u7528\u6570\u636e\u9700\u6c42\u589e\u52a0\u548c\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u53d1\u5c55\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u5728\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u65b0\u8d8b\u52bf\u3002</li>\n    <li>\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u4f7f\u7528LLM\u6280\u672f\u8fdb\u884c\u6570\u636e\u51c6\u5907\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u6574\u5408\u548c\u6570\u636e\u4e30\u5bcc\u3002</li>\n    <li>\u6211\u4eec\u5206\u6790\u4e86\u6bcf\u4e2a\u4efb\u52a1\u7684\u4ee3\u8868\u6027\u6280\u672f\uff0c\u6307\u51fa\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5982\u63d0\u9ad8\u7406\u89e3\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u7684\u6210\u672c\u7b49\u95ee\u9898\u3002</li>\n    <li>\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u6ce8\u53ef\u6269\u5c55\u7684LLM\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u7684\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u56fe\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation helps clean raw datasets, find relationships between them, and extract useful insights for various applications.</li>\n    <li>There is a growing need for ready-to-use data due to advancements in LLM (Large Language Model) techniques and new flexible infrastructures.</li>\n    <li>This paper reviews recent studies on using LLMs for data preparation, moving from traditional methods to more interactive and context-aware processes.</li>\n    <li>It categorizes data preparation into three main tasks: cleaning, integration, and enrichment, discussing techniques and their pros and cons.</li>\n    <li>The paper also highlights research challenges and proposes a roadmap for creating scalable and reliable data systems using LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22060", "authors": [{"_id": "69817968ce18b186280960f0", "user": {"_id": "67dc162ec8c00778e8689f42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png", "isPro": false, "fullname": "Wenxuan Huang", "user": "Osilly", "type": "user"}, "name": "Wenxuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:37.268Z", "hidden": false}, {"_id": "69817968ce18b186280960f1", "user": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "name": "Yu Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:52.098Z", "hidden": false}, {"_id": "69817968ce18b186280960f2", "name": "Qiuchen Wang", "hidden": false}, {"_id": "69817968ce18b186280960f3", "name": "Zhen Fang", "hidden": false}, {"_id": "69817968ce18b186280960f4", "name": "Shaosheng Cao", "hidden": false}, {"_id": "69817968ce18b186280960f5", "name": "Zheng Chu", "hidden": false}, {"_id": "69817968ce18b186280960f6", "name": "Qingyu Yin", "hidden": false}, {"_id": "69817968ce18b186280960f7", "name": "Shuang Chen", "hidden": false}, {"_id": "69817968ce18b186280960f8", "name": "Zhenfei Yin", "hidden": false}, {"_id": "69817968ce18b186280960f9", "user": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "name": "Lin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:49.925Z", "hidden": false}, {"_id": "69817968ce18b186280960fa", "name": "Zehui Chen", "hidden": false}, {"_id": "69817968ce18b186280960fb", "name": "Yao Hu", "hidden": false}, {"_id": "69817968ce18b186280960fc", "name": "Philip Torr", "hidden": false}, {"_id": "69817968ce18b186280960fd", "name": "Feng Zhao", "hidden": false}, {"_id": "69817968ce18b186280960fe", "name": "Wanli Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "publishedAt": "2026-01-29T17:58:40.000Z", "submittedOnDailyAt": "2026-02-03T02:05:47.568Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "submittedOnDailyBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "upvotes": 127, "discussionId": "69817968ce18b186280960ff", "projectPage": "https://osilly.github.io/Vision-DeepResearch/", "githubRepo": "https://github.com/Osilly/Vision-DeepResearch", "githubRepoAddedBy": "user", "ai_summary": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.", "ai_keywords": ["multimodal large language models", "visual and textual search engines", "reasoning-then-tool-call", "multimodal deep-research", "multi-turn search", "multi-entity search", "multi-scale search", "cold-start supervision", "reinforcement learning", "end-to-end multimodal deep-research"], "githubStars": 96, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n    <li>\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u201c\u63a8\u7406-\u7136\u540e-\u5de5\u5177\u8c03\u7528\u201d\u7684\u7b56\u7565\u6765\u589e\u5f3a\u6a21\u578b\uff0c\u4f46\u5047\u8bbe\u67e5\u8be2\u65b9\u5f0f\u8fc7\u4e8e\u7b80\u5355\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e2d\u7684\u590d\u6742\u60c5\u51b5\u3002</li>\n    <li>\u65b0\u63d0\u51fa\u7684Vision-DeepResearch\u91c7\u7528\u591a\u8f6e\u3001\u591a\u5b9e\u4f53\u548c\u591a\u5c3a\u5ea6\u7684\u641c\u7d22\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u89c6\u89c9\u566a\u58f0\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u66f4\u591a\u7684\u63a8\u7406\u6b65\u9aa4\u548c\u5f15\u64ce\u4ea4\u4e92\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u3002</li>\n    <li>Vision-DeepResearch\u5728\u6027\u80fd\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\uff0c\u5e76\u5c06\u5728GitHub\u4e0a\u53d1\u5e03\u4ee3\u7801\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal large language models (MLLMs) are good at vision tasks but need improvements for factual information retrieval.</li>\n    <li>Current methods often assume one simple query is enough, which doesn't work well in real-life situations with a lot of visual noise.</li>\n    <li>The proposed solution, Vision-DeepResearch, allows for multiple searches and reasoning steps to gather information from various sources.</li>\n    <li>This new approach has better performance than existing multimodal models and integrates deep-research capabilities effectively.</li>\n    <li>The code for Vision-DeepResearch will be available for public use on GitHub.</li>\n</ul>"}, "publishedAt": "2026-01-29T12:58:40.000Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22060.png", "numComments": 2, "submittedBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "fullname": "Yu Zeng", "name": "YuZeng260", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 05, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Mixture-of-Experts (MoE) \u6a21\u578b\u7f3a\u4e4f\u7ea6\u675f\uff0c\u5bfc\u81f4\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u7684\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\u635f\u5931\uff08ERC\u635f\u5931\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f85\u52a9\u635f\u5931\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u786e\u4fdd\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u5176\u4ee3\u7406\u6807\u8bb0\u7684\u6fc0\u6d3b\u9ad8\u4e8e\u5bf9\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406\u6807\u8bb0\uff0c\u5e76\u4f7f\u6bcf\u4e2a\u4ee3\u7406\u6807\u8bb0\u4ece\u5176\u5bf9\u5e94\u7684\u4e13\u5bb6\u90a3\u91cc\u83b7\u5f97\u66f4\u5f3a\u7684\u6fc0\u6d3b\u3002</li>\n    <li>\u8be5\u635f\u5931\u8ba1\u7b97\u9ad8\u6548\uff0c\u4ec5\u9700\u5904\u7406n\u00b2\u4e2a\u6fc0\u6d3b\uff0c\u5176\u4e2dn\u4e3a\u4e13\u5bb6\u6570\u91cf\uff0c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u53c2\u6570\u4ece30\u4ebf\u5230150\u4ebf\u7684MoE-LLM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u4e86ERC\u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u4e13\u5bb6\u4e13\u95e8\u5316\u6c34\u5e73\u7684\u63a7\u5236\u548c\u5b9a\u91cf\u8ddf\u8e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models need better ways to connect the router's choices with the abilities of each expert to improve performance.</li>\n    <li>We introduce a new loss function called expert-router coupling (ERC) loss that helps link the router's decisions to the experts' strengths.</li>\n    <li>The ERC loss makes sure that each expert is more responsive to its own assigned tokens than to those of other experts.</li>\n    <li>This method is efficient, using a fixed amount of resources regardless of the number of tokens, unlike older methods that required more resources as the number of tokens increased.</li>\n    <li>Our tests on large MoE models show that ERC loss enhances expert specialization and provides insights during training.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23576", "authors": [{"_id": "69534f1e89916ff627aa3fe3", "name": "Ethan Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe4", "name": "Zhulin Hu", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe5", "name": "Bohao Tang", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe6", "name": "Jiadi Su", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe7", "name": "Steffi Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe8", "name": "Zhijie Deng", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe9", "name": "Pengfei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "publishedAt": "2025-12-29T16:17:36.000Z", "submittedOnDailyAt": "2025-12-30T02:36:23.479Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "submittedOnDailyBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "isPro": false, "fullname": "Ethan Chern", "user": "ethanchern", "type": "user"}, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "upvotes": 51, "discussionId": "69534f1e89916ff627aa3fea", "githubRepo": "https://github.com/GAIR-NLP/LiveTalk", "githubRepoAddedBy": "user", "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.", "ai_keywords": ["diffusion models", "bidirectional attention", "distillation methods", "on-policy distillation", "Self Forcing", "audio language models", "Anchor-Heavy Identity Sinks", "multimodal conditioning", "autoregressive", "on-policy optimization"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u4e2d\u7684\u591a\u6a21\u6001\u4e92\u52a8\u95ee\u9898\uff0c\u5305\u62ec\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u3002</li>\n    <li>\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u5728\u53bb\u566a\u89c6\u9891\u5e27\u65f6\u6548\u7387\u4f4e\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u4e92\u52a8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u91cd\u70b9\u63d0\u9ad8\u6761\u4ef6\u8f93\u5165\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u4e0e\u4f20\u7edf\u6a21\u578b\u76f8\u4f3c\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u964d\u4f4e\u4e8620\u500d\u3002</li>\n    <li>\u6574\u5408\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u9891\u63a8\u65ad\u6280\u672f\u540e\uff0c\u521b\u5efa\u4e86\u5b9e\u65f6\u4e92\u52a8\u7684\u865a\u62df\u5934\u50cf\u7cfb\u7edfLiveTalk\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8f6e\u4e92\u52a8\u7684\u89c6\u9891\u8fde\u8d2f\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper focuses on improving real-time video generation for interactive AI systems using diffusion models.</li>\n    <li>Current methods struggle with natural human-AI interaction due to slow video frame processing.</li>\n    <li>The authors propose a better way to distill models that enhances video quality and reduces issues like flickering and black frames.</li>\n    <li>The new model provides high visual quality while being 20 times faster and less costly in terms of resources.</li>\n    <li>They created a system called LiveTalk that allows for real-time, multimodal interactions with avatars, outperforming existing models.</li>\n</ul>"}, "publishedAt": "2025-12-29T11:17:36.000Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png", "numComments": 1, "submittedBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "fullname": "Ethan Chern", "name": "ethanchern", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22096", "authors": [{"_id": "695206a8746a34b55dd548dd", "name": "Xiaofeng Mao", "hidden": false}, {"_id": "695206a8746a34b55dd548de", "name": "Zhen Li", "hidden": false}, {"_id": "695206a8746a34b55dd548df", "name": "Chuanhao Li", "hidden": false}, {"_id": "695206a8746a34b55dd548e0", "name": "Xiaojie Xu", "hidden": false}, {"_id": "695206a8746a34b55dd548e1", "name": "Kaining Ying", "hidden": false}, {"_id": "695206a8746a34b55dd548e2", "name": "Tong He", "hidden": false}, {"_id": "695206a8746a34b55dd548e3", "name": "Jiangmiao Pang", "hidden": false}, {"_id": "695206a8746a34b55dd548e4", "name": "Yu Qiao", "hidden": false}, {"_id": "695206a8746a34b55dd548e5", "name": "Kaipeng Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "publishedAt": "2025-12-26T17:52:49.000Z", "submittedOnDailyAt": "2025-12-30T01:50:23.447Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "upvotes": 50, "discussionId": "695206a8746a34b55dd548e6", "projectPage": "https://stdstu12.github.io/YUME-Project/", "githubRepo": "https://github.com/stdstu12/YUME", "githubRepoAddedBy": "user", "githubStars": 426, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u663e\u793a\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u4e92\u52a8\u548c\u53ef\u63a2\u7d22\u7684\u4e16\u754c\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u53c2\u6570\u8fc7\u5927\u3001\u63a8\u7406\u6b65\u9aa4\u957f\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u589e\u957f\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u5b9e\u65f6\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\\method\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u7247\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u771f\u5b9e\u7684\u4e92\u52a8\u4e16\u754c\u3002</li>\n    <li>\u8be5\u6846\u67b6\u652f\u6301\u952e\u76d8\u63a2\u7d22\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u957f\u89c6\u9891\u751f\u6210\u3001\u5b9e\u65f6\u52a0\u901f\u7b56\u7565\u548c\u6587\u672c\u63a7\u5236\u7684\u4e16\u754c\u4e8b\u4ef6\u751f\u6210\u3002</li>\n    <li>\u4ee3\u7801\u5e93\u5df2\u5728\u8865\u5145\u6750\u6599\u4e2d\u63d0\u4f9b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods using diffusion models can create interactive virtual worlds from images or text.</li>\n    <li>Current approaches struggle with large size, slow processing, and limited text control.</li>\n    <li>The proposed framework, called \\method, allows for realistic and continuous world generation.</li>\n    <li>\\method includes features for keyboard exploration and real-time performance improvements.</li>\n    <li>It consists of three main parts: video generation, a streaming acceleration strategy, and a text-controlled event generation method.</li>\n</ul>"}, "publishedAt": "2025-12-26T12:52:49.000Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22096.png", "numComments": 1, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22322", "authors": [{"_id": "69533fb889916ff627aa3ecb", "name": "Shaofei Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ecc", "name": "Yulei Qin", "hidden": false}, {"_id": "69533fb889916ff627aa3ecd", "name": "Haojia Lin", "hidden": false}, {"_id": "69533fb889916ff627aa3ece", "name": "Zihan Xu", "hidden": false}, {"_id": "69533fb889916ff627aa3ecf", "name": "Gang Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed0", "name": "Yuchen Shi", "hidden": false}, {"_id": "69533fb889916ff627aa3ed1", "name": "Zongyi Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed2", "name": "Yong Mao", "hidden": false}, {"_id": "69533fb889916ff627aa3ed3", "name": "Siqi Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ed4", "name": "Xiaoyu Tan", "hidden": false}, {"_id": "69533fb889916ff627aa3ed5", "name": "Yitao Liang", "hidden": false}, {"_id": "69533fb889916ff627aa3ed6", "name": "Ke Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed7", "name": "Xing Sun", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "publishedAt": "2025-12-26T14:51:39.000Z", "submittedOnDailyAt": "2025-12-30T01:07:21.942Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "submittedOnDailyBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "upvotes": 33, "discussionId": "69533fb889916ff627aa3ed8", "projectPage": "https://huggingface.co/collections/yolay/smartsnap", "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u590d\u6742GUI\u4efb\u52a1\u4e2d\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u4efb\u52a1\u5b8c\u6210\u7684\u9a8c\u8bc1\u5f71\u54cd\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>\u73b0\u6709\u7684\u4efb\u52a1\u9a8c\u8bc1\u662f\u88ab\u52a8\u7684\uff0c\u5206\u6790\u4ee3\u7406\u7684\u6574\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u6765\u5224\u65ad\u662f\u5426\u6210\u529f\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u548c\u53ef\u9760\u6027\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SmartSnap\uff0c\u901a\u8fc7\u4e3b\u52a8\u7684\u81ea\u6211\u9a8c\u8bc1\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u4ee3\u7406\u4e0d\u4ec5\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u8981\u63d0\u4f9b\u8bc1\u636e\u8bc1\u660e\u5176\u5b8c\u6210\u60c5\u51b5\u3002</li>\n    <li>\u4ee3\u7406\u4f7f\u75283C\u539f\u5219\uff08\u5b8c\u6574\u6027\u3001\u7b80\u6d01\u6027\u548c\u521b\u9020\u6027\uff09\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u5173\u952e\u5feb\u7167\u4f5c\u4e3a\u8bc1\u636e\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSmartSnap\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c8B\u548c30B\u6a21\u578b\u7684\u8868\u73b0\u5206\u522b\u63d0\u9ad8\u4e8626.08%\u548c16.66%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic reinforcement learning can help create autonomous agents for complex tasks, but verifying task completion is a big challenge.</li>\n    <li>Current verification methods are passive and analyze the entire task history, which can be costly and unreliable due to irrelevant information.</li>\n    <li>SmartSnap introduces a new approach where agents proactively verify their own success using key evidences, called snapshots.</li>\n    <li>The Self-Verifying Agent follows 3C Principles (Completeness, Conciseness, Creativity) to improve the verification process.</li>\n    <li>Tests show that SmartSnap can enhance performance of agents significantly, making them more efficient and competitive.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:39.000Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22322.png", "numComments": 2, "submittedBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "fullname": "Yulei Qin", "name": "yolay", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21185", "authors": [{"_id": "694e459a746a34b55dd54592", "user": {"_id": "62fbe6cfa80632fbd47bb8ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fbe6cfa80632fbd47bb8ca/lIOS1wWIiBkfjfY3cNJRb.jpeg", "isPro": false, "fullname": "thj2333", "user": "infinith", "type": "user"}, "name": "Tanghui Jia", "status": "claimed_verified", "statusLastChangedAt": "2025-12-31T20:57:47.389Z", "hidden": false}, {"_id": "694e459a746a34b55dd54593", "user": {"_id": "64049ae20ab5e22719f35103", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg", "isPro": false, "fullname": "Dongyu Yan", "user": "StarYDY", "type": "user"}, "name": "Dongyu Yan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-31T20:57:43.637Z", "hidden": false}, {"_id": "694e459a746a34b55dd54594", "name": "Dehao Hao", "hidden": false}, {"_id": "694e459a746a34b55dd54595", "name": "Yang Li", "hidden": false}, {"_id": "694e459a746a34b55dd54596", "name": "Kaiyi Zhang", "hidden": false}, {"_id": "694e459a746a34b55dd54597", "name": "Xianyi He", "hidden": false}, {"_id": "694e459a746a34b55dd54598", "name": "Lanjiong Li", "hidden": false}, {"_id": "694e459a746a34b55dd54599", "name": "Jinnan Chen", "hidden": false}, {"_id": "694e459a746a34b55dd5459a", "user": {"_id": "65d61d8fd484956b5acc89fe", "avatarUrl": "/avatars/47954232c90780ffe898a5a445f7fb0a.svg", "isPro": false, "fullname": "Lutao Jiang", "user": "LutaoJiang", "type": "user"}, "name": "Lutao Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-31T20:57:40.803Z", "hidden": false}, {"_id": "694e459a746a34b55dd5459b", "name": "Qishen Yin", "hidden": false}, {"_id": "694e459a746a34b55dd5459c", "name": "Long Quan", "hidden": false}, {"_id": "694e459a746a34b55dd5459d", "name": "Ying-Cong Chen", "hidden": false}, {"_id": "694e459a746a34b55dd5459e", "name": "Li Yuan", "hidden": false}], "publishedAt": "2025-12-24T14:08:38.000Z", "submittedOnDailyAt": "2025-12-31T06:02:48.033Z", "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement", "submittedOnDailyBy": {"_id": "64049ae20ab5e22719f35103", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg", "isPro": false, "fullname": "Dongyu Yan", "user": "StarYDY", "type": "user"}, "summary": "In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.", "upvotes": 14, "discussionId": "694e459a746a34b55dd5459f", "projectPage": "https://pku-yuangroup.github.io/UltraShape-1.0/", "githubRepo": "https://github.com/PKU-YuanGroup/UltraShape-1.0", "githubRepoAddedBy": "user", "githubStars": 110, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86UltraShape 1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u76843D\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f3D\u51e0\u4f55\u4f53\u751f\u6210\u3002</li>\n    <li>\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u6d41\u7a0b\uff1a\u5148\u5408\u6210\u7c97\u7565\u7684\u5168\u5c40\u7ed3\u6784\uff0c\u518d\u8fdb\u884c\u7ec6\u5316\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u51e0\u4f55\u4f53\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u7efc\u5408\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u65b0\u9896\u7684\u5bc6\u95ed\u5904\u7406\u65b9\u6cd5\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u8fc7\u6ee4\uff0c\u63d0\u5347\u516c\u5f003D\u6570\u636e\u96c6\u7684\u51e0\u4f55\u8d28\u91cf\u3002</li>\n    <li>\u901a\u8fc7\u5728\u56fa\u5b9a\u7a7a\u95f4\u4f4d\u7f6e\u8fdb\u884c\u4f53\u7d20\u57fa\u7840\u7684\u7ec6\u5316\uff0c\u89e3\u8026\u7a7a\u95f4\u5b9a\u4f4d\u4e0e\u51e0\u4f55\u7ec6\u8282\u5408\u6210\uff0c\u805a\u7126\u4e8e\u5c40\u90e8\u51e0\u4f55\u7ec6\u8282\u7684\u751f\u6210\u3002</li>\n    <li>\u6a21\u578b\u5728\u516c\u5f00\u76843D\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u8d44\u6e90\u6709\u9650\uff0c\u4ecd\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u51e0\u4f55\u8d28\u91cf\uff0c\u5e76\u4e0e\u73b0\u6709\u5f00\u6e90\u65b9\u6cd5\u8868\u73b0\u7ade\u4e89\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UltraShape 1.0 is a new tool for creating detailed 3D shapes using a two-step process: first creating a rough shape, then refining it for better quality.</li>\n    <li>A special data processing method improves the quality of 3D datasets by removing poor-quality parts and fixing issues while keeping important details.</li>\n    <li>The refinement process separates where to place details on the 3D shape from how to create those details, making it more efficient.</li>\n    <li>UltraShape 1.0 uses only publicly available 3D data for training and still achieves high-quality results.</li>\n    <li>The tool has been tested extensively and performs well compared to other open-source methods, with plans to share the code and models for future use.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:08:38.000Z", "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement", "summary": "In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21185.png", "numComments": 1, "submittedBy": {"_id": "64049ae20ab5e22719f35103", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678023295407-noauth.jpeg", "fullname": "Dongyu Yan", "name": "StarYDY", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "isAuthorParticipating": true}, {"paper": {"id": "2512.22525", "authors": [{"_id": "69549ff6869a8627b452c956", "name": "Bin Xia", "hidden": false}, {"_id": "69549ff6869a8627b452c957", "name": "Bohao Peng", "hidden": false}, {"_id": "69549ff6869a8627b452c958", "name": "Jiyang Liu", "hidden": false}, {"_id": "69549ff6869a8627b452c959", "name": "Sitong Wu", "hidden": false}, {"_id": "69549ff6869a8627b452c95a", "name": "Jingyao Li", "hidden": false}, {"_id": "69549ff6869a8627b452c95b", "name": "Junjia Huang", "hidden": false}, {"_id": "69549ff6869a8627b452c95c", "name": "Xu Zhao", "hidden": false}, {"_id": "69549ff6869a8627b452c95d", "name": "Yitong Wang", "hidden": false}, {"_id": "69549ff6869a8627b452c95e", "name": "Ruihang Chu", "hidden": false}, {"_id": "69549ff6869a8627b452c95f", "name": "Bei Yu", "hidden": false}, {"_id": "69549ff6869a8627b452c960", "name": "Jiaya Jia", "hidden": false}], "publishedAt": "2025-12-27T09:07:12.000Z", "submittedOnDailyAt": "2025-12-31T01:36:44.468Z", "title": "DreamOmni3: Scribble-based Editing and Generation", "submittedOnDailyBy": {"_id": "64e42a700ecc1ecca77b1db9", "avatarUrl": "/avatars/c1228db09b88c9246aab48da7ae82f7c.svg", "isPro": false, "fullname": "binxia", "user": "binxia", "type": "user"}, "summary": "Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.", "upvotes": 10, "discussionId": "69549ff6869a8627b452c961", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n  <li>\u63d0\u51fa\u4e86\u57fa\u4e8e\u6d82\u9e26\u7684\u7f16\u8f91\u548c\u751f\u6210\u4efb\u52a1\uff0c\u4f7f\u5f97\u7528\u6237\u53ef\u4ee5\u66f4\u52a0\u7075\u6d3b\u5730\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\u4e0a\u8fdb\u884c\u521b\u4f5c\u3002</li>\n  <li>DreamOmni3 \u89e3\u51b3\u4e86\u6570\u636e\u521b\u5efa\u548c\u6846\u67b6\u8bbe\u8ba1\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\u3002</li>\n  <li>\u6570\u636e\u5408\u6210\u7ba1\u9053\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u6d82\u9e26\u7f16\u8f91\u548c\u751f\u6210\uff0c\u5b9a\u4e49\u4e86\u591a\u4e2a\u4efb\u52a1\u6765\u652f\u6301\u4e0d\u540c\u7684\u7f16\u8f91\u548c\u751f\u6210\u65b9\u5f0f\u3002</li>\n  <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u8f93\u5165\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u7f16\u8f91\uff0c\u63d0\u5347\u6a21\u578b\u7684\u7cbe\u786e\u6027\u3002</li>\n  <li>\u5efa\u7acb\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDreamOmni3\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models for editing and generating images use text prompts but often miss user intentions for detailed edits.</li>\n    <li>We propose two tasks: scribble-based editing and generation, allowing users to combine text, images, and sketches.</li>\n    <li>DreamOmni3 addresses challenges in creating data and designing the framework for these tasks.</li>\n    <li>We define specific tasks for editing and generating images based on user scribbles and instructions.</li>\n    <li>DreamOmni3 shows excellent results and will have its models and code available for public use.</li>\n</ul>"}, "publishedAt": "2025-12-27T04:07:12.000Z", "title": "DreamOmni3: Scribble-based Editing and Generation", "summary": "Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22525.png", "numComments": 2, "submittedBy": {"_id": "64e42a700ecc1ecca77b1db9", "avatarUrl": "/avatars/c1228db09b88c9246aab48da7ae82f7c.svg", "fullname": "binxia", "name": "binxia", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23675", "authors": [{"_id": "6954c9c9869a8627b452c99b", "name": "Arnuv Tandon", "hidden": false}, {"_id": "6954c9c9869a8627b452c99c", "name": "Karan Dalal", "hidden": false}, {"_id": "6954c9c9869a8627b452c99d", "name": "Xinhao Li", "hidden": false}, {"_id": "6954c9c9869a8627b452c99e", "name": "Daniel Koceja", "hidden": false}, {"_id": "6954c9c9869a8627b452c99f", "name": "Marcel R\u00f8d", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a0", "name": "Sam Buchanan", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a1", "name": "Xiaolong Wang", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a2", "name": "Jure Leskovec", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a3", "name": "Sanmi Koyejo", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a4", "name": "Tatsunori Hashimoto", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a5", "name": "Carlos Guestrin", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a6", "name": "Jed McCaleb", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a7", "name": "Yejin Choi", "hidden": false}, {"_id": "6954c9c9869a8627b452c9a8", "name": "Yu Sun", "hidden": false}], "publishedAt": "2025-12-29T18:30:14.000Z", "submittedOnDailyAt": "2025-12-31T04:29:26.053Z", "title": "End-to-End Test-Time Training for Long Context", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.", "upvotes": 6, "discussionId": "6954c9c9869a8627b452c9a9", "githubRepo": "https://github.com/test-time-training/e2e", "githubRepoAddedBy": "user", "githubStars": 96, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u5c06\u957f\u671f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u89c6\u4e3a\u6301\u7eed\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u67b6\u6784\u8bbe\u8ba1\u3002</li>\n    <li>\u4f7f\u7528\u6807\u51c6\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</li>\n    <li>\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u7ee7\u7eed\u5b66\u4e60\uff0c\u5e76\u5c06\u8bfb\u53d6\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u5230\u6743\u91cd\u4e2d\u3002</li>\n    <li>\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u7684\u5143\u5b66\u4e60\u6539\u5584\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u521d\u59cb\u5316\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u7684\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u901f\u5ea6\u66f4\u5feb\uff0c\u63a8\u7406\u5ef6\u8fdf\u6052\u5b9a\uff0c\u6bd4\u5168\u6ce8\u610f\u529b\u5feb2.7\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study approaches long-context language modeling as a continual learning problem using a standard Transformer model with sliding-window attention.</li>\n    <li>The model learns during test time by predicting the next token based on the provided context, adjusting its weights accordingly.</li>\n    <li>It enhances learning at test time through meta-learning during training, making the process efficient and end-to-end.</li>\n    <li>Experiments show that this method scales well with longer contexts, matching the performance of full attention Transformers.</li>\n    <li>TTT-E2E offers faster inference times than traditional methods, being 2.7 times quicker for large context lengths.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:30:14.000Z", "title": "End-to-End Test-Time Training for Long Context", "summary": "We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23675.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23165", "authors": [{"_id": "6954cfbc869a8627b452c9ab", "name": "Qingyu Yin", "hidden": false}, {"_id": "6954cfbc869a8627b452c9ac", "name": "Yulun Wu", "hidden": false}, {"_id": "6954cfbc869a8627b452c9ad", "name": "Zhennan Shen", "hidden": false}, {"_id": "6954cfbc869a8627b452c9ae", "name": "Sunbowen Li", "hidden": false}, {"_id": "6954cfbc869a8627b452c9af", "name": "Zhilin Wang", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b0", "name": "Yanshu Li", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b1", "name": "Chak Tou Leong", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b2", "name": "Jiale Kang", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b3", "name": "Jinjin Gu", "hidden": false}], "publishedAt": "2025-12-29T03:13:08.000Z", "submittedOnDailyAt": "2025-12-31T04:55:40.592Z", "title": "Evaluating Parameter Efficient Methods for RLVR", "submittedOnDailyBy": {"_id": "6453cb22908e259483c0a061", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6453cb22908e259483c0a061/hMgdwZUsUbgquGalzPGzV.jpeg", "isPro": false, "fullname": "Qingyu_Yin", "user": "MikaStars39", "type": "user"}, "summary": "We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (e.g., PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (e.g., VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.", "upvotes": 5, "discussionId": "6954cfbc869a8627b452c9b4", "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6846\u67b6\u4e0b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u3002</li>\n    <li>\u5c3d\u7ba1LoRA\u5e38\u88ab\u4f7f\u7528\uff0c\u4f46\u6700\u4f18\u7684PEFT\u67b6\u6784\u4ecd\u672a\u786e\u5b9a\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u7ed3\u6784\u53d8\u4f53\uff08\u5982DoRA\u3001AdaLoRA\u548cMiSS\uff09\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eLoRA\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u5728\u67d0\u4e9b\u521d\u59cb\u5316\u7b56\u7565\u4e2d\u51fa\u73b0\u4e86\u8c31\u5d29\u6e83\u73b0\u8c61\uff0c\u5bfc\u81f4\u5b83\u4eec\u7684\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u6781\u7aef\u53c2\u6570\u51cf\u5c11\uff08\u5982VeRA\u3001Rank-1\uff09\u663e\u8457\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u793a\u9700\u8981\u66f4\u591a\u63a2\u7d22\u53c2\u6570\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study evaluates different methods of Parameter-Efficient Fine-Tuning (PEFT) for improving reasoning in language models using Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>It finds that alternative methods like DoRA, AdaLoRA, and MiSS perform better than the commonly used LoRA method.</li>\n    <li>There is a problem called spectral collapse in certain initialization strategies, which hinders performance due to misalignment with RL optimization.</li>\n    <li>Extreme reductions in parameters significantly limit reasoning abilities in models.</li>\n    <li>The research encourages further exploration into effective parameter-efficient methods for reinforcement learning.</li>\n</ul>"}, "publishedAt": "2025-12-28T22:13:08.000Z", "title": "Evaluating Parameter Efficient Methods for RLVR", "summary": "We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (e.g., PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (e.g., VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23165.png", "numComments": 1, "submittedBy": {"_id": "6453cb22908e259483c0a061", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6453cb22908e259483c0a061/hMgdwZUsUbgquGalzPGzV.jpeg", "fullname": "Qingyu_Yin", "name": "MikaStars39", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22469", "authors": [{"_id": "6954c129869a8627b452c97c", "name": "Wei Liu", "hidden": false}, {"_id": "6954c129869a8627b452c97d", "user": {"_id": "64425a502f4abae43fc0446c", "avatarUrl": "/avatars/7448a8d024813d8a20e09c162a189304.svg", "isPro": false, "fullname": "Chao Peng", "user": "pengchao", "type": "user"}, "name": "Chao Peng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-31T20:54:27.882Z", "hidden": false}, {"_id": "6954c129869a8627b452c97e", "name": "Pengfei Gao", "hidden": false}, {"_id": "6954c129869a8627b452c97f", "name": "Aofan Liu", "hidden": false}, {"_id": "6954c129869a8627b452c980", "name": "Wei Zhang", "hidden": false}, {"_id": "6954c129869a8627b452c981", "name": "Haiyan Zhao", "hidden": false}, {"_id": "6954c129869a8627b452c982", "name": "Zhi Jin", "hidden": false}], "publishedAt": "2025-12-27T05:02:53.000Z", "submittedOnDailyAt": "2025-12-31T04:28:11.263Z", "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization", "submittedOnDailyBy": {"_id": "64425a502f4abae43fc0446c", "avatarUrl": "/avatars/7448a8d024813d8a20e09c162a189304.svg", "isPro": false, "fullname": "Chao Peng", "user": "pengchao", "type": "user"}, "summary": "The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.", "upvotes": 2, "discussionId": "6954c129869a8627b452c983", "summary_zh": "<ul>\n    <li>\u95ee\u9898\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u6839\u636e\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u627e\u51fa\u8f6f\u4ef6\u4ee3\u7801\u5e93\u4e2d\u9700\u8981\u4fee\u6539\u7684\u4f4d\u7f6e\u3002</li>\n    <li>\u8be5\u4efb\u52a1\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u75c7\u72b6\u4e0e\u539f\u56e0\u4e0d\u5339\u914d\u4ee5\u53ca\u4e00\u4e2a\u95ee\u9898\u5bf9\u5e94\u591a\u4e2a\u4ee3\u7801\u5b9e\u4f53\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86GraphLocator\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d1\u73b0\u56e0\u679c\u7ed3\u6784\u6765\u7f13\u89e3\u75c7\u72b6\u4e0e\u539f\u56e0\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002</li>\n    <li>GraphLocator\u7684\u6838\u5fc3\u662f\u56e0\u679c\u95ee\u9898\u56fe\uff08CIG\uff09\uff0c\u5176\u4e2d\u9876\u70b9\u8868\u793a\u53d1\u73b0\u7684\u5b50\u95ee\u9898\u53ca\u5176\u76f8\u5173\u4ee3\u7801\uff0c\u8fb9\u8868\u793a\u5b83\u4eec\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGraphLocator\u5728\u51c6\u786e\u5b9a\u4f4d\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u529f\u80fd\u7ea7\u53ec\u56de\u7387\u63d0\u9ad8\u4e8619.49%\uff0c\u7cbe\u786e\u5ea6\u63d0\u9ad8\u4e8611.89%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The task of issue localization aims to find where changes are needed in software based on problem descriptions.</li>\n    <li>There are two main challenges: understanding the root cause of issues and dealing with multiple code parts linked to a single issue.</li>\n    <li>GraphLocator is a new method that helps solve these challenges by creating a causal issue graph (CIG) that maps sub-issues to their related code.</li>\n    <li>GraphLocator works in two steps: first, it finds symptom locations in the code, and then it builds the CIG by exploring code relationships.</li>\n    <li>Tests show that GraphLocator is more effective than previous methods, with significant improvements in accuracy for finding and resolving issues.</li>\n</ul>"}, "publishedAt": "2025-12-27T00:02:53.000Z", "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization", "summary": "The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22469.png", "numComments": 1, "submittedBy": {"_id": "64425a502f4abae43fc0446c", "avatarUrl": "/avatars/7448a8d024813d8a20e09c162a189304.svg", "fullname": "Chao Peng", "name": "pengchao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21008", "authors": [{"_id": "6954ddee869a8627b452c9d2", "user": {"_id": "6786535551cd69b65efd2bee", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png", "isPro": false, "fullname": "Lichao Wu", "user": "woorkhaarder", "type": "user"}, "name": "Lichao Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-31T20:54:19.177Z", "hidden": false}, {"_id": "6954ddee869a8627b452c9d3", "name": "Sasha Behrouzi", "hidden": false}, {"_id": "6954ddee869a8627b452c9d4", "name": "Mohamadreza Rostami", "hidden": false}, {"_id": "6954ddee869a8627b452c9d5", "name": "Stjepan Picek", "hidden": false}, {"_id": "6954ddee869a8627b452c9d6", "name": "Ahmad-Reza Sadeghi", "hidden": false}], "publishedAt": "2025-12-24T07:13:24.000Z", "submittedOnDailyAt": "2025-12-31T05:56:16.694Z", "title": "GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs", "submittedOnDailyBy": {"_id": "6786535551cd69b65efd2bee", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png", "isPro": false, "fullname": "Lichao Wu", "user": "woorkhaarder", "type": "user"}, "summary": "Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.\n  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.", "upvotes": 0, "discussionId": "6954ddee869a8627b452c9d7", "organization": {"_id": "656dd6ea7c934a7b3c4c59c2", "name": "is-tuda", "fullname": "Technical University of Darmstadt - Information Systems", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6464e09d0e6c7618f611d9d6/WXo06EWxaZCtWdSUftCzi.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u901a\u8fc7\u6bcf\u6b21\u8f93\u5165\u6fc0\u6d3b\u5c11\u91cf\u53c2\u6570\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u3002</li>\n    <li>\u73b0\u6709\u7684\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5bc6\u96c6\u67b6\u6784\u4e0a\uff0cMoE\u7684\u72ec\u7279\u5b89\u5168\u7279\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86GateBreaker\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u653b\u51fb\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u524a\u5f31MoE LLM\u7684\u5b89\u5168\u5bf9\u9f50\u3002</li>\n    <li>GateBreaker\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u5de5\u4f5c\uff1a\u8bc6\u522b\u6709\u5bb3\u8f93\u5165\u7684\u5b89\u5168\u4e13\u5bb6\u3001\u5b9a\u4f4d\u5b89\u5168\u7ed3\u6784\u4ee5\u53ca\u9009\u62e9\u6027\u7981\u7528\u5b89\u5168\u7ed3\u6784\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u7981\u7528\u7ea63%\u7684\u795e\u7ecf\u5143\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u4ece7.4%\u63d0\u5347\u81f364.9%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models improve Large Language Models by using fewer parameters, making them more efficient and cost-effective.</li>\n    <li>Understanding MoE safety mechanisms is important as they are used in sensitive areas, but most research has focused on traditional dense models.</li>\n    <li>The paper introduces GateBreaker, a new tool that attacks MoE models' safety features without needing prior training.</li>\n    <li>GateBreaker works in three steps: finding harmful inputs, locating safety features, and disabling them to compromise safety.</li>\n    <li>The study shows that disabling just 3% of certain neurons can greatly increase the success rate of attacks on MoE models, while still keeping performance relatively stable.</li>\n</ul>"}, "publishedAt": "2025-12-24T02:13:24.000Z", "title": "GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs", "summary": "Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.\n  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21008.png", "numComments": 1, "submittedBy": {"_id": "6786535551cd69b65efd2bee", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png", "fullname": "Lichao Wu", "name": "woorkhaarder", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "656dd6ea7c934a7b3c4c59c2", "name": "is-tuda", "fullname": "Technical University of Darmstadt - Information Systems", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6464e09d0e6c7618f611d9d6/WXo06EWxaZCtWdSUftCzi.png"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Mixture-of-Experts (MoE) \u6a21\u578b\u7f3a\u4e4f\u7ea6\u675f\uff0c\u5bfc\u81f4\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u7684\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\u635f\u5931\uff08ERC\u635f\u5931\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f85\u52a9\u635f\u5931\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u786e\u4fdd\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u5176\u4ee3\u7406\u6807\u8bb0\u7684\u6fc0\u6d3b\u9ad8\u4e8e\u5bf9\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406\u6807\u8bb0\uff0c\u5e76\u4f7f\u6bcf\u4e2a\u4ee3\u7406\u6807\u8bb0\u4ece\u5176\u5bf9\u5e94\u7684\u4e13\u5bb6\u90a3\u91cc\u83b7\u5f97\u66f4\u5f3a\u7684\u6fc0\u6d3b\u3002</li>\n    <li>\u8be5\u635f\u5931\u8ba1\u7b97\u9ad8\u6548\uff0c\u4ec5\u9700\u5904\u7406n\u00b2\u4e2a\u6fc0\u6d3b\uff0c\u5176\u4e2dn\u4e3a\u4e13\u5bb6\u6570\u91cf\uff0c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u53c2\u6570\u4ece30\u4ebf\u5230150\u4ebf\u7684MoE-LLM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u4e86ERC\u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u4e13\u5bb6\u4e13\u95e8\u5316\u6c34\u5e73\u7684\u63a7\u5236\u548c\u5b9a\u91cf\u8ddf\u8e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models need better ways to connect the router's choices with the abilities of each expert to improve performance.</li>\n    <li>We introduce a new loss function called expert-router coupling (ERC) loss that helps link the router's decisions to the experts' strengths.</li>\n    <li>The ERC loss makes sure that each expert is more responsive to its own assigned tokens than to those of other experts.</li>\n    <li>This method is efficient, using a fixed amount of resources regardless of the number of tokens, unlike older methods that required more resources as the number of tokens increased.</li>\n    <li>Our tests on large MoE models show that ERC loss enhances expert specialization and provides insights during training.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23576", "authors": [{"_id": "69534f1e89916ff627aa3fe3", "name": "Ethan Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe4", "name": "Zhulin Hu", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe5", "name": "Bohao Tang", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe6", "name": "Jiadi Su", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe7", "name": "Steffi Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe8", "name": "Zhijie Deng", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe9", "name": "Pengfei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "publishedAt": "2025-12-29T16:17:36.000Z", "submittedOnDailyAt": "2025-12-30T02:36:23.479Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "submittedOnDailyBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "isPro": false, "fullname": "Ethan Chern", "user": "ethanchern", "type": "user"}, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "upvotes": 51, "discussionId": "69534f1e89916ff627aa3fea", "githubRepo": "https://github.com/GAIR-NLP/LiveTalk", "githubRepoAddedBy": "user", "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.", "ai_keywords": ["diffusion models", "bidirectional attention", "distillation methods", "on-policy distillation", "Self Forcing", "audio language models", "Anchor-Heavy Identity Sinks", "multimodal conditioning", "autoregressive", "on-policy optimization"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u4e2d\u7684\u591a\u6a21\u6001\u4e92\u52a8\u95ee\u9898\uff0c\u5305\u62ec\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u3002</li>\n    <li>\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u5728\u53bb\u566a\u89c6\u9891\u5e27\u65f6\u6548\u7387\u4f4e\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u4e92\u52a8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u91cd\u70b9\u63d0\u9ad8\u6761\u4ef6\u8f93\u5165\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u4e0e\u4f20\u7edf\u6a21\u578b\u76f8\u4f3c\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u964d\u4f4e\u4e8620\u500d\u3002</li>\n    <li>\u6574\u5408\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u9891\u63a8\u65ad\u6280\u672f\u540e\uff0c\u521b\u5efa\u4e86\u5b9e\u65f6\u4e92\u52a8\u7684\u865a\u62df\u5934\u50cf\u7cfb\u7edfLiveTalk\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8f6e\u4e92\u52a8\u7684\u89c6\u9891\u8fde\u8d2f\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper focuses on improving real-time video generation for interactive AI systems using diffusion models.</li>\n    <li>Current methods struggle with natural human-AI interaction due to slow video frame processing.</li>\n    <li>The authors propose a better way to distill models that enhances video quality and reduces issues like flickering and black frames.</li>\n    <li>The new model provides high visual quality while being 20 times faster and less costly in terms of resources.</li>\n    <li>They created a system called LiveTalk that allows for real-time, multimodal interactions with avatars, outperforming existing models.</li>\n</ul>"}, "publishedAt": "2025-12-29T11:17:36.000Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png", "numComments": 1, "submittedBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "fullname": "Ethan Chern", "name": "ethanchern", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22096", "authors": [{"_id": "695206a8746a34b55dd548dd", "name": "Xiaofeng Mao", "hidden": false}, {"_id": "695206a8746a34b55dd548de", "name": "Zhen Li", "hidden": false}, {"_id": "695206a8746a34b55dd548df", "name": "Chuanhao Li", "hidden": false}, {"_id": "695206a8746a34b55dd548e0", "name": "Xiaojie Xu", "hidden": false}, {"_id": "695206a8746a34b55dd548e1", "name": "Kaining Ying", "hidden": false}, {"_id": "695206a8746a34b55dd548e2", "name": "Tong He", "hidden": false}, {"_id": "695206a8746a34b55dd548e3", "name": "Jiangmiao Pang", "hidden": false}, {"_id": "695206a8746a34b55dd548e4", "name": "Yu Qiao", "hidden": false}, {"_id": "695206a8746a34b55dd548e5", "name": "Kaipeng Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "publishedAt": "2025-12-26T17:52:49.000Z", "submittedOnDailyAt": "2025-12-30T01:50:23.447Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "upvotes": 50, "discussionId": "695206a8746a34b55dd548e6", "projectPage": "https://stdstu12.github.io/YUME-Project/", "githubRepo": "https://github.com/stdstu12/YUME", "githubRepoAddedBy": "user", "githubStars": 426, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u663e\u793a\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u4e92\u52a8\u548c\u53ef\u63a2\u7d22\u7684\u4e16\u754c\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u53c2\u6570\u8fc7\u5927\u3001\u63a8\u7406\u6b65\u9aa4\u957f\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u589e\u957f\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u5b9e\u65f6\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\\method\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u7247\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u771f\u5b9e\u7684\u4e92\u52a8\u4e16\u754c\u3002</li>\n    <li>\u8be5\u6846\u67b6\u652f\u6301\u952e\u76d8\u63a2\u7d22\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u957f\u89c6\u9891\u751f\u6210\u3001\u5b9e\u65f6\u52a0\u901f\u7b56\u7565\u548c\u6587\u672c\u63a7\u5236\u7684\u4e16\u754c\u4e8b\u4ef6\u751f\u6210\u3002</li>\n    <li>\u4ee3\u7801\u5e93\u5df2\u5728\u8865\u5145\u6750\u6599\u4e2d\u63d0\u4f9b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods using diffusion models can create interactive virtual worlds from images or text.</li>\n    <li>Current approaches struggle with large size, slow processing, and limited text control.</li>\n    <li>The proposed framework, called \\method, allows for realistic and continuous world generation.</li>\n    <li>\\method includes features for keyboard exploration and real-time performance improvements.</li>\n    <li>It consists of three main parts: video generation, a streaming acceleration strategy, and a text-controlled event generation method.</li>\n</ul>"}, "publishedAt": "2025-12-26T12:52:49.000Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22096.png", "numComments": 1, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22322", "authors": [{"_id": "69533fb889916ff627aa3ecb", "name": "Shaofei Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ecc", "name": "Yulei Qin", "hidden": false}, {"_id": "69533fb889916ff627aa3ecd", "name": "Haojia Lin", "hidden": false}, {"_id": "69533fb889916ff627aa3ece", "name": "Zihan Xu", "hidden": false}, {"_id": "69533fb889916ff627aa3ecf", "name": "Gang Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed0", "name": "Yuchen Shi", "hidden": false}, {"_id": "69533fb889916ff627aa3ed1", "name": "Zongyi Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed2", "name": "Yong Mao", "hidden": false}, {"_id": "69533fb889916ff627aa3ed3", "name": "Siqi Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ed4", "name": "Xiaoyu Tan", "hidden": false}, {"_id": "69533fb889916ff627aa3ed5", "name": "Yitao Liang", "hidden": false}, {"_id": "69533fb889916ff627aa3ed6", "name": "Ke Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed7", "name": "Xing Sun", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "publishedAt": "2025-12-26T14:51:39.000Z", "submittedOnDailyAt": "2025-12-30T01:07:21.942Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "submittedOnDailyBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "upvotes": 33, "discussionId": "69533fb889916ff627aa3ed8", "projectPage": "https://huggingface.co/collections/yolay/smartsnap", "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u590d\u6742GUI\u4efb\u52a1\u4e2d\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u4efb\u52a1\u5b8c\u6210\u7684\u9a8c\u8bc1\u5f71\u54cd\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>\u73b0\u6709\u7684\u4efb\u52a1\u9a8c\u8bc1\u662f\u88ab\u52a8\u7684\uff0c\u5206\u6790\u4ee3\u7406\u7684\u6574\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\u6765\u5224\u65ad\u662f\u5426\u6210\u529f\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u548c\u53ef\u9760\u6027\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SmartSnap\uff0c\u901a\u8fc7\u4e3b\u52a8\u7684\u81ea\u6211\u9a8c\u8bc1\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u4ee3\u7406\u4e0d\u4ec5\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u8981\u63d0\u4f9b\u8bc1\u636e\u8bc1\u660e\u5176\u5b8c\u6210\u60c5\u51b5\u3002</li>\n    <li>\u4ee3\u7406\u4f7f\u75283C\u539f\u5219\uff08\u5b8c\u6574\u6027\u3001\u7b80\u6d01\u6027\u548c\u521b\u9020\u6027\uff09\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u5173\u952e\u5feb\u7167\u4f5c\u4e3a\u8bc1\u636e\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSmartSnap\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c8B\u548c30B\u6a21\u578b\u7684\u8868\u73b0\u5206\u522b\u63d0\u9ad8\u4e8626.08%\u548c16.66%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic reinforcement learning can help create autonomous agents for complex tasks, but verifying task completion is a big challenge.</li>\n    <li>Current verification methods are passive and analyze the entire task history, which can be costly and unreliable due to irrelevant information.</li>\n    <li>SmartSnap introduces a new approach where agents proactively verify their own success using key evidences, called snapshots.</li>\n    <li>The Self-Verifying Agent follows 3C Principles (Completeness, Conciseness, Creativity) to improve the verification process.</li>\n    <li>Tests show that SmartSnap can enhance performance of agents significantly, making them more efficient and competitive.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:39.000Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22322.png", "numComments": 2, "submittedBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "fullname": "Yulei Qin", "name": "yolay", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23705", "authors": [{"_id": "6953546989916ff627aa4002", "name": "Shaocong Xu", "hidden": false}, {"_id": "6953546989916ff627aa4003", "name": "Songlin Wei", "hidden": false}, {"_id": "6953546989916ff627aa4004", "name": "Qizhe Wei", "hidden": false}, {"_id": "6953546989916ff627aa4005", "name": "Zheng Geng", "hidden": false}, {"_id": "6953546989916ff627aa4006", "name": "Hong Li", "hidden": false}, {"_id": "6953546989916ff627aa4007", "name": "Licheng Shen", "hidden": false}, {"_id": "6953546989916ff627aa4008", "name": "Qianpu Sun", "hidden": false}, {"_id": "6953546989916ff627aa4009", "name": "Shu Han", "hidden": false}, {"_id": "6953546989916ff627aa400a", "name": "Bin Ma", "hidden": false}, {"_id": "6953546989916ff627aa400b", "name": "Bohan Li", "hidden": false}, {"_id": "6953546989916ff627aa400c", "name": "Chongjie Ye", "hidden": false}, {"_id": "6953546989916ff627aa400d", "name": "Yuhang Zheng", "hidden": false}, {"_id": "6953546989916ff627aa400e", "name": "Nan Wang", "hidden": false}, {"_id": "6953546989916ff627aa400f", "name": "Saining Zhang", "hidden": false}, {"_id": "6953546989916ff627aa4010", "name": "Hao Zhao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "publishedAt": "2025-12-29T18:59:24.000Z", "submittedOnDailyAt": "2025-12-30T01:56:18.708Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "submittedOnDailyBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "isPro": true, "fullname": "Shaocong.Xu", "user": "Daniellesry", "type": "user"}, "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "upvotes": 32, "discussionId": "6953546a89916ff627aa4011", "projectPage": "https://daniellli.github.io/projects/DKT/", "githubRepo": "https://github.com/Daniellli/DKT", "githubRepoAddedBy": "user", "githubStars": 94, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u900f\u660e\u7269\u4f53\u5bf9\u4e8e\u611f\u77e5\u7cfb\u7edf\u6765\u8bf4\u5f88\u96be\u5904\u7406\uff0c\u56e0\u4e3a\u6298\u5c04\u3001\u53cd\u5c04\u548c\u900f\u5c04\u4f1a\u7834\u574f\u6df1\u5ea6\u4f30\u8ba1\u7684\u5047\u8bbe\u3002</li>\n    <li>\u73b0\u4ee3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u80fd\u591f\u5408\u6210\u53ef\u4fe1\u7684\u900f\u660e\u73b0\u8c61\uff0c\u8868\u660e\u5b83\u4eec\u638c\u63e1\u4e86\u5149\u5b66\u89c4\u5f8b\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86TransPhy3D\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u900f\u660e\u548c\u53cd\u5c04\u573a\u666f\u7684\u5408\u6210\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b11,000\u4e2a\u5e8f\u5217\u3002</li>\n    <li>\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u89c6\u9891\u5230\u89c6\u9891\u7684\u7ffb\u8bd1\u6a21\u578bDKT\uff0c\u53ef\u4ee5\u5728\u900f\u660e\u7269\u4f53\u7684\u6df1\u5ea6\u4f30\u8ba1\u4e0a\u5b9e\u73b0\u96f6-shot\u7684\u6700\u4f73\u6027\u80fd\u3002</li>\n    <li>DKT\u7684\u6df1\u5ea6\u4f30\u8ba1\u63d0\u9ad8\u4e86\u5728\u4e0d\u540c\u8868\u9762\uff08\u900f\u660e\u3001\u53cd\u5c04\u548c\u6f2b\u53cd\u5c04\uff09\u4e0a\u7684\u6210\u529f\u7387\uff0c\u663e\u793a\u51fa\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u900f\u660e\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Transparent objects are difficult for perception systems to analyze due to issues like refraction and reflection.</li>\n    <li>The authors created a new dataset called TransPhy3D, which includes 11,000 video sequences of transparent and reflective scenes using realistic materials.</li>\n    <li>They developed a model called DKT that translates video input to depth and normals, improving the predictions for videos involving transparency.</li>\n    <li>DKT achieves state-of-the-art results on various benchmarks, showing better accuracy and consistency than previous methods.</li>\n    <li>The findings suggest that generative models, like video diffusion models, can effectively handle complex visual challenges in real-world applications.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:24.000Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23705.png", "numComments": 1, "submittedBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "fullname": "Shaocong.Xu", "name": "Daniellesry", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23709", "authors": [{"_id": "69537f4189916ff627aa40c0", "name": "Hau-Shiang Shiu", "hidden": false}, {"_id": "69537f4189916ff627aa40c1", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "69537f4189916ff627aa40c2", "name": "Zhixiang Wang", "hidden": false}, {"_id": "69537f4189916ff627aa40c3", "name": "Chi-Wei Hsiao", "hidden": false}, {"_id": "69537f4189916ff627aa40c4", "name": "Po-Fan Yu", "hidden": false}, {"_id": "69537f4189916ff627aa40c5", "name": "Yu-Chih Chen", "hidden": false}, {"_id": "69537f4189916ff627aa40c6", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "publishedAt": "2025-12-29T18:59:57.000Z", "submittedOnDailyAt": "2025-12-30T05:04:09.292Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "upvotes": 29, "discussionId": "69537f4289916ff627aa40c7", "projectPage": "https://jamichss.github.io/stream-diffvsr-project-page/", "summary_zh": "<ul>\n    <li>Stream-DiffVSR \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u5728\u7ebf\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u80fd\u591f\u5feb\u901f\u5904\u7406\u89c6\u9891\u3002</li>\n    <li>\u5b83\u53ea\u4f9d\u8d56\u4e8e\u8fc7\u53bb\u7684\u5e27\uff0c\u907f\u514d\u4e86\u5bf9\u672a\u6765\u5e27\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6027\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5feb\u901f\u63a8\u7406\u7684\u56db\u6b65\u53bb\u566a\u5668\u548c\u8fd0\u52a8\u5bf9\u9f50\u63d0\u793a\u6a21\u5757\uff0c\u589e\u5f3a\u4e86\u56fe\u50cf\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5728 RTX4090 GPU \u4e0a\uff0c\u5904\u7406 720p \u5e27\u4ec5\u9700 0.328 \u79d2\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6269\u6563\u65b9\u6cd5\u3002</li>\n    <li>Stream-DiffVSR \u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u5ef6\u8fdf\uff0c\u9002\u5408\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Stream-DiffVSR is a new method for improving video quality that works quickly and efficiently.</li>\n    <li>It only uses past video frames, which helps it perform better in real-time situations.</li>\n    <li>The method includes a fast four-step denoiser and a module that aligns motion for better video quality.</li>\n    <li>Stream-DiffVSR processes 720p video frames in just 0.328 seconds, making it much faster than previous methods.</li>\n    <li>It significantly lowers the time delay for video processing, making it suitable for applications that need quick responses.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:57.000Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23709.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22615", "authors": [{"_id": "6953489889916ff627aa3f25", "name": "Jiacheng Ye", "hidden": false}, {"_id": "6953489889916ff627aa3f26", "name": "Shansan Gong", "hidden": false}, {"_id": "6953489889916ff627aa3f27", "name": "Jiahui Gao", "hidden": false}, {"_id": "6953489889916ff627aa3f28", "name": "Junming Fan", "hidden": false}, {"_id": "6953489889916ff627aa3f29", "name": "Shuang Wu", "hidden": false}, {"_id": "6953489889916ff627aa3f2a", "name": "Wei Bi", "hidden": false}, {"_id": "6953489889916ff627aa3f2b", "name": "Haoli Bai", "hidden": false}, {"_id": "6953489889916ff627aa3f2c", "name": "Lifeng Shang", "hidden": false}, {"_id": "6953489889916ff627aa3f2d", "name": "Lingpeng Kong", "hidden": false}], "publishedAt": "2025-12-27T14:46:24.000Z", "submittedOnDailyAt": "2025-12-30T03:42:33.237Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "submittedOnDailyBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "isPro": false, "fullname": "Shansan Gong", "user": "Sansa", "type": "user"}, "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "upvotes": 27, "discussionId": "6953489889916ff627aa3f2e", "projectPage": "https://hkunlp.github.io/blog/2025/dream-vlx/", "githubRepo": "https://github.com/DreamLM/Dream-VLX", "githubRepoAddedBy": "user", "ai_summary": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines.", "ai_keywords": ["diffusion-based large language models (dLLMs)", "Vision-Language Models (VLMs)", "Dream-VL", "Vision-Language-Action model (dVLA)", "Dream-VLA", "action chunking", "parallel generation", "LIBERO", "SimplerEnv-Bridge", "SimplerEnv-Fractal", "continuous pre-training"], "githubStars": 40, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7684\u89c6\u89c9\u89c4\u5212\u548c\u52a8\u6001\u673a\u5668\u4eba\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u79f0\u4e3aDream-VL\uff0c\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u7684\u6a21\u578b\u3002</li>\n    <li>Dream-VL\u5728\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u9876\u7ea7\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u57fa\u4e8eDream-VL\uff0c\u6211\u4eec\u5f00\u53d1\u4e86Dream-VLA\uff0c\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7684\u6a21\u578b\uff0c\u7ecf\u8fc7\u6301\u7eed\u7684\u9884\u8bad\u7ec3\uff0c\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002</li>\n    <li>Dream-VLA\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5e76\u4e14\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u6a21\u578b\uff0c\u6211\u4eec\u5c06Dream-VL\u548cDream-VLA\u53d1\u5e03\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Autoregressive Large Vision-Language Models (VLMs) have challenges with complex visual planning and dynamic robotic control.</li>\n  <li>This study introduces Dream-VL, a new vision-language model that uses diffusion-based large language models (dLLMs) and performs better than previous models.</li>\n  <li>Dream-VL is effective in visual planning tasks and competes well with top autoregressive models on various benchmarks.</li>\n  <li>Building on Dream-VL, Dream-VLA is a model that integrates vision, language, and action, showing faster training and better performance on specific robotic tasks.</li>\n  <li>Both Dream-VL and Dream-VLA are released for public use to encourage further research in this area.</li>\n</ul>"}, "publishedAt": "2025-12-27T09:46:24.000Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22615.png", "numComments": 1, "submittedBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "fullname": "Shansan Gong", "name": "Sansa", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22323", "authors": [{"_id": "6953692989916ff627aa4065", "name": "Zhibin Qin", "hidden": false}, {"_id": "6953692989916ff627aa4066", "name": "Zhenxiong Tan", "hidden": false}, {"_id": "6953692989916ff627aa4067", "name": "Zeqing Wang", "hidden": false}, {"_id": "6953692989916ff627aa4068", "name": "Songhua Liu", "hidden": false}, {"_id": "6953692989916ff627aa4069", "name": "Xinchao Wang", "hidden": false}], "publishedAt": "2025-12-26T14:59:41.000Z", "submittedOnDailyAt": "2025-12-30T03:43:24.884Z", "title": "SpotEdit: Selective Region Editing in Diffusion Transformers", "submittedOnDailyBy": {"_id": "640ebdfefdeaae139086f4d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg", "isPro": true, "fullname": "Zhenxiong Tan", "user": "Yuanshi", "type": "user"}, "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.", "upvotes": 27, "discussionId": "6953692989916ff627aa406a", "projectPage": "https://biangbiang0321.github.io/SpotEdit.github.io", "githubRepo": "https://github.com/Biangbiang0321/SpotEdit", "githubRepoAddedBy": "user", "githubStars": 36, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "summary_zh": "<ul>\n    <li>Diffusion Transformer \u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5904\u7406\u6240\u6709\u533a\u57df\u65f6\u6548\u7387\u4f4e\u3002</li>\n    <li>\u5927\u591a\u6570\u7f16\u8f91\u53ea\u9700\u4fee\u6539\u5c0f\u533a\u57df\uff0c\u73b0\u6709\u65b9\u6cd5\u5374\u4f1a\u91cd\u590d\u8ba1\u7b97\u672a\u4fee\u6539\u7684\u533a\u57df\u3002</li>\n    <li>SpotEdit \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u53ea\u66f4\u65b0\u88ab\u4fee\u6539\u7684\u533a\u57df\u3002</li>\n    <li>SpotEdit \u5305\u542b\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1aSpotSelector \u8bc6\u522b\u7a33\u5b9a\u533a\u57df\u5e76\u91cd\u7528\u6761\u4ef6\u56fe\u50cf\u7279\u5f81\uff0cSpotFusion \u52a8\u6001\u878d\u5408\u7279\u5f81\u548c\u7f16\u8f91\u5185\u5bb9\u3002</li>\n    <li>SpotEdit \u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0c\u786e\u4fdd\u672a\u4fee\u6539\u533a\u57df\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Transformer models improve image editing but often process all areas, wasting resources.</li>\n    <li>Many edits only change small parts of an image, making it inefficient to regenerate everything.</li>\n    <li>SpotEdit is a new framework that only updates the parts of the image that are modified.</li>\n    <li>It has two main parts: SpotSelector identifies stable regions to avoid recalculating them, and SpotFusion blends edited and unedited areas.</li>\n    <li>SpotEdit makes image editing faster and keeps the quality of unchanged areas high.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:59:41.000Z", "title": "SpotEdit: Selective Region Editing in Diffusion Transformers", "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22323.png", "numComments": 2, "submittedBy": {"_id": "640ebdfefdeaae139086f4d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg", "fullname": "Zhenxiong Tan", "name": "Yuanshi", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 170}, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23541", "authors": [{"_id": "69534cef89916ff627aa3f77", "name": "Pengfei Zhou", "hidden": false}, {"_id": "69534cef89916ff627aa3f78", "name": "Liliang Chen", "hidden": false}, {"_id": "69534cef89916ff627aa3f79", "name": "Shengcong Chen", "hidden": false}, {"_id": "69534cef89916ff627aa3f7a", "name": "Di Chen", "hidden": false}, {"_id": "69534cef89916ff627aa3f7b", "name": "Wenzhi Zhao", "hidden": false}, {"_id": "69534cef89916ff627aa3f7c", "name": "Rongjun Jin", "hidden": false}, {"_id": "69534cef89916ff627aa3f7d", "name": "Guanghui Ren", "hidden": false}, {"_id": "69534cef89916ff627aa3f7e", "name": "Jianlan Luo", "hidden": false}], "publishedAt": "2025-12-29T15:28:42.000Z", "submittedOnDailyAt": "2025-12-30T05:09:42.689Z", "title": "Act2Goal: From World Model To General Goal-conditioned Policy", "submittedOnDailyBy": {"_id": "646ec9b135f55eb49e405faa", "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg", "isPro": false, "fullname": "Guanghui Ren", "user": "sundrops", "type": "user"}, "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/", "upvotes": 19, "discussionId": "69534cf089916ff627aa3f7f", "projectPage": "https://act2goal.github.io/", "ai_summary": "Act2Goal employs a goal-conditioned visual world model with multi-scale temporal control and cross-attention to achieve robust long-horizon robotic manipulation through structured planning and adaptive execution.", "ai_keywords": ["goal-conditioned visual world model", "Multi-Scale Temporal Hashing (MSTH)", "cross-attention", "end-to-end cross-attention", "LoRA-based finetuning", "hindsight goal relabeling"], "organization": {"_id": "676fc7c31c48eff17fac3135", "name": "agibot-world", "fullname": "AgiBot World", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565Act2Goal\uff0c\u65e8\u5728\u66f4\u597d\u5730\u5b9e\u73b0\u957f\u65f6\u95f4\u7684\u64cd\u4f5c\u4efb\u52a1\u3002</li>\n    <li>\u8be5\u7b56\u7565\u7ed3\u5408\u4e86\u76ee\u6807\u5bfc\u5411\u7684\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a7\u5236\uff0c\u4ee5\u751f\u6210\u5408\u7406\u7684\u89c6\u89c9\u72b6\u6001\u5e8f\u5217\u3002</li>\n    <li>\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u95f4\u54c8\u5e0c\uff08MSTH\uff09\u6280\u672f\uff0c\u7ec6\u5316\u4e86\u64cd\u4f5c\u8fc7\u7a0b\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5c40\u90e8\u5e72\u6270\u3002</li>\n    <li>Act2Goal\u5728\u5904\u7406\u65b0\u7684\u7269\u4f53\u548c\u73af\u5883\u65f6\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6-shot\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc1\u660e\uff0cAct2Goal\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u4ece30%\u63d0\u9ad8\u523090%\uff0c\u663e\u793a\u4e86\u5176\u4f18\u79c0\u7684\u64cd\u4f5c\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Act2Goal is a new policy for robotic manipulation that helps robots understand and achieve complex tasks using visual goals.</li>\n    <li>It uses a world model to create a sequence of visual steps that guide the robot over longer tasks, improving its ability to predict actions.</li>\n    <li>The method includes Multi-Scale Temporal Hashing (MSTH) to manage both detailed and broader task aspects for better control.</li>\n    <li>Act2Goal can adapt quickly to new objects and environments without needing external help, improving performance from 30% to 90% in challenging situations.</li>\n    <li>Real-robot tests show that this approach significantly enhances the robot's ability to handle difficult tasks autonomously.</li>\n</ul>"}, "publishedAt": "2025-12-29T10:28:42.000Z", "title": "Act2Goal: From World Model To General Goal-conditioned Policy", "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23541.png", "numComments": 1, "submittedBy": {"_id": "646ec9b135f55eb49e405faa", "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg", "fullname": "Guanghui Ren", "name": "sundrops", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "676fc7c31c48eff17fac3135", "name": "agibot-world", "fullname": "AgiBot World", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22047", "authors": [{"_id": "6951e7d4746a34b55dd548a7", "name": "Hanzhang Zhou", "hidden": false}, {"_id": "6951e7d4746a34b55dd548a8", "name": "Xu Zhang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548a9", "name": "Panrong Tong", "hidden": false}, {"_id": "6951e7d4746a34b55dd548aa", "name": "Jianan Zhang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ab", "name": "Liangyu Chen", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ac", "name": "Quyu Kong", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ad", "name": "Chenglin Cai", "hidden": false}, {"_id": "6951e7d4746a34b55dd548ae", "name": "Chen Liu", "hidden": false}, {"_id": "6951e7d4746a34b55dd548af", "name": "Yue Wang", "hidden": false}, {"_id": "6951e7d4746a34b55dd548b0", "name": "Jingren Zhou", "hidden": false}, {"_id": "6951e7d4746a34b55dd548b1", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-26T14:51:52.000Z", "submittedOnDailyAt": "2025-12-29T00:01:11.405Z", "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.", "upvotes": 19, "discussionId": "6951e7d4746a34b55dd548b2", "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>MAI-UI\u662f\u4e00\u79cd\u65b0\u578b\u7684GUI\u4ee3\u7406\uff0c\u5177\u6709\u591a\u79cd\u89c4\u6a21\u7684\u7248\u672c\uff0c\u65e8\u5728\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u89e3\u51b3\u4e86\u7528\u6237\u4ea4\u4e92\u7f3a\u5931\u3001\u754c\u9762\u64cd\u4f5c\u9650\u5236\u3001\u7f3a\u4e4f\u90e8\u7f72\u67b6\u6784\u548c\u52a8\u6001\u73af\u5883\u8106\u5f31\u6027\u7b49\u56db\u4e2a\u4e3b\u8981\u6311\u6218\u3002</li>\n    <li>MAI-UI\u901a\u8fc7\u81ea\u6211\u6f14\u53d8\u7684\u6570\u636e\u7ba1\u9053\u548c\u8bbe\u5907-\u4e91\u534f\u4f5c\u7cfb\u7edf\u6765\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u6267\u884c\u3002</li>\n    <li>\u5728GUI\u5bf9\u63a5\u548c\u79fb\u52a8\u5bfc\u822a\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMAI-UI\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f73\u6210\u7ee9\uff0c\u8d85\u8d8a\u4e86\u591a\u4e2a\u7ade\u4e89\u5bf9\u624b\u3002</li>\n    <li>\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u663e\u793a\uff0c\u6269\u5c55\u5e76\u884c\u73af\u5883\u548c\u589e\u52a0\u6b65\u6570\u9884\u7b97\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u540c\u65f6\u8fd8\u63d0\u9ad8\u4e86\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MAI-UI is a new type of GUI agent designed to improve how people interact with computers.</li>\n    <li>It overcomes four main challenges: agent-user interaction, UI-only operation limitations, deployment issues, and problems in dynamic settings.</li>\n    <li>MAI-UI uses a unique approach that includes a data pipeline for user interactions, a device-cloud collaboration system, and an online reinforcement learning framework.</li>\n    <li>It achieves state-of-the-art results in GUI grounding and mobile navigation, surpassing other models in performance benchmarks.</li>\n    <li>The system enhances performance on devices, reduces reliance on cloud calls, and helps protect user privacy.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:52.000Z", "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents", "summary": "The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22047.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 196}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86DeepSeek-V3.2\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u91c7\u7528\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cDeepSeek-V3.2\u7684\u6027\u80fd\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u672c\u8d85\u8d8aGPT-5\uff0c\u63a8\u7406\u80fd\u529b\u4e0eGemini-3.0-Pro\u76f8\u5f53\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u667a\u80fd\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u6267\u884c\u7684\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines fast performance with excellent reasoning abilities.</li>\n    <li>It features an efficient attention mechanism called DeepSeek Sparse Attention (DSA) that lowers computation needs while maintaining effectiveness with long data contexts.</li>\n    <li>The model uses a strong reinforcement learning approach, performing similarly to GPT-5, and its advanced version exceeds GPT-5's performance.</li>\n    <li>DeepSeek-V3.2 has demonstrated top-level reasoning skills, winning gold in prestigious competitions like the International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a unique pipeline for creating large amounts of training data, improving its ability to follow instructions and adapt in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u8feb\u5207\u9700\u8981\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u3002</li>\n    <li>\u5f53\u524d\u7684\u6570\u636e\u51c6\u5907\u65b9\u5f0f\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\u548c\u677e\u6563\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u539f\u5219\u6027\u62bd\u8c61\uff0c\u5bfc\u81f4\u53ef\u91cd\u590d\u6027\u5dee\uff0c\u652f\u6301\u6709\u9650\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDataFlow\u7684\u7edf\u4e00\u6269\u5c55\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u3001\u53ef\u91cd\u7528\u548c\u53ef\u7ec4\u5408\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u8de8\u9886\u57df\u7684\u7ba1\u9053\uff0c\u9002\u7528\u4e8e\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u591a\u4e2a\u9886\u57df\u3002</li>\n    <li>\u901a\u8fc7DataFlow-Agent\uff0c\u7528\u6237\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u9ad8\u4e0b\u6e38LLM\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-quality data is essential for Large Language Models (LLMs), but current data preparation methods are often chaotic and hard to reproduce.</li>\n    <li>DataFlow is a new framework designed to create better data preparation pipelines that are modular, reusable, and easy to debug.</li>\n    <li>The framework includes nearly 200 reusable operators and supports various tasks like text processing, mathematical reasoning, and code generation.</li>\n    <li>DataFlow-Agent can automatically convert natural language instructions into working data pipelines, making it user-friendly.</li>\n    <li>Tests show that DataFlow significantly boosts LLM performance, outperforming traditional datasets and methods in various benchmarks.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u201d\u6846\u67b6\uff0c\u4f7f\u7528\u4e00\u4e2a\u542b\u6709140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u52a0\u901f\u53bb\u566a\u6b65\u9aa4\uff0c\u6253\u7834\u81ea\u56de\u5f52\u74f6\u9888\uff0c\u5b9e\u73b0\u7a33\u5b9a\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u4e3a\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u6821\u51c6\u5916\u89c2\u6765\u7ef4\u62a4\u5e8f\u5217\u7684\u4fdd\u771f\u5ea6\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6cd5\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u9996\u6b21\u5728\u8fd9\u4e2a\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for generating realistic avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>It uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up the process by using multiple GPUs, allowing for low-latency streaming.</li>\n    <li>To improve consistency in the avatars' appearance, it employs a method called Rolling Sink Frame Mechanism (RSFM) that keeps images looking stable over time.</li>\n    <li>Live Avatar can generate video at 20 frames per second on five GPUs, achieving high-quality results quickly.</li>\n    <li>This work sets a new standard for using advanced models in long videos, making it practical for real-time applications. </li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4e86\u771f\u5b9e\u4f01\u4e1a\u7684\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u6d89\u53ca\u5bf9\u5de5\u4e1a\u6a21\u5f0f\u8fdb\u884crepository\u7ea7\u7684\u5de5\u7a0b\uff0c\u9700\u8bbe\u8ba1\u548c\u6784\u5efa\u591a\u9636\u6bb5SQL\u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u9700\u8981\u89e3\u51b3\u5f00\u653e\u5f0f\u4e1a\u52a1\u95ee\u9898\uff0c\u8fdb\u884c\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4ee3\u7406\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e5f\u4e0d\u4f73\uff0c\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u5de5\u7a0b\u548c\u5206\u6790\u80fd\u529b\u7684\u4e0d\u540c\uff0c\u63d0\u4f9b\u4e86\u63a8\u52a8\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u53d1\u5c55\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark consisting of 210 tasks that replicate complex data workflows in businesses.</li>\n    <li>It includes data engineering tasks that involve creating and improving SQL pipelines and managing data schemas.</li>\n    <li>Data analysis tasks focus on solving open-ended business problems through strategic planning and iterative coding.</li>\n    <li>Results show that even advanced agents struggle with these tasks, especially in data engineering, with success rates below 20%.</li>\n    <li>DAComp helps identify weaknesses in current data processing capabilities and aims to improve autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u5b8c\u6574\u7684\u7cfb\u7edf\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\u548c\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u652f\u6301\u591a\u6a21\u6001\u89c6\u9891\u521b\u4f5c\u3002</li>\n    <li>Kling-Omni \u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u5b83\u88ab\u89c6\u4e3a\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one unified framework.</li>\n    <li>The system can process different user inputs like text, images, and videos to produce cinematic-quality content.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training methods for better performance.</li>\n    <li>It shows great skills in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u63a7\u5236\u7cbe\u5ea6\u4f4e\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u800cWan-Move\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u5c06\u7269\u4f53\u8fd0\u52a8\u8868\u793a\u4e3a\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\uff0cWan-Move\u80fd\u591f\u5bf9\u573a\u666f\u8fdb\u884c\u7ec6\u81f4\u7684\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u80fd\u591f\u751f\u62105\u79d2\u3001480p\u7684\u89c6\u9891\uff0c\u5176\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0e\u5546\u4e1a\u4ea7\u54c1Kling 1.5 Pro\u7684Motion Brush\u76f8\u5f53\u3002</li>\n    <li>\u4e3a\u4e86\u5168\u9762\u8bc4\u4f30\uff0cWan-Move\u8fd8\u8bbe\u8ba1\u4e86MoveBench\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u5185\u5bb9\u7c7b\u522b\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u6ce8\u91ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It achieves better precision and quality in motion control compared to existing methods.</li>\n    <li>The framework uses dense point trajectories to represent object motions, allowing for detailed scene control.</li>\n    <li>Wan-Move integrates easily with existing image-to-video models without needing extra components.</li>\n    <li>A new benchmark called MoveBench has been created to evaluate motion quality, showing that Wan-Move performs better in tests.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u80fd\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u4e14\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u80fd\u53ef\u9760\u6a21\u62df\u4e16\u754c\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7684\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5982Frechet Video Distance\uff08FVD\uff09\u6ce8\u91cd\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u63a8\u7406\u5931\u8d25\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u8868\u73b0\u5dee\u8ddd\u3002</li>\n    <li>\u5206\u6790\u6307\u51fa\u4e86\u6a21\u578b\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4f9d\u8d56\u611f\u77e5\u6570\u636e\u3001\u5168\u5c40\u72b6\u6001\u4e00\u81f4\u6027\u5dee\u7b49\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic videos, but they need to follow physical and logical rules to be reliable.</li>\n    <li>Current evaluation methods focus too much on visual quality and ignore important reasoning issues like causality and consistency.</li>\n    <li>MMGR is a new evaluation framework that tests five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>It assesses performance in areas like Abstract Reasoning, real-world navigation, and commonsense understanding in sports.</li>\n    <li>Results show that while models perform moderately well in commonsense tasks, they struggle significantly with abstract reasoning and long-term planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u662f\u6307\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u81ea\u4e3b\u6784\u601d\u3001\u8c03\u67e5\u548c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u8df5\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u5177\u4f53\u5316\uff1a\u6df1\u5165\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72\u5b9e\u9a8c/\u6e7f\u5b9e\u9a8c\u548c\u5b9e\u9a8c\u63a8\u7406\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>\u7ed3\u679c\u663e\u793a\uff1a\u5728\u6df1\u5165\u7814\u7a76\u4e2d\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff1b\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff1b\u5e72\u5b9e\u9a8c\u7684\u4ee3\u7801\u53ef\u6267\u884c\u6027\u9ad8\u4f46\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\uff1b\u6e7f\u5b9e\u9a8c\u7684\u6d41\u7a0b\u987a\u5e8f\u4fdd\u771f\u5ea6\u4f4e\uff1b\u591a\u6a21\u6001\u6bd4\u8f83\u63a8\u7406\u4ecd\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u65f6\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u4ee5\u589e\u5f3a\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u4ece\u800c\u4e3a\u771f\u6b63\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u7684AI\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to conduct scientific work on its own.</li>\n    <li>We define SGI based on a model that includes steps like thinking, creating ideas, taking actions, and observing results.</li>\n    <li>We created SGI-Bench, a set of over 1,000 tasks designed to evaluate AI in various scientific areas, based on important scientific questions.</li>\n    <li>Our findings show that AI struggles with deep research, idea feasibility, and accurate results in experiments.</li>\n    <li>We propose a new method called Test-Time Reinforcement Learning to improve AI's ability to generate new and useful hypotheses during research.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>SemanticGen \u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u6162\u6536\u655b\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\uff0c\u5148\u5b9a\u4e49\u5168\u7403\u5e03\u5c40\uff0c\u518d\u6dfb\u52a0\u9ad8\u9891\u7ec6\u8282\u3002</li>\n    <li>SemanticGen \u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u7684\u6536\u655b\u901f\u5ea6\u6bd4\u5728 VAE \u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen \u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>State-of-the-art video generative models use a method that can be slow and costly for creating long videos.</li>\n    <li>SemanticGen is a new solution that generates videos in a simpler, high-level semantic space to improve speed and efficiency.</li>\n    <li>The generation process has two stages: first, it creates a basic outline of the video, and then it adds detailed features.</li>\n    <li>This approach leads to faster results than traditional methods and works well for longer videos.</li>\n    <li>Tests show that SemanticGen creates high-quality videos and outperforms existing leading methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728GUI\u81ea\u52a8\u5316\u65b9\u9762\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u83b7\u53d6\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5229\u7528\u6821\u51c6\u6b65\u5956\u52b1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e8690%\u4ee5\u4e0a\u7684\u6807\u6ce8\u51c6\u786e\u7387\uff0c\u6210\u672c\u964d\u4f4e10-100\u500d\u3002</li>\n    <li>\u57fa\u4e8e\u6b64\u6d41\u7a0b\uff0c\u6211\u4eec\u63a8\u51fa\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e1a\u5185\u9886\u5148\u7684GUI\u6027\u80fd\u3002</li>\n    <li>\u4e3a\u6ee1\u8db3\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86AndroidDaily\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u4ee3\u7406\u5728\u771f\u5b9e\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b3146\u4e2a\u9759\u6001\u52a8\u4f5c\u548c235\u4e2a\u7aef\u5230\u7aef\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models for automating graphical user interfaces (GUIs) have been developed, making it easier to gather high-quality training data.</li>\n    <li>The self-evolving training pipeline uses a system that ensures reliable training signals while reducing costs significantly.</li>\n    <li>Step-GUI models have achieved top performance in GUI tasks while also being capable in other areas.</li>\n    <li>To ensure user privacy, a new protocol (GUI-MCP) has been created, allowing sensitive data to remain on devices during automation.</li>\n    <li>A new benchmark, AndroidDaily, tests the models with real-world mobile usage patterns to assess their effectiveness in daily tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 01, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2511.21087", "authors": [{"_id": "6928a2681f3fae537858b4a4", "name": "Ziyun Zeng", "hidden": false}, {"_id": "6928a2681f3fae537858b4a5", "name": "Hang Hua", "hidden": false}, {"_id": "6928a2681f3fae537858b4a6", "name": "Jiebo Luo", "hidden": false}], "publishedAt": "2025-11-26T06:13:32.000Z", "submittedOnDailyAt": "2025-11-28T00:06:07.849Z", "title": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing", "submittedOnDailyBy": {"_id": "639f8277beb95d698de007dd", "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg", "isPro": false, "fullname": "HangHua", "user": "hhua2", "type": "user"}, "summary": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.", "upvotes": 4, "discussionId": "6928a2691f3fae537858b4a7", "ai_summary": "MIRA, a multimodal reasoning agent, enhances diffusion-based image editing by iteratively interpreting complex instructions, improving both semantic consistency and perceptual quality.", "ai_keywords": ["diffusion-based editing models", "multimodal reasoning agent", "iterative perception-reasoning-action loop", "atomic edit instructions", "visual feedback", "multimodal tool-use dataset", "MIRA-Editing", "two-stage SFT + GRPO training pipeline", "semantic consistency", "perceptual quality", "Flux.1-Kontext", "Step1X-Edit", "Qwen-Image-Edit", "GPT-Image", "Nano-Banana"], "summary_zh": "<ul>\n    <li>\u6307\u4ee4\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u5141\u8bb8\u7528\u6237\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u76f4\u89c2\u7f16\u8f91\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u7528\u6237\u6307\u4ee4\u65f6\u5e38\u5e38\u6548\u679c\u4e0d\u4f73\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MIRA\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001\u63a8\u7406\u4ee3\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u611f\u77e5-\u63a8\u7406-\u884c\u52a8\u5faa\u73af\u8fdb\u884c\u7f16\u8f91\u3002</li>\n    <li>MIRA\u9010\u6b65\u9884\u6d4b\u539f\u5b50\u7f16\u8f91\u6307\u4ee4\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u53cd\u9988\u6765\u505a\u51fa\u51b3\u7b56\u3002</li>\n    <li>\u4e0e\u5f00\u6e90\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7ed3\u5408\u65f6\uff0cMIRA\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MIRA is a new tool for image editing that helps users edit images using natural language instructions.</li>\n    <li>Current image editing models struggle with complex instructions, which can lead to incorrect edits.</li>\n    <li>MIRA uses a step-by-step process to make editing decisions based on visual feedback, similar to how humans interact with models.</li>\n    <li>It was trained on a large dataset and uses a specific training method to understand and perform complex edits better.</li>\n    <li>MIRA improves the quality and accuracy of image edits when used with popular editing models, matching or surpassing some commercial tools.</li>\n</ul>"}, "publishedAt": "2025-11-26T01:13:32.000Z", "title": "MIRA: Multimodal Iterative Reasoning Agent for Image Editing", "summary": "Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21087.png", "numComments": 1, "submittedBy": {"_id": "639f8277beb95d698de007dd", "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg", "fullname": "HangHua", "name": "hhua2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21691", "authors": [{"_id": "6927cfb1243b2216fb75cd97", "name": "Yusuf Dalva", "hidden": false}, {"_id": "6927cfb1243b2216fb75cd98", "name": "Guocheng Gordon Qian", "hidden": false}, {"_id": "6927cfb1243b2216fb75cd99", "name": "Maya Goldenberg", "hidden": false}, {"_id": "6927cfb1243b2216fb75cd9a", "name": "Tsai-Shien Chen", "hidden": false}, {"_id": "6927cfb1243b2216fb75cd9b", "name": "Kfir Aberman", "hidden": false}, {"_id": "6927cfb1243b2216fb75cd9c", "name": "Sergey Tulyakov", "hidden": false}, {"_id": "6927cfb1243b2216fb75cd9d", "name": "Pinar Yanardag", "hidden": false}, {"_id": "6927cfb1243b2216fb75cd9e", "name": "Kuan-Chieh Jackson Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65454d7c117ecae648892170/wYbC0015-eECU_RvLEOZD.mp4"], "publishedAt": "2025-11-26T18:59:56.000Z", "submittedOnDailyAt": "2025-11-28T03:38:16.212Z", "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls", "submittedOnDailyBy": {"_id": "65454d7c117ecae648892170", "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg", "isPro": false, "fullname": "Yusuf Dalva", "user": "ydalva", "type": "user"}, "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.", "upvotes": 2, "discussionId": "6927cfb2243b2216fb75cd9f", "projectPage": "https://snap-research.github.io/canvas-to-image/", "ai_summary": "Canvas-to-Image is a unified framework that encodes diverse control signals into a composite canvas image for high-fidelity multimodal image generation, outperforming existing methods in various benchmarks.", "ai_keywords": ["diffusion models", "high-fidelity compositional", "multimodal control", "text prompts", "subject references", "spatial arrangements", "pose constraints", "layout annotations", "unified framework", "composite canvas image", "visual-spatial reasoning", "multi-task datasets", "Multi-Task Canvas Training", "text-to-image generation", "identity preservation", "control adherence", "multi-person composition", "pose-controlled composition", "layout-constrained generation", "multi-control generation"], "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7684\u7528\u6237\u63a7\u5236\u65f6\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u753b\u5e03\u5230\u56fe\u50cf\u201d\u6846\u67b6\uff0c\u5c06\u591a\u79cd\u63a7\u5236\u65b9\u5f0f\u6574\u5408\u5230\u4e00\u4e2a\u753b\u5e03\u754c\u9762\u4e2d\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u610f\u56fe\u7684\u56fe\u50cf\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u7f16\u7801\u4e0d\u540c\u7684\u63a7\u5236\u4fe1\u53f7\u4e3a\u4e00\u4e2a\u590d\u5408\u753b\u5e03\u56fe\u50cf\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u7efc\u5408\u7684\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5236\u5b9a\u4e86\u591a\u4efb\u52a1\u6570\u636e\u96c6\u548c\u591a\u4efb\u52a1\u753b\u5e03\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5316\u6269\u6563\u6a21\u578b\u4ee5\u5171\u540c\u7406\u89e3\u548c\u6574\u5408\u4e0d\u540c\u7684\u63a7\u5236\u65b9\u5f0f\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u201c\u753b\u5e03\u5230\u56fe\u50cf\u201d\u5728\u8eab\u4efd\u4fdd\u6301\u548c\u63a7\u5236\u9075\u5faa\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u591a\u4eba\u7269\u6784\u56fe\u548c\u59ff\u52bf\u63a7\u5236\u751f\u6210\u7b49\u590d\u6742\u573a\u666f\u4e2d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Modern diffusion models are good at creating quality images but have trouble with detailed controls when users want specific features.</li>\n    <li>Canvas-to-Image is a new system that allows users to combine different controls in one easy-to-use interface to create images that match their needs.</li>\n    <li>The system uses a single composite canvas image to help the model understand and generate visuals based on multiple different inputs.</li>\n    <li>Canvas-to-Image includes a special training method that helps it learn to handle various controls together instead of needing separate approaches for each task.</li>\n    <li>Tests show that Canvas-to-Image works better than other methods in keeping identities and following user instructions in complex scenarios.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:59:56.000Z", "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls", "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65454d7c117ecae648892170/wYbC0015-eECU_RvLEOZD.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21691.png", "numComments": 1, "submittedBy": {"_id": "65454d7c117ecae648892170", "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg", "fullname": "Yusuf Dalva", "name": "ydalva", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21662", "authors": [{"_id": "6927d4bd243b2216fb75cdb6", "name": "Tianyi Xiong", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdb7", "name": "Yi Ge", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdb8", "name": "Ming Li", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdb9", "name": "Zuolong Zhang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdba", "name": "Pranav Kulkarni", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbb", "name": "Kaishen Wang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbc", "name": "Qi He", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbd", "name": "Zeying Zhu", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbe", "name": "Chenxi Liu", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdbf", "name": "Ruibo Chen", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc0", "name": "Tong Zheng", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc1", "name": "Yanshuo Chen", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc2", "name": "Xiyao Wang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc3", "name": "Renrui Zhang", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc4", "name": "Wenhu Chen", "hidden": false}, {"_id": "6927d4bd243b2216fb75cdc5", "name": "Heng Huang", "hidden": false}], "publishedAt": "2025-11-26T18:35:17.000Z", "submittedOnDailyAt": "2025-11-28T00:23:35.734Z", "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following", "submittedOnDailyBy": {"_id": "6570977f87a92b76922c9950", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg", "isPro": false, "fullname": "Tianyi Xiong", "user": "txiong23", "type": "user"}, "summary": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.", "upvotes": 2, "discussionId": "6927d4be243b2216fb75cdc6", "projectPage": "https://multi-crit.github.io/", "ai_summary": "Multi-Crit evaluates multimodal models on following diverse criteria with metrics for pluralistic adherence, criterion-switching flexibility, and recognizing preference conflicts, revealing gaps in model capabilities.", "ai_keywords": ["LMMs", "multimodal evaluation systems", "instruction following", "human preferences", "multi-criterion human annotations", "pluralistic criteria", "criterion-level judgments", "open-ended generation", "verifiable reasoning tasks", "holistic judgment signals", "visual grounding", "reasoning fine-tuning", "test-time scaling", "boundary consistency"], "organization": {"_id": "68b3c3bbc375e05b059370b2", "name": "UMCP", "fullname": "University of Maryland College Park", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\u4e2d\u8d8a\u6765\u8d8a\u5e38\u88ab\u7528\u4f5c\u8bc4\u5224\u8005\uff0c\u56e0\u5176\u80fd\u591f\u6709\u6548\u8ddf\u968f\u6307\u4ee4\u5e76\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86Multi-Crit\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u80fd\u5426\u9075\u5faa\u591a\u79cd\u8bc4\u4f30\u6807\u51c6\u5e76\u4ea7\u751f\u53ef\u9760\u7684\u5224\u65ad\u3002</li>\n    <li>Multi-Crit\u5305\u62ec\u5f00\u653e\u5f0f\u751f\u6210\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u5efa\u7acb\uff0c\u6536\u96c6\u4e86\u5177\u6709\u591a\u6807\u51c6\u4eba\u5de5\u6807\u6ce8\u7684\u6311\u6218\u6027\u54cd\u5e94\u5bf9\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u4e13\u6709\u6a21\u578b\u5728\u9075\u5faa\u591a\u5143\u6807\u51c6\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u5f0f\u8bc4\u4f30\u4e2d\uff1b\u5f00\u6e90\u6a21\u578b\u5728\u7075\u6d3b\u8ddf\u968f\u591a\u6837\u6807\u51c6\u65b9\u9762\u8868\u73b0\u66f4\u5dee\u3002</li>\n    <li>Multi-Crit\u4e3a\u5efa\u7acb\u53ef\u9760\u4e14\u53ef\u8c03\u8282\u7684\u591a\u6a21\u6001AI\u8bc4\u4f30\u6253\u4e0b\u4e86\u57fa\u7840\uff0c\u662f\u4e00\u9879\u5f00\u521b\u6027\u7684\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) are becoming popular for judging multimodal evaluation systems because they follow instructions well and match human preferences.</li>\n    <li>Multi-Crit is a new benchmark designed to test how well these models can adhere to multiple evaluation criteria and make reliable judgments.</li>\n    <li>This benchmark includes both creative tasks and reasoning tasks, and uses carefully curated data with human feedback on responses.</li>\n    <li>Analysis of 25 LMMs shows that proprietary models struggle with following diverse criteria, while open-source models do even worse.</li>\n    <li>The study suggests that improvements in fine-tuning for holistic judgments help with visual tasks but do not improve performance on varied criteria.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:35:17.000Z", "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following", "summary": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21662.png", "numComments": 1, "submittedBy": {"_id": "6570977f87a92b76922c9950", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg", "fullname": "Tianyi Xiong", "name": "txiong23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "68b3c3bbc375e05b059370b2", "name": "UMCP", "fullname": "University of Maryland College Park", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21678", "authors": [{"_id": "692918571f3fae537858b537", "name": "Weihao Bo", "hidden": false}, {"_id": "692918571f3fae537858b538", "name": "Shan Zhang", "hidden": false}, {"_id": "692918571f3fae537858b539", "name": "Yanpeng Sun", "hidden": false}, {"_id": "692918571f3fae537858b53a", "name": "Jingjing Wu", "hidden": false}, {"_id": "692918571f3fae537858b53b", "name": "Qunyi Xie", "hidden": false}, {"_id": "692918571f3fae537858b53c", "name": "Xiao Tan", "hidden": false}, {"_id": "692918571f3fae537858b53d", "name": "Kunbin Chen", "hidden": false}, {"_id": "692918571f3fae537858b53e", "name": "Wei He", "hidden": false}, {"_id": "692918571f3fae537858b53f", "name": "Xiaofan Li", "hidden": false}, {"_id": "692918571f3fae537858b540", "name": "Na Zhao", "hidden": false}, {"_id": "692918571f3fae537858b541", "name": "Jingdong Wang", "hidden": false}, {"_id": "692918571f3fae537858b542", "name": "Zechao Li", "hidden": false}], "publishedAt": "2025-11-26T18:55:08.000Z", "submittedOnDailyAt": "2025-11-28T01:10:12.370Z", "title": "Agentic Learner with Grow-and-Refine Multimodal Semantic Memory", "submittedOnDailyBy": {"_id": "64297212e5f33939cf3a3d9b", "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg", "isPro": false, "fullname": "yanpeng_sun", "user": "syp115", "type": "user"}, "summary": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.", "upvotes": 0, "discussionId": "692918571f3fae537858b543", "ai_summary": "ViLoMem, a dual-stream memory framework, enhances MLLMs by preserving multimodal semantic knowledge, reducing errors, and improving accuracy across benchmarks.", "ai_keywords": ["memory-augmented agents", "trajectory-based memory", "brevity bias", "semantic memory", "multimodal problem-solving", "visual attention", "logical reasoning", "ViLoMem", "dual-stream memory", "schema-based memory", "visual distraction patterns", "logical reasoning errors", "pass@1 accuracy", "catastrophic forgetting", "lifelong learning", "cross-domain learning"], "summary_zh": "<ul>\n    <li>MLLMs\uff08\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u72ec\u7acb\u67e5\u8be2\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u72ec\u7acb\u89e3\u51b3\u6bcf\u4e2a\u95ee\u9898\uff0c\u5e38\u5e38\u91cd\u590d\u9519\u8bef\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u589e\u5f3a\u4ee3\u7406\u4e3b\u8981\u5b58\u50a8\u8fc7\u53bb\u7684\u8f68\u8ff9\uff0c\u7136\u800c\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bb0\u5fc6\u5bb9\u6613\u4e22\u5931\u91cd\u8981\u7684\u9886\u57df\u77e5\u8bc6\u3002</li>\n    <li>ViLoMem \u662f\u4e00\u79cd\u53cc\u6d41\u8bb0\u5fc6\u6846\u67b6\uff0c\u80fd\u591f\u5206\u522b\u7f16\u7801\u89c6\u89c9\u5e72\u6270\u6a21\u5f0f\u548c\u903b\u8f91\u63a8\u7406\u9519\u8bef\uff0c\u4ece\u800c\u5e2e\u52a9 MLLMs \u5b66\u4e60\u6210\u529f\u548c\u5931\u8d25\u7684\u7ecf\u9a8c\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u901a\u8fc7\u9010\u6b65\u79ef\u7d2f\u548c\u66f4\u65b0\u591a\u6a21\u6001\u8bed\u4e49\u77e5\u8bc6\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u3001\u53ef\u63a8\u5e7f\u7684\u7b56\u7565\uff0c\u907f\u514d\u4e25\u91cd\u9057\u5fd8\u3002</li>\n    <li>\u5728\u516d\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViLoMem \u4e00\u81f4\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u89c6\u89c9\u548c\u903b\u8f91\u9519\u8bef\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>MLLMs can solve problems well in isolation, but they don\u2019t learn from past mistakes effectively.</li>\n  <li>Current memory systems mostly keep track of past actions, but they can lose important knowledge over time.</li>\n  <li>These systems often fail to combine visual and logical reasoning, which is how humans think.</li>\n  <li>ViLoMem is a new memory system that improves learning by separately storing visual distractions and reasoning errors.</li>\n  <li>This method has been shown to enhance accuracy and reduce mistakes in multimodal tasks through better memory management.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:55:08.000Z", "title": "Agentic Learner with Grow-and-Refine Multimodal Semantic Memory", "summary": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21678.png", "numComments": 1, "submittedBy": {"_id": "64297212e5f33939cf3a3d9b", "avatarUrl": "/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg", "fullname": "yanpeng_sun", "name": "syp115", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.19304", "authors": [{"_id": "6925274e16eb3a9f13103998", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:05:29.887Z", "hidden": false}, {"_id": "6925274e16eb3a9f13103999", "name": "Yiran Peng", "hidden": false}, {"_id": "6925274e16eb3a9f1310399a", "user": {"_id": "6621e02cf34ab6caed18e9c6", "avatarUrl": "/avatars/15888b2060d1cc56be9fa55fd4b34005.svg", "isPro": false, "fullname": "Fanqi Kong", "user": "Fancylalala", "type": "user"}, "name": "Fanqi Kong", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:05:25.655Z", "hidden": false}, {"_id": "6925274e16eb3a9f1310399b", "user": {"_id": "67c443afb753bd020f9c97d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png", "isPro": false, "fullname": "Cheng", "user": "YangC777", "type": "user"}, "name": "Yang Cheng", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:05:18.820Z", "hidden": false}, {"_id": "6925274e16eb3a9f1310399c", "name": "Yifan Wu", "hidden": false}, {"_id": "6925274e16eb3a9f1310399d", "name": "Zhaoyang Yu", "hidden": false}, {"_id": "6925274e16eb3a9f1310399e", "user": {"_id": "649ea7106282cb41e77760bc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649ea7106282cb41e77760bc/HlWjaqxr03ob93vdKg_LQ.jpeg", "isPro": false, "fullname": "Isaac", "user": "XiangJinYu", "type": "user"}, "name": "Jinyu Xiang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:05:23.607Z", "hidden": false}, {"_id": "6925274e16eb3a9f1310399f", "user": {"_id": "68a435cc22fdf7356962ccb9", "avatarUrl": "/avatars/467f4732ade5f47b42433ff354acdeef.svg", "isPro": false, "fullname": "jianhao ruan", "user": "Aurorra1123", "type": "user"}, "name": "Jianhao Ruan", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T15:53:17.306Z", "hidden": false}, {"_id": "6925274e16eb3a9f131039a0", "name": "Jinlin Wang", "hidden": false}, {"_id": "6925274e16eb3a9f131039a1", "name": "Maojia Song", "hidden": false}, {"_id": "6925274e16eb3a9f131039a2", "user": {"_id": "6632160088f75d987d1a156f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6632160088f75d987d1a156f/mYlMQfK1BGWeEbOSMmeSb.jpeg", "isPro": false, "fullname": "Hongzhang Liu", "user": "Alphamasterliu", "type": "user"}, "name": "HongZhang Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:05:27.575Z", "hidden": false}, {"_id": "6925274e16eb3a9f131039a3", "name": "Xiangru Tang", "hidden": false}, {"_id": "6925274e16eb3a9f131039a4", "name": "Bang Liu", "hidden": false}, {"_id": "6925274e16eb3a9f131039a5", "name": "Chenglin Wu", "hidden": false}, {"_id": "6925274e16eb3a9f131039a6", "name": "Yuyu Luo", "hidden": false}], "publishedAt": "2025-11-24T16:54:23.000Z", "submittedOnDailyAt": "2025-11-25T01:26:13.029Z", "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning", "submittedOnDailyBy": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.", "upvotes": 85, "discussionId": "6925274f16eb3a9f131039a7", "ai_summary": "AutoEnv and AutoEnv-36 provide a standardized framework and dataset for evaluating cross-environment learning in agents, highlighting the challenges and limitations of existing learning methods.", "ai_keywords": ["AutoEnv", "factorizable distributions", "heterogeneous environments", "AutoEnv-36", "language models", "normalized reward", "component-centric process", "Selection", "Optimization", "Evaluation", "learning methods", "environment-adaptive selection"], "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u80fd\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u73af\u5883\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u4e16\u754c\u7684\u89c4\u5219\u3002</li>\n    <li>\u73b0\u6709\u7684\u667a\u80fd\u4f53\u901a\u5e38\u53ea\u5728\u5355\u4e00\u73af\u5883\u4e2d\u81ea\u6211\u8fdb\u5316\uff0c\u5047\u8bbe\u73af\u5883\u662f\u56fa\u5b9a\u7684\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86AutoEnv\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u4f4e\u6210\u672c\u751f\u6210\u591a\u6837\u5316\u7684\u73af\u5883\u3002</li>\n    <li>\u4f7f\u7528AutoEnv\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u5305\u542b36\u4e2a\u73af\u5883\u548c358\u4e2a\u9a8c\u8bc1\u5173\u5361\u7684\u6570\u636e\u96c6AutoEnv-36\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u56fa\u5b9a\u7684\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u6548\u679c\u4e0b\u964d\uff0c\u800c\u73af\u5883\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans can adapt to different environments by learning rules, but current agents mainly improve in one fixed environment.</li>\n    <li>There is a lack of standardized environments and methods for measuring learning across different settings.</li>\n    <li>AutoEnv is introduced as a new framework to easily create diverse environments for testing agents.</li>\n    <li>AutoEnv-36 is a dataset with 36 environments, where agents achieved only modest rewards, highlighting its challenges.</li>\n    <li>Learning methods need to adapt to different environments, but their effectiveness decreases as complexity increases, indicating limitations in current approaches.</li>\n</ul>"}, "publishedAt": "2025-11-24T11:54:23.000Z", "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning", "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19304.png", "numComments": 3, "submittedBy": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "fullname": "Jiayi Zhang", "name": "didiforhugface", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20639", "authors": [{"_id": "6927c504243b2216fb75cd62", "user": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "name": "Jiaru Zou", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:58:44.029Z", "hidden": false}, {"_id": "6927c504243b2216fb75cd63", "name": "Xiyuan Yang", "hidden": false}, {"_id": "6927c504243b2216fb75cd64", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6927c504243b2216fb75cd65", "name": "Gaotang Li", "hidden": false}, {"_id": "6927c504243b2216fb75cd66", "name": "Katherine Tieu", "hidden": false}, {"_id": "6927c504243b2216fb75cd67", "user": {"_id": "60f5f68fa7fd83d025749234", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg", "isPro": false, "fullname": "Pan Lu", "user": "lupantech", "type": "user"}, "name": "Pan Lu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:58:40.924Z", "hidden": false}, {"_id": "6927c504243b2216fb75cd68", "name": "Ke Shen", "hidden": false}, {"_id": "6927c504243b2216fb75cd69", "name": "Hanghang Tong", "hidden": false}, {"_id": "6927c504243b2216fb75cd6a", "name": "Yejin Choi", "hidden": false}, {"_id": "6927c504243b2216fb75cd6b", "name": "Jingrui He", "hidden": false}, {"_id": "6927c504243b2216fb75cd6c", "name": "James Zou", "hidden": false}, {"_id": "6927c504243b2216fb75cd6d", "name": "Mengdi Wang", "hidden": false}, {"_id": "6927c504243b2216fb75cd6e", "name": "Ling Yang", "hidden": false}], "publishedAt": "2025-11-25T18:56:57.000Z", "submittedOnDailyAt": "2025-11-27T01:00:26.981Z", "title": "Latent Collaboration in Multi-Agent Systems", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "upvotes": 64, "discussionId": "6927c504243b2216fb75cd6f", "githubRepo": "https://github.com/Gen-Verse/LatentMAS", "ai_summary": "LatentMAS enables efficient and effective collaboration among LLM agents using latent space representations, enhancing reasoning quality and reducing computational costs.", "ai_keywords": ["multi-agent systems", "large language models", "latent space", "LatentMAS", "auto-regressive latent thoughts generation", "last-layer hidden embeddings", "shared latent working memory", "expressiveness", "information preservation", "complexity", "math and science reasoning", "commonsense understanding", "code generation", "accuracy", "output token usage", "end-to-end inference"], "githubStars": 115, "organization": {"_id": "67a21d7efeeacb7707bf40de", "name": "Gen-Verse", "fullname": "Princeton-AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"}, "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u901a\u8fc7\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u534f\u4f5c\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6587\u672c\u57fa\u7840\u63a8\u7406\u548c\u4ea4\u6d41\u3002</li>\n    <li>LatentMAS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u652f\u6301LLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u7eaf\u6f5c\u5728\u534f\u4f5c\u3002</li>\n    <li>\u6bcf\u4e2a\u667a\u80fd\u4f53\u901a\u8fc7\u6700\u540e\u4e00\u5c42\u7684\u9690\u85cf\u5d4c\u5165\u751f\u6210\u6f5c\u5728\u601d\u7ef4\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u7684\u6f5c\u5728\u5de5\u4f5c\u8bb0\u5fc6\u8fdb\u884c\u4fe1\u606f\u4ea4\u6362\u3002</li>\n    <li>\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cLatentMAS\u5728\u4fe1\u606f\u8868\u8fbe\u548c\u4fdd\u7559\u65b9\u9762\u7684\u590d\u6742\u6027\u66f4\u4f4e\uff0c\u4e14\u8868\u73b0\u66f4\u4f18\u3002</li>\n    <li>\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLatentMAS\u5728\u51c6\u786e\u6027\u4e0a\u63d0\u9ad8\u4e8614.6%\uff0c\u51cf\u5c11\u4e86\u8f93\u51fa\u4ee4\u724c\u4f7f\u7528\u91cf\uff0c\u5e76\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems (MAS) improve collaboration among large language models (LLMs) beyond just single-model thinking.</li>\n    <li>LatentMAS allows LLM agents to work together directly in a shared space without relying on text communication.</li>\n    <li>It uses a shared memory to exchange information seamlessly while maintaining the quality of data.</li>\n    <li>LatentMAS shows better performance in various tasks, with up to 14.6% higher accuracy and significantly faster processing times.</li>\n    <li>The framework is open-source and does not require additional training to operate effectively.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:56:57.000Z", "title": "Latent Collaboration in Multi-Agent Systems", "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20639.png", "numComments": 5, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "67a21d7efeeacb7707bf40de", "name": "Gen-Verse", "fullname": "Princeton-AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.19365", "authors": [{"_id": "69251d6b16eb3a9f13103933", "user": {"_id": "65d851096769b3a9c9376134", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg", "isPro": false, "fullname": "ZehongMa", "user": "zehongma", "type": "user"}, "name": "Zehong Ma", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T09:37:20.953Z", "hidden": false}, {"_id": "69251d6b16eb3a9f13103934", "name": "Longhui Wei", "hidden": false}, {"_id": "69251d6b16eb3a9f13103935", "user": {"_id": "66615c855fd9d736e670e0a9", "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg", "isPro": false, "fullname": "wangshuai", "user": "wangsssssss", "type": "user"}, "name": "Shuai Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T15:53:18.956Z", "hidden": false}, {"_id": "69251d6b16eb3a9f13103936", "user": {"_id": "681c126ad15c979cc4c8cad1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/rpuwzjv2qXzrA4hfbdXJ8.png", "isPro": false, "fullname": "shiliang zhang", "user": "slade96", "type": "user"}, "name": "Shiliang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T09:37:37.350Z", "hidden": false}, {"_id": "69251d6b16eb3a9f13103937", "name": "Qi Tian", "hidden": false}], "publishedAt": "2025-11-24T17:59:06.000Z", "submittedOnDailyAt": "2025-11-25T01:03:11.081Z", "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation", "submittedOnDailyBy": {"_id": "65d851096769b3a9c9376134", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg", "isPro": false, "fullname": "ZehongMa", "user": "zehongma", "type": "user"}, "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.", "upvotes": 57, "discussionId": "69251d6b16eb3a9f13103938", "projectPage": "https://zehong-ma.github.io/DeCo/", "githubRepo": "https://github.com/Zehong-Ma/DeCo", "ai_summary": "The frequency-DeCoupled pixel diffusion framework improves image generation efficiency and quality by separating high-frequency details and low-frequency semantics, achieving superior performance compared to existing pixel diffusion models.", "ai_keywords": ["pixel diffusion", "pixel space", "VAE", "latent diffusion", "diffusion transformer (DiT)", "frequency-DeCoupled pixel diffusion", "lightweight pixel decoder", "semantic guidance", "frequency-aware flow-matching loss", "FID", "ImageNet", "GenEval"], "githubStars": 68, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u50cf\u7d20\u6269\u6563\u76f4\u63a5\u5728\u50cf\u7d20\u7a7a\u95f4\u751f\u6210\u56fe\u50cf\uff0c\u907f\u514d\u4e86VAE\u7684\u9650\u5236\uff0c\u6a21\u578b\u80fd\u529b\u66f4\u5f3a\u3002</li>\n    <li>\u73b0\u6709\u7684\u50cf\u7d20\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u8f83\u6162\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u540c\u4e00\u4e2a\u6269\u6563\u53d8\u6362\u5668\u4e2d\u5904\u7406\u9ad8\u9891\u548c\u4f4e\u9891\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u9891\u7387\u89e3\u8026\u50cf\u7d20\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u4f4e\u9891\u7ec4\u4ef6\u7684\u751f\u6210\uff0c\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>\u5f15\u5165\u9891\u7387\u611f\u77e5\u6d41\u5339\u914d\u635f\u5931\uff0c\u5f3a\u8c03\u91cd\u8981\u9891\u7387\uff0c\u6291\u5236\u4e0d\u91cd\u8981\u7684\u9891\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cDeCo\u5728\u50cf\u7d20\u6269\u6563\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0cFID\u5f97\u5206\u5206\u522b\u4e3a1.62\u548c2.22\uff0c\u5e76\u5728\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u53d6\u5f97\u4e860.86\u7684\u9886\u5148\u5206\u6570\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Pixel diffusion creates images directly in pixel space, improving on traditional two-stage methods.</li>\n    <li>Current models are slow because they handle both high and low-frequency image details together.</li>\n    <li>The new framework, called frequency-DeCoupled pixel diffusion, separates the generation of high and low-frequency components for efficiency.</li>\n    <li>A lightweight decoder focuses on adding high-frequency details while a main model handles low-frequency information.</li>\n    <li>Experiments show this approach outperforms others, achieving impressive scores on image quality benchmarks.</li>\n</ul>"}, "publishedAt": "2025-11-24T12:59:06.000Z", "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation", "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19365.png", "numComments": 3, "submittedBy": {"_id": "65d851096769b3a9c9376134", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d851096769b3a9c9376134/j_d2RSa-3rRmSAmRICSk0.jpeg", "fullname": "ZehongMa", "name": "zehongma", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.19399", "authors": [{"_id": "692531b316eb3a9f13103a46", "name": "Rulin Shao", "hidden": false}, {"_id": "692531b316eb3a9f13103a47", "user": {"_id": "6266e0cb7a1f5a1562c4e86e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6266e0cb7a1f5a1562c4e86e/kkJZPssa76mdRPnKH-q3e.jpeg", "isPro": false, "fullname": "Akari Asai", "user": "akariasai", "type": "user"}, "name": "Akari Asai", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T15:53:14.992Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a48", "user": {"_id": "613f897ffbfd59f147a88c81", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659455519119-613f897ffbfd59f147a88c81.jpeg", "isPro": false, "fullname": "Shannon Shen", "user": "shannons", "type": "user"}, "name": "Shannon Zejiang Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:05:11.372Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a49", "user": {"_id": "62608fc2ffe8827cb1d89f9f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png", "isPro": false, "fullname": "Hamish Ivison", "user": "hamishivi", "type": "user"}, "name": "Hamish Ivison", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:05:13.345Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a4a", "name": "Varsha Kishore", "hidden": false}, {"_id": "692531b316eb3a9f13103a4b", "user": {"_id": "6530c7263976e5f4412ba737", "avatarUrl": "/avatars/8b4d9d847a9e115c3da8cad629bd0a41.svg", "isPro": false, "fullname": "Jingming Zhuo", "user": "JingmingZ", "type": "user"}, "name": "Jingming Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T09:34:32.721Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a4c", "name": "Xinran Zhao", "hidden": false}, {"_id": "692531b316eb3a9f13103a4d", "name": "Molly Park", "hidden": false}, {"_id": "692531b316eb3a9f13103a4e", "name": "Samuel G. Finlayson", "hidden": false}, {"_id": "692531b316eb3a9f13103a4f", "name": "David Sontag", "hidden": false}, {"_id": "692531b316eb3a9f13103a50", "user": {"_id": "65e5fefbe8cbae176d9ca005", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5fefbe8cbae176d9ca005/5SLREDsVzycEwVsPNv765.jpeg", "isPro": false, "fullname": "Tyler Murray", "user": "undfined", "type": "user"}, "name": "Tyler Murray", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T09:41:12.772Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a51", "user": {"_id": "63a76d0de27a6dbd485fe863", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg", "isPro": false, "fullname": "Sewon Min", "user": "sewon", "type": "user"}, "name": "Sewon Min", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T09:41:07.253Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a52", "user": {"_id": "6408fcc93461c51cf735a61e", "avatarUrl": "/avatars/619f3653911d111f046a5a6c30fc8319.svg", "isPro": false, "fullname": "Pradeep Dasigi", "user": "pradeepd", "type": "user"}, "name": "Pradeep Dasigi", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T09:41:00.782Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a53", "name": "Luca Soldaini", "hidden": false}, {"_id": "692531b316eb3a9f13103a54", "user": {"_id": "65282b8d578679aac7888aec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65282b8d578679aac7888aec/dibBkhH-z1c70mJZZxJ7u.jpeg", "isPro": false, "fullname": "Faeze Brahman", "user": "faezeb", "type": "user"}, "name": "Faeze Brahman", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T09:41:17.953Z", "hidden": false}, {"_id": "692531b316eb3a9f13103a55", "name": "Wen-tau Yih", "hidden": false}, {"_id": "692531b316eb3a9f13103a56", "name": "Tongshuang Wu", "hidden": false}, {"_id": "692531b316eb3a9f13103a57", "name": "Luke Zettlemoyer", "hidden": false}, {"_id": "692531b316eb3a9f13103a58", "name": "Yoon Kim", "hidden": false}, {"_id": "692531b316eb3a9f13103a59", "name": "Hannaneh Hajishirzi", "hidden": false}, {"_id": "692531b316eb3a9f13103a5a", "user": {"_id": "641b4263abfce26bcf7b27de", "avatarUrl": "/avatars/e91b4205e4f74b0dd8c333c23203a924.svg", "isPro": false, "fullname": "Pang Wei Koh", "user": "pangwei", "type": "user"}, "name": "Pang Wei Koh", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T09:38:49.292Z", "hidden": false}], "publishedAt": "2025-11-24T18:35:54.000Z", "submittedOnDailyAt": "2025-11-25T02:20:40.847Z", "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research", "submittedOnDailyBy": {"_id": "62608fc2ffe8827cb1d89f9f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png", "isPro": false, "fullname": "Hamish Ivison", "user": "hamishivi", "type": "user"}, "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.", "upvotes": 47, "discussionId": "692531b316eb3a9f13103a5b", "projectPage": "https://github.com/rlresearch/dr-tulu", "githubRepo": "https://github.com/rlresearch/dr-tulu", "ai_summary": "Reinforcement Learning with Evolving Rubrics (RLER) enables training of deep research models for long-form tasks, outperforming existing models and proprietary systems while being more cost-effective.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "Reinforcement Learning with Evolving Rubrics (RLER)", "Deep Research Tulu (DR Tulu-8B)", "long-form deep research", "deep research models", "open-ended tasks", "science benchmarks", "healthcare benchmarks", "general domain benchmarks", "MCP-based agent infrastructure"], "githubStars": 386, "organization": {"_id": "6916602cb89e7abe20e1da38", "name": "rl-research", "fullname": "RL ReSearch", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6334a0bd31a2be3938c59537/iySh9RAgtqs7m77mwfFOi.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\u53ef\u4ee5\u8fdb\u884c\u591a\u6b65\u9aa4\u7684\u7814\u7a76\uff0c\u751f\u6210\u957f\u7bc7\u3001\u51c6\u786e\u7684\u7b54\u6848\u3002</li>\n    <li>\u5927\u591a\u6570\u5f00\u653e\u7684\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\u53ea\u5728\u77ed\u671f\u95ee\u7b54\u4efb\u52a1\u4e0a\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5904\u7406\u771f\u5b9e\u7684\u957f\u7bc7\u4efb\u52a1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6f14\u53d8\u8bc4\u5206\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLER\uff09\uff0c\u4f7f\u8bc4\u5206\u6807\u51c6\u4e0e\u8bad\u7ec3\u4e2d\u7684\u6a21\u578b\u5171\u540c\u6f14\u53d8\u3002</li>\n    <li>\u4f7f\u7528RLER\uff0c\u6211\u4eec\u5f00\u53d1\u4e86DR Tulu\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u5f00\u653e\u5f0f\u957f\u7bc7\u6df1\u5ea6\u7814\u7a76\u8bad\u7ec3\u7684\u5f00\u653e\u6a21\u578b\u3002</li>\n    <li>DR Tulu\u5728\u79d1\u5b66\u3001\u533b\u7597\u548c\u4e00\u822c\u9886\u57df\u7684\u957f\u7bc7\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6bd4\u73b0\u6709\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u66f4\u5c0f\u3001\u66f4\u4fbf\u5b9c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep research models are designed to give detailed answers but struggle with long-form tasks.</li>\n    <li>Most existing models are trained on short questions and answers, which limits their ability for complex research.</li>\n    <li>The new method, Reinforcement Learning with Evolving Rubrics (RLER), helps improve the training process by adapting feedback as the model learns.</li>\n    <li>DR Tulu-8B is the first open model trained specifically for long-form research and performs better than other models in various fields.</li>\n    <li>All related data, models, and code are shared publicly to support future research efforts.</li>\n</ul>"}, "publishedAt": "2025-11-24T13:35:54.000Z", "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research", "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19399.png", "numComments": 3, "submittedBy": {"_id": "62608fc2ffe8827cb1d89f9f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png", "fullname": "Hamish Ivison", "name": "hamishivi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "organization": {"_id": "6916602cb89e7abe20e1da38", "name": "rl-research", "fullname": "RL ReSearch", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6334a0bd31a2be3938c59537/iySh9RAgtqs7m77mwfFOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.19046", "authors": [{"_id": "69267465243b2216fb75ca9d", "user": {"_id": "66214b651fc3a06144ef8f4b", "avatarUrl": "/avatars/dab669ad56b67805d55fef8c4fcf1326.svg", "isPro": false, "fullname": "Anglin Liu", "user": "lal-Joey", "type": "user"}, "name": "Anglin Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-26T09:27:33.112Z", "hidden": false}, {"_id": "69267465243b2216fb75ca9e", "name": "Rundong Xue", "hidden": false}, {"_id": "69267465243b2216fb75ca9f", "user": {"_id": "5fc9f05d52770aca770bd3d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc9f05d52770aca770bd3d9/axiKCXPYttXEWC-RqCSfh.jpeg", "isPro": true, "fullname": "Xu Cao", "user": "IrohXu", "type": "user"}, "name": "Xu R. Cao", "status": "claimed_verified", "statusLastChangedAt": "2025-11-26T09:27:30.698Z", "hidden": false}, {"_id": "69267465243b2216fb75caa0", "user": {"_id": "65e387095132c2edd193ae49", "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg", "isPro": false, "fullname": "Yifan Shen", "user": "SivanSX", "type": "user"}, "name": "Yifan Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:55.425Z", "hidden": false}, {"_id": "69267465243b2216fb75caa1", "name": "Yi Lu", "hidden": false}, {"_id": "69267465243b2216fb75caa2", "name": "Xiang Li", "hidden": false}, {"_id": "69267465243b2216fb75caa3", "name": "Qianqian Chen", "hidden": false}, {"_id": "69267465243b2216fb75caa4", "name": "Jintai Chen", "hidden": false}], "publishedAt": "2025-11-24T12:34:38.000Z", "submittedOnDailyAt": "2025-11-26T01:01:06.095Z", "title": "MedSAM3: Delving into Segment Anything with Medical Concepts", "submittedOnDailyBy": {"_id": "65e387095132c2edd193ae49", "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg", "isPro": false, "fullname": "Yifan Shen", "user": "SivanSX", "type": "user"}, "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.", "upvotes": 44, "discussionId": "69267466243b2216fb75caa5", "githubRepo": "https://github.com/Joey-S-Liu/MedSAM3", "ai_summary": "MedSAM-3, a text-promptable medical segmentation model fine-tuned on SAM 3 architecture, achieves superior performance across various medical imaging modalities using semantic conceptual labels and multimodal large language models.", "ai_keywords": ["Segment Anything Model (SAM)", "text promptable", "medical segmentation", "Promptable Concept Segmentation (PCS)", "Multimodal Large Language Models (MLLMs)", "X-ray", "MRI", "Ultrasound", "CT", "video"], "githubStars": 59, "summary_zh": "<ul>\n    <li>\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5bf9\u751f\u7269\u533b\u5b66\u7814\u7a76\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9700\u5927\u91cf\u624b\u52a8\u6807\u6ce8\u65b0\u4e34\u5e8a\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MedSAM-3\uff0c\u4e00\u4e2a\u53ef\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u7684\u6a21\u578b\u3002</li>\n    <li>MedSAM-3\u53ef\u4ee5\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u7684\u6587\u672c\u63cf\u8ff0\u7cbe\u786e\u5b9a\u4f4d\u89e3\u5256\u7ed3\u6784\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cMedSAM-3\u5728\u5404\u7c7b\u533b\u5b66\u6210\u50cf\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MedSAM-3 is a new model for segmenting medical images and videos, making it easier to identify important parts of the body.</li>\n    <li>It uses text descriptions instead of just geometric shapes, allowing for more precise targeting of anatomical structures.</li>\n    <li>The model is fine-tuned on medical images with labeled concepts, enhancing its ability to understand medical content.</li>\n    <li>MedSAM-3 works with a new system called the MedSAM-3 Agent that combines language models for better reasoning and improvement during use.</li>\n    <li>Tests show that MedSAM-3 performs better than existing models across various medical imaging types like X-ray, MRI, and CT.</li>\n</ul>"}, "publishedAt": "2025-11-24T07:34:38.000Z", "title": "MedSAM3: Delving into Segment Anything with Medical Concepts", "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19046.png", "numComments": 3, "submittedBy": {"_id": "65e387095132c2edd193ae49", "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg", "fullname": "Yifan Shen", "name": "SivanSX", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}, {"paper": {"id": "2511.19900", "authors": [{"_id": "6926877a243b2216fb75caca", "user": {"_id": "684ff37fa383bc5d6b0ff77f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png", "isPro": false, "fullname": "JiaqiLiu", "user": "JiaaqiLiu", "type": "user"}, "name": "Jiaqi Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-26T09:27:22.964Z", "hidden": false}, {"_id": "6926877a243b2216fb75cacb", "name": "Kaiwen Xiong", "hidden": false}, {"_id": "6926877a243b2216fb75cacc", "user": {"_id": "643e9ee6f6bb3c31a26e7bc4", "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg", "isPro": false, "fullname": "Peng Xia", "user": "richardxp888", "type": "user"}, "name": "Peng Xia", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:53.380Z", "hidden": false}, {"_id": "6926877a243b2216fb75cacd", "name": "Yiyang Zhou", "hidden": false}, {"_id": "6926877a243b2216fb75cace", "name": "Haonian Ji", "hidden": false}, {"_id": "6926877a243b2216fb75cacf", "name": "Lu Feng", "hidden": false}, {"_id": "6926877a243b2216fb75cad0", "name": "Siwei Han", "hidden": false}, {"_id": "6926877a243b2216fb75cad1", "name": "Mingyu Ding", "hidden": false}, {"_id": "6926877a243b2216fb75cad2", "name": "Huaxiu Yao", "hidden": false}], "publishedAt": "2025-11-25T04:15:14.000Z", "submittedOnDailyAt": "2025-11-26T02:25:56.245Z", "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "684ff37fa383bc5d6b0ff77f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png", "isPro": false, "fullname": "JiaqiLiu", "user": "JiaaqiLiu", "type": "user"}, "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.", "upvotes": 42, "discussionId": "6926877a243b2216fb75cad3", "projectPage": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL", "githubRepo": "https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL", "ai_summary": "Agent0-VL, a self-evolving vision-language agent, incorporates tool usage into both reasoning and self-evaluation, enabling continual improvement through evidence-grounded analysis and reinforcement learning.", "ai_keywords": ["self-rewarding approaches", "tool-integrated reasoning", "self-evaluation", "self-repair", "introspect", "evidence-grounded analysis", "Solver", "Verifier", "Self-Evolving Reasoning Cycle", "reinforcement learning", "geometric problem solving", "visual scientific analysis"], "githubStars": 544, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u76d1\u7763\u3002</li>\n    <li>\u8fd1\u671f\u7684\u81ea\u6211\u5956\u52b1\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u8ba9\u6a21\u578b\u81ea\u6211\u8bc4\u4ef7\u6765\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u4f46\u7eaf\u6587\u672c\u8bc4\u4ef7\u96be\u4ee5\u9a8c\u8bc1\u590d\u6742\u89c6\u89c9\u63a8\u7406\u6b65\u9aa4\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Agent0-VL\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff0c\u5229\u7528\u5de5\u5177\u96c6\u6210\u63a8\u7406\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>Agent0-VL\u7ed3\u5408\u4e86\u63a8\u7406\u3001\u81ea\u6211\u8bc4\u4ef7\u548c\u81ea\u6211\u4fee\u6b63\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bc1\u636e\u7684\u5206\u6790\u6765\u63d0\u9ad8\u5176\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cAgent0-VL\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u548c\u89c6\u89c9\u79d1\u5b66\u5206\u6790\u4e2d\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u9ad8\u4e8612.5%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language agents have made great progress but still rely on human supervision.</li>\n    <li>Self-rewarding methods allow models to evaluate themselves, but they can struggle with complex visual tasks.</li>\n    <li>Agent0-VL is a new agent that improves itself by using tools for reasoning, evaluation, and correction.</li>\n    <li>It has two main roles: a Solver for reasoning and a Verifier for giving feedback and rewards.</li>\n    <li>Agent0-VL shows a 12.5% improvement in tasks without needing human help or external rewards.</li>\n</ul>"}, "publishedAt": "2025-11-24T23:15:14.000Z", "title": "Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning", "summary": "Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19900.png", "numComments": 2, "submittedBy": {"_id": "684ff37fa383bc5d6b0ff77f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png", "fullname": "JiaqiLiu", "name": "JiaaqiLiu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "669f9d1fec8789263c0e355a", "name": "UNC-ChapelHill", "fullname": "University of North Carolina at Chapel Hill", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20714", "authors": [{"_id": "6927bc37243b2216fb75cd3c", "name": "Inferix Team", "hidden": false}, {"_id": "6927bc37243b2216fb75cd3d", "name": "Tianyu Feng", "hidden": false}, {"_id": "6927bc37243b2216fb75cd3e", "name": "Yizeng Han", "hidden": false}, {"_id": "6927bc37243b2216fb75cd3f", "name": "Jiahao He", "hidden": false}, {"_id": "6927bc37243b2216fb75cd40", "name": "Yuanyu He", "hidden": false}, {"_id": "6927bc37243b2216fb75cd41", "name": "Xi Lin", "hidden": false}, {"_id": "6927bc37243b2216fb75cd42", "name": "Teng Liu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd43", "name": "Hanfeng Lu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd44", "name": "Jiasheng Tang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd45", "name": "Wei Wang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd46", "name": "Zhiyuan Wang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd47", "name": "Jichao Wu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd48", "name": "Mingyang Yang", "hidden": false}, {"_id": "6927bc37243b2216fb75cd49", "name": "Yinghao Yu", "hidden": false}, {"_id": "6927bc37243b2216fb75cd4a", "user": {"_id": "64ec877bb93654d4ca5c92e9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg", "isPro": false, "fullname": "Zeyu Zhang", "user": "SteveZeyuZhang", "type": "user"}, "name": "Zeyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:06.036Z", "hidden": false}, {"_id": "6927bc37243b2216fb75cd4b", "name": "Bohan Zhuang", "hidden": false}], "publishedAt": "2025-11-25T01:45:04.000Z", "submittedOnDailyAt": "2025-11-27T00:19:36.886Z", "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.", "upvotes": 38, "discussionId": "6927bc38243b2216fb75cd4c", "githubRepo": "https://github.com/alibaba-damo-academy/Inferix", "ai_summary": "Inferix is a next-generation inference engine designed for immersive world synthesis using semi-autoregressive decoding, combining diffusion and autoregressive methods for high-quality, real-time video generation and interaction.", "ai_keywords": ["world models", "agentic AI", "embodied AI", "gaming", "visual perception", "understanding", "reasoning", "semi-autoregressive", "block-diffusion", "diffusion", "autoregressive methods", "video tokens", "KV Cache management", "Inferix", "vLLM", "SGLang", "xDiTs", "interactive video streaming", "profiling", "LV-Bench", "minute-long video generation"], "githubStars": 24, "summary_zh": "<ul>\n    <li>\u4e16\u754c\u6a21\u578b\u662f\u4ee3\u7406 AI\u3001\u5177\u8eab AI \u548c\u6e38\u620f\u7b49\u9886\u57df\u7684\u6838\u5fc3\u6a21\u62df\u5668\uff0c\u80fd\u591f\u751f\u6210\u957f\u800c\u771f\u5b9e\u4e92\u52a8\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u6269\u5c55\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u4f1a\u89e3\u9501\u65b0\u7684\u89c6\u89c9\u611f\u77e5\u3001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52a8\u8d85\u8d8a\u5f53\u524d LLM \u89c6\u89d2\u7684\u57fa\u7840\u6a21\u578b\u7684\u65b0\u8303\u5f0f\u3002</li>\n    <li>\u534a\u81ea\u56de\u5f52\uff08\u5757\u6269\u6563\uff09\u89e3\u7801\u65b9\u6cd5\u662f\u5173\u952e\u7a81\u7834\uff0c\u5b83\u7ed3\u5408\u4e86\u6269\u6563\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u751f\u6210\u66f4\u8fde\u8d2f\u548c\u7a33\u5b9a\u7684\u89c6\u9891\u5e8f\u5217\u3002</li>\n    <li>Inferix \u662f\u4e0b\u4e00\u4ee3\u63a8\u7406\u5f15\u64ce\uff0c\u4e13\u4e3a\u4f18\u5316\u534a\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u800c\u8bbe\u8ba1\uff0c\u80fd\u591f\u5b9e\u73b0\u6c89\u6d78\u5f0f\u7684\u4e16\u754c\u5408\u6210\u3002</li>\n    <li>\u5b83\u652f\u6301\u5b9e\u65f6\u4e92\u52a8\u548c\u73b0\u5b9e\u6a21\u62df\uff0c\u5e76\u901a\u8fc7 LV-Bench \u96c6\u6210\u9ad8\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a8\u52a8\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>World models can create realistic and interactive videos, useful in AI, gaming, and simulation.</li>\n    <li>Scaling these models can improve visual perception and reasoning, moving beyond current methods focused on language models.</li>\n    <li>The new semi-autoregressive decoding combines diffusion and autoregressive techniques to create better video sequences.</li>\n    <li>Inferix is a new engine designed for efficient world simulation using this advanced decoding method.</li>\n    <li>It offers real-time video streaming and integrates a new evaluation benchmark for testing video generation quality.</li>\n</ul>"}, "publishedAt": "2025-11-24T20:45:04.000Z", "title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation", "summary": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20714.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.19320", "authors": [{"_id": "692525d916eb3a9f13103974", "user": {"_id": "65645169ec7e239899136895", "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg", "isPro": false, "fullname": "Jiaming Zhang", "user": "jiamingZ", "type": "user"}, "name": "Jiaming Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-25T12:10:05.653Z", "hidden": false}, {"_id": "692525d916eb3a9f13103975", "name": "Shengming Cao", "hidden": false}, {"_id": "692525d916eb3a9f13103976", "name": "Rui Li", "hidden": false}, {"_id": "692525d916eb3a9f13103977", "name": "Xiaotong Zhao", "hidden": false}, {"_id": "692525d916eb3a9f13103978", "name": "Yutao Cui", "hidden": false}, {"_id": "692525d916eb3a9f13103979", "name": "Xinglin Hou", "hidden": false}, {"_id": "692525d916eb3a9f1310397a", "name": "Gangshan Wu", "hidden": false}, {"_id": "692525d916eb3a9f1310397b", "name": "Haolan Chen", "hidden": false}, {"_id": "692525d916eb3a9f1310397c", "name": "Yu Xu", "hidden": false}, {"_id": "692525d916eb3a9f1310397d", "name": "Limin Wang", "hidden": false}, {"_id": "692525d916eb3a9f1310397e", "name": "Kai Ma", "hidden": false}], "publishedAt": "2025-11-24T17:15:55.000Z", "submittedOnDailyAt": "2025-11-26T00:24:22.899Z", "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation", "submittedOnDailyBy": {"_id": "65645169ec7e239899136895", "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg", "isPro": false, "fullname": "Jiaming Zhang", "user": "jiamingZ", "type": "user"}, "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.", "upvotes": 37, "discussionId": "692525d916eb3a9f1310397f", "projectPage": "https://mcg-nju.github.io/steadydancer-web", "githubRepo": "https://github.com/MCG-NJU/SteadyDancer", "ai_summary": "SteadyDancer, an Image-to-Video framework, ensures first-frame identity preservation and precise motion control through harmonized conditions, adaptive pose representation, and hierarchical training objectives.", "ai_keywords": ["Image-to-Motion Binding", "Reference-to-Video", "Image-to-Video", "Condition-Reconciliation Mechanism", "Synergistic Pose Modulation Modules", "Staged Decoupled-Objective Training Pipeline", "motion fidelity", "visual quality", "temporal coherence"], "githubStars": 116, "organization": {"_id": "62c77fde1e080b83746468bd", "name": "MCG-NJU", "fullname": "Multimedia Computing Group-Nanjing University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"}, "summary_zh": "<ul>\n    <li>\u4eba\u50cf\u52a8\u753b\u4e2d\uff0c\u4fdd\u6301\u7b2c\u4e00\u5e27\u7684\u8eab\u4efd\u548c\u7cbe\u786e\u7684\u8fd0\u52a8\u63a7\u5236\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u56fe\u50cf\u5230\u89c6\u9891\uff08R2V\uff09\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5e38\u89c1\u7684\u65f6\u7a7a\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5bfc\u81f4\u8eab\u4efd\u6f02\u79fb\u548c\u89c6\u89c9\u5931\u771f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684SteadyDancer\u65b9\u6cd5\u80fd\u591f\u534f\u8c03\u52a8\u753b\u6548\u679c\uff0c\u5e76\u7a33\u56fa\u5730\u4fdd\u6301\u7b2c\u4e00\u5e27\u7684\u8eab\u4efd\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u6761\u4ef6\u8c03\u548c\u673a\u5236\u548c\u534f\u540c\u59ff\u6001\u8c03\u5236\u6a21\u5757\u6765\u751f\u6210\u4e0e\u53c2\u8003\u56fe\u50cf\u9ad8\u5ea6\u517c\u5bb9\u7684\u59ff\u6001\u8868\u793a\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSteadyDancer\u5728\u5916\u89c2\u4fdd\u771f\u5ea6\u548c\u8fd0\u52a8\u63a7\u5236\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u66f4\u5c11\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Human image animation faces challenges in keeping the identity consistent while controlling motion accurately.</li>\n    <li>The traditional method, Reference-to-Video (R2V), often leads to issues like identity drift and visual errors due to misalignments.</li>\n    <li>SteadyDancer is a new framework that focuses on Image-to-Video (I2V) and successfully preserves the identity from the first frame.</li>\n    <li>It includes a special mechanism to balance conflicting conditions for better control and visual quality.</li>\n    <li>SteadyDancer outperforms existing methods in quality and motion control while needing less training effort.</li>\n</ul>"}, "publishedAt": "2025-11-24T12:15:55.000Z", "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation", "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19320.png", "numComments": 2, "submittedBy": {"_id": "65645169ec7e239899136895", "avatarUrl": "/avatars/59f7f485643f71aff078347d99ed9765.svg", "fullname": "Jiaming Zhang", "name": "jiamingZ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "62c77fde1e080b83746468bd", "name": "MCG-NJU", "fullname": "Multimedia Computing Group-Nanjing University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2511.04570", "authors": [{"_id": "690d5b51ad2597bf6c464ca9", "name": "Jingqi Tong", "hidden": false}, {"_id": "690d5b51ad2597bf6c464caa", "name": "Yurong Mou", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cab", "name": "Hangcheng Li", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cac", "user": {"_id": "65e4134472e748aae53e24f3", "avatarUrl": "/avatars/3346f4f4cdbffc4c51276be01a6c5f10.svg", "isPro": false, "fullname": "Mingzhe Li", "user": "Mubuky", "type": "user"}, "name": "Mingzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2025-11-07T10:28:47.508Z", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cad", "user": {"_id": "65ab2dd614d782df061265cd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ab2dd614d782df061265cd/7T8kMx0wFNTa5zsVQrnOr.jpeg", "isPro": false, "fullname": "Yongzhuo Yang", "user": "YangYongzhuo", "type": "user"}, "name": "Yongzhuo Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-10T09:31:39.261Z", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cae", "name": "Ming Zhang", "hidden": false}, {"_id": "690d5b51ad2597bf6c464caf", "name": "Qiguang Chen", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cb0", "name": "Tianyi Liang", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cb1", "name": "Xiaomeng Hu", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cb2", "name": "Yining Zheng", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cb3", "name": "Xinchi Chen", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cb4", "name": "Jun Zhao", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cb5", "name": "Xuanjing Huang", "hidden": false}, {"_id": "690d5b51ad2597bf6c464cb6", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2025-11-06T17:25:23.000Z", "submittedOnDailyAt": "2025-11-07T00:18:17.549Z", "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm", "submittedOnDailyBy": {"_id": "6690e13ccbcaf7ab0ec1c971", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/e8KDV6J29tviXlIpLZPq6.png", "isPro": false, "fullname": "Tony.Li", "user": "lkdhy", "type": "user"}, "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.", "upvotes": 203, "discussionId": "690d5b51ad2597bf6c464cb7", "projectPage": "https://thinking-with-video.github.io/", "githubRepo": "https://github.com/tongjingqi/Thinking-with-Video", "ai_summary": "The \"Thinking with Video\" paradigm enhances multimodal reasoning by integrating video generation models, demonstrated through the Video Thinking Benchmark and improved performance on both vision and text tasks.", "ai_keywords": ["Thinking with Text", "Thinking with Images", "large language models", "Vision Language Models", "Thinking with Video", "video generation models", "Video Thinking Benchmark", "vision-centric tasks", "text-centric tasks", "Eyeballing Puzzles", "GSM8K", "MMMU", "self-consistency", "in-context learning"], "githubStars": 211, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u201c\u89c6\u9891\u601d\u7ef4\u201d\u65b0\u8303\u5f0f\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u6587\u672c\u548c\u56fe\u50cf\u601d\u7ef4\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>\u89c6\u9891\u601d\u7ef4\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u5982Sora-2\uff09\u6765\u5b9e\u73b0\u89c6\u89c9\u548c\u6587\u672c\u63a8\u7406\u7684\u7edf\u4e00\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u89c6\u9891\u601d\u7ef4\u57fa\u51c6\uff08VideoThinkBench\uff09\uff0c\u5305\u542b\u89c6\u89c9\u548c\u6587\u672c\u4efb\u52a1\u4e24\u7c7b\u3002</li>\n    <li>Sora-2\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684VLMs\u76f8\u5f53\uff0c\u5e76\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5b83\u4eec\u3002</li>\n    <li>Sora-2\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8fbe\u523092%\u548c75.53%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li> \"Thinking with Text\" and \"Thinking with Images\" improve reasoning in language and vision models but have limitations.</li>\n    <li> Images can only show single moments and don't capture ongoing changes; separating text and images limits understanding.</li>\n    <li> The new \"Thinking with Video\" approach uses video generation to connect visual and textual reasoning over time.</li>\n    <li> We created the Video Thinking Benchmark to test this approach with vision-centric and text-centric tasks.</li>\n    <li> The Sora-2 model performs well, matching or exceeding other models in many tasks, showing potential for unified multimodal reasoning.</li>\n</ul>"}, "publishedAt": "2025-11-06T12:25:23.000Z", "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm", "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.04570.png", "numComments": 4, "submittedBy": {"_id": "6690e13ccbcaf7ab0ec1c971", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/e8KDV6J29tviXlIpLZPq6.png", "fullname": "Tony.Li", "name": "lkdhy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.08892", "authors": [{"_id": "69154dffa1b06ca3cc81351e", "name": "Weihao Tan", "hidden": false}, {"_id": "69154dffa1b06ca3cc81351f", "name": "Xiangyang Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813520", "name": "Yunhao Fang", "hidden": false}, {"_id": "69154dffa1b06ca3cc813521", "name": "Heyuan Yao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813522", "name": "Shi Yan", "hidden": false}, {"_id": "69154dffa1b06ca3cc813523", "name": "Hao Luo", "hidden": false}, {"_id": "69154dffa1b06ca3cc813524", "name": "Tenglong Ao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813525", "name": "Huihui Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813526", "name": "Hongbin Ren", "hidden": false}, {"_id": "69154dffa1b06ca3cc813527", "user": {"_id": "6369d92f64aad59d4d44d362", "avatarUrl": "/avatars/73956400cfbfd53116aefc17b3c9f0fd.svg", "isPro": false, "fullname": "Yi", "user": "Bairen", "type": "user"}, "name": "Bairen Yi", "status": "claimed_verified", "statusLastChangedAt": "2025-11-17T10:31:49.043Z", "hidden": false}, {"_id": "69154dffa1b06ca3cc813528", "name": "Yujia Qin", "hidden": false}, {"_id": "69154dffa1b06ca3cc813529", "name": "Bo An", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352a", "name": "Libin Liu", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352b", "name": "Guang Shi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "publishedAt": "2025-11-12T02:01:26.000Z", "submittedOnDailyAt": "2025-11-13T00:49:21.639Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "upvotes": 185, "discussionId": "69154dffa1b06ca3cc81352c", "projectPage": "https://www.lumine-ai.org/", "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.", "ai_keywords": ["vision-language model", "end-to-end", "3D open-world environments", "human-like interaction", "real-time", "zero-shot cross-game generalization"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Lumine\u662f\u7b2c\u4e00\u4e2a\u5f00\u653e\u7684\u901a\u7528\u667a\u80fd\u4f53\u5f00\u53d1\u914d\u65b9\uff0c\u80fd\u591f\u5728\u590d\u6742\u76843D\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u65f6\u5b8c\u6210\u957f\u8fbe\u6570\u5c0f\u65f6\u7684\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u91c7\u7528\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u7edf\u4e00\u5728\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u3002</li>\n    <li>Lumine\u5728\u300a\u539f\u795e\u300b\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u6548\u7387\u76f8\u5f53\u5730\u5b8c\u6210\u4e94\u5c0f\u65f6\u7684\u4e3b\u7ebf\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u5404\u79cd\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u4e0d\u4ec5\u5728\u7279\u5b9a\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u591f\u5728\u6ca1\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5176\u4ed6\u6e38\u620f\u4e2d\u5b8c\u6210\u957f\u8fbe100\u5206\u949f\u7684\u4efb\u52a1\u3002</li>\n    <li>Lumine\u7684\u6210\u529f\u5c55\u793a\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5f00\u53d1\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Lumine is a new open recipe for creating generalist agents that can handle long, complex tasks in 3D open-world games.</li>\n    <li>It uses a human-like approach to combine sensing, thinking, and acting, using a vision-language model.</li>\n    <li>Lumine processes visual information quickly and efficiently to perform actions based on natural language commands.</li>\n    <li>It can complete the five-hour main storyline of Genshin Impact at a level similar to human players and follow various instructions across different tasks.</li>\n    <li>Lumine shows strong performance in other games without needing extra training, successfully completing missions in Wuthering Waves and Honkai: Star Rail.</li>\n</ul>"}, "publishedAt": "2025-11-11T21:01:26.000Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08892.png", "numComments": 12, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.11793", "authors": [{"_id": "691be81b6bfd5965c0fd37e2", "name": "MiroMind Team", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e3", "name": "Song Bai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e4", "name": "Lidong Bing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e5", "name": "Carson Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e6", "name": "Guanzheng Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e7", "user": {"_id": "632dab84fdb35759ea6646a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632dab84fdb35759ea6646a0/IxO5mbtzHJsr0YHW-YtVk.jpeg", "isPro": false, "fullname": "Yuntao Chen", "user": "YuntaoChen", "type": "user"}, "name": "Yuntao Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:45.403Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e8", "name": "Zhe Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e9", "name": "Ziyi Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ea", "name": "Jifeng Dai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37eb", "name": "Xuan Dong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ec", "name": "Yue Deng", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ed", "name": "Yunjie Fu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ee", "name": "Junqi Ge", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ef", "name": "Chenxia Han", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f0", "name": "Tammy Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f1", "name": "Zhenhang Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f2", "name": "Jerry Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f3", "name": "Shilei Jiang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f4", "name": "Tianyu Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f5", "user": {"_id": "64be2455b567ae97c34bb948", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be2455b567ae97c34bb948/QuCdStDDGaXjDmp4V-dBj.jpeg", "isPro": false, "fullname": "Xiaoqi Jian", "user": "mx1024", "type": "user"}, "name": "Xiaoqi Jian", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:52.417Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f6", "name": "Lei Lei", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f7", "user": {"_id": "6466e7be1343dce20e59191b", "avatarUrl": "/avatars/6779560b203c3773dc76372c0b8cbe4e.svg", "isPro": false, "fullname": "Li Ruilin", "user": "Eric-LRL-130", "type": "user"}, "name": "Ruilin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:43.774Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f8", "name": "Ryan Luo", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f9", "name": "Tiantong Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fa", "name": "Xiang Lin", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fb", "name": "Ziyuan Liu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fc", "name": "Zhiqi Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fd", "name": "Jie Ni", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fe", "name": "Qiang Ren", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ff", "name": "Pax Sun", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3800", "name": "Shiqian Su", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3801", "name": "Chenxin Tao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3802", "name": "Bin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3803", "name": "Hellen Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3804", "name": "Haonan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3805", "name": "James Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3806", "name": "Jin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3807", "name": "Jojo Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3808", "name": "Letian Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3809", "name": "Shizun Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380a", "user": {"_id": "63d34004b734eaa4d4faeccf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg", "isPro": false, "fullname": "Weizhi Wang", "user": "weizhiwang", "type": "user"}, "name": "Weizhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:47.000Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380b", "name": "Zixuan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380c", "name": "Jinfan Xu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380d", "name": "Sen Xing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380e", "user": {"_id": "637f347a52229c639211bee8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg", "isPro": false, "fullname": "Chenyu Yang", "user": "cyyang822", "type": "user"}, "name": "Chenyu Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:48.746Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380f", "user": {"_id": "6239888e7fef05b7bdd5fcff", "avatarUrl": "/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg", "isPro": false, "fullname": "Hai Ye", "user": "oceanpty", "type": "user"}, "name": "Hai Ye", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:50.623Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3810", "name": "Jiaheng Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3811", "name": "Yue Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3812", "name": "Muyan Zhong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3813", "name": "Tianchen Zhao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3814", "name": "Xizhou Zhu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3815", "name": "Yanpeng Zhou", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3816", "name": "Yifan Zhang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3817", "name": "Zhi Zhu", "hidden": false}], "publishedAt": "2025-11-14T18:52:07.000Z", "submittedOnDailyAt": "2025-11-18T02:00:07.077Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "upvotes": 153, "discussionId": "691be81b6bfd5965c0fd3818", "projectPage": "https://dr.miromind.ai/", "githubRepo": "https://github.com/MiroMindAI/MiroThinker", "githubStars": 1133, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MiroThinker v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u548c\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002</li>\n    <li>MiroThinker\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u652f\u6301\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u9002\u5e94\u590d\u6742\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>MiroThinker\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u4ea4\u4e92\u6df1\u5ea6\u7684\u6269\u5c55\u4e0e\u6a21\u578b\u7684\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u7279\u6027\uff0c\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u91cd\u8981\u7ef4\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MiroThinker v1.0 is an open-source research agent aimed at improving reasoning and information-seeking skills.</li>\n    <li>It focuses on enhancing how the model interacts with its environment, rather than just increasing model size or context length.</li>\n    <li>The model uses reinforcement learning to effectively manage many tool calls (up to 600) during tasks, supporting complex research activities.</li>\n    <li>MiroThinker outperforms previous open-source agents in various benchmarks, showing accuracy rates that approach those of advanced commercial models.</li>\n    <li>The research highlights that better performance comes from deeper and more frequent interactions with the environment, suggesting that this interaction is crucial for future research agent development.</li>\n</ul>"}, "publishedAt": "2025-11-14T13:52:07.000Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11793.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.13254", "authors": [{"_id": "691c1c836bfd5965c0fd39e4", "user": {"_id": "6419b34a9a27800807c34a63", "avatarUrl": "/avatars/ee1391a9a153bae0dd04323b1fa5b5d6.svg", "isPro": false, "fullname": "Shalini M", "user": "shalinimaiti", "type": "user"}, "name": "Shalini Maiti", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:47.486Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e5", "user": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "name": "Amar Budhiraja", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:49.892Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e6", "user": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "isPro": false, "fullname": "Bhavul Gauri", "user": "bhavul", "type": "user"}, "name": "Bhavul Gauri", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:31.124Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e7", "user": {"_id": "691c6b9b660c15d270b5838a", "avatarUrl": "/avatars/2dc40e079bd8b7af7ae4a4ac43acc552.svg", "isPro": false, "fullname": "Gaurav Chaurasia", "user": "gchauras", "type": "user"}, "name": "Gaurav Chaurasia", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:45.558Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e8", "name": "Anton Protopopov", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e9", "name": "Alexis Audran-Reiss", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ea", "name": "Michael Slater", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39eb", "name": "Despoina Magka", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ec", "name": "Tatiana Shavrina", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ed", "name": "Roberta Raileanu", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ee", "name": "Yoram Bachrach", "hidden": false}], "publishedAt": "2025-11-17T11:13:34.000Z", "submittedOnDailyAt": "2025-11-18T04:47:00.815Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "submittedOnDailyBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "upvotes": 131, "discussionId": "691c1c846bfd5965c0fd39fc", "githubRepo": "https://github.com/facebookresearch/llm_souping", "githubStars": 60, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>\u6a21\u578b\u201c\u6df7\u5408\u201d\uff08model souping\uff09\u901a\u8fc7\u5bf9\u591a\u4e2a\u76f8\u540c\u67b6\u6784\u7684\u6a21\u578b\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u80fd\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86\u201c\u7c7b\u522b\u4e13\u5bb6\u6df7\u5408\u201d\uff08SoCE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u51c6\u7ec4\u5408\u6765\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u6765\u4f18\u5316\u6027\u80fd\u3002</li>\n    <li>SoCE\u65b9\u6cd5\u5229\u7528\u57fa\u51c6\u7c7b\u522b\u4e4b\u95f4\u7684\u4f4e\u76f8\u5173\u6027\u6765\u8bc6\u522b\u201c\u4e13\u5bb6\u201d\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4f18\u5316\u52a0\u6743\u5408\u5e76\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u80fd\u529b\u3001\u5de5\u5177\u8c03\u7528\u548c\u6570\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are powerful but require a lot of resources and time to train.</li>\n    <li>Model souping, or averaging weights from multiple models, can improve performance without needing extensive retraining.</li>\n    <li>This paper presents Soup Of Category Experts (SoCE), a new method for model souping that selects the best models based on benchmark categories.</li>\n    <li>SoCE uses weighted averaging to combine models, focusing on those that perform well in related areas instead of using equal weights for all models.</li>\n    <li>The method shows better performance and reliability in various tasks, achieving top results in certain benchmarks.</li>\n</ul>"}, "publishedAt": "2025-11-17T06:13:34.000Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13254.png", "numComments": 4, "submittedBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "fullname": "Amar Budhiraja", "name": "ambud26", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.13612", "authors": [{"_id": "691bfdc96bfd5965c0fd3951", "user": {"_id": "65352acb7139c5dd8d9a8590", "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg", "isPro": false, "fullname": "JiachengChen", "user": "JC-Chen", "type": "user"}, "name": "Jiacheng Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:07:04.293Z", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3952", "name": "Qianjia Cheng", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3953", "name": "Fangchen Yu", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3954", "user": {"_id": "691b0f528411a45dc9ee9de8", "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg", "isPro": false, "fullname": "Haiyuan Wan", "user": "HY-Wan", "type": "user"}, "name": "Haiyuan Wan", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:34.294Z", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3955", "name": "Yuchen Zhang", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3956", "name": "Shenghe Zheng", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3957", "name": "Junchi Yao", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3958", "user": {"_id": "66554e83a2d7a882a876ffc3", "avatarUrl": "/avatars/9803448705f282b72d82a66cb22338b2.svg", "isPro": false, "fullname": "qingyang zhang", "user": "qingyangzhang", "type": "user"}, "name": "Qingyang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:50:04.900Z", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3959", "name": "Haonan He", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd395a", "name": "Yun Luo", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd395b", "name": "Yufeng Zhao", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd395c", "name": "Futing Wang", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd395d", "name": "Li Sheng", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd395e", "name": "Chengxing Xie", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd395f", "name": "Yuxin Zuo", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3960", "name": "Yizhuo Li", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3961", "name": "Wenxauan Zeng", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3962", "name": "Yulun Wu", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3963", "user": {"_id": "6731caae58ba0f0984c188ea", "avatarUrl": "/avatars/445275cca29f97c5f8754d6c27075887.svg", "isPro": false, "fullname": "Rui Huang", "user": "Rui1121", "type": "user"}, "name": "Rui Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:07:02.211Z", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3964", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3965", "name": "Kai Chen", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3966", "name": "Yu Qiao", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3967", "name": "Lei Bai", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3968", "name": "Yu Cheng", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd3969", "name": "Ning Ding", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd396a", "name": "Bowen Zhou", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd396b", "name": "Peng Ye", "hidden": false}, {"_id": "691bfdc96bfd5965c0fd396c", "name": "Ganqu Cui", "hidden": false}], "publishedAt": "2025-11-17T17:18:13.000Z", "submittedOnDailyAt": "2025-11-18T02:37:44.601Z", "title": "P1: Mastering Physics Olympiads with Reinforcement Learning", "submittedOnDailyBy": {"_id": "65352acb7139c5dd8d9a8590", "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg", "isPro": false, "fullname": "JiachengChen", "user": "JC-Chen", "type": "user"}, "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.", "upvotes": 128, "discussionId": "691bfdca6bfd5965c0fd396d", "projectPage": "https://prime-rl.github.io/P1/", "githubRepo": "https://github.com/PRIME-RL/P1", "githubStars": 59, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\u4e0a\u3002</li>\n    <li>P1\u662f\u4e00\u4e2a\u5f00\u6e90\u7269\u7406\u63a8\u7406\u6a21\u578b\u7cfb\u5217\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u800c\u6210\u3002</li>\n    <li>P1-235B-A22B\u662f\u9996\u4e2a\u5728\u56fd\u9645\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u83b7\u5f97\u91d1\u724c\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u57282024/2025\u5e74\u8d62\u5f97\u4e8612\u679a\u91d1\u724c\u3002</li>\n    <li>P1-30B-A3B\u5728\u56fd\u9645\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u83b7\u5f97\u94f6\u724c\uff0c\u8d85\u8d8a\u4e86\u51e0\u4e4e\u6240\u6709\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002</li>\n    <li>P1\u7cfb\u5217\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u663e\u793a\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in large language models (LLMs) have improved their ability to solve complex scientific problems, especially in physics.</li>\n    <li>The P1 models are a new series of open-source models designed specifically for physics reasoning, trained using reinforcement learning.</li>\n    <li>P1-235B-A22B is the first open-source model to achieve Gold-medal performance at the International Physics Olympiad (IPhO) 2025.</li>\n    <li>The P1 models also excel in other reasoning tasks such as math and coding, demonstrating their versatility.</li>\n    <li>P1-30B-A3B earned a silver medal at the IPhO 2025, showing strong performance among open-source models.</li>\n</ul>"}, "publishedAt": "2025-11-17T12:18:13.000Z", "title": "P1: Mastering Physics Olympiads with Reinforcement Learning", "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13612.png", "numComments": 4, "submittedBy": {"_id": "65352acb7139c5dd8d9a8590", "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg", "fullname": "JiachengChen", "name": "JC-Chen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": true}, {"paper": {"id": "2511.06221", "authors": [{"_id": "6912a1c7a644ba07c499c6e1", "user": {"_id": "67486775ed2e4d9e50fc9117", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67486775ed2e4d9e50fc9117/WrCtPqY9X67ASbkUeloDF.jpeg", "isPro": false, "fullname": "Sen Xu", "user": "SenXbjtu", "type": "user"}, "name": "Sen Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-11T19:44:27.021Z", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e2", "user": {"_id": "6406991ec3ab325efa9b6732", "avatarUrl": "/avatars/5ba29d9e25820c1172b5a98b078e416f.svg", "isPro": false, "fullname": "DenseHub", "user": "YiZhouDenseHub", "type": "user"}, "name": "Yi Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-11-11T19:44:32.245Z", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e3", "name": "Wei Wang", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e4", "user": {"_id": "6646fa14b096df5b522fd5f9", "avatarUrl": "/avatars/98f67b6f8cb9776575916a2ba027f738.svg", "isPro": false, "fullname": "MIN JIXIN", "user": "JIXIN0121", "type": "user"}, "name": "Jixin Min", "status": "claimed_verified", "statusLastChangedAt": "2025-11-12T12:21:33.041Z", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e5", "user": {"_id": "64d1faaa1ed6649d70d1fa2f", "avatarUrl": "/avatars/388ba18df077eaa8e16a89e59bf852fa.svg", "isPro": false, "fullname": "YinZhiBin", "user": "YinZhiBin", "type": "user"}, "name": "Zhibin Yin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-11T19:44:34.446Z", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e6", "name": "Yingwei Dai", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e7", "user": {"_id": "6422eba32f38c0a50cfdc77d", "avatarUrl": "/avatars/0b1137ff258ba578c8b0e257a43716fa.svg", "isPro": false, "fullname": "lsx", "user": "lsx666", "type": "user"}, "name": "Shixi Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-13T13:07:14.701Z", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e8", "user": {"_id": "636b5433d524b220830e7a61", "avatarUrl": "/avatars/09e9b392a189195aa09b93fc17b70cc3.svg", "isPro": false, "fullname": "pangly", "user": "pangly", "type": "user"}, "name": "Lianyu Pang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-12T12:21:31.027Z", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6e9", "name": "Yirong Chen", "hidden": false}, {"_id": "6912a1c7a644ba07c499c6ea", "user": {"_id": "668b5090101353874ced73d0", "avatarUrl": "/avatars/b2ec34a321890140e97ddd69884132a8.svg", "isPro": false, "fullname": "junlin zhang", "user": "junlinzhang", "type": "user"}, "name": "Junlin Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-11T19:44:24.601Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6406991ec3ab325efa9b6732/4yVJjn-Y2UONHvzebYMkU.png"], "publishedAt": "2025-11-09T04:37:36.000Z", "submittedOnDailyAt": "2025-11-12T00:53:44.764Z", "title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5B", "submittedOnDailyBy": {"_id": "6406991ec3ab325efa9b6732", "avatarUrl": "/avatars/5ba29d9e25820c1172b5a98b078e416f.svg", "isPro": false, "fullname": "DenseHub", "user": "YiZhouDenseHub", "type": "user"}, "summary": "Challenging the prevailing consensus that small models inherently lack robust\nreasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense\nmodel developed via our Spectrum-to-Signal Principle (SSP). This challenges the\nprevailing approach of scaling model parameters to enhance capabilities, as\nseen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework\nfirst employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a\nbroad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)\nto amplify the correct signal. With a total training cost of only $7,800,\nVibeThinker-1.5B demonstrates superior reasoning capabilities compared to\nclosed-source models like Magistral Medium and Claude Opus 4, and performs on\npar with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses\nthe 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),\nAIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial\nimprovement over its base model (6.7, 4.3, and 0.6, respectively). On\nLiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its\nbase model's 0.0. These findings demonstrate that small models can achieve\nreasoning capabilities comparable to large models, drastically reducing\ntraining and inference costs and thereby democratizing advanced AI research.", "upvotes": 120, "discussionId": "6912a1c7a644ba07c499c6eb", "projectPage": "https://github.com/WeiboAI/VibeThinker", "githubRepo": "https://github.com/WeiboAI/VibeThinker", "ai_summary": "VibeThinker-1.5B, a 1.5B-parameter model using the Spectrum-to-Signal Principle, achieves superior reasoning capabilities compared to larger models at a significantly lower cost.", "ai_keywords": ["Spectrum-to-Signal Principle", "Two-Stage Diversity-Exploring Distillation", "MaxEnt-Guided Policy Optimization", "VibeThinker-1.5B", "DeepSeek R1", "Kimi k2", "Magistral Medium", "Claude Opus 4", "GPT OSS-20B Medium", "AIME24", "AIME25", "HMMT25", "LiveCodeBench V6"], "githubStars": 508, "organization": {"_id": "68c8059479c43cfa50f36156", "name": "WeiboAI", "fullname": "WeiboAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d1faaa1ed6649d70d1fa2f/lZVm6Yuiif9cdr5KsnfZr.png"}, "summary_zh": "<ul>\n    <li>\u62a5\u544a\u4ecb\u7ecd\u4e86VibeThinker-1.5B\uff0c\u4e00\u4e2a\u5177\u670915\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u6311\u6218\u4e86\u5c0f\u6a21\u578b\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u7684\u4f20\u7edf\u89c2\u70b9\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7Spectrum-to-Signal Principle (SSP)\u5f00\u53d1\uff0c\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7684\u591a\u6837\u6027\u63a2\u7d22\u84b8\u998f\u548c\u6700\u5927\u71b5\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u3002</li>\n    <li>VibeThinker-1.5B\u7684\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a7800\u7f8e\u5143\uff0c\u63a8\u7406\u80fd\u529b\u4f18\u4e8e\u4e00\u4e9b\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u5728\u4e09\u9879\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVibeThinker-1.5B\u7684\u8868\u73b0\u8d85\u8fc7\u4e86400\u500d\u66f4\u5927\u7684DeepSeek R1\u3002</li>\n    <li>\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u4f3c\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u4fc3\u8fdb\u5148\u8fdbAI\u7814\u7a76\u7684\u666e\u53ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VibeThinker-1.5B is a new 1.5 billion parameter model that challenges the idea that smaller models can't reason well.</li>\n    <li>This model uses a unique approach called the Spectrum-to-Signal Principle (SSP) to improve its performance without needing a massive number of parameters.</li>\n    <li>It was trained at a low cost of $7,800 and shows better reasoning than some larger closed-source models.</li>\n    <li>VibeThinker-1.5B outperformed the much larger DeepSeek R1 on three math tests, achieving better scores.</li>\n    <li>This research suggests that smaller models can match the reasoning abilities of larger ones, making advanced AI more accessible and cost-effective.</li>\n</ul>"}, "publishedAt": "2025-11-08T23:37:36.000Z", "title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5B", "summary": "Challenging the prevailing consensus that small models inherently lack robust\nreasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense\nmodel developed via our Spectrum-to-Signal Principle (SSP). This challenges the\nprevailing approach of scaling model parameters to enhance capabilities, as\nseen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework\nfirst employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a\nbroad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)\nto amplify the correct signal. With a total training cost of only $7,800,\nVibeThinker-1.5B demonstrates superior reasoning capabilities compared to\nclosed-source models like Magistral Medium and Claude Opus 4, and performs on\npar with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses\nthe 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),\nAIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial\nimprovement over its base model (6.7, 4.3, and 0.6, respectively). On\nLiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its\nbase model's 0.0. These findings demonstrate that small models can achieve\nreasoning capabilities comparable to large models, drastically reducing\ntraining and inference costs and thereby democratizing advanced AI research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6406991ec3ab325efa9b6732/4yVJjn-Y2UONHvzebYMkU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.06221.png", "numComments": 11, "submittedBy": {"_id": "6406991ec3ab325efa9b6732", "avatarUrl": "/avatars/5ba29d9e25820c1172b5a98b078e416f.svg", "fullname": "DenseHub", "name": "YiZhouDenseHub", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "68c8059479c43cfa50f36156", "name": "WeiboAI", "fullname": "WeiboAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d1faaa1ed6649d70d1fa2f/lZVm6Yuiif9cdr5KsnfZr.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.03276", "authors": [{"_id": "690c1cdc60494e4fa76756a9", "user": {"_id": "62a7362fd1e7a011fd4e31a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a7362fd1e7a011fd4e31a7/ZY_mwH-sI0o05SHpLqwc7.png", "isPro": false, "fullname": "Jinjie Ni", "user": "jinjieni", "type": "user"}, "name": "Jinjie Ni", "status": "claimed_verified", "statusLastChangedAt": "2025-11-06T11:47:00.935Z", "hidden": false}, {"_id": "690c1cdc60494e4fa76756aa", "user": {"_id": "612ee6a7b960e78c6d2319d4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg", "isPro": false, "fullname": "Qian Liu", "user": "SivilTaram", "type": "user"}, "name": "Qian Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-11-07T10:31:23.866Z", "hidden": false}, {"_id": "690c1cdc60494e4fa76756ab", "user": {"_id": "6214e4ee1e35c843d42d1f88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6214e4ee1e35c843d42d1f88/fj-9wuIdPhvogh3BrcXTB.jpeg", "isPro": false, "fullname": "Longxu Dou", "user": "dreamerdeo", "type": "user"}, "name": "Longxu Dou", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:09:50.728Z", "hidden": false}, {"_id": "690c1cdc60494e4fa76756ac", "name": "Chao Du", "hidden": false}, {"_id": "690c1cdc60494e4fa76756ad", "name": "Zili Wang", "hidden": false}, {"_id": "690c1cdc60494e4fa76756ae", "name": "Hang Yan", "hidden": false}, {"_id": "690c1cdc60494e4fa76756af", "name": "Tianyu Pang", "hidden": false}, {"_id": "690c1cdc60494e4fa76756b0", "name": "Michael Qizhe Shieh", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/62a7362fd1e7a011fd4e31a7/QrHOFNxOkfy93f6tPGJdN.jpeg"], "publishedAt": "2025-11-05T08:17:42.000Z", "submittedOnDailyAt": "2025-11-06T01:32:43.418Z", "title": "Diffusion Language Models are Super Data Learners", "submittedOnDailyBy": {"_id": "62a7362fd1e7a011fd4e31a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a7362fd1e7a011fd4e31a7/ZY_mwH-sI0o05SHpLqwc7.png", "isPro": false, "fullname": "Jinjie Ni", "user": "jinjieni", "type": "user"}, "summary": "Under strictly controlled pre-training settings, we observe a Crossover: when\nunique data is limited, diffusion language models (DLMs) consistently surpass\nautoregressive (AR) models by training for more epochs. The crossover shifts\nlater with more or higher-quality data, earlier with larger models, and\npersists across dense and sparse architectures. We attribute the gains to three\ncompounding factors: (1) any-order modeling, (2) super-dense compute from\niterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;\ninput or parameter noise improves AR under data constraint but cannot close the\ngap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B\nunique Python tokens overtakes an AR coder trained with strictly matched\nsettings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag\nand > 33% on MMLU using only 1B tokens, without any special tricks, just by\nrepeating standard pre-training data. We also show that rising validation\ncross-entropy does not imply degraded downstream performance in this regime.", "upvotes": 118, "discussionId": "690c1cdc60494e4fa76756b1", "projectPage": "https://github.com/JinjieNi/dlms-are-super-data-learners", "githubRepo": "https://github.com/JinjieNi/MegaDLMs", "ai_summary": "Diffusion language models outperform autoregressive models in low-data settings due to any-order modeling, iterative bidirectional denoising, and Monte Carlo augmentation, and maintain advantages even at scale.", "ai_keywords": ["diffusion language models", "autoregressive models", "any-order modeling", "iterative bidirectional denoising", "Monte Carlo augmentation", "HellaSwag", "MMLU"], "githubStars": 283, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "summary_zh": "<ul>\n    <li>\u5728\u4e25\u683c\u63a7\u5236\u7684\u9884\u8bad\u7ec3\u6761\u4ef6\u4e0b\uff0c\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLMs\uff09\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u8d85\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u3002</li>\n    <li>\u968f\u7740\u6570\u636e\u91cf\u589e\u52a0\uff0c\u4ea4\u53c9\u70b9\u4f1a\u63a8\u8fdf\uff1b\u4f7f\u7528\u66f4\u5927\u6a21\u578b\u65f6\uff0c\u4ea4\u53c9\u70b9\u63d0\u524d\u3002</li>\n    <li>DLM\u7684\u4f18\u52bf\u5f52\u56e0\u4e8e\u4e09\u4e2a\u56e0\u7d20\uff1a\u65e0\u5e8f\u5efa\u6a21\u3001\u8fed\u4ee3\u53cc\u5411\u53bb\u566a\u7684\u8d85\u5bc6\u96c6\u8ba1\u7b97\u3001\u4ee5\u53ca\u5185\u7f6e\u7684\u8499\u7279\u5361\u6d1b\u589e\u5f3a\u3002</li>\n    <li>\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\uff0c1.7B\u53c2\u6570\u7684DLM\u5728\u4f7f\u752815\u4e07\u4ebf\u6807\u8bb0\u8ba1\u7b97\u9884\u7b97\u65f6\u8d85\u8d8a\u4e86\u81ea\u56de\u5f52\u7f16\u7801\u5668\u3002</li>\n    <li>1B\u53c2\u6570\u7684DLM\u5728\u6ca1\u6709\u7279\u6b8a\u6280\u5de7\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f7f\u752810\u4ebf\u6807\u8bb0\u5c31\u80fd\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion language models (DLMs) perform better than autoregressive (AR) models when trained for longer with limited unique data.</li>\n    <li>The advantage of DLMs increases with better data quality and larger model sizes, but can decrease with more unique data.</li>\n    <li>The improved performance of DLMs is due to three main factors: allowing any order of input, efficient computation from iterative denoising, and using noise for augmentation.</li>\n    <li>A 1.7 billion parameter DLM trained on a large dataset outperforms a matched AR model.</li>\n    <li>A smaller 1 billion parameter DLM achieves high accuracy on specific tasks using only standard pre-training data without special techniques.</li>\n</ul>"}, "publishedAt": "2025-11-05T03:17:42.000Z", "title": "Diffusion Language Models are Super Data Learners", "summary": "Under strictly controlled pre-training settings, we observe a Crossover: when\nunique data is limited, diffusion language models (DLMs) consistently surpass\nautoregressive (AR) models by training for more epochs. The crossover shifts\nlater with more or higher-quality data, earlier with larger models, and\npersists across dense and sparse architectures. We attribute the gains to three\ncompounding factors: (1) any-order modeling, (2) super-dense compute from\niterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;\ninput or parameter noise improves AR under data constraint but cannot close the\ngap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B\nunique Python tokens overtakes an AR coder trained with strictly matched\nsettings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag\nand > 33% on MMLU using only 1B tokens, without any special tricks, just by\nrepeating standard pre-training data. We also show that rising validation\ncross-entropy does not imply degraded downstream performance in this regime.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/62a7362fd1e7a011fd4e31a7/QrHOFNxOkfy93f6tPGJdN.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.03276.png", "numComments": 15, "submittedBy": {"_id": "62a7362fd1e7a011fd4e31a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a7362fd1e7a011fd4e31a7/ZY_mwH-sI0o05SHpLqwc7.png", "fullname": "Jinjie Ni", "name": "jinjieni", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 18}, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Nov 29, 2025";
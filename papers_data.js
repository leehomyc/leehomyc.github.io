window.trendingPapers = {
    "today": [{"paper": {"id": "2512.10430", "authors": [{"_id": "693bba8d9874a2a5e4ffb3ab", "user": {"_id": "64f4c8739ee58d48e8507e0e", "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg", "isPro": false, "fullname": "Dmitrii Stoianov", "user": "heylimon", "type": "user"}, "name": "Dmitrii Stoianov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:40.198Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ac", "user": {"_id": "64fb054ebb362cbf2fe53159", "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg", "isPro": false, "fullname": "Danil Taranets", "user": "taranetsdan", "type": "user"}, "name": "Danil Taranets", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:29.281Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ad", "user": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "name": "Olga Tsymboi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:38.180Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ae", "user": {"_id": "6780dcd6acf8d824c03864da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png", "isPro": false, "fullname": "Ramil Latypov", "user": "kylecr4ne", "type": "user"}, "name": "Ramil Latypov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:23.437Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3af", "user": {"_id": "6513f03e86d74f32ed65e3b8", "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg", "isPro": false, "fullname": "Almaz Dautov", "user": "the-hir0", "type": "user"}, "name": "Almaz Dautov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:30.990Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b0", "user": {"_id": "621a8daf325b927e60fcef08", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg", "isPro": false, "fullname": "Vladislav Kruglikov", "user": "vladislavkruglikov", "type": "user"}, "name": "Vladislav Kruglikov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:21.170Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b1", "name": "Nikita Surkov", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b2", "user": {"_id": "63188c428d698d8c1642a0d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg", "isPro": false, "fullname": "German Abramov", "user": "germanjke", "type": "user"}, "name": "German Abramov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:28.579Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b3", "name": "Pavel Gein", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b4", "user": {"_id": "636a9a07e3ad78bc68b1a5a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg", "isPro": false, "fullname": "Dmitry Abulkhanov", "user": "mponty", "type": "user"}, "name": "Dmitry Abulkhanov", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:02:42.107Z", "hidden": true}, {"_id": "693bba8d9874a2a5e4ffb3b5", "user": {"_id": "658bc20cfdf2279d4721f218", "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg", "isPro": false, "fullname": "Mikhail Gashkov", "user": "MikeGashkov", "type": "user"}, "name": "Mikhail Gashkov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:32.809Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b6", "name": "Viktor Zelenkovskiy", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b7", "user": {"_id": "644f64bc17b6189cda54cae8", "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg", "isPro": false, "fullname": "Artem Batalov", "user": "batalovme", "type": "user"}, "name": "Artem Batalov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:27.497Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b8", "user": {"_id": "62609d224e6e4b84475eb8d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg", "isPro": false, "fullname": "Alex Medvedev", "user": "kenkaneki", "type": "user"}, "name": "Aleksandr Medvedev", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:36.035Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b9", "user": {"_id": "63f26358be95ed4c9a9b0583", "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg", "isPro": false, "fullname": "Anatoly Potapov", "user": "AnatoliiPotapov", "type": "user"}, "name": "Anatolii Potapov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:25.666Z", "hidden": false}], "publishedAt": "2025-12-11T08:40:10.000Z", "submittedOnDailyAt": "2025-12-12T08:33:32.798Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "submittedOnDailyBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "upvotes": 60, "discussionId": "693bba8e9874a2a5e4ffb3ba", "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.", "ai_keywords": ["Cyrillic-dense tokenizer", "EAGLE speculative-decoding pipeline", "hybrid reasoning", "efficient inference", "reasoning-trace generation"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86T-pro 2.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u6743\u91cd\u7684\u4fc4\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u7406\u3002</li>\n    <li>\u8be5\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\uff0c\u91c7\u7528\u4e86\u5bc6\u96c6\u7684\u897f\u91cc\u5c14\u6587\u6807\u8bb0\u5668\u548c\u8c03\u6574\u540e\u7684EAGLE\u89e3\u7801\u7ba1\u9053\uff0c\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u6211\u4eec\u5728Hugging Face\u4e0a\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001T-Wix 500k\u6307\u4ee4\u8bed\u6599\u5e93\u3001T-Math\u63a8\u7406\u57fa\u51c6\u548cEAGLE\u6743\u91cd\uff0c\u4fbf\u4e8e\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u7814\u7a76\u3002</li>\n    <li>\u516c\u5171\u7f51\u9875\u6f14\u793a\u5c55\u793a\u4e86\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u8bf4\u660e\u4e86\u6211\u4eec\u63a8\u7406\u7cfb\u7edf\u5728\u5404\u4e2a\u9886\u57df\u4e2d\u5b9e\u73b0\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n    <li>T-pro 2.0\u4e3a\u6784\u5efa\u548c\u8bc4\u4f30\u9ad8\u6548\u5b9e\u7528\u7684\u4fc4\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u5f00\u653e\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>T-pro 2.0 is a new open-source Russian language model designed for reasoning and quick responses.</li>\n    <li>The model can answer questions directly and generate reasoning steps, using specialized tools to speed up processing.</li>\n    <li>Researchers can access the model, a large instruction dataset, a math reasoning benchmark, and additional tools on Hugging Face for their studies.</li>\n    <li>A public demo shows how the model works in different modes and highlights its fast performance.</li>\n    <li>T-pro 2.0 aims to help users create and test effective Russian language model applications.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:40:10.000Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png", "numComments": 1, "submittedBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "fullname": "Olga Tsymboi", "name": "oltsy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10739", "authors": [{"_id": "693b91d89874a2a5e4ffb329", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32a", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:56.632Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32b", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32c", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32d", "user": {"_id": "64e8505321540e1da3226b54", "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg", "isPro": false, "fullname": "Wenwei Zhang", "user": "ZwwWayne", "type": "user"}, "name": "Wenwei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:21.987Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32e", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32f", "name": "Fan Zheng", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb330", "user": {"_id": "670f8df2005a358fdc6c2fb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qw2yocepl2vhC5T2ae49b.png", "isPro": false, "fullname": "tianyou", "user": "matianyou", "type": "user"}, "name": "Tianyou Ma", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:57.205Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb331", "user": {"_id": "687f853bb39262ba84f3eeff", "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg", "isPro": false, "fullname": "Junhao Shen", "user": "shenjunhao", "type": "user"}, "name": "Junhao Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:00.496Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb332", "name": "Haiteng Zhao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb333", "user": {"_id": "6454b1073aaeff9f3d330ef6", "avatarUrl": "/avatars/331fcbbf18a72b0dc8419ca3a77299bb.svg", "isPro": false, "fullname": "Duanyang Zhang", "user": "KKKDaniel", "type": "user"}, "name": "Duanyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:12.911Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb334", "name": "Huilun Zhang", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb335", "user": {"_id": "63fd691794cc8f815d50c112", "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg", "isPro": false, "fullname": "liu", "user": "Harold-lkk", "type": "user"}, "name": "Kuikun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:58.650Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb336", "name": "Chengqi Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb337", "name": "Yanhui Duan", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb338", "name": "Chiyu Chen", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb339", "name": "Ningsheng Ma", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33a", "name": "Jianfei Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33b", "name": "Han Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33c", "user": {"_id": "636317ed80c1a705a6eff396", "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg", "isPro": false, "fullname": "Dahua Lin", "user": "lindahua", "type": "user"}, "name": "Dahua Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:20.346Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33d", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:26:28.000Z", "submittedOnDailyAt": "2025-12-12T01:27:55.307Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "upvotes": 37, "discussionId": "693b91d99874a2a5e4ffb33e", "ai_summary": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "long reasoning chains of thought (CoTs)", "iterative active learning", "Rejection Fine-Tuning (RFT)", "F1 score", "accuracy", "DeepSeek-R1-Distill-Qwen-32B", "AIME2025"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u5f53\u524d\u7684\u7ed3\u679c\u57fa\u7840\u9a8c\u8bc1\u5668\uff08OVs\uff09\u65e0\u6cd5\u68c0\u67e5\u957f\u671f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\u3002</li>\n    <li>\u8fc7\u7a0b\u57fa\u7840\u9a8c\u8bc1\u5668\uff08PVs\uff09\u5728\u68c0\u6d4b\u590d\u6742\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u65f6\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u7684\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\u901a\u8fc7\u603b\u7ed3\u7ed3\u679c\u7684\u63a8\u7406\u8fc7\u7a0b\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u9a8c\u8bc1\uff0c\u5e76\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>OPV\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0cF1\u5f97\u5206\u8fbe\u523083.1\uff0c\u8d85\u8fc7\u4e86\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u80fd\u6709\u6548\u68c0\u6d4b\u5047\u9633\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are improving at complex reasoning tasks using a method called Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>Current verifiers struggle to check unreliable steps in long reasoning processes due to the lack of good quality human annotations.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) aims to verify reasoning processes efficiently and accurately by summarizing outcomes from long reasoning chains.</li>\n    <li>OPV uses a learning method that involves expert annotations to continually improve its verification abilities with fewer costs.</li>\n    <li>Experiments show OPV outperforms larger models and effectively detects errors, achieving a high F1 score and improving accuracy when used with other models.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:26:28.000Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10739.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10949", "authors": [{"_id": "693b895d9874a2a5e4ffb302", "user": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "name": "Yiwen Tang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:55.504Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb303", "user": {"_id": "642a8302d651bae3c11b72b1", "avatarUrl": "/avatars/4d2d422613e274d80482fed9a7d3f785.svg", "isPro": false, "fullname": "Zoey Guo", "user": "Purple1288", "type": "user"}, "name": "Zoey Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:01.705Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb304", "user": {"_id": "6708920aeae29d1cd41a703b", "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg", "isPro": false, "fullname": "kaixin zhu", "user": "czkk566", "type": "user"}, "name": "Kaixin Zhu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:10.275Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb305", "name": "Ray Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb306", "user": {"_id": "6535045a910b844786a6642f", "avatarUrl": "/avatars/37a94864a7a348151837b421ea6d77e3.svg", "isPro": false, "fullname": "Qizhi Chen", "user": "Tavish9", "type": "user"}, "name": "Qizhi Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:11.060Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb307", "user": {"_id": "6349214f8146350b3a4c5cdf", "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg", "isPro": false, "fullname": "Dongzhi Jiang", "user": "CaraJ", "type": "user"}, "name": "Dongzhi Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:06:02.910Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb308", "name": "Junli Liu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb309", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:08.733Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30a", "user": {"_id": "6662d2c9de4c4e1f04bd29c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QnnO_KOyZjFd-iXuPnHqR.png", "isPro": false, "fullname": "HaomingSong", "user": "HaomingSong", "type": "user"}, "name": "Haoming Song", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:30.062Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30b", "user": {"_id": "64daecec888b7e9c400f59b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png", "isPro": false, "fullname": "Delin Qu", "user": "delinqu", "type": "user"}, "name": "Delin Qu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:34.085Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30c", "name": "Tianyi Bai", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30d", "name": "Dan Xu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30e", "name": "Wentao Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30f", "name": "Bin Zhao", "hidden": false}], "publishedAt": "2025-12-11T18:59:52.000Z", "submittedOnDailyAt": "2025-12-12T00:49:43.850Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "submittedOnDailyBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "upvotes": 36, "discussionId": "693b895d9874a2a5e4ffb310", "githubRepo": "https://github.com/Ivan-Tang-3D/3DGen-R1", "githubRepoAddedBy": "user", "ai_summary": "This study investigates reinforcement learning for text-to-3D generation, focusing on reward designs, RL algorithms, benchmarking, and hierarchical optimization, introducing AR3D-R1 as the first RL-enhanced model for 3D generation.", "ai_keywords": ["reinforcement learning", "text-to-3D generation", "reward designs", "GRPO variants", "token-level optimization", "MME-3DR", "Hi-GRPO", "hierarchical 3D generation", "AR3D-R1"], "githubStars": 40, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u57282D\u56fe\u50cf\u751f\u6210\u4e2d\u5df2\u6210\u529f\u5e94\u7528\uff0c\u4f46\u57283D\u751f\u6210\u4e2d\u4ecd\u7136\u8f83\u5c11\u63a2\u7d22\u3002</li>\n    <li>3D\u5bf9\u8c61\u7684\u590d\u6742\u6027\u4f7f\u5f97\u5176\u751f\u6210\u5bf9\u5956\u52b1\u8bbe\u8ba1\u548cRL\u7b97\u6cd5\u975e\u5e38\u654f\u611f\u3002</li>\n    <li>\u6211\u4eec\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u8bc4\u4f30\u5956\u52b1\u8bbe\u8ba1\u3001\u7b97\u6cd5\u548c\u6570\u636e\u89c4\u6a21\u5bf9\u6587\u672c\u52303D\u751f\u6210\u7684\u5f71\u54cd\u3002</li>\n    <li>\u5f15\u5165MME-3DR\u57fa\u51c6\uff0c\u4ee5\u8861\u91cf3D\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1AR3D-R1\u6a21\u578b\uff0c\u4f18\u5316\u4ece\u7c97\u7565\u5f62\u72b6\u5230\u7eb9\u7406\u7ec6\u5316\u76843D\u751f\u6210\u8fc7\u7a0b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) has been successfully used for 2D image generation and is now being explored for 3D generation, which is more complex.</li>\n    <li>The study focuses on improving 3D generation using RL by examining reward designs that align with human preferences and using effective RL algorithms.</li>\n    <li>New benchmarks like MME-3DR are introduced to assess reasoning abilities in 3D generation models.</li>\n    <li>A new hierarchical approach called Hi-GRPO is proposed to improve the process of generating 3D objects from general shapes to detailed textures.</li>\n    <li>The result is AR3D-R1, the first RL-enhanced model for generating 3D objects from text, with the aim of advancing 3D generation techniques.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:59:52.000Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10949.png", "numComments": 2, "submittedBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "fullname": "Ivan Tang", "name": "IvanTang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10756", "authors": [{"_id": "693b91539874a2a5e4ffb318", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91539874a2a5e4ffb319", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31a", "name": "Wenwei Zhang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31b", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31c", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:02.946Z", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31d", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31e", "name": "Tianyou Ma", "hidden": false}, {"_id": "693b91539874a2a5e4ffb31f", "name": "Yuhong Liu", "hidden": false}, {"_id": "693b91539874a2a5e4ffb320", "name": "Zhi Wang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb321", "name": "Runyuan Ma", "hidden": false}, {"_id": "693b91539874a2a5e4ffb322", "name": "Guangyu Wang", "hidden": false}, {"_id": "693b91539874a2a5e4ffb323", "name": "Wei Li", "hidden": false}, {"_id": "693b91539874a2a5e4ffb324", "name": "Conghui He", "hidden": false}, {"_id": "693b91539874a2a5e4ffb325", "name": "Dahua Lin", "hidden": false}, {"_id": "693b91539874a2a5e4ffb326", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:47:38.000Z", "submittedOnDailyAt": "2025-12-12T01:23:10.732Z", "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "upvotes": 30, "discussionId": "693b91539874a2a5e4ffb327", "ai_summary": "The Outcome-based Process Verifier (OPV) improves the verification of complex reasoning chains in large language models by combining outcome-based and process-based verification with iterative active learning and Rejection Fine-Tuning, achieving state-of-the-art performance on various benchmarks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "verifiers", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "CoTs", "iterative active learning", "Rejection Fine-Tuning (RFT)", "OPV-Bench", "AIME2025", "DeepSeek-R1-Distill-Qwen-32B"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u5f97\u76ca\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u548c\u53ef\u9760\u9a8c\u8bc1\u8005\u7684\u76d1\u7763\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u7ed3\u679c\u7684\u9a8c\u8bc1\u5668\u65e0\u6cd5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\uff0c\u800c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u9a8c\u8bc1\u5668\u5728\u590d\u6742\u957f\u63a8\u7406\u94fe\u4e2d\u68c0\u6d4b\u9519\u8bef\u65f6\u9762\u4e34\u56f0\u96be\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\uff0c\u5b83\u80fd\u591f\u9a8c\u8bc1\u957f\u63a8\u7406\u94fe\u7684\u603b\u7ed3\u7ed3\u679c\uff0c\u4ece\u800c\u5b9e\u73b0\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u9a8c\u8bc1\u3002</li>\n    <li>\u4f7f\u7528\u4e00\u79cd\u8fed\u4ee3\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u4e0e\u4e13\u5bb6\u6807\u6ce8\uff0c\u9010\u6b65\u63d0\u9ad8OPV\u7684\u9a8c\u8bc1\u80fd\u529b\uff0c\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cOPV\u5728\u6027\u80fd\u548c\u5e94\u7528\u8303\u56f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5047\u9633\u6027\uff0c\u5e76\u5728\u4e0e\u7b56\u7565\u6a21\u578b\u5408\u4f5c\u65f6\u63d0\u9ad8\u51c6\u786e\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models have improved at solving complex tasks using a method called Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>Current verifiers struggle to check the reliability of the steps in long reasoning processes due to limitations in available data and high costs of human annotations.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) aims to improve verification by summarizing outcomes from long reasoning chains while making the process more efficient and scalable.</li>\n    <li>OPV uses an iterative learning approach to enhance its verification ability with fewer annotations, focusing on the most uncertain cases for training.</li>\n    <li>Experiments show that OPV outperforms larger models and effectively detects errors, leading to improved accuracy in various applications.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:47:38.000Z", "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10756.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10534", "authors": [{"_id": "693b94359874a2a5e4ffb340", "name": "Haiteng Zhao", "hidden": false}, {"_id": "693b94359874a2a5e4ffb341", "user": {"_id": "687f853bb39262ba84f3eeff", "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg", "isPro": false, "fullname": "Junhao Shen", "user": "shenjunhao", "type": "user"}, "name": "Junhao Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:51.355Z", "hidden": false}, {"_id": "693b94359874a2a5e4ffb342", "user": {"_id": "64ccaa4687ec96aa4752e754", "avatarUrl": "/avatars/d2dd2040a521de4f55c7335cb7771c75.svg", "isPro": false, "fullname": "Yiming Zhang", "user": "ymzhang319", "type": "user"}, "name": "Yiming Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:27.118Z", "hidden": false}, {"_id": "693b94359874a2a5e4ffb343", "name": "Songyang Gao", "hidden": false}, {"_id": "693b94359874a2a5e4ffb344", "user": {"_id": "63fd691794cc8f815d50c112", "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg", "isPro": false, "fullname": "liu", "user": "Harold-lkk", "type": "user"}, "name": "Kuikun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:54.577Z", "hidden": false}, {"_id": "693b94359874a2a5e4ffb345", "name": "Tianyou Ma", "hidden": false}, {"_id": "693b94359874a2a5e4ffb346", "name": "Fan Zheng", "hidden": false}, {"_id": "693b94359874a2a5e4ffb347", "name": "Dahua Lin", "hidden": false}, {"_id": "693b94359874a2a5e4ffb348", "name": "Wenwei Zhang", "hidden": false}, {"_id": "693b94359874a2a5e4ffb349", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T11:05:04.000Z", "submittedOnDailyAt": "2025-12-12T03:29:52.050Z", "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.", "upvotes": 25, "discussionId": "693b94359874a2a5e4ffb34a", "ai_summary": "InternGeometry, an LLM agent, surpasses human performance on IMO geometry problems using a heuristic-driven approach with iterative proposition verification and a dynamic memory mechanism, significantly outperforming AlphaGeometry 2 with limited training data.", "ai_keywords": ["LLM agents", "mathematical problem-solving", "International Mathematical Olympiad", "AI for geometry", "AlphaGeometry 2", "large-scale data synthesis", "search", "heuristic limitations", "propositions", "auxiliary constructions", "symbolic engine", "dynamic memory mechanism", "Complexity-Boosting Reinforcement Learning", "CBRL", "training examples", "InternThinker-32B", "expert-level geometry tasks"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u89e3\u51b3\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\uff08IMO\uff09\u7ea7\u522b\u7684\u95ee\u9898\u3002</li>\n    <li>\u867d\u7136AI\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u4ecd\u7136\u53d7\u9650\u4e8e\u4e13\u5bb6\u6a21\u578b\uff0c\u4f46\u6211\u4eec\u9996\u6b21\u5c1d\u8bd5\u6784\u5efa\u4e00\u4e2a\u51e0\u4f55\u9886\u57df\u7684LLM\u4ee3\u7406\u2014\u2014InternGeometry\u3002</li>\n    <li>InternGeometry\u901a\u8fc7\u53cd\u590d\u63d0\u51fa\u547d\u9898\u548c\u8f85\u52a9\u6784\u9020\uff0c\u5e76\u5229\u7528\u7b26\u53f7\u5f15\u64ce\u9a8c\u8bc1\u6765\u514b\u670d\u51e0\u4f55\u4e2d\u7684\u542f\u53d1\u5f0f\u9650\u5236\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u89e3\u51b32000-2024\u5e74\u95f4\u7684IMO\u51e0\u4f55\u95ee\u9898\u4e2d\uff0c\u6210\u529f\u89e3\u51b3\u4e8644\u4e2a\uff0c\u8d85\u51fa\u4e86\u5e73\u5747\u91d1\u724c\u5f97\u5206\u3002</li>\n    <li>\u6211\u4eec\u5c06\u53d1\u5e03\u8be5\u6a21\u578b\u3001\u6570\u636e\u548c\u7b26\u53f7\u5f15\u64ce\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>InternGeometry is a new AI model designed to solve geometry problems at a high level, similar to medalists in competitions.</li>\n    <li>It works by suggesting ideas, checking them with a symbolic engine, and using feedback to improve its next suggestions.</li>\n    <li>The model engages in over 200 interactions with the symbolic engine for each problem to refine its solutions.</li>\n    <li>It uses a special training method called Complexity-Boosting Reinforcement Learning to gradually increase problem difficulty.</li>\n    <li>InternGeometry successfully solved 44 out of 50 geometry problems from the International Mathematical Olympiad, showing its effectiveness with very little training data compared to other models.</li>\n</ul>"}, "publishedAt": "2025-12-11T06:05:04.000Z", "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning", "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10534.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2512.10881", "authors": [{"_id": "693b87039874a2a5e4ffb2f5", "user": {"_id": "6412d08e027aea38bc90c802", "avatarUrl": "/avatars/86e3fa33193305af591d7d3cc79feb5c.svg", "isPro": false, "fullname": "Gongkehong", "user": "kehong", "type": "user"}, "name": "Kehong Gong", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:11.844Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f6", "user": {"_id": "62f4b9edd6d189cbd25fc7f8", "avatarUrl": "/avatars/12428e2ca213bf64c86dc7ed26b4dbcc.svg", "isPro": false, "fullname": "Zhengyu Wen", "user": "wzy27", "type": "user"}, "name": "Zhengyu Wen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:21.767Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f7", "user": {"_id": "68faf7fcc8107da320682a57", "avatarUrl": "/avatars/324fb883e23acc65ce8a14af87a37c65.svg", "isPro": false, "fullname": "He Weixia", "user": "weixia111111", "type": "user"}, "name": "Weixia He", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:32.488Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f8", "name": "Mingxi Xu", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2f9", "name": "Qi Wang", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fa", "name": "Ning Zhang", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fb", "name": "Zhengyu Li", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fc", "user": {"_id": "64ac10d2c08741fc97bda2e9", "avatarUrl": "/avatars/ffe028f28e2284a44b0a0617f4abe387.svg", "isPro": false, "fullname": "Dongze Lian", "user": "DonaldLian", "type": "user"}, "name": "Dongze Lian", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:07:42.284Z", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fd", "name": "Wei Zhao", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2fe", "name": "Xiaoyu He", "hidden": false}, {"_id": "693b87039874a2a5e4ffb2ff", "name": "Mingyuan Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xNfWwMFlhegi3RqwZxqPG.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xdSriPIZ-k8QLE8m0A8c0.qt"], "publishedAt": "2025-12-11T18:09:48.000Z", "submittedOnDailyAt": "2025-12-12T00:41:38.576Z", "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/", "upvotes": 20, "discussionId": "693b87049874a2a5e4ffb300", "ai_summary": "MoCapAnything is a reference-guided framework that reconstructs rotation-based animations from monocular video for arbitrary rigged 3D assets, enabling cross-species retargeting and scalable 3D motion capture.", "ai_keywords": ["Category-Agnostic Motion Capture", "CAMoCap", "MoCapAnything", "Reference Prompt Encoder", "Video Feature Extractor", "Unified Motion Decoder", "3D joint trajectories", "inverse kinematics", "BVH", "Truebones Zoo", "skeletal animations", "cross-species retargeting"], "summary_zh": "<ul>\n    <li>\u8fd0\u52a8\u6355\u6349\u6280\u672f\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\uff0c\u4f46\u73b0\u6709\u6d41\u7a0b\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u7269\u79cd\u6216\u6a21\u677f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u65e0\u7c7b\u522b\u8fd0\u52a8\u6355\u6349\uff08CAMoCap\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u5355\u76ee\u89c6\u9891\u548c\u4efb\u610f3D\u6a21\u578b\u751f\u6210\u52a8\u753b\u3002</li>\n    <li>MoCapAnything\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b3D\u5173\u8282\u8f68\u8ff9\u5e76\u4f7f\u7528\u9006\u8fd0\u52a8\u5b66\u6062\u590d\u7279\u5b9a\u8d44\u4ea7\u7684\u65cb\u8f6c\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u53ef\u5b66\u4e60\u6a21\u5757\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u9006\u8fd0\u52a8\u5b66\u9636\u6bb5\uff0c\u80fd\u591f\u63d0\u53d6\u548c\u878d\u5408\u4e0d\u540c\u6570\u636e\u4ee5\u751f\u6210\u52a8\u753b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMoCapAnything\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u9aa8\u9abc\u52a8\u753b\uff0c\u5e76\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u6709\u6548\u8f6c\u6362\u8fd0\u52a8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Motion capture is used for creating content beyond just digital humans, but existing methods are often limited to specific types of characters.</li>\n    <li>The new approach, called Category-Agnostic Motion Capture (CAMoCap), aims to create animations from any 3D asset using a single video input.</li>\n    <li>MoCapAnything is a framework that predicts 3D joint movements and converts them into animations specific to the chosen asset.</li>\n    <li>The system includes three main components: a Reference Prompt Encoder, a Video Feature Extractor, and a Unified Motion Decoder.</li>\n    <li>Tests show that MoCapAnything produces high-quality animations and can adapt movements across different character types, making motion capture more flexible.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:09:48.000Z", "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos", "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xNfWwMFlhegi3RqwZxqPG.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xdSriPIZ-k8QLE8m0A8c0.qt"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10881.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 182}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05439", "authors": [{"_id": "693c5619f516c69324680c76", "name": "Tarun Suresh", "hidden": false}, {"_id": "693c5619f516c69324680c77", "name": "Nalin Wadhwa", "hidden": false}, {"_id": "693c5619f516c69324680c78", "name": "Debangshu Banerjee", "hidden": false}, {"_id": "693c5619f516c69324680c79", "name": "Gagandeep Singh", "hidden": false}], "publishedAt": "2025-12-05T05:34:06.000Z", "submittedOnDailyAt": "2025-12-12T15:32:48.549Z", "title": "BEAVER: An Efficient Deterministic LLM Verifier", "submittedOnDailyBy": {"_id": "65e7bb35e5e78134ab049942", "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg", "isPro": false, "fullname": "Tarun Suresh", "user": "tarsur909", "type": "user"}, "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.", "upvotes": 13, "discussionId": "693c561af516c69324680c7a", "ai_summary": "BEAVER is a framework that provides deterministic and sound probability bounds for verifying constraints in large language models, achieving tighter bounds and identifying more high-risk instances than baseline methods.", "ai_keywords": ["large language models", "LLMs", "BEAVER", "prefix-closed semantic constraint", "token trie", "frontier data structures", "sound probability bounds", "correctness verification", "privacy verification", "secure code generation", "high-risk instances"], "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>BEAVER\u662f\u7b2c\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6ee1\u8db3\u7ea6\u675f\u7684\u786e\u5b9a\u6027\u6982\u7387\u754c\u9650\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u751f\u6210\u7a7a\u95f4\uff0c\u4fdd\u6301\u6bcf\u6b21\u8fed\u4ee3\u90fd\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u754c\u9650\u3002</li>\n    <li>BEAVER\u5728\u6b63\u786e\u6027\u9a8c\u8bc1\u3001\u9690\u79c1\u9a8c\u8bc1\u548c\u5b89\u5168\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cBEAVER\u7684\u6982\u7387\u754c\u9650\u66f4\u7d27\uff0c\u53d1\u73b0\u7684\u9ad8\u98ce\u9669\u5b9e\u4f8b\u66f4\u591a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4e3a\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6bd4\u677e\u6563\u754c\u9650\u6216\u7ecf\u9a8c\u8bc4\u4f30\u66f4\u7cbe\u786e\u7684\u7279\u5f81\u63cf\u8ff0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>BEAVER is a new framework that helps check if large language models (LLMs) meet required constraints.</li>\n    <li>It provides reliable, deterministic probability bounds instead of just estimates, ensuring sound results.</li>\n    <li>BEAVER systematically explores how models generate outputs using advanced data structures.</li>\n    <li>It has been tested on tasks like correctness verification, privacy checking, and secure code generation.</li>\n    <li>BEAVER shows 6 to 8 times tighter probability bounds and identifies more high-risk instances than existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-05T00:34:06.000Z", "title": "BEAVER: An Efficient Deterministic LLM Verifier", "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05439.png", "numComments": 1, "submittedBy": {"_id": "65e7bb35e5e78134ab049942", "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg", "fullname": "Tarun Suresh", "name": "tarsur909", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.10867", "authors": [{"_id": "693bb6089874a2a5e4ffb39e", "user": {"_id": "670f86e4d75f114352916a35", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g3zZ6TTbgV-Xu789lPaYN.png", "isPro": false, "fullname": "Li", "user": "zongzhao", "type": "user"}, "name": "Zongzhao Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:42.257Z", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb39f", "name": "Xiangzhe Kong", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a0", "name": "Jiahui Su", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a1", "name": "Zongyang Ma", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a2", "name": "Mingze Li", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a3", "name": "Songyou Li", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a4", "name": "Yuelin Zhang", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a5", "name": "Yu Rong", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a6", "name": "Tingyang Xu", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a7", "name": "Deli Zhao", "hidden": false}, {"_id": "693bb6089874a2a5e4ffb3a8", "name": "Wenbing Huang", "hidden": false}], "publishedAt": "2025-12-11T18:00:21.000Z", "submittedOnDailyAt": "2025-12-12T04:05:11.878Z", "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models", "submittedOnDailyBy": {"_id": "61bb00f6c4ac95d207b25f1b", "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg", "isPro": false, "fullname": "Zongyang Ma", "user": "zyma", "type": "user"}, "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.", "upvotes": 11, "discussionId": "693bb6099874a2a5e4ffb3a9", "ai_summary": "A benchmark framework evaluates Vision-Language Models in understanding microscopic spatial relationships, showing potential but highlighting the need for domain-specific knowledge integration.", "ai_keywords": ["Vision-Language Models", "VLMs", "Microscopic Spatial Intelligence", "MiSI", "MiSI-Bench", "spatial transformations", "relational identifications", "hydrogen bond recognition", "scientific AGI"], "organization": {"_id": "622177ac43826d6f261f8208", "name": "RUC", "fullname": "Renmin University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86\u5fae\u89c2\u7a7a\u95f4\u667a\u80fd\uff08MiSI\uff09\u7684\u6982\u5ff5\uff0c\u5373\u611f\u77e5\u548c\u63a8\u7406\u4e0d\u53ef\u89c1\u7684\u5fae\u89c2\u5b9e\u4f53\u7684\u7a7a\u95f4\u5173\u7cfb\u7684\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u57fa\u51c6\u6846\u67b6MiSI-Bench\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5305\u542b\u8d85\u8fc7163,000\u4e2a\u95ee\u7b54\u5bf9\u548c587,000\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6\u7ea64,000\u79cd\u5206\u5b50\u7ed3\u6784\uff0c\u8bc4\u4f30\u4e5d\u4e2a\u4e92\u8865\u4efb\u52a1\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u524d\u7684VLMs\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u7ecf\u8fc7\u5fae\u8c03\u76847B\u6a21\u578b\u5728\u7a7a\u95f4\u53d8\u6362\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002</li>\n    <li>\u5728\u79d1\u5b66\u4efb\u52a1\uff08\u5982\u6c22\u952e\u8bc6\u522b\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u6574\u5408\u660e\u786e\u9886\u57df\u77e5\u8bc6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63a8\u52a8\u79d1\u5b66\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces Microscopic Spatial Intelligence (MiSI), which helps understand and reason about tiny, invisible entities important for science.</li>\n    <li>A new benchmark called MiSI-Bench is created to evaluate Vision-Language Models (VLMs) with over 163,000 questions and 587,000 images related to molecular structures.</li>\n    <li>The benchmark tests various skills, from simple spatial tasks to complex relational tasks.</li>\n    <li>Current top VLMs do not perform as well as humans on this benchmark, but a specially fine-tuned 7B model shows promise in spatial transformation tasks.</li>\n    <li>However, the model struggles with tasks needing scientific knowledge, highlighting the need for better integration of domain knowledge for advancements in scientific AI.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:00:21.000Z", "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models", "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10867.png", "numComments": 1, "submittedBy": {"_id": "61bb00f6c4ac95d207b25f1b", "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg", "fullname": "Zongyang Ma", "name": "zyma", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "622177ac43826d6f261f8208", "name": "RUC", "fullname": "Renmin University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.23386", "authors": [{"_id": "693ba6879874a2a5e4ffb362", "name": "Sinan Du", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb363", "name": "Jiahao Guo", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb364", "user": {"_id": "68fed13597cae9c0f484ebed", "avatarUrl": "/avatars/8df380d81daab4fae8a52d0816bf8099.svg", "isPro": false, "fullname": "libo", "user": "libo31", "type": "user"}, "name": "Bo Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:49.405Z", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb365", "name": "Shuhao Cui", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb366", "name": "Zhengzhuo Xu", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb367", "name": "Yifu Luo", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb368", "name": "Yongxian Wei", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb369", "name": "Kun Gai", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb36a", "name": "Xinggang Wang", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb36b", "user": {"_id": "68e741ea3edb0ff47e20084e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg", "isPro": false, "fullname": "Wu Kai", "user": "KaiiWuu1993", "type": "user"}, "name": "Kai Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:47.442Z", "hidden": false}, {"_id": "693ba6879874a2a5e4ffb36c", "name": "Chun Yuan", "hidden": false}], "publishedAt": "2025-11-28T17:26:34.000Z", "submittedOnDailyAt": "2025-12-12T02:54:20.848Z", "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction", "submittedOnDailyBy": {"_id": "68e741ea3edb0ff47e20084e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg", "isPro": false, "fullname": "Wu Kai", "user": "KaiiWuu1993", "type": "user"}, "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.", "upvotes": 10, "discussionId": "693ba6879874a2a5e4ffb36d", "ai_summary": "VQRAE, a Vector Quantization Representation AutoEncoder, unifies multimodal understanding, generation, and reconstruction using a unified tokenizer with continuous semantic features and discrete tokens.", "ai_keywords": ["multimodal understanding", "generation", "reconstruction", "tokenizer", "dual encoder paradigm", "VQRAE", "Representation AutoEncoders", "Vector Quantization", "symmetric ViT decoder", "two-stage training strategy", "encoder", "self-distillation constraints", "semantic VQ codebook", "pixel reconstruction objective", "multimodal understanding", "discrete tokens", "fine-grained reconstruction", "autoregressive paradigm"], "organization": {"_id": "665f02ce9f9e5b38d0a256a8", "name": "Kwai-Kolors", "fullname": "Kolors Team, Kuaishou Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5VQRAE\uff0c\u65e8\u5728\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u3001\u751f\u6210\u548c\u91cd\u5efa\u8868\u793a\u3002</li>\n    <li>VQRAE\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u7684\u81ea\u7f16\u7801\u5668\uff0c\u80fd\u591f\u540c\u65f6\u4ea7\u751f\u8fde\u7eed\u7684\u8bed\u4e49\u7279\u5f81\u548c\u79bb\u6563\u7684\u751f\u6210\u6807\u8bb0\u3002</li>\n    <li>\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u51bb\u7ed3\u7f16\u7801\u5668\u5b66\u4e60\u9ad8\u7ef4\u8bed\u4e49\u4ee3\u7801\u672c\uff0c\u518d\u4f18\u5316\u7f16\u7801\u5668\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u591a\u6a21\u6001\u7406\u89e3\u6240\u9700\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u540c\u65f6\u517c\u5bb9\u751f\u6210\u548c\u7ec6\u7c92\u5ea6\u91cd\u5efa\u3002</li>\n    <li>\u5728\u591a\u4e2a\u89c6\u89c9\u7406\u89e3\u3001\u751f\u6210\u548c\u91cd\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVQRAE\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces VQRAE, a new model for handling different types of data (multimodal understanding, generation, and reconstruction) using a single tokenizer.</li>\n    <li>VQRAE uses a special technique called Vector Quantization to create a unified representation that provides both continuous semantic features and discrete tokens.</li>\n    <li>The model is built on existing vision models and uses a two-step training process to improve performance in understanding images and generating visual content.</li>\n    <li>It achieves effective use of a high-dimensional semantic codebook, which is different from the usual low-dimensional approach, enabling better image reconstruction.</li>\n    <li>VQRAE shows strong results in various tests related to visual understanding, generation, and reconstruction, making it a promising solution for these tasks.</li>\n</ul>"}, "publishedAt": "2025-11-28T12:26:34.000Z", "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction", "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.23386.png", "numComments": 1, "submittedBy": {"_id": "68e741ea3edb0ff47e20084e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68e741ea3edb0ff47e20084e/OyBgFqcU4QWyPF_K58Gt5.jpeg", "fullname": "Wu Kai", "name": "KaiiWuu1993", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "665f02ce9f9e5b38d0a256a8", "name": "Kwai-Kolors", "fullname": "Kolors Team, Kuaishou Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62f0babaef9cc6810cec02ff/sVnELkcfVo5kxg5308rkr.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10675", "authors": [{"_id": "693b83939874a2a5e4ffb2a4", "name": "Gemini Robotics Team", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a5", "name": "Coline Devin", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a6", "user": {"_id": "63c9bd445fdc575773c732fe", "avatarUrl": "/avatars/def472d1ab3fbf751225357c0932ae7e.svg", "isPro": false, "fullname": "Yilun Du", "user": "yilundu", "type": "user"}, "name": "Yilun Du", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:15.439Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a7", "name": "Debidatta Dwibedi", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a8", "name": "Ruiqi Gao", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2a9", "name": "Abhishek Jindal", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2aa", "user": {"_id": "6413572b6cd62eb3ba2024e9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6413572b6cd62eb3ba2024e9/UCtMdeV8_N6w-SBnTDXBy.jpeg", "isPro": false, "fullname": "Thomas Kipf", "user": "tkipf", "type": "user"}, "name": "Thomas Kipf", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:33.451Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ab", "user": {"_id": "648c85f33ffc11989bddcce1", "avatarUrl": "/avatars/bdde73b95a36e32cb2975656cca46022.svg", "isPro": false, "fullname": "Sean Kirmani", "user": "skirmani", "type": "user"}, "name": "Sean Kirmani", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:39.448Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ac", "user": {"_id": "63609ef63605bd411c1c7509", "avatarUrl": "/avatars/3fadc3c4117cdf17d25f3efc545388cd.svg", "isPro": false, "fullname": "Fangchen Liu", "user": "fangchenliu", "type": "user"}, "name": "Fangchen Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:44.842Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ad", "name": "Anirudha Majumdar", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2ae", "user": {"_id": "60ac2ec408805574fcf076f4", "avatarUrl": "/avatars/a8cd60f39ba89939c5841252dc9d742b.svg", "isPro": false, "fullname": "Andrew Marmon", "user": "acoadmarmon", "type": "user"}, "name": "Andrew Marmon", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:54.023Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2af", "user": {"_id": "65b7c3e87817e067a5b66cf1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/KysLXVoFmkPM2pQMgDlsb.jpeg", "isPro": false, "fullname": "Carolina Parada", "user": "CarolinaParada", "type": "user"}, "name": "Carolina Parada", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:14:59.800Z", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b0", "name": "Yulia Rubanova", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b1", "name": "Dhruv Shah", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b2", "name": "Vikas Sindhwani", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b3", "name": "Jie Tan", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b4", "name": "Fei Xia", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b5", "name": "Ted Xiao", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b6", "name": "Sherry Yang", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b7", "name": "Wenhao Yu", "hidden": false}, {"_id": "693b83939874a2a5e4ffb2b8", "name": "Allan Zhou", "hidden": false}], "publishedAt": "2025-12-11T14:22:14.000Z", "submittedOnDailyAt": "2025-12-12T00:23:17.104Z", "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.", "upvotes": 8, "discussionId": "693b83939874a2a5e4ffb2b9", "ai_summary": "A generative evaluation system using a frontier video model (Veo) enables comprehensive policy evaluation in robotics, including nominal performance, out-of-distribution generalization, and safety checks.", "ai_keywords": ["generative world models", "video models", "policy evaluation", "out-of-distribution (OOD) generalization", "generative evaluation system", "frontier video foundation model", "Veo", "robot action conditioning", "multi-view consistency", "generative image-editing", "multi-view completion", "bimanual manipulator"], "organization": {"_id": "60f6cbb2852126bac698c89e", "name": "deepmind", "fullname": "Deepmind", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"}, "summary_zh": "<ul>\n    <li>\u751f\u6210\u6027\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u6a21\u62df\u673a\u5668\u4eba\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u548c\u8fd0\u52a8\u4ea4\u4e92\u3002</li>\n    <li>\u89c6\u9891\u6a21\u578b\u80fd\u751f\u6210\u771f\u5b9e\u7684\u89c2\u5bdf\u548c\u73af\u5883\u4ea4\u4e92\uff0c\u4f46\u76ee\u524d\u4e3b\u8981\u7528\u4e8e\u76f8\u4f3c\u573a\u666f\u7684\u8bc4\u4f30\u3002</li>\n    <li>\u672c\u62a5\u544a\u5c55\u793a\u89c6\u9891\u6a21\u578b\u53ef\u7528\u4e8e\u673a\u5668\u4eba\u653f\u7b56\u8bc4\u4f30\u7684\u6240\u6709\u573a\u666f\uff0c\u5305\u62ec\u6b63\u5e38\u6027\u80fd\u548c\u8d85\u51fa\u5206\u5e03\u7684\u6cdb\u5316\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u6cbf\u89c6\u9891\u6a21\u578b\uff08Veo\uff09\u7684\u751f\u6210\u8bc4\u4f30\u7cfb\u7edf\uff0c\u652f\u6301\u673a\u5668\u4eba\u52a8\u4f5c\u6761\u4ef6\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002</li>\n    <li>\u901a\u8fc7\u5bf91600\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u7684\u9a8c\u8bc1\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u4e0d\u540c\u653f\u7b56\u7684\u76f8\u5bf9\u8868\u73b0\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Generative world models can help simulate how robots interact with their environments.</li>\n    <li>Video models can create realistic scenarios for testing robots, but have mostly been used in familiar situations.</li>\n    <li>This report shows how video models can evaluate robot performance in various situations, including new and unusual ones.</li>\n    <li>A new system called Veo was developed to improve robot testing by generating realistic scene variations.</li>\n    <li>The system has been tested on over 1600 real-world evaluations with various robot tasks, showing its effectiveness in predicting robot performance.</li>\n</ul>"}, "publishedAt": "2025-12-11T09:22:14.000Z", "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator", "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10675.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 182}, "organization": {"_id": "60f6cbb2852126bac698c89e", "name": "deepmind", "fullname": "Deepmind", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u5b83\u89e3\u51b3\u4e86\u73b0\u6709\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u7cbe\u5ea6\u4f4e\u548c\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0cWan-Move\u53ef\u4ee5\u7cbe\u7ec6\u63a7\u5236\u573a\u666f\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4e0e\u73b0\u6709\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u517c\u5bb9\uff0c\u6613\u4e8e\u6269\u5c55\uff0c\u4e0d\u9700\u8981\u989d\u5916\u7684\u8fd0\u52a8\u7f16\u7801\u5668\u3002</li>\n    <li>Wan-Move\u751f\u6210\u7684480p\u89c6\u9891\u5728\u8fd0\u52a8\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5546\u4e1a\u8f6f\u4ef6Kling 1.5 Pro\u7684Motion Brush\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6MoveBench\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality motion control, addressing issues of existing methods that lack detail and scalability.</li>\n    <li>The framework uses dense point trajectories to represent object motions, leading to better control over scene movements.</li>\n    <li>Wan-Move can easily integrate into existing image-to-video models without changing their architecture.</li>\n    <li>It has been tested and shown to produce high-quality 5-second videos, outperforming other tools in user studies.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.08478", "authors": [{"_id": "6938e00fdfc35938ba129f4f", "name": "Yuning Gong", "hidden": false}, {"_id": "6938e00fdfc35938ba129f50", "name": "Yifei Liu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f51", "name": "Yifan Zhan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f52", "name": "Muyao Niu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f53", "name": "Xueying Li", "hidden": false}, {"_id": "6938e00fdfc35938ba129f54", "name": "Yuanjun Liao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f55", "name": "Jiaming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f56", "name": "Yuanyuan Gao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f57", "name": "Jiaqi Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f58", "name": "Minming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f59", "name": "Li Zhou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5a", "name": "Yuning Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5b", "name": "Wei Wang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5c", "name": "Xiaoqing Hou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5d", "name": "Huaxi Huang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5e", "name": "Shixiang Tang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5f", "name": "Le Ma", "hidden": false}, {"_id": "6938e00fdfc35938ba129f60", "name": "Dingwen Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f61", "name": "Xue Yang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f62", "name": "Junchi Yan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f63", "name": "Yanchi Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f64", "name": "Yinqiang Zheng", "hidden": false}, {"_id": "6938e00fdfc35938ba129f65", "name": "Xiao Sun", "hidden": false}, {"_id": "6938e00fdfc35938ba129f66", "user": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "name": "Zhihang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:56:28.162Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "publishedAt": "2025-12-09T10:54:58.000Z", "submittedOnDailyAt": "2025-12-10T07:43:37.566Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "submittedOnDailyBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "upvotes": 64, "discussionId": "6938e00fdfc35938ba129f67", "projectPage": "https://visionary-laboratory.github.io/visionary/", "githubRepo": "https://github.com/Visionary-Laboratory/visionary", "githubRepoAddedBy": "user", "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.", "ai_keywords": ["Neural rendering", "3D Gaussian Splatting", "3DGS", "WebGPU", "ONNX inference", "Gaussian Generator contract", "three.js", "TypeScript API", "MLP-based 3DGS", "4DGS", "neural avatars", "style transformation", "GPU-based primitive sorting", "World Model Carrier"], "githubStars": 162, "summary_zh": "<ul>\n    <li>Visionary\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u3001\u57fa\u4e8e\u7f51\u9875\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u5b9e\u65f6\u6e32\u67d3\u5404\u79cd\u9ad8\u65af\u70b9\u548c\u7f51\u683c\u3002</li>\n    <li>\u5b83\u4f7f\u7528\u9ad8\u6548\u7684WebGPU\u6e32\u67d3\u5668\uff0c\u652f\u6301\u52a8\u6001\u795e\u7ecf\u5904\u7406\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u6d4f\u89c8\u5668\u4f53\u9a8c\u3002</li>\n    <li>Visionary\u5f15\u5165\u4e86\u6807\u51c6\u5316\u7684\u9ad8\u65af\u751f\u6210\u5668\u534f\u8bae\uff0c\u5141\u8bb8\u7b97\u6cd5\u5728\u6bcf\u4e00\u5e27\u751f\u6210\u6216\u66f4\u65b0\u9ad8\u65af\u6570\u636e\u3002</li>\n    <li>\u8be5\u5e73\u53f0\u652f\u6301\u591a\u79cd3D\u9ad8\u65af\u6e32\u67d3\u53d8\u4f53\uff0c\u5e76\u63d0\u4f9b\u7b80\u6d01\u7684TypeScript API\uff0c\u65b9\u4fbf\u4e0e\u73b0\u6709\u7f51\u9875\u5e94\u7528\u96c6\u6210\u3002</li>\n    <li>\u4e0e\u5f53\u524d\u7684Web\u67e5\u770b\u5668\u76f8\u6bd4\uff0cVisionary\u5728\u6e32\u67d3\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u964d\u4f4e\u4e863DGS\u65b9\u6cd5\u7684\u4f7f\u7528\u95e8\u69db\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visionary is a new platform designed for easy and real-time rendering of 3D Gaussian Splatting and meshes.</li>\n    <li>It uses a WebGPU renderer for fast processing and can run directly in web browsers without heavy installations.</li>\n    <li>The platform allows users to generate and update 3D visuals dynamically with a simple plug-and-play approach.</li>\n    <li>Visionary integrates well with existing web applications through a TypeScript API and offers better rendering efficiency than current solutions.</li>\n    <li>It supports various advanced techniques, making it a versatile tool for both creating and analyzing 3D models.</li>\n</ul>"}, "publishedAt": "2025-12-09T05:54:58.000Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08478.png", "numComments": 3, "submittedBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "fullname": "Zhihang Zhong", "name": "Zuica96", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10430", "authors": [{"_id": "693bba8d9874a2a5e4ffb3ab", "user": {"_id": "64f4c8739ee58d48e8507e0e", "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg", "isPro": false, "fullname": "Dmitrii Stoianov", "user": "heylimon", "type": "user"}, "name": "Dmitrii Stoianov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:40.198Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ac", "user": {"_id": "64fb054ebb362cbf2fe53159", "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg", "isPro": false, "fullname": "Danil Taranets", "user": "taranetsdan", "type": "user"}, "name": "Danil Taranets", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:29.281Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ad", "user": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "name": "Olga Tsymboi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:38.180Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ae", "user": {"_id": "6780dcd6acf8d824c03864da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png", "isPro": false, "fullname": "Ramil Latypov", "user": "kylecr4ne", "type": "user"}, "name": "Ramil Latypov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:23.437Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3af", "user": {"_id": "6513f03e86d74f32ed65e3b8", "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg", "isPro": false, "fullname": "Almaz Dautov", "user": "the-hir0", "type": "user"}, "name": "Almaz Dautov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:30.990Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b0", "user": {"_id": "621a8daf325b927e60fcef08", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg", "isPro": false, "fullname": "Vladislav Kruglikov", "user": "vladislavkruglikov", "type": "user"}, "name": "Vladislav Kruglikov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:21.170Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b1", "name": "Nikita Surkov", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b2", "user": {"_id": "63188c428d698d8c1642a0d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg", "isPro": false, "fullname": "German Abramov", "user": "germanjke", "type": "user"}, "name": "German Abramov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:28.579Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b3", "name": "Pavel Gein", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b4", "user": {"_id": "636a9a07e3ad78bc68b1a5a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg", "isPro": false, "fullname": "Dmitry Abulkhanov", "user": "mponty", "type": "user"}, "name": "Dmitry Abulkhanov", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:02:42.107Z", "hidden": true}, {"_id": "693bba8d9874a2a5e4ffb3b5", "user": {"_id": "658bc20cfdf2279d4721f218", "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg", "isPro": false, "fullname": "Mikhail Gashkov", "user": "MikeGashkov", "type": "user"}, "name": "Mikhail Gashkov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:32.809Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b6", "name": "Viktor Zelenkovskiy", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b7", "user": {"_id": "644f64bc17b6189cda54cae8", "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg", "isPro": false, "fullname": "Artem Batalov", "user": "batalovme", "type": "user"}, "name": "Artem Batalov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:27.497Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b8", "user": {"_id": "62609d224e6e4b84475eb8d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg", "isPro": false, "fullname": "Alex Medvedev", "user": "kenkaneki", "type": "user"}, "name": "Aleksandr Medvedev", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:36.035Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b9", "user": {"_id": "63f26358be95ed4c9a9b0583", "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg", "isPro": false, "fullname": "Anatoly Potapov", "user": "AnatoliiPotapov", "type": "user"}, "name": "Anatolii Potapov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:25.666Z", "hidden": false}], "publishedAt": "2025-12-11T08:40:10.000Z", "submittedOnDailyAt": "2025-12-12T08:33:32.798Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "submittedOnDailyBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "upvotes": 60, "discussionId": "693bba8e9874a2a5e4ffb3ba", "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.", "ai_keywords": ["Cyrillic-dense tokenizer", "EAGLE speculative-decoding pipeline", "hybrid reasoning", "efficient inference", "reasoning-trace generation"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86T-pro 2.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u6743\u91cd\u7684\u4fc4\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u7406\u3002</li>\n    <li>\u8be5\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\uff0c\u91c7\u7528\u4e86\u5bc6\u96c6\u7684\u897f\u91cc\u5c14\u6587\u6807\u8bb0\u5668\u548c\u8c03\u6574\u540e\u7684EAGLE\u89e3\u7801\u7ba1\u9053\uff0c\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u6211\u4eec\u5728Hugging Face\u4e0a\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001T-Wix 500k\u6307\u4ee4\u8bed\u6599\u5e93\u3001T-Math\u63a8\u7406\u57fa\u51c6\u548cEAGLE\u6743\u91cd\uff0c\u4fbf\u4e8e\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u7814\u7a76\u3002</li>\n    <li>\u516c\u5171\u7f51\u9875\u6f14\u793a\u5c55\u793a\u4e86\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u8bf4\u660e\u4e86\u6211\u4eec\u63a8\u7406\u7cfb\u7edf\u5728\u5404\u4e2a\u9886\u57df\u4e2d\u5b9e\u73b0\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n    <li>T-pro 2.0\u4e3a\u6784\u5efa\u548c\u8bc4\u4f30\u9ad8\u6548\u5b9e\u7528\u7684\u4fc4\u8bed\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u5f00\u653e\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>T-pro 2.0 is a new open-source Russian language model designed for reasoning and quick responses.</li>\n    <li>The model can answer questions directly and generate reasoning steps, using specialized tools to speed up processing.</li>\n    <li>Researchers can access the model, a large instruction dataset, a math reasoning benchmark, and additional tools on Hugging Face for their studies.</li>\n    <li>A public demo shows how the model works in different modes and highlights its fast performance.</li>\n    <li>T-pro 2.0 aims to help users create and test effective Russian language model applications.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:40:10.000Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png", "numComments": 1, "submittedBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "fullname": "Olga Tsymboi", "name": "oltsy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07461", "authors": [{"_id": "6937b96219d912300c34a398", "user": {"_id": "626b889ff451470f861d8c78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651214465695-noauth.jpeg", "isPro": false, "fullname": "victor wu", "user": "victor-wu", "type": "user"}, "name": "Tong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:22.731Z", "hidden": false}, {"_id": "6937b96219d912300c34a399", "user": {"_id": "6191cc9e6d34e827404cebab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg", "isPro": false, "fullname": "Yang", "user": "jacklanda", "type": "user"}, "name": "Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:20.278Z", "hidden": false}, {"_id": "6937b96219d912300c34a39a", "user": {"_id": "624505fcd083d28d314de3dd", "avatarUrl": "/avatars/92cf6b6a1d81d7958dbbd21f0bf63f8f.svg", "isPro": false, "fullname": "bai jun", "user": "ba1jun", "type": "user"}, "name": "Jun Bai", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:17.404Z", "hidden": false}, {"_id": "6937b96219d912300c34a39b", "name": "Zixia Jia", "hidden": false}, {"_id": "6937b96219d912300c34a39c", "name": "Shuyi Zhang", "hidden": false}, {"_id": "6937b96219d912300c34a39d", "name": "Ziyong Lin", "hidden": false}, {"_id": "6937b96219d912300c34a39e", "user": {"_id": "64b119c4372d43407723136b", "avatarUrl": "/avatars/d523e181993eea06b7f6a71a592c995e.svg", "isPro": false, "fullname": "YANTING WANG", "user": "Noane", "type": "user"}, "name": "Yanting Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:14.418Z", "hidden": false}, {"_id": "6937b96219d912300c34a39f", "name": "Song-Chun Zhu", "hidden": false}, {"_id": "6937b96219d912300c34a3a0", "name": "Zilong Zheng", "hidden": false}], "publishedAt": "2025-12-08T11:39:43.000Z", "submittedOnDailyAt": "2025-12-09T04:12:55.960Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "submittedOnDailyBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "isPro": false, "fullname": "Zilong Zheng", "user": "zlzheng", "type": "user"}, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "upvotes": 49, "discussionId": "6937b96219d912300c34a3a1", "projectPage": "https://bigai-nlco.github.io/Native-Parallel-Reasoner/", "githubRepo": "https://github.com/bigai-nlco/Native-Parallel-Reasoner", "ai_summary": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.", "ai_keywords": ["Native Parallel Reasoner", "Large Language Models", "self-evolve", "parallel reasoning", "self-distilled progressive training", "cold-start format discovery", "topological constraints", "Parallel-Aware Policy Optimization", "branching policies", "execution graph", "adaptive decomposition", "trial and error", "NPR Engine", "memory management", "flow control", "parallel RL training", "reasoning benchmarks", "Qwen3-4B", "genuine parallel execution", "autoregressive decoding", "agentic reasoning"], "githubStars": 18, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNative Parallel Reasoner (NPR)\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u884c\u53d1\u5c55\u5e76\u5177\u5907\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>NPR\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u5b9e\u73b0\u6a21\u578b\u4ece\u987a\u5e8f\u6a21\u62df\u5230\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u7684\u8f6c\u53d8\uff0c\u5305\u62ec\u65e0\u76d1\u7763\u7684\u81ea\u6211\u84b8\u998f\u8fdb\u9636\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u65b0\u7684\u5e76\u884c\u610f\u8bc6\u7b56\u7565\u4f18\u5316\uff08PAPO\uff09\u7b97\u6cd5\uff0c\u5141\u8bb8\u6a21\u578b\u5728\u6267\u884c\u56fe\u4e2d\u76f4\u63a5\u4f18\u5316\u5206\u652f\u7b56\u7565\uff0c\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u81ea\u9002\u5e94\u5206\u89e3\u3002</li>\n    <li>NPR\u5f15\u5165\u4e86\u5f3a\u5927\u7684\u5f15\u64ce\uff0c\u6539\u5584\u4e86\u5185\u5b58\u7ba1\u7406\u548c\u6d41\u63a7\u5236\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u5927\u89c4\u6a21\u5e76\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002</li>\n    <li>NPR\u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u63d0\u5347\uff0c\u6027\u80fd\u63d0\u9ad8\u6700\u591a\u8fbe24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u6700\u591a\u8fbe4.6\u500d\uff0c\u5c55\u73b0\u4e86100%\u7684\u771f\u6b63\u5e76\u884c\u6267\u884c\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NPR is a new framework that helps Large Language Models (LLMs) improve their ability to think in parallel without needing a teacher.</li>\n    <li>It uses three main innovations: a self-training method that develops reasoning skills, a new algorithm that helps the model learn better through practice, and a system that manages memory and control for efficient training.</li>\n    <li>NPR has been tested on eight reasoning tasks and shows performance improvements of up to 24.5% and speed increases of up to 4.6 times.</li>\n    <li>Unlike previous methods that often rely on sequential processing, NPR achieves true parallel reasoning, setting a new benchmark for efficient and scalable reasoning in AI.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:39:43.000Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07461.png", "numComments": 1, "submittedBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "fullname": "Zilong Zheng", "name": "zlzheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.09363", "authors": [{"_id": "693a379e74fced5bf9c32412", "user": {"_id": "6486ff6561053da6442fef1a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg", "isPro": false, "fullname": "KeXing", "user": "KXingLab", "type": "user"}, "name": "Ke Xing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:26.656Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32413", "name": "Longfei Li", "hidden": false}, {"_id": "693a379e74fced5bf9c32414", "user": {"_id": "64b7ab4c037d6452a31910eb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png", "isPro": false, "fullname": "yuyangyin", "user": "yuyangyin", "type": "user"}, "name": "Yuyang Yin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:43:30.256Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32415", "name": "Hanwen Liang", "hidden": false}, {"_id": "693a379e74fced5bf9c32416", "name": "Guixun Luo", "hidden": false}, {"_id": "693a379e74fced5bf9c32417", "name": "Chen Fang", "hidden": false}, {"_id": "693a379e74fced5bf9c32418", "name": "Jue Wang", "hidden": false}, {"_id": "693a379e74fced5bf9c32419", "name": "Konstantinos N. Plataniotis", "hidden": false}, {"_id": "693a379e74fced5bf9c3241a", "name": "Xiaojie Jin", "hidden": false}, {"_id": "693a379e74fced5bf9c3241b", "name": "Yao Zhao", "hidden": false}, {"_id": "693a379e74fced5bf9c3241c", "name": "Yunchao Wei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "publishedAt": "2025-12-10T06:50:16.000Z", "submittedOnDailyAt": "2025-12-11T00:46:52.612Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "upvotes": 44, "discussionId": "693a379e74fced5bf9c3241d", "projectPage": "https://ke-xing.github.io/StereoWorld/", "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.", "ai_keywords": ["stereo video", "monocular-to-stereo", "pretrained video generator", "geometry-aware regularization", "spatio-temporal tiling", "high-definition stereo video dataset", "natural human interpupillary distance (IPD)", "visual fidelity", "geometric consistency"], "summary_zh": "<ul>\n    <li>XR\u8bbe\u5907\u7684\u666e\u53ca\u589e\u52a0\u4e86\u5bf9\u9ad8\u8d28\u91cf\u7acb\u4f53\u89c6\u9891\u7684\u9700\u6c42\uff0c\u4f46\u5176\u5236\u4f5c\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u73b0\u7455\u75b5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86StereoWorld\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u5668\u5c06\u5355\u76ee\u89c6\u9891\u8f6c\u5316\u4e3a\u9ad8\u4fdd\u771f\u7684\u7acb\u4f53\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5bf9\u5355\u76ee\u89c6\u9891\u8fdb\u884c\u6761\u4ef6\u5904\u7406\uff0c\u5e76\u4f7f\u7528\u51e0\u4f55\u611f\u77e5\u7684\u6b63\u5219\u5316\u6765\u786e\u4fdd3D\u7ed3\u6784\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7a7a\u95f4\u65f6\u95f4\u5206\u5757\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u7684\u6548\u7387\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc71100\u4e07\u5e27\u7684\u9ad8\u6e05\u7acb\u4f53\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8868\u660eStereoWorld\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The use of XR devices is increasing the demand for high-quality stereo videos, but making these videos is expensive and often produces errors.</li>\n    <li>StereoWorld is a new system that improves the process by converting regular videos into high-quality stereo videos using a pre-trained video generator.</li>\n    <li>This system uses special techniques to ensure that the 3D structures in the videos look correct and are visually appealing.</li>\n    <li>To support large training and testing, a new dataset with over 11 million high-definition stereo video frames was created.</li>\n    <li>Tests show that StereoWorld produces much better stereo videos compared to older methods, with improved visual quality and 3D accuracy.</li>\n</ul>"}, "publishedAt": "2025-12-10T01:50:16.000Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09363.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07525", "authors": [{"_id": "693794d319d912300c34a291", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:38:01.257Z", "hidden": false}, {"_id": "693794d319d912300c34a292", "name": "Yuerong Song", "hidden": false}, {"_id": "693794d319d912300c34a293", "name": "Zhigeng Liu", "hidden": false}, {"_id": "693794d319d912300c34a294", "user": {"_id": "64805e6dde559d48dbb00627", "avatarUrl": "/avatars/29ca34546411dcc28bbc934e3c26a2ba.svg", "isPro": false, "fullname": "Zengfeng", "user": "ZengfengHuang", "type": "user"}, "name": "Zengfeng Huang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:11.754Z", "hidden": false}, {"_id": "693794d319d912300c34a295", "user": {"_id": "6491cd52b1e5d3444528edb1", "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg", "isPro": false, "fullname": "Qipeng Guo", "user": "QipengGuo", "type": "user"}, "name": "Qipeng Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:18.117Z", "hidden": false}, {"_id": "693794d319d912300c34a296", "name": "Zhaoxiang Liu", "hidden": false}, {"_id": "693794d319d912300c34a297", "name": "Shiguo Lian", "hidden": false}, {"_id": "693794d319d912300c34a298", "user": {"_id": "64de18f41d826d7355c285e7", "avatarUrl": "/avatars/23e2c44e3a593415becc02463980f6e8.svg", "isPro": false, "fullname": "Ziwei He", "user": "ziweihe", "type": "user"}, "name": "Ziwei He", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T20:17:41.271Z", "hidden": false}, {"_id": "693794d319d912300c34a299", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:28.411Z", "hidden": false}], "publishedAt": "2025-12-08T12:59:54.000Z", "submittedOnDailyAt": "2025-12-09T00:49:29.234Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "upvotes": 41, "discussionId": "693794d419d912300c34a29a", "githubRepo": "https://github.com/OpenMOSS/rope_pp", "ai_summary": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.", "ai_keywords": ["Rotary Position Embeddings", "Large Language Models", "complex-valued dot product", "attention score", "long-context dependencies", "positional information"], "githubStars": 12, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7528\u4e8e\u7f16\u7801\u5e8f\u5217\u987a\u5e8f\u7684\u6807\u51c6\u65b9\u6cd5\u3002</li>\n    <li>\u73b0\u6709\u7684\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u5305\u542b\u91cd\u8981\u76f8\u4f4d\u4fe1\u606f\u7684\u865a\u90e8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u5f15\u5165\u88ab\u4e22\u5f03\u7684\u865a\u90e8\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u521b\u5efa\u53cc\u7ec4\u4ef6\u6ce8\u610f\u529b\u5206\u6570\u3002</li>\n    <li>\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5efa\u6a21\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u4fdd\u7559\u66f4\u591a\u4f4d\u7f6e\u4fe1\u606f\u3002</li>\n    <li>\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6807\u51c6RoPE\uff0c\u5c24\u5176\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u660e\u663e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Rotary Position Embeddings (RoPE) are used in Large Language Models to understand the order of sequences by rotating vectors.</li>\n    <li>Current methods only use the real part of complex calculations, ignoring the imaginary part that holds important information.</li>\n    <li>This paper introduces a new method that includes the imaginary component to improve attention score calculations.</li>\n    <li>The new approach enhances the model's ability to understand long-context relationships in language.</li>\n    <li>Tests show that this method performs better than standard RoPE, especially with longer contexts.</li>\n</ul>"}, "publishedAt": "2025-12-08T07:59:54.000Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07525.png", "numComments": 1, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07951", "authors": [{"_id": "6938e892dfc35938ba129ff5", "name": "Zekai Luo", "hidden": false}, {"_id": "6938e892dfc35938ba129ff6", "name": "Zongze Du", "hidden": false}, {"_id": "6938e892dfc35938ba129ff7", "name": "Zhouhang Zhu", "hidden": false}, {"_id": "6938e892dfc35938ba129ff8", "name": "Hao Zhong", "hidden": false}, {"_id": "6938e892dfc35938ba129ff9", "user": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "name": "Muzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T10:51:43.541Z", "hidden": false}, {"_id": "6938e892dfc35938ba129ffa", "name": "Wen Wang", "hidden": false}, {"_id": "6938e892dfc35938ba129ffb", "name": "Yuling Xi", "hidden": false}, {"_id": "6938e892dfc35938ba129ffc", "name": "Chenchen Jing", "hidden": false}, {"_id": "6938e892dfc35938ba129ffd", "name": "Hao Chen", "hidden": false}, {"_id": "6938e892dfc35938ba129ffe", "name": "Chunhua Shen", "hidden": false}], "publishedAt": "2025-12-08T19:00:04.000Z", "submittedOnDailyAt": "2025-12-10T00:59:39.366Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "submittedOnDailyBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "upvotes": 40, "discussionId": "6938e892dfc35938ba129fff", "projectPage": "https://aim-uofa.github.io/LivingSwap", "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.", "ai_keywords": ["keyframe conditioning", "video reference guidance", "temporal stitching", "identity preservation", "high-fidelity reconstruction", "Face2Face dataset"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u5728\u5f71\u89c6\u5236\u4f5c\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5b9e\u73b0\u9ad8\u4fdd\u771f\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86LivingSwap\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u53c2\u8003\u7684\u89c6\u9891\u6362\u8138\u6a21\u578b\uff0c\u5229\u7528\u6e90\u89c6\u9891\u7684\u4e30\u5bcc\u89c6\u89c9\u7279\u5f81\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u5173\u952e\u5e27\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u4ee5\u7075\u6d3b\u548c\u53ef\u63a7\u7684\u65b9\u5f0f\u6ce8\u5165\u76ee\u6807\u8eab\u4efd\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u5408\u5173\u952e\u5e27\u6761\u4ef6\u548c\u89c6\u9891\u53c2\u8003\u6307\u5bfc\uff0c\u6a21\u578b\u80fd\u591f\u7a33\u5b9a\u5730\u4fdd\u7559\u8eab\u4efd\u5e76\u9ad8\u4fdd\u771f\u91cd\u5efa\u957f\u89c6\u9891\u5e8f\u5217\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u5236\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4eba\u5de5\u5de5\u4f5c\uff0c\u540c\u65f6\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video face swapping is important in film but has challenges with quality and consistency in long videos.</li>\n    <li>The new model, LivingSwap, uses keyframes from videos to improve the face swapping process.</li>\n    <li>It combines keyframes with video references to maintain stable identities and high quality throughout the video.</li>\n    <li>A new dataset called Face2Face was created to help train the model effectively.</li>\n    <li>Tests show LivingSwap achieves top results, making it easier to integrate faces while reducing manual work in video production.</li>\n</ul>"}, "publishedAt": "2025-12-08T14:00:04.000Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07951.png", "numComments": 1, "submittedBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "fullname": "zhumuzhi", "name": "Z-MU-Z", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10739", "authors": [{"_id": "693b91d89874a2a5e4ffb329", "name": "Songyang Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32a", "user": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "name": "Yuzhe Gu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:56.632Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32b", "name": "Zijian Wu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32c", "name": "Lingkai Kong", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32d", "user": {"_id": "64e8505321540e1da3226b54", "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg", "isPro": false, "fullname": "Wenwei Zhang", "user": "ZwwWayne", "type": "user"}, "name": "Wenwei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:21.987Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32e", "name": "Zhongrui Cai", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb32f", "name": "Fan Zheng", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb330", "user": {"_id": "670f8df2005a358fdc6c2fb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qw2yocepl2vhC5T2ae49b.png", "isPro": false, "fullname": "tianyou", "user": "matianyou", "type": "user"}, "name": "Tianyou Ma", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:03:57.205Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb331", "user": {"_id": "687f853bb39262ba84f3eeff", "avatarUrl": "/avatars/cdfc44fde8237f08f10192553fe5a075.svg", "isPro": false, "fullname": "Junhao Shen", "user": "shenjunhao", "type": "user"}, "name": "Junhao Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:00.496Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb332", "name": "Haiteng Zhao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb333", "user": {"_id": "6454b1073aaeff9f3d330ef6", "avatarUrl": "/avatars/331fcbbf18a72b0dc8419ca3a77299bb.svg", "isPro": false, "fullname": "Duanyang Zhang", "user": "KKKDaniel", "type": "user"}, "name": "Duanyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:12.911Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb334", "name": "Huilun Zhang", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb335", "user": {"_id": "63fd691794cc8f815d50c112", "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg", "isPro": false, "fullname": "liu", "user": "Harold-lkk", "type": "user"}, "name": "Kuikun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:58.650Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb336", "name": "Chengqi Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb337", "name": "Yanhui Duan", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb338", "name": "Chiyu Chen", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb339", "name": "Ningsheng Ma", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33a", "name": "Jianfei Gao", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33b", "name": "Han Lyu", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33c", "user": {"_id": "636317ed80c1a705a6eff396", "avatarUrl": "/avatars/3db090e101b916d9256d0d3e043db71d.svg", "isPro": false, "fullname": "Dahua Lin", "user": "lindahua", "type": "user"}, "name": "Dahua Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:20.346Z", "hidden": false}, {"_id": "693b91d89874a2a5e4ffb33d", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-11T15:26:28.000Z", "submittedOnDailyAt": "2025-12-12T01:27:55.307Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "submittedOnDailyBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "isPro": false, "fullname": "Yuzhe Gu", "user": "vanilla1116", "type": "user"}, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "upvotes": 37, "discussionId": "693b91d99874a2a5e4ffb33e", "ai_summary": "OPV, an iterative active learning framework with Rejection Fine-Tuning, enhances verification of long reasoning chains in large language models, achieving state-of-the-art results and improving accuracy in collaborative tasks.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards (RLVR)", "outcome-based verifiers (OVs)", "process-based verifiers (PVs)", "long reasoning chains of thought (CoTs)", "iterative active learning", "Rejection Fine-Tuning (RFT)", "F1 score", "accuracy", "DeepSeek-R1-Distill-Qwen-32B", "AIME2025"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u5f53\u524d\u7684\u7ed3\u679c\u57fa\u7840\u9a8c\u8bc1\u5668\uff08OVs\uff09\u65e0\u6cd5\u68c0\u67e5\u957f\u671f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\u3002</li>\n    <li>\u8fc7\u7a0b\u57fa\u7840\u9a8c\u8bc1\u5668\uff08PVs\uff09\u5728\u68c0\u6d4b\u590d\u6742\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u65f6\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u7684\u7ed3\u679c\u57fa\u7840\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\u901a\u8fc7\u603b\u7ed3\u7ed3\u679c\u7684\u63a8\u7406\u8fc7\u7a0b\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u9a8c\u8bc1\uff0c\u5e76\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>OPV\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0cF1\u5f97\u5206\u8fbe\u523083.1\uff0c\u8d85\u8fc7\u4e86\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u80fd\u6709\u6548\u68c0\u6d4b\u5047\u9633\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are improving at complex reasoning tasks using a method called Reinforcement Learning with Verifiable Rewards (RLVR).</li>\n    <li>Current verifiers struggle to check unreliable steps in long reasoning processes due to the lack of good quality human annotations.</li>\n    <li>The proposed Outcome-based Process Verifier (OPV) aims to verify reasoning processes efficiently and accurately by summarizing outcomes from long reasoning chains.</li>\n    <li>OPV uses a learning method that involves expert annotations to continually improve its verification abilities with fewer costs.</li>\n    <li>Experiments show OPV outperforms larger models and effectively detects errors, achieving a high F1 score and improving accuracy when used with other models.</li>\n</ul>"}, "publishedAt": "2025-12-11T10:26:28.000Z", "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\thisbench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10739.png", "numComments": 1, "submittedBy": {"_id": "6601196cc91ba4c08ad6e270", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6601196cc91ba4c08ad6e270/venywO3WPi2fNi5WUJTH0.jpeg", "fullname": "Yuzhe Gu", "name": "vanilla1116", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.10949", "authors": [{"_id": "693b895d9874a2a5e4ffb302", "user": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "name": "Yiwen Tang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:04:55.504Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb303", "user": {"_id": "642a8302d651bae3c11b72b1", "avatarUrl": "/avatars/4d2d422613e274d80482fed9a7d3f785.svg", "isPro": false, "fullname": "Zoey Guo", "user": "Purple1288", "type": "user"}, "name": "Zoey Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:01.705Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb304", "user": {"_id": "6708920aeae29d1cd41a703b", "avatarUrl": "/avatars/922427a86523b0aa810412fd2d75f88e.svg", "isPro": false, "fullname": "kaixin zhu", "user": "czkk566", "type": "user"}, "name": "Kaixin Zhu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:10.275Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb305", "name": "Ray Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb306", "user": {"_id": "6535045a910b844786a6642f", "avatarUrl": "/avatars/37a94864a7a348151837b421ea6d77e3.svg", "isPro": false, "fullname": "Qizhi Chen", "user": "Tavish9", "type": "user"}, "name": "Qizhi Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:11.060Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb307", "user": {"_id": "6349214f8146350b3a4c5cdf", "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg", "isPro": false, "fullname": "Dongzhi Jiang", "user": "CaraJ", "type": "user"}, "name": "Dongzhi Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:06:02.910Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb308", "name": "Junli Liu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb309", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:15:08.733Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30a", "user": {"_id": "6662d2c9de4c4e1f04bd29c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QnnO_KOyZjFd-iXuPnHqR.png", "isPro": false, "fullname": "HaomingSong", "user": "HaomingSong", "type": "user"}, "name": "Haoming Song", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:05:30.062Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30b", "user": {"_id": "64daecec888b7e9c400f59b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png", "isPro": false, "fullname": "Delin Qu", "user": "delinqu", "type": "user"}, "name": "Delin Qu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:16:34.085Z", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30c", "name": "Tianyi Bai", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30d", "name": "Dan Xu", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30e", "name": "Wentao Zhang", "hidden": false}, {"_id": "693b895d9874a2a5e4ffb30f", "name": "Bin Zhao", "hidden": false}], "publishedAt": "2025-12-11T18:59:52.000Z", "submittedOnDailyAt": "2025-12-12T00:49:43.850Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "submittedOnDailyBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "isPro": false, "fullname": "Ivan Tang", "user": "IvanTang", "type": "user"}, "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "upvotes": 36, "discussionId": "693b895d9874a2a5e4ffb310", "githubRepo": "https://github.com/Ivan-Tang-3D/3DGen-R1", "githubRepoAddedBy": "user", "ai_summary": "This study investigates reinforcement learning for text-to-3D generation, focusing on reward designs, RL algorithms, benchmarking, and hierarchical optimization, introducing AR3D-R1 as the first RL-enhanced model for 3D generation.", "ai_keywords": ["reinforcement learning", "text-to-3D generation", "reward designs", "GRPO variants", "token-level optimization", "MME-3DR", "Hi-GRPO", "hierarchical 3D generation", "AR3D-R1"], "githubStars": 40, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u57282D\u56fe\u50cf\u751f\u6210\u4e2d\u5df2\u6210\u529f\u5e94\u7528\uff0c\u4f46\u57283D\u751f\u6210\u4e2d\u4ecd\u7136\u8f83\u5c11\u63a2\u7d22\u3002</li>\n    <li>3D\u5bf9\u8c61\u7684\u590d\u6742\u6027\u4f7f\u5f97\u5176\u751f\u6210\u5bf9\u5956\u52b1\u8bbe\u8ba1\u548cRL\u7b97\u6cd5\u975e\u5e38\u654f\u611f\u3002</li>\n    <li>\u6211\u4eec\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u8bc4\u4f30\u5956\u52b1\u8bbe\u8ba1\u3001\u7b97\u6cd5\u548c\u6570\u636e\u89c4\u6a21\u5bf9\u6587\u672c\u52303D\u751f\u6210\u7684\u5f71\u54cd\u3002</li>\n    <li>\u5f15\u5165MME-3DR\u57fa\u51c6\uff0c\u4ee5\u8861\u91cf3D\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1AR3D-R1\u6a21\u578b\uff0c\u4f18\u5316\u4ece\u7c97\u7565\u5f62\u72b6\u5230\u7eb9\u7406\u7ec6\u5316\u76843D\u751f\u6210\u8fc7\u7a0b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) has been successfully used for 2D image generation and is now being explored for 3D generation, which is more complex.</li>\n    <li>The study focuses on improving 3D generation using RL by examining reward designs that align with human preferences and using effective RL algorithms.</li>\n    <li>New benchmarks like MME-3DR are introduced to assess reasoning abilities in 3D generation models.</li>\n    <li>A new hierarchical approach called Hi-GRPO is proposed to improve the process of generating 3D objects from general shapes to detailed textures.</li>\n    <li>The result is AR3D-R1, the first RL-enhanced model for generating 3D objects from text, with the aim of advancing 3D generation techniques.</li>\n</ul>"}, "publishedAt": "2025-12-11T13:59:52.000Z", "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation", "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10949.png", "numComments": 2, "submittedBy": {"_id": "6552f1ad5d55ccb20e9142a0", "avatarUrl": "/avatars/0e3e80cba64b5ae0bc5638694ac33dbf.svg", "fullname": "Ivan Tang", "name": "IvanTang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07469", "authors": [{"_id": "69379e0319d912300c34a2fb", "user": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "name": "Xiangpeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T10:34:33.919Z", "hidden": false}, {"_id": "69379e0319d912300c34a2fc", "name": "Ji Xie", "hidden": false}, {"_id": "69379e0319d912300c34a2fd", "name": "Yiyuan Yang", "hidden": false}, {"_id": "69379e0319d912300c34a2fe", "name": "Yan Huang", "hidden": false}, {"_id": "69379e0319d912300c34a2ff", "name": "Min Xu", "hidden": false}, {"_id": "69379e0319d912300c34a300", "name": "Qiang Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "publishedAt": "2025-12-08T11:50:18.000Z", "submittedOnDailyAt": "2025-12-09T01:34:28.853Z", "title": "Unified Video Editing with Temporal Reasoner", "submittedOnDailyBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "upvotes": 31, "discussionId": "69379e0319d912300c34a301", "projectPage": "https://videocof.github.io/", "githubRepo": "https://github.com/knightyxp/VideoCoF", "ai_summary": "VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.", "ai_keywords": ["chain-of-frames", "chain-of-thought reasoning", "video diffusion model", "reasoning tokens", "edit-region latents", "target video tokens", "instruction-to-region alignment", "fine-grained video editing", "RoPE alignment", "motion alignment", "length extrapolation"], "githubStars": 25, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u9762\u4e34\u7cbe\u786e\u6027\u4e0e\u7edf\u4e00\u6027\u7684\u77db\u76fe\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86VideoCoF\uff0c\u4e00\u79cd\u65b0\u7684\u94fe\u5e27\u65b9\u6cd5\uff0c\u91c7\u7528\u201c\u5148\u770b\u3001\u518d\u63a8\u7406\u3001\u6700\u540e\u7f16\u8f91\u201d\u7684\u6d41\u7a0b\u3002</li>\n    <li>This approach eliminates\u5bf9\u7528\u6237\u63d0\u4f9b\u7684\u63a9\u7801\u7684\u9700\u6c42\uff0c\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u7684\u6307\u4ee4\u5230\u533a\u57df\u7684\u5bf9\u9f50\u548c\u7ec6\u81f4\u7684\u89c6\u9891\u7f16\u8f91\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86RoPE\u5bf9\u9f50\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u8fd0\u52a8\u5bf9\u9f50\u5e76\u6269\u5c55\u8d85\u51fa\u8bad\u7ec3\u65f6\u957f\u3002</li>\n    <li>\u901a\u8fc7\u4ec5\u4f7f\u752850k\u4e2a\u89c6\u9891\u5bf9\uff0cVideoCoF\u5728VideoCoF-Bench\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video editing methods struggle between precision and flexibility, using either expert models needing specific inputs or unified models lacking spatial details.</li>\n    <li>VideoCoF is a new approach inspired by how we think, promoting a \"see, reason, then edit\" process for better video editing.</li>\n    <li>This method allows the video model to first understand what needs editing without needing user-defined masks, improving accuracy in editing tasks.</li>\n    <li>VideoCoF includes a special alignment method to ensure smooth motion in videos and can extend video length beyond what it was trained on.</li>\n    <li>With just 50,000 video pairs, VideoCoF shows top performance on a benchmark, proving it is both efficient and effective.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:50:18.000Z", "title": "Unified Video Editing with Temporal Reasoner", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07469.png", "numComments": 5, "submittedBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "fullname": "Xiangpeng Yang", "name": "XiangpengYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u8f6f\u4ef6\u5f00\u53d1\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u7ffb\u8bd1\u4e3a\u529f\u80fd\u4ee3\u7801\u3002</li>\n    <li>\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u5168\u9762\u6307\u5357\uff0c\u6db5\u76d6\u4ece\u6570\u636e\u5904\u7406\u5230\u540e\u671f\u8bad\u7ec3\u7684\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u5206\u6790\u4e86\u901a\u7528LLMs\uff08\u5982GPT-4\uff09\u548c\u4e13\u95e8\u7684\u4ee3\u7801LLMs\uff08\u5982StarCoder\uff09\u7684\u80fd\u529b\u548c\u8bbe\u8ba1\u51b3\u7b56\u3002</li>\n    <li>\u63a2\u8ba8\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5305\u62ec\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u4e0e\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u6574\u5408\u3002</li>\n    <li>\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u5206\u6790\u4ee3\u7801\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c\u53ca\u5176\u53c2\u6570\u654f\u611f\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are changing how software is developed by turning natural language descriptions into working code.</li>\n    <li>Tools like Github Copilot and others are driving the use of these models, which have improved significantly in performance.</li>\n    <li>This work provides a detailed guide on how to use code LLMs, covering everything from data preparation to post-training techniques.</li>\n    <li>It compares general LLMs and specialized code LLMs, looking at their strengths and weaknesses in coding tasks.</li>\n    <li>The research identifies gaps between academic findings and real-world software development needs, suggesting future research directions.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86DeepSeek-V3.2\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u5353\u8d8a\u7684\u63a8\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b(DSA)\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u534f\u8bae\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u672cDeepSeek-V3.2-Speciale\u8d85\u8d8a\u4e86GPT-5\uff0c\u5e76\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u4e0eGemini-3.0-Pro\u76f8\u5f53\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u4ee3\u7406\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u6027\u5730\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines efficiency with strong reasoning and performance.</li>\n    <li>It features a new attention mechanism called DeepSeek Sparse Attention (DSA) that reduces complexity while maintaining performance in long contexts.</li>\n    <li>DeepSeek-V3.2 uses a scalable reinforcement learning approach and performs similarly to GPT-5, with a special version outperforming it and matching the reasoning skills of Gemini-3.0-Pro.</li>\n    <li>This model achieved top results in prestigious competitions like the 2025 International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a unique data generation pipeline to improve reasoning in tool-use tasks, enhancing its ability to generalize and follow instructions in complex environments.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5f00\u6e90\u66ff\u4ee3\u54c1\uff08\u5982Qwen-Image\u548cHunyuan-Image-3.0\uff09\u53c2\u6570\u91cf\u5927\uff0820B\u523080B\uff09\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u7ecf\u8fc7\u4f18\u5316\u8bad\u7ec3\uff0c\u4f7f\u7528314K H800 GPU\u5c0f\u65f6\u5b8c\u6210\u8bad\u7ec3\uff0c\u6210\u672c\u7ea6\u4e3a630K\u7f8e\u5143\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u5e76\u4e14\u4ee3\u7801\u548c\u6743\u91cd\u5df2\u516c\u5f00\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-performance image generation models are mostly proprietary, with open-source options being too large and impractical for regular users.</li>\n    <li>Z-Image is a new 6B-parameter model that uses a unique architecture to challenge the trend of making models bigger and bigger.</li>\n    <li>It was trained efficiently, taking only 314K GPU hours and costing about $630K, making it more accessible.</li>\n    <li>Z-Image provides fast performance on both high-end and consumer-grade hardware, and includes an editing model with good instruction-following abilities.</li>\n    <li>The model shows strong capabilities in creating realistic images and bilingual text, and the team has made their code and demo available to help others build similar models.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.11793", "authors": [{"_id": "691be81b6bfd5965c0fd37e2", "name": "MiroMind Team", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e3", "name": "Song Bai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e4", "name": "Lidong Bing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e5", "name": "Carson Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e6", "name": "Guanzheng Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e7", "user": {"_id": "632dab84fdb35759ea6646a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632dab84fdb35759ea6646a0/IxO5mbtzHJsr0YHW-YtVk.jpeg", "isPro": false, "fullname": "Yuntao Chen", "user": "YuntaoChen", "type": "user"}, "name": "Yuntao Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:45.403Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e8", "name": "Zhe Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e9", "name": "Ziyi Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ea", "name": "Jifeng Dai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37eb", "name": "Xuan Dong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ec", "name": "Yue Deng", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ed", "name": "Yunjie Fu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ee", "name": "Junqi Ge", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ef", "name": "Chenxia Han", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f0", "name": "Tammy Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f1", "name": "Zhenhang Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f2", "name": "Jerry Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f3", "name": "Shilei Jiang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f4", "name": "Tianyu Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f5", "user": {"_id": "64be2455b567ae97c34bb948", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be2455b567ae97c34bb948/QuCdStDDGaXjDmp4V-dBj.jpeg", "isPro": false, "fullname": "Xiaoqi Jian", "user": "mx1024", "type": "user"}, "name": "Xiaoqi Jian", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:52.417Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f6", "name": "Lei Lei", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f7", "user": {"_id": "6466e7be1343dce20e59191b", "avatarUrl": "/avatars/6779560b203c3773dc76372c0b8cbe4e.svg", "isPro": false, "fullname": "Li Ruilin", "user": "Eric-LRL-130", "type": "user"}, "name": "Ruilin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:43.774Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f8", "name": "Ryan Luo", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f9", "name": "Tiantong Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fa", "name": "Xiang Lin", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fb", "name": "Ziyuan Liu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fc", "name": "Zhiqi Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fd", "name": "Jie Ni", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fe", "name": "Qiang Ren", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ff", "name": "Pax Sun", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3800", "name": "Shiqian Su", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3801", "name": "Chenxin Tao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3802", "name": "Bin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3803", "name": "Hellen Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3804", "name": "Haonan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3805", "name": "James Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3806", "name": "Jin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3807", "name": "Jojo Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3808", "name": "Letian Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3809", "name": "Shizun Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380a", "user": {"_id": "63d34004b734eaa4d4faeccf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg", "isPro": false, "fullname": "Weizhi Wang", "user": "weizhiwang", "type": "user"}, "name": "Weizhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:47.000Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380b", "name": "Zixuan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380c", "name": "Jinfan Xu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380d", "name": "Sen Xing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380e", "user": {"_id": "637f347a52229c639211bee8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg", "isPro": false, "fullname": "Chenyu Yang", "user": "cyyang822", "type": "user"}, "name": "Chenyu Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:48.746Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380f", "user": {"_id": "6239888e7fef05b7bdd5fcff", "avatarUrl": "/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg", "isPro": false, "fullname": "Hai Ye", "user": "oceanpty", "type": "user"}, "name": "Hai Ye", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:50.623Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3810", "name": "Jiaheng Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3811", "name": "Yue Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3812", "name": "Muyan Zhong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3813", "name": "Tianchen Zhao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3814", "name": "Xizhou Zhu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3815", "name": "Yanpeng Zhou", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3816", "name": "Yifan Zhang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3817", "name": "Zhi Zhu", "hidden": false}], "publishedAt": "2025-11-14T18:52:07.000Z", "submittedOnDailyAt": "2025-11-18T02:00:07.077Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "upvotes": 153, "discussionId": "691be81b6bfd5965c0fd3818", "projectPage": "https://dr.miromind.ai/", "githubRepo": "https://github.com/MiroMindAI/MiroThinker", "githubStars": 1133, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MiroThinker v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u548c\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002</li>\n    <li>MiroThinker\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u652f\u6301\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u9002\u5e94\u590d\u6742\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>MiroThinker\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u4ea4\u4e92\u6df1\u5ea6\u7684\u6269\u5c55\u4e0e\u6a21\u578b\u7684\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u7279\u6027\uff0c\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u91cd\u8981\u7ef4\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MiroThinker v1.0 is an open-source research agent aimed at improving reasoning and information-seeking skills.</li>\n    <li>It focuses on enhancing how the model interacts with its environment, rather than just increasing model size or context length.</li>\n    <li>The model uses reinforcement learning to effectively manage many tool calls (up to 600) during tasks, supporting complex research activities.</li>\n    <li>MiroThinker outperforms previous open-source agents in various benchmarks, showing accuracy rates that approach those of advanced commercial models.</li>\n    <li>The research highlights that better performance comes from deeper and more frequent interactions with the environment, suggesting that this interaction is crucial for future research agent development.</li>\n</ul>"}, "publishedAt": "2025-11-14T13:52:07.000Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11793.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u5c24\u5176\u662f\u8bc1\u636e\u7a00\u5c11\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u7406\u89e3\u957f\u89c6\u9891\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u5168\u5c40\u6d4f\u89c8\u7136\u540e\u8be6\u7ec6\u67e5\u770b\u76f8\u5173\u7247\u6bb5\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u8fdb\u884c\u89c6\u9891\u526a\u8f91\u548c\u7ec6\u7c92\u5ea6\u753b\u9762\u91cd\u91c7\u6837\uff0c\u76f4\u5230\u5f97\u5230\u6709\u6839\u636e\u7684\u7b54\u6848\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u95ee\u7b54\u6570\u636e\u7684\u95ee\u9898\uff0c\u6211\u4eec\u521b\u5efa\u4e86VideoSIAH\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0d\u540c\u7528\u9014\u7684\u6837\u672c\u3002</li>\n    <li>LongVT\u5728\u7ecf\u8fc7\u4e25\u683c\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u540e\uff0c\u5728\u56db\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongVT is a new system that helps large multimodal models better understand long videos by mimicking human video comprehension.</li>\n    <li>It uses a strategy of first looking at the overall video and then focusing on specific clips for detailed information.</li>\n    <li>The system improves video reasoning by using video cropping to zoom in on important parts of the video.</li>\n    <li>A new dataset called VideoSIAH will be released, containing 247,900 samples for training and 1,280 question-answer pairs for evaluation.</li>\n    <li>LongVT shows better performance than existing methods on various long video understanding tasks, and its resources are available online.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5316\u8eab\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5316\u8eab\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u8fdb\u884c\u53bb\u566a\u6b65\u9aa4\u7684\u7ba1\u9053\u5316\uff0c\u786e\u4fdd\u7a33\u5b9a\u7684\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u91c7\u7528\u6eda\u52a8\u4e0b\u6c89\u5e27\u673a\u5236\uff08RSFM\uff09\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u5f00\u521b\u4e86\u5de5\u4e1a\u957f\u89c6\u9891\u5408\u6210\u7684\u65b0\u6a21\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for generating high-quality avatars in real-time using a powerful 14-billion-parameter diffusion model.</li>\n    <li>It uses a method called Timestep-forcing Pipeline Parallelism (TPP) to improve speed and reduce delays by processing multiple steps at once on different GPUs.</li>\n    <li>The Rolling Sink Frame Mechanism (RSFM) helps maintain consistent appearance and reduces visual errors by using a cached reference image.</li>\n    <li>Live Avatar can generate avatars at 20 frames per second using 5 GPUs, achieving high-quality results quickly.</li>\n    <li>This technology sets a new standard for using advanced models in long videos, making real-time avatar generation practical for various applications.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4e86\u771f\u5b9e\u4f01\u4e1a\u4e2d\u7684\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\uff08DE\uff09\u4efb\u52a1\u6d89\u53ca\u5728\u5de5\u4e1a\u67b6\u6784\u4e0a\u8fdb\u884c\u4ed3\u5e93\u7ea7\u7684\u5de5\u7a0b\uff0c\u5305\u62ec\u4ece\u96f6\u8bbe\u8ba1\u548c\u6784\u5efaSQL\u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\uff08DA\uff09\u4efb\u52a1\u89e3\u51b3\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728DE\u4efb\u52a1\u7684\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u5de5\u7a0b\u548c\u5206\u6790\u80fd\u529b\u7684\u7f3a\u9677\uff0c\u63a8\u52a8\u771f\u6b63\u5177\u5907\u81ea\u4e3b\u6570\u636e\u5904\u7406\u80fd\u529b\u7684\u4ee3\u7406\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks that reflect complex data workflows in businesses.</li>\n    <li>Data engineering tasks involve creating and improving SQL pipelines and managing data structures.</li>\n    <li>Data analysis tasks focus on solving business problems through planning and coding while interpreting results.</li>\n    <li>Results show that even advanced systems struggle with these tasks, especially in data engineering.</li>\n    <li>DAComp helps identify challenges in data processes and encourages the development of better data management tools.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 14, 2025";
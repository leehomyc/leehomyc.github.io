[{"paper":{"id":"2602.07085","authors":[{"_id":"698ab6f91b2dc6b37d61b031","name":"Jun Han","hidden":false},{"_id":"698ab6f91b2dc6b37d61b032","name":"Shuo Zhang","hidden":false},{"_id":"698ab6f91b2dc6b37d61b033","name":"Wei Li","hidden":false},{"_id":"698ab6f91b2dc6b37d61b034","user":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","isPro":false,"fullname":"Zhi Yang","user":"yangzhi1","type":"user"},"name":"Zhi Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:58.707Z","hidden":false},{"_id":"698ab6f91b2dc6b37d61b035","name":"Yifan Dong","hidden":false},{"_id":"698ab6f91b2dc6b37d61b036","name":"Tu Hu","hidden":false},{"_id":"698ab6f91b2dc6b37d61b037","name":"Jialuo Yuan","hidden":false},{"_id":"698ab6f91b2dc6b37d61b038","user":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"name":"Xiaomin Yu","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:00.954Z","hidden":false},{"_id":"698ab6f91b2dc6b37d61b039","name":"Yumo Zhu","hidden":false},{"_id":"698ab6f91b2dc6b37d61b03a","name":"Fangqi Lou","hidden":false},{"_id":"698ab6f91b2dc6b37d61b03b","name":"Xin Guo","hidden":false},{"_id":"698ab6f91b2dc6b37d61b03c","name":"Zhaowei Liu","hidden":false},{"_id":"698ab6f91b2dc6b37d61b03d","name":"Tianyi Jiang","hidden":false},{"_id":"698ab6f91b2dc6b37d61b03e","name":"Ruichuan An","hidden":false},{"_id":"698ab6f91b2dc6b37d61b03f","name":"Jingping Liu","hidden":false},{"_id":"698ab6f91b2dc6b37d61b040","name":"Biao Wu","hidden":false},{"_id":"698ab6f91b2dc6b37d61b041","name":"Rongze Chen","hidden":false},{"_id":"698ab6f91b2dc6b37d61b042","name":"Kunyi Wang","hidden":false},{"_id":"698ab6f91b2dc6b37d61b043","name":"Yifan Wang","hidden":false},{"_id":"698ab6f91b2dc6b37d61b044","name":"Sen Hu","hidden":false},{"_id":"698ab6f91b2dc6b37d61b045","name":"Xinbing Kong","hidden":false},{"_id":"698ab6f91b2dc6b37d61b046","name":"Liwen Zhang","hidden":false},{"_id":"698ab6f91b2dc6b37d61b047","name":"Ronghao Chen","hidden":false},{"_id":"698ab6f91b2dc6b37d61b048","name":"Huacan Wang","hidden":false}],"publishedAt":"2026-02-06T08:08:04.000Z","submittedOnDailyAt":"2026-02-10T02:19:22.216Z","title":"QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining","submittedOnDailyBy":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","isPro":false,"fullname":"Zhi Yang","user":"yangzhi1","type":"user"},"summary":"Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.","upvotes":141,"discussionId":"698ab6fa1b2dc6b37d61b049","githubRepo":"https://github.com/QuantaAlpha/QuantaAlpha","githubRepoAddedBy":"user","githubStars":63,"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}},"publishedAt":"2026-02-06T03:08:04.000Z","title":"QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining","summary":"Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png","numComments":1,"submittedBy":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","fullname":"Zhi Yang","name":"yangzhi1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.08794","authors":[{"_id":"698ac65d1b2dc6b37d61b1c2","name":"SII-OpenMOSS Team","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1c4","name":"Donghua Yu","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1c5","name":"Mingshu Chen","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1c6","name":"Qi Chen","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1c7","name":"Qi Luo","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1c8","name":"Qianyi Wu","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1c9","user":{"_id":"63ec4715c81b6a52391c46b8","avatarUrl":"/avatars/496819b5075a1a834a2b9edeb068c80e.svg","isPro":false,"fullname":"QinyuanCheng","user":"Cqy2019","type":"user"},"name":"Qinyuan Cheng","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:07.400Z","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1ca","name":"Ruixiao Li","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1cb","user":{"_id":"62c14609ac1b639c2d87192c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png","isPro":false,"fullname":"SII-liangtianyi","user":"tianyilt","type":"user"},"name":"Tianyi Liang","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:10.522Z","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1cc","name":"Wenbo Zhang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1cd","name":"Wenming Tu","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1ce","name":"Xiangyu Peng","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1cf","name":"Yang Gao","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d0","name":"Yanru Huo","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d1","user":{"_id":"69158ffc0153b85a677dcc46","avatarUrl":"/avatars/c9c5f60522f2a8f370d790ea9938b090.svg","isPro":false,"fullname":"Ying Zhu","user":"Auraithm","type":"user"},"name":"Ying Zhu","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:27:41.440Z","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d2","name":"Yinze Luo","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d3","name":"Yiyang Zhang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d4","name":"Yuerong Song","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d5","name":"Zhe Xu","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d6","name":"Zhiyu Zhang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d7","name":"Chenchen Yang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d8","name":"Cheng Chang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1d9","name":"Chushu Zhou","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1da","name":"Hanfu Chen","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1db","name":"Hongnan Ma","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1dc","name":"Jiaxi Li","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1dd","name":"Jingqi Tong","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1de","name":"Junxi Liu","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1df","name":"Ke Chen","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e0","name":"Shimin Li","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e1","name":"Songlin Wang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e2","name":"Wei Jiang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e3","name":"Zhaoye Fei","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e4","name":"Zhiyuan Ning","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e5","name":"Chunguo Li","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e6","name":"Chenhui Li","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e7","name":"Ziwei He","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e8","name":"Zengfeng Huang","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1e9","name":"Xie Chen","hidden":false},{"_id":"698ac65d1b2dc6b37d61b1ea","name":"Xipeng Qiu","hidden":false}],"publishedAt":"2026-02-09T15:31:54.000Z","submittedOnDailyAt":"2026-02-10T03:18:59.260Z","title":"MOVA: Towards Scalable and Synchronized Video-Audio Generation","submittedOnDailyBy":{"_id":"62c14609ac1b639c2d87192c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png","isPro":false,"fullname":"SII-liangtianyi","user":"tianyilt","type":"user"},"summary":"Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.","upvotes":131,"discussionId":"698ac65e1b2dc6b37d61b1eb","projectPage":"https://mosi.cn/models/mova","githubRepo":"https://github.com/OpenMOSS/MOVA","githubRepoAddedBy":"user","ai_summary":"MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.","ai_keywords":["Mixture-of-Experts","MoE","audio-visual content","lip-synced speech","sound effects","content-aligned music","IT2VA","efficient inference","LoRA fine-tuning","prompt enhancement"],"githubStars":579,"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}},"publishedAt":"2026-02-09T10:31:54.000Z","title":"MOVA: Towards Scalable and Synchronized Video-Audio Generation","summary":"Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08794.png","numComments":1,"submittedBy":{"_id":"62c14609ac1b639c2d87192c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png","fullname":"SII-liangtianyi","name":"tianyilt","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.07026","authors":[{"_id":"698a98541b2dc6b37d61af09","user":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"name":"Xiaomin Yu","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:07:26.323Z","hidden":false},{"_id":"698a98541b2dc6b37d61af0a","name":"Yi Xin","hidden":false},{"_id":"698a98541b2dc6b37d61af0b","name":"Wenjie Zhang","hidden":false},{"_id":"698a98541b2dc6b37d61af0c","name":"Chonghan Liu","hidden":false},{"_id":"698a98541b2dc6b37d61af0d","name":"Hanzhen Zhao","hidden":false},{"_id":"698a98541b2dc6b37d61af0e","name":"Xiaoxing Hu","hidden":false},{"_id":"698a98541b2dc6b37d61af0f","name":"Xinlei Yu","hidden":false},{"_id":"698a98541b2dc6b37d61af10","name":"Ziyue Qiao","hidden":false},{"_id":"698a98541b2dc6b37d61af11","name":"Hao Tang","hidden":false},{"_id":"698a98541b2dc6b37d61af12","name":"Xue Yang","hidden":false},{"_id":"698a98541b2dc6b37d61af13","name":"Xiaobin Hu","hidden":false},{"_id":"698a98541b2dc6b37d61af14","name":"Chengwei Qin","hidden":false},{"_id":"698a98541b2dc6b37d61af15","name":"Hui Xiong","hidden":false},{"_id":"698a98541b2dc6b37d61af16","name":"Yu Qiao","hidden":false},{"_id":"698a98541b2dc6b37d61af17","name":"Shuicheng Yan","hidden":false}],"publishedAt":"2026-02-02T13:59:39.000Z","submittedOnDailyAt":"2026-02-10T00:01:56.908Z","title":"Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models","submittedOnDailyBy":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"summary":"Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.","upvotes":120,"discussionId":"698a98541b2dc6b37d61af18","githubRepo":"https://github.com/Yu-xm/ReVision.git","githubRepoAddedBy":"user","ai_summary":"Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.","ai_keywords":["multimodal contrastive learning","modality gap","geometric anomaly","isotropic assumptions","Fixed-frame Modality Gap Theory","ReAlign","Anchor Alignment","Trace Alignment","Centroid Alignment","ReVision","Multimodal Large Language Models","unpaired data","visual representation distribution","pretraining stage","visual instruction tuning"],"githubStars":41},"publishedAt":"2026-02-02T08:59:39.000Z","title":"Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models","summary":"Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07026.png","numComments":5,"submittedBy":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","fullname":"Yu_xm","name":"Yu2020","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.08222","authors":[{"_id":"698ad20e1b2dc6b37d61b227","user":{"_id":"64afe1653361f887816da303","avatarUrl":"/avatars/320d71adacfad9dd5db064b4ed3dec2b.svg","isPro":false,"fullname":"chenzehao","user":"chhao","type":"user"},"name":"Zehao Chen","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:04:42.021Z","hidden":false},{"_id":"698ad20e1b2dc6b37d61b228","name":"Gongxun Li","hidden":false},{"_id":"698ad20e1b2dc6b37d61b229","name":"Tianxiang Ai","hidden":false},{"_id":"698ad20e1b2dc6b37d61b22a","name":"Yifei Li","hidden":false},{"_id":"698ad20e1b2dc6b37d61b22b","name":"Zixuan Huang","hidden":false},{"_id":"698ad20e1b2dc6b37d61b22c","name":"Wang Zhou","hidden":false},{"_id":"698ad20e1b2dc6b37d61b22d","name":"Fuzhen Zhuang","hidden":false},{"_id":"698ad20e1b2dc6b37d61b22e","name":"Xianglong Liu","hidden":false},{"_id":"698ad20e1b2dc6b37d61b22f","name":"Jianxin Li","hidden":false},{"_id":"698ad20e1b2dc6b37d61b230","name":"Deqing Wang","hidden":false},{"_id":"698ad20e1b2dc6b37d61b231","user":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun B","user":"Yikunb","type":"user"},"name":"Yikun Ban","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:04:44.617Z","hidden":false}],"publishedAt":"2026-02-09T02:50:40.000Z","submittedOnDailyAt":"2026-02-10T04:36:28.975Z","title":"Weak-Driven Learning: How Weak Agents make Strong Agents Stronger","submittedOnDailyBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun B","user":"Yikunb","type":"user"},"summary":"As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.","upvotes":118,"discussionId":"698ad20e1b2dc6b37d61b232","githubRepo":"https://github.com/chenzehao82/Weak-Driven-Learning","githubRepoAddedBy":"user","ai_summary":"WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.","ai_keywords":["post-training optimization","large language models","saturation bottleneck","weak checkpoints","entropy dynamics","compensatory learning","learning gaps"],"githubStars":39},"publishedAt":"2026-02-08T21:50:40.000Z","title":"Weak-Driven Learning: How Weak Agents make Strong Agents Stronger","summary":"As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08222.png","numComments":4,"submittedBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","fullname":"Yikun B","name":"Yikunb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.06855","authors":[{"_id":"698b0ed21b2dc6b37d61b3d0","name":"Alisia Lupidi","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d1","name":"Bhavul Gauri","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d2","name":"Thomas Simon Foster","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d3","name":"Bassel Al Omari","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d4","name":"Despoina Magka","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d5","name":"Alberto Pepe","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d6","name":"Alexis Audran-Reiss","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d7","name":"Muna Aghamelu","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d8","name":"Nicolas Baldwin","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3d9","name":"Lucia Cipolina-Kun","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3da","name":"Jean-Christophe Gagnon-Audet","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3db","name":"Chee Hau Leow","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3dc","name":"Sandra Lefdal","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3dd","name":"Hossam Mossalam","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3de","name":"Abhinav Moudgil","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3df","name":"Saba Nazir","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e0","name":"Emanuel Tewolde","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e1","name":"Isabel Urrego","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e2","name":"Jordi Armengol Estape","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e3","name":"Amar Budhiraja","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e4","name":"Gaurav Chaurasia","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e5","name":"Abhishek Charnalia","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e6","name":"Derek Dunfield","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e7","name":"Karen Hambardzumyan","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e8","name":"Daniel Izcovich","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3e9","name":"Martin Josifoski","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3ea","name":"Ishita Mediratta","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3eb","name":"Kelvin Niu","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3ec","name":"Parth Pathak","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3ed","name":"Michael Shvartsman","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3ee","name":"Edan Toledo","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3ef","name":"Anton Protopopov","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3f0","name":"Roberta Raileanu","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3f1","name":"Alexander Miller","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3f2","name":"Tatiana Shavrina","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3f3","name":"Jakob Foerster","hidden":false},{"_id":"698b0ed21b2dc6b37d61b3f4","name":"Yoram Bachrach","hidden":false}],"publishedAt":"2026-02-06T16:45:02.000Z","submittedOnDailyAt":"2026-02-10T08:48:50.684Z","title":"AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents","submittedOnDailyBy":{"_id":"60720704227ff331937110f4","avatarUrl":"/avatars/8010bfb98256c138049aa3d237737b37.svg","isPro":false,"fullname":"Bhavul Gauri","user":"bhavul","type":"user"},"summary":"LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.","upvotes":54,"discussionId":"698b0ed31b2dc6b37d61b3f5","githubRepo":"https://github.com/facebookresearch/airs-bench","githubRepoAddedBy":"user","ai_summary":"AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancing autonomous scientific research.","ai_keywords":["AI Research Science Benchmark","agentic capabilities","research lifecycle","sequential scaffolds","parallel scaffolds","autonomous scientific research"],"githubStars":16,"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}},"publishedAt":"2026-02-06T11:45:02.000Z","title":"AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents","summary":"LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06855.png","numComments":1,"submittedBy":{"_id":"60720704227ff331937110f4","avatarUrl":"/avatars/8010bfb98256c138049aa3d237737b37.svg","fullname":"Bhavul Gauri","name":"bhavul","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07845","authors":[{"_id":"698ab2ef1b2dc6b37d61af7b","name":"Yalcin Tur","hidden":false},{"_id":"698ab2ef1b2dc6b37d61af7c","name":"Jalal Naghiyev","hidden":false},{"_id":"698ab2ef1b2dc6b37d61af7d","name":"Haoquan Fang","hidden":false},{"_id":"698ab2ef1b2dc6b37d61af7e","name":"Wei-Chuan Tsai","hidden":false},{"_id":"698ab2ef1b2dc6b37d61af7f","name":"Jiafei Duan","hidden":false},{"_id":"698ab2ef1b2dc6b37d61af80","name":"Dieter Fox","hidden":false},{"_id":"698ab2ef1b2dc6b37d61af81","name":"Ranjay Krishna","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/QxggswtZU5KkZQawFarz8.mp4"],"publishedAt":"2026-02-08T07:21:01.000Z","submittedOnDailyAt":"2026-02-10T02:01:42.213Z","title":"Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning","submittedOnDailyBy":{"_id":"632b42626110e37dba3d5bcb","avatarUrl":"/avatars/ca70a15def71ee84f4f149db5e954843.svg","isPro":false,"fullname":"Duan","user":"Jiafei1224","type":"user"},"summary":"Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/","upvotes":47,"discussionId":"698ab2ef1b2dc6b37d61af82","projectPage":"https://rd-vla.github.io/","githubRepo":"https://github.com/rd-vla/rd-vla","githubRepoAddedBy":"user","ai_summary":"RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.","ai_keywords":["Vision-Language-Action models","Chain-of-Thought prompting","recurrent architecture","weight-tied action head","truncated backpropagation through time","latent iterative refinement","adaptive stopping criterion","latent convergence","computational adaptivity"],"githubStars":3,"organization":{"_id":"5e70f3648ce3c604d78fe132","name":"allenai","fullname":"Ai2","avatar":"https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}},"publishedAt":"2026-02-08T02:21:01.000Z","title":"Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning","summary":"Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/QxggswtZU5KkZQawFarz8.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07845.png","numComments":1,"submittedBy":{"_id":"632b42626110e37dba3d5bcb","avatarUrl":"/avatars/ca70a15def71ee84f4f149db5e954843.svg","fullname":"Duan","name":"Jiafei1224","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"5e70f3648ce3c604d78fe132","name":"allenai","fullname":"Ai2","avatar":"https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08676","authors":[{"_id":"698ab6fd1b2dc6b37d61b04b","name":"Tiwei Bie","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b04c","name":"Maosong Cao","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b04d","name":"Xiang Cao","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b04e","name":"Bingsen Chen","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b04f","name":"Fuyuan Chen","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b050","name":"Kun Chen","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b051","name":"Lun Du","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b052","name":"Daozhuo Feng","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b053","name":"Haibo Feng","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b054","name":"Mingliang Gong","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b055","name":"Zhuocheng Gong","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b056","name":"Yanmei Gu","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b057","name":"Jian Guan","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b058","user":{"_id":"6494f6dc32f2c0d7ef28315c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6494f6dc32f2c0d7ef28315c/y4TwcrT6UQ9Zfy6BfUMUl.jpeg","isPro":false,"fullname":"Kaiyuan Guan","user":"WilfredG","type":"user"},"name":"Kaiyuan Guan","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:52.604Z","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b059","name":"Hongliang He","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b05a","name":"Zenan Huang","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b05b","name":"Juyong Jiang","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b05c","name":"Zhonghui Jiang","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b05d","name":"Zhenzhong Lan","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b05e","name":"Chengxi Li","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b05f","name":"Jianguo Li","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b060","name":"Zehuan Li","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b061","user":{"_id":"6470600790482b0e0f65a393","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fOUWWhf9jjsC9fgIiFlPc.png","isPro":false,"fullname":"Huabin","user":"Rookie-Liu","type":"user"},"name":"Huabin Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:42.985Z","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b062","name":"Lin Liu","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b063","name":"Guoshan Lu","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b064","name":"Yuan Lu","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b065","name":"Yuxin Ma","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b066","name":"Xingyu Mou","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b067","name":"Zhenxuan Pan","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b068","name":"Kaida Qiu","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b069","name":"Yuji Ren","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b06a","name":"Jianfeng Tan","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b06b","user":{"_id":"5f94dea4cf95e81b6854e223","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5f94dea4cf95e81b6854e223/r3VW8rb0wlJ7t_Lcpqi5f.jpeg","isPro":false,"fullname":"Dean Tian","user":"killa1218","type":"user"},"name":"Yiding Tian","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:47.064Z","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b06c","name":"Zian Wang","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b06d","name":"Lanning Wei","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b06e","name":"Tao Wu","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b06f","name":"Yipeng Xing","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b070","name":"Wentao Ye","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b071","name":"Liangyu Zha","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b072","name":"Tianze Zhang","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b073","name":"Xiaolu Zhang","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b074","user":{"_id":"6725f5a7f05f62659e3615f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iiXdzhaSnExw2dwQQtLZ8.png","isPro":false,"fullname":"Junbo Zhao","user":"jakezhao2024","type":"user"},"name":"Junbo Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:45.032Z","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b075","name":"Da Zheng","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b076","name":"Hao Zhong","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b077","name":"Wanli Zhong","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b078","name":"Jun Zhou","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b079","user":{"_id":"63eb008e5c837d9968f1eb71","avatarUrl":"/avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg","isPro":false,"fullname":"Junlin Zhou","user":"jlzhou","type":"user"},"name":"Junlin Zhou","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:49.495Z","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b07a","name":"Liwang Zhu","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b07b","user":{"_id":"632179745fc60c44fd91fc33","avatarUrl":"/avatars/37d4fefbcc19f091dccffefec9706de2.svg","isPro":false,"fullname":"zhumuzhi","user":"Z-MU-Z","type":"user"},"name":"Muzhi Zhu","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:33.817Z","hidden":false},{"_id":"698ab6fd1b2dc6b37d61b07c","user":{"_id":"673b5f24e863f1d28b402efc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png","isPro":false,"fullname":"yihongzhuang","user":"utdawn","type":"user"},"name":"Yihong Zhuang","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:55.143Z","hidden":false}],"publishedAt":"2026-02-09T14:00:07.000Z","submittedOnDailyAt":"2026-02-10T02:14:10.029Z","title":"LLaDA2.1: Speeding Up Text Diffusion via Token Editing","submittedOnDailyBy":{"_id":"673b5f24e863f1d28b402efc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png","isPro":false,"fullname":"yihongzhuang","user":"utdawn","type":"user"},"summary":"While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.","upvotes":42,"discussionId":"698ab6fd1b2dc6b37d61b07d","githubRepo":"https://github.com/inclusionAI/LLaDA2.X","githubRepoAddedBy":"user","ai_summary":"LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.","ai_keywords":["block-diffusion models","decoding speed","generation quality","Token-to-Token editing","Mask-to-Token scheme","threshold-decoding scheme","Speedy Mode","Quality Mode","Reinforcement Learning","gradient estimation","reasoning precision","instruction-following","large language diffusion models","HumanEval+","BigCodeBench","LiveCodeBench"],"githubStars":247,"organization":{"_id":"67aea5c8f086ab0f70ed97c9","name":"inclusionAI","fullname":"inclusionAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}},"publishedAt":"2026-02-09T09:00:07.000Z","title":"LLaDA2.1: Speeding Up Text Diffusion via Token Editing","summary":"While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08676.png","numComments":2,"submittedBy":{"_id":"673b5f24e863f1d28b402efc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png","fullname":"yihongzhuang","name":"utdawn","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"67aea5c8f086ab0f70ed97c9","name":"inclusionAI","fullname":"inclusionAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.06422","authors":[{"_id":"698a9c501b2dc6b37d61af2f","name":"Yunze Tong","hidden":false},{"_id":"698a9c501b2dc6b37d61af30","name":"Mushui Liu","hidden":false},{"_id":"698a9c501b2dc6b37d61af31","user":{"_id":"646efd223dd912a539e0bd46","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png","isPro":false,"fullname":"Canyu Zhao","user":"Canyu","type":"user"},"name":"Canyu Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:46.484Z","hidden":false},{"_id":"698a9c501b2dc6b37d61af32","name":"Wanggui He","hidden":false},{"_id":"698a9c501b2dc6b37d61af33","name":"Shiyi Zhang","hidden":false},{"_id":"698a9c501b2dc6b37d61af34","name":"Hongwei Zhang","hidden":false},{"_id":"698a9c501b2dc6b37d61af35","name":"Peng Zhang","hidden":false},{"_id":"698a9c501b2dc6b37d61af36","name":"Jinlong Liu","hidden":false},{"_id":"698a9c501b2dc6b37d61af37","name":"Ju Huang","hidden":false},{"_id":"698a9c501b2dc6b37d61af38","name":"Jiamang Wang","hidden":false},{"_id":"698a9c501b2dc6b37d61af39","name":"Hao Jiang","hidden":false},{"_id":"698a9c501b2dc6b37d61af3a","name":"Pipei Huang","hidden":false}],"publishedAt":"2026-02-06T06:37:10.000Z","submittedOnDailyAt":"2026-02-10T00:19:39.681Z","title":"Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO","submittedOnDailyBy":{"_id":"646efd223dd912a539e0bd46","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png","isPro":false,"fullname":"Canyu Zhao","user":"Canyu","type":"user"},"summary":"Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.","upvotes":37,"discussionId":"698a9c501b2dc6b37d61af3b","githubRepo":"https://github.com/YunzeTong/TurningPoint-GRPO","githubRepoAddedBy":"user","ai_summary":"TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.","ai_keywords":["GRPO","flow matching models","text-to-image generation","denoising steps","reward sparsity","incremental rewards","turning points","denoising trajectory","delayed impact","reward evolution"],"githubStars":13,"organization":{"_id":"61bac2af530e5c78d7b99667","name":"zju","fullname":"Zhejiang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}},"publishedAt":"2026-02-06T01:37:10.000Z","title":"Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO","summary":"Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06422.png","numComments":1,"submittedBy":{"_id":"646efd223dd912a539e0bd46","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png","fullname":"Canyu Zhao","name":"Canyu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":16,"isUserFollowing":false},"organization":{"_id":"61bac2af530e5c78d7b99667","name":"zju","fullname":"Zhejiang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09007","authors":[{"_id":"698ad8ac1b2dc6b37d61b275","user":{"_id":"65ddea8b2d26e59a5a33330f","avatarUrl":"/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg","isPro":false,"fullname":"li haodong","user":"mickyhimself","type":"user"},"name":"Haodong Li","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:28:30.775Z","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b276","name":"Jingwei Wu","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b277","name":"Quan Sun","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b278","name":"Guopeng Li","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b279","user":{"_id":"670880950e79a8b46f7ff9dd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg","isPro":false,"fullname":"Juanxi Tian","user":"Juanxi","type":"user"},"name":"Juanxi Tian","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:04:13.048Z","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b27a","name":"Huanyu Zhang","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b27b","name":"Yanlin Lai","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b27c","name":"Ruichuan An","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b27d","name":"Hongbo Peng","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b27e","user":{"_id":"65d70e775e971572da16c05b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65d70e775e971572da16c05b/8Cv71Clfk_C7k6U4yI6ln.jpeg","isPro":false,"fullname":"YuHong Dai","user":"BroAlanTaps","type":"user"},"name":"Yuhong Dai","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:04:30.092Z","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b27f","name":"Chenxi Li","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b280","name":"Chunmei Qing","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b281","name":"Jia Wang","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b282","name":"Ziyang Meng","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b283","name":"Zheng Ge","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b284","name":"Xiangyu Zhang","hidden":false},{"_id":"698ad8ac1b2dc6b37d61b285","name":"Daxin Jiang","hidden":false}],"publishedAt":"2026-02-09T18:52:02.000Z","submittedOnDailyAt":"2026-02-10T04:37:26.012Z","title":"GEBench: Benchmarking Image Generation Models as GUI Environments","submittedOnDailyBy":{"_id":"65ddea8b2d26e59a5a33330f","avatarUrl":"/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg","isPro":false,"fullname":"li haodong","user":"mickyhimself","type":"user"},"summary":"Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.","upvotes":32,"discussionId":"698ad8ad1b2dc6b37d61b286","ai_summary":"A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.","ai_keywords":["GUI generation","temporal coherence","dynamic interaction","visual fidelity","GUI-specific contexts","GEBench","GE-Score","goal achievement","interaction logic","content consistency","UI plausibility","visual quality"],"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}},"publishedAt":"2026-02-09T13:52:02.000Z","title":"GEBench: Benchmarking Image Generation Models as GUI Environments","summary":"Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09007.png","numComments":1,"submittedBy":{"_id":"65ddea8b2d26e59a5a33330f","avatarUrl":"/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg","fullname":"li haodong","name":"mickyhimself","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":0,"isUserFollowing":false},"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08439","authors":[{"_id":"698ab3e51b2dc6b37d61afaa","user":{"_id":"652965773a416e1f2173443b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg","isPro":true,"fullname":"Yuhao Dong","user":"THUdyh","type":"user"},"name":"Yuhao Dong","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:16.048Z","hidden":false},{"_id":"698ab3e51b2dc6b37d61afab","name":"Shulin Tian","hidden":false},{"_id":"698ab3e51b2dc6b37d61afac","name":"Shuai Liu","hidden":false},{"_id":"698ab3e51b2dc6b37d61afad","name":"Shuangrui Ding","hidden":false},{"_id":"698ab3e51b2dc6b37d61afae","name":"Yuhang Zang","hidden":false},{"_id":"698ab3e51b2dc6b37d61afaf","name":"Xiaoyi Dong","hidden":false},{"_id":"698ab3e51b2dc6b37d61afb0","name":"Yuhang Cao","hidden":false},{"_id":"698ab3e51b2dc6b37d61afb1","name":"Jiaqi Wang","hidden":false},{"_id":"698ab3e51b2dc6b37d61afb2","name":"Ziwei Liu","hidden":false}],"publishedAt":"2026-02-09T09:51:29.000Z","submittedOnDailyAt":"2026-02-10T02:03:30.996Z","title":"Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition","submittedOnDailyBy":{"_id":"652965773a416e1f2173443b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg","isPro":true,"fullname":"Yuhao Dong","user":"THUdyh","type":"user"},"summary":"Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.","upvotes":28,"discussionId":"698ab3e51b2dc6b37d61afb3","githubRepo":"https://github.com/dongyh20/Demo-ICL","githubRepoAddedBy":"user","ai_summary":"Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.","ai_keywords":["Multimodal Large Language Models","video understanding","in-context learning","video benchmarks","Demo-ICL-Bench","video-supervised fine-tuning","direct preference optimization"],"githubStars":25,"organization":{"_id":"62d55f243bf5e059f7ca25ba","name":"mmlab-ntu","fullname":"MMLab@NTU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}},"publishedAt":"2026-02-09T04:51:29.000Z","title":"Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition","summary":"Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08439.png","numComments":1,"submittedBy":{"_id":"652965773a416e1f2173443b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg","fullname":"Yuhao Dong","name":"THUdyh","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":50,"isUserFollowing":false},"organization":{"_id":"62d55f243bf5e059f7ca25ba","name":"mmlab-ntu","fullname":"MMLab@NTU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06025","authors":[{"_id":"698608f09c78be977c104b8c","name":"Haozhen Zhang","hidden":false},{"_id":"698608f09c78be977c104b8d","name":"Haodong Yue","hidden":false},{"_id":"698608f09c78be977c104b8e","name":"Tao Feng","hidden":false},{"_id":"698608f09c78be977c104b8f","name":"Quanyu Long","hidden":false},{"_id":"698608f09c78be977c104b90","name":"Jianzhu Bao","hidden":false},{"_id":"698608f09c78be977c104b91","name":"Bowen Jin","hidden":false},{"_id":"698608f09c78be977c104b92","name":"Weizhi Zhang","hidden":false},{"_id":"698608f09c78be977c104b93","name":"Xiao Li","hidden":false},{"_id":"698608f09c78be977c104b94","name":"Jiaxuan You","hidden":false},{"_id":"698608f09c78be977c104b95","name":"Chengwei Qin","hidden":false},{"_id":"698608f09c78be977c104b96","name":"Wenya Wang","hidden":false}],"publishedAt":"2026-02-05T18:57:09.000Z","submittedOnDailyAt":"2026-02-10T05:16:01.749Z","title":"Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory","submittedOnDailyBy":{"_id":"64f58f3468047192d6c7f335","avatarUrl":"/avatars/88be16ee80da7d2eaa0feae878375001.svg","isPro":false,"fullname":"XaiverZ","user":"XaiverZ","type":"user"},"summary":"Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.","upvotes":27,"discussionId":"698608f09c78be977c104b97","projectPage":"https://viktoraxelsen.github.io/BudgetMem/","githubRepo":"https://github.com/ViktorAxelsen/BudgetMem","githubRepoAddedBy":"user","ai_summary":"BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.","ai_keywords":["runtime memory utilization","query-aware performance-cost control","memory modules","budget tiers","lightweight router","neural policy","reinforcement learning","LoCoMo","LongMemEval","HotpotQA"],"githubStars":6,"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"}},"publishedAt":"2026-02-05T13:57:09.000Z","title":"Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory","summary":"Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06025.png","numComments":2,"submittedBy":{"_id":"64f58f3468047192d6c7f335","avatarUrl":"/avatars/88be16ee80da7d2eaa0feae878375001.svg","fullname":"XaiverZ","name":"XaiverZ","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07962","authors":[{"_id":"698a9f631b2dc6b37d61af47","name":"Weihao Zeng","hidden":false},{"_id":"698a9f631b2dc6b37d61af48","user":{"_id":"6462def82a83863b97c0611e","avatarUrl":"/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg","isPro":false,"fullname":"Yuzhen Huang","user":"yuzhen17","type":"user"},"name":"Yuzhen Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:27.902Z","hidden":false},{"_id":"698a9f631b2dc6b37d61af49","name":"Junxian He","hidden":false}],"publishedAt":"2026-02-08T13:20:39.000Z","submittedOnDailyAt":"2026-02-10T00:32:12.677Z","title":"LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth","submittedOnDailyBy":{"_id":"62751082b43ccfeef483424f","avatarUrl":"/avatars/fec83e4478e7d1731ba6033328131852.svg","isPro":false,"fullname":"WeihaoZeng","user":"AndrewZeng","type":"user"},"summary":"Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench","upvotes":23,"discussionId":"698a9f631b2dc6b37d61af4a","githubRepo":"https://github.com/hkust-nlp/LOCA-bench","githubRepoAddedBy":"user","ai_summary":"LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.","ai_keywords":["large language models","context rot","long-context benchmarks","language agents","LOCA-bench","environment states","context management strategies","agent performance"],"githubStars":22,"organization":{"_id":"647693e442e5f529745b9ba6","name":"hkust-nlp","fullname":"HKUST NLP Group"}},"publishedAt":"2026-02-08T08:20:39.000Z","title":"LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth","summary":"Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07962.png","numComments":1,"submittedBy":{"_id":"62751082b43ccfeef483424f","avatarUrl":"/avatars/fec83e4478e7d1731ba6033328131852.svg","fullname":"WeihaoZeng","name":"AndrewZeng","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"647693e442e5f529745b9ba6","name":"hkust-nlp","fullname":"HKUST NLP Group"},"isAuthorParticipating":false},{"paper":{"id":"2602.08990","authors":[{"_id":"698abe481b2dc6b37d61b12c","user":{"_id":"667cf204268f6622dac71961","avatarUrl":"/avatars/90e1928beb2a685e82e19758e4a6b7ae.svg","isPro":false,"fullname":"shiyang","user":"sY713","type":"user"},"name":"Shiyang Feng","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:21.332Z","hidden":false},{"_id":"698abe481b2dc6b37d61b12d","name":"Runmin Ma","hidden":false},{"_id":"698abe481b2dc6b37d61b12e","name":"Xiangchao Yan","hidden":false},{"_id":"698abe481b2dc6b37d61b12f","name":"Yue Fan","hidden":false},{"_id":"698abe481b2dc6b37d61b130","name":"Yusong Hu","hidden":false},{"_id":"698abe481b2dc6b37d61b131","user":{"_id":"67cfd5f4d8cb8688d7e2df22","avatarUrl":"/avatars/099139aac6d803fa47579a1152da39ef.svg","isPro":false,"fullname":"Songtao Huang","user":"huangst","type":"user"},"name":"Songtao Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:17.495Z","hidden":false},{"_id":"698abe481b2dc6b37d61b132","name":"Shuaiyu Zhang","hidden":false},{"_id":"698abe481b2dc6b37d61b133","name":"Zongsheng Cao","hidden":false},{"_id":"698abe481b2dc6b37d61b134","name":"Tianshuo Peng","hidden":false},{"_id":"698abe481b2dc6b37d61b135","user":{"_id":"64a3d1ddb3239f3e3892b24b","avatarUrl":"/avatars/7ce585f5fc1d077fb1d70cc18c4da2c1.svg","isPro":false,"fullname":"Jiakang Yuan","user":"JiakangYuan","type":"user"},"name":"Jiakang Yuan","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:19.387Z","hidden":false},{"_id":"698abe481b2dc6b37d61b136","name":"Zijie Guo","hidden":false},{"_id":"698abe481b2dc6b37d61b137","name":"Zhijie Zhong","hidden":false},{"_id":"698abe481b2dc6b37d61b138","name":"Shangheng Du","hidden":false},{"_id":"698abe481b2dc6b37d61b139","name":"Weida Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b13a","name":"Jinxin Shi","hidden":false},{"_id":"698abe481b2dc6b37d61b13b","name":"Yuhao Zhou","hidden":false},{"_id":"698abe481b2dc6b37d61b13c","name":"Xiaohan He","hidden":false},{"_id":"698abe481b2dc6b37d61b13d","name":"Zhiyin Yu","hidden":false},{"_id":"698abe481b2dc6b37d61b13e","name":"Fangchen Yu","hidden":false},{"_id":"698abe481b2dc6b37d61b13f","name":"Qihao Zheng","hidden":false},{"_id":"698abe481b2dc6b37d61b140","name":"Jiamin Wu","hidden":false},{"_id":"698abe481b2dc6b37d61b141","name":"Mianxin Liu","hidden":false},{"_id":"698abe481b2dc6b37d61b142","name":"Chi Zhang","hidden":false},{"_id":"698abe481b2dc6b37d61b143","name":"Shaowei Hou","hidden":false},{"_id":"698abe481b2dc6b37d61b144","name":"Shuya Li","hidden":false},{"_id":"698abe481b2dc6b37d61b145","name":"Yankai Jiang","hidden":false},{"_id":"698abe481b2dc6b37d61b146","name":"Wenjie Lou","hidden":false},{"_id":"698abe481b2dc6b37d61b147","name":"Lilong Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b148","name":"Zifu Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b149","name":"Jiong Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b14a","name":"Wanghan Xu","hidden":false},{"_id":"698abe481b2dc6b37d61b14b","name":"Yue Deng","hidden":false},{"_id":"698abe481b2dc6b37d61b14c","name":"Dongrui Liu","hidden":false},{"_id":"698abe481b2dc6b37d61b14d","name":"Yiheng Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b14e","name":"Wenlong Zhang","hidden":false},{"_id":"698abe481b2dc6b37d61b14f","name":"Fenghua Ling","hidden":false},{"_id":"698abe481b2dc6b37d61b150","name":"Shufei Zhang","hidden":false},{"_id":"698abe481b2dc6b37d61b151","name":"Xiaosong Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b152","name":"Shuangjia Zheng","hidden":false},{"_id":"698abe481b2dc6b37d61b153","name":"Xun Huang","hidden":false},{"_id":"698abe481b2dc6b37d61b154","name":"Siqi Sun","hidden":false},{"_id":"698abe481b2dc6b37d61b155","name":"Shuyue Hu","hidden":false},{"_id":"698abe481b2dc6b37d61b156","name":"Peng Ye","hidden":false},{"_id":"698abe481b2dc6b37d61b157","name":"Chunfeng Song","hidden":false},{"_id":"698abe481b2dc6b37d61b158","name":"Bin Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b159","name":"Conghui He","hidden":false},{"_id":"698abe481b2dc6b37d61b15a","name":"Yihao Liu","hidden":false},{"_id":"698abe481b2dc6b37d61b15b","name":"Xin Li","hidden":false},{"_id":"698abe481b2dc6b37d61b15c","name":"Qibin Hou","hidden":false},{"_id":"698abe481b2dc6b37d61b15d","name":"Tao Chen","hidden":false},{"_id":"698abe481b2dc6b37d61b15e","name":"Xiangyu Yue","hidden":false},{"_id":"698abe481b2dc6b37d61b15f","name":"Bin Wang","hidden":false},{"_id":"698abe481b2dc6b37d61b160","name":"Liang He","hidden":false},{"_id":"698abe481b2dc6b37d61b161","name":"Dahua Lin","hidden":false},{"_id":"698abe481b2dc6b37d61b162","name":"Bowen Zhou","hidden":false},{"_id":"698abe481b2dc6b37d61b163","name":"Bo Zhang","hidden":false},{"_id":"698abe481b2dc6b37d61b164","name":"Lei Bai","hidden":false}],"publishedAt":"2026-02-09T18:36:06.000Z","submittedOnDailyAt":"2026-02-10T02:55:11.896Z","title":"InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.","upvotes":18,"discussionId":"698abe481b2dc6b37d61b165","projectPage":"https://discovery.intern-ai.org.cn/home","githubRepo":"https://github.com/InternScience/InternAgent","githubRepoAddedBy":"user","ai_summary":"InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.","ai_keywords":["scientific discovery","computational modeling","laboratory experimentation","unified system","deep research","solution optimization","long horizon memory","scientific reasoning benchmarks","algorithm discovery","empirical discovery"],"githubStars":864},"publishedAt":"2026-02-09T13:36:06.000Z","title":"InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery","summary":"We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08990.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.08543","authors":[{"_id":"698abf921b2dc6b37d61b16a","name":"Yutao Zhu","hidden":false},{"_id":"698abf921b2dc6b37d61b16b","name":"Xingshuo Zhang","hidden":false},{"_id":"698abf921b2dc6b37d61b16c","name":"Maosen Zhang","hidden":false},{"_id":"698abf921b2dc6b37d61b16d","name":"Jiajie Jin","hidden":false},{"_id":"698abf921b2dc6b37d61b16e","name":"Liancheng Zhang","hidden":false},{"_id":"698abf921b2dc6b37d61b16f","name":"Xiaoshuai Song","hidden":false},{"_id":"698abf921b2dc6b37d61b170","name":"Kangzhi Zhao","hidden":false},{"_id":"698abf921b2dc6b37d61b171","name":"Wencong Zeng","hidden":false},{"_id":"698abf921b2dc6b37d61b172","name":"Ruiming Tang","hidden":false},{"_id":"698abf921b2dc6b37d61b173","name":"Han Li","hidden":false},{"_id":"698abf921b2dc6b37d61b174","name":"Ji-Rong Wen","hidden":false},{"_id":"698abf921b2dc6b37d61b175","user":{"_id":"66f0bf59e9d50ec57febf751","avatarUrl":"/avatars/be97941e60064e5dd806c6fe9db3c537.svg","isPro":false,"fullname":"Zhicheng Dou","user":"douzc","type":"user"},"name":"Zhicheng Dou","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:05:14.771Z","hidden":false}],"publishedAt":"2026-02-09T11:44:15.000Z","submittedOnDailyAt":"2026-02-10T02:50:16.823Z","title":"GISA: A Benchmark for General Information-Seeking Assistant","submittedOnDailyBy":{"_id":"625e62452a7279d3c77b5c38","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg","isPro":false,"fullname":"Yutao Zhu","user":"yutaozhu94","type":"user"},"summary":"The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.","upvotes":18,"discussionId":"698abf921b2dc6b37d61b176","ai_summary":"A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.","ai_keywords":["large language models","search agents","information-seeking assistants","benchmarks","deep reasoning","information aggregation","exact match score","complex planning"],"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}},"publishedAt":"2026-02-09T06:44:15.000Z","title":"GISA: A Benchmark for General Information-Seeking Assistant","summary":"The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08543.png","numComments":1,"submittedBy":{"_id":"625e62452a7279d3c77b5c38","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg","fullname":"Yutao Zhu","name":"yutaozhu94","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.00169","authors":[{"_id":"698b5df76052d3bed9630816","name":"Huan Zhang","hidden":false},{"_id":"698b5df76052d3bed9630817","name":"Yizhan Li","hidden":false},{"_id":"698b5df76052d3bed9630818","name":"Wenhao Huang","hidden":false},{"_id":"698b5df76052d3bed9630819","name":"Ziyu Hou","hidden":false},{"_id":"698b5df76052d3bed963081a","name":"Yu Song","hidden":false},{"_id":"698b5df76052d3bed963081b","name":"Xuye Liu","hidden":false},{"_id":"698b5df76052d3bed963081c","name":"Farshid Effaty","hidden":false},{"_id":"698b5df76052d3bed963081d","name":"Jinya Jiang","hidden":false},{"_id":"698b5df76052d3bed963081e","name":"Sifan Wu","hidden":false},{"_id":"698b5df76052d3bed963081f","name":"Qianggang Ding","hidden":false},{"_id":"698b5df76052d3bed9630820","name":"Izumi Takahara","hidden":false},{"_id":"698b5df76052d3bed9630821","name":"Leonard R. MacGillivray","hidden":false},{"_id":"698b5df76052d3bed9630822","name":"Teruyasu Mizoguchi","hidden":false},{"_id":"698b5df76052d3bed9630823","name":"Tianshu Yu","hidden":false},{"_id":"698b5df76052d3bed9630824","name":"Lizi Liao","hidden":false},{"_id":"698b5df76052d3bed9630825","name":"Yuyu Luo","hidden":false},{"_id":"698b5df76052d3bed9630826","name":"Yu Rong","hidden":false},{"_id":"698b5df76052d3bed9630827","name":"Jia Li","hidden":false},{"_id":"698b5df76052d3bed9630828","name":"Ying Diao","hidden":false},{"_id":"698b5df76052d3bed9630829","name":"Heng Ji","hidden":false},{"_id":"698b5df76052d3bed963082a","name":"Bang Liu","hidden":false}],"publishedAt":"2026-01-29T23:48:43.000Z","submittedOnDailyAt":"2026-02-10T14:10:13.591Z","title":"Towards Agentic Intelligence for Materials Science","submittedOnDailyBy":{"_id":"658f310a89145cbc7c967e35","avatarUrl":"/avatars/9e3baa66f7292bb6d42670bc3246f6b7.svg","isPro":false,"fullname":"Huan Zhang","user":"Huan0825","type":"user"},"summary":"The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.\n  To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.","upvotes":18,"discussionId":"698b5df86052d3bed963082b","ai_summary":"AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.","ai_keywords":["artificial intelligence","materials science","agentic systems","pipeline-centric view","domain adaptation","instruction tuning","goal-conditioned agents","simulation platforms","experimental platforms","credit assignment","LLMs","pattern recognition","predictive analytics","natural language processing","literature mining","materials characterization","property prediction","materials design","process optimization","computational workflows","DFT","robotic labs","passive approaches","reactive approaches","autonomous agents","safety-aware agents"]},"publishedAt":"2026-01-29T18:48:43.000Z","title":"Towards Agentic Intelligence for Materials Science","summary":"The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.\n  To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00169.png","numComments":1,"submittedBy":{"_id":"658f310a89145cbc7c967e35","avatarUrl":"/avatars/9e3baa66f7292bb6d42670bc3246f6b7.svg","fullname":"Huan Zhang","name":"Huan0825","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07055","authors":[{"_id":"698a94fc1b2dc6b37d61aecd","user":{"_id":"66d32d853c8270397e6c2f0a","avatarUrl":"/avatars/3b99da95769673f773dff9ccbdb6887d.svg","isPro":false,"fullname":"pingyue zhang","user":"williamzhangNU","type":"user"},"name":"Pingyue Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:07:28.765Z","hidden":false},{"_id":"698a94fc1b2dc6b37d61aece","name":"Zihan Huang","hidden":false},{"_id":"698a94fc1b2dc6b37d61aecf","name":"Yue Wang","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed0","name":"Jieyu Zhang","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed1","name":"Letian Xue","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed2","name":"Zihan Wang","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed3","name":"Qineng Wang","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed4","name":"Keshigeyan Chandrasegaran","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed5","name":"Ruohan Zhang","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed6","name":"Yejin Choi","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed7","name":"Ranjay Krishna","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed8","name":"Jiajun Wu","hidden":false},{"_id":"698a94fc1b2dc6b37d61aed9","name":"Li Fei-Fei","hidden":false},{"_id":"698a94fc1b2dc6b37d61aeda","name":"Manling Li","hidden":false}],"publishedAt":"2026-02-04T19:06:40.000Z","submittedOnDailyAt":"2026-02-10T03:40:15.544Z","title":"Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?","submittedOnDailyBy":{"_id":"640131b08ba76abe4b71b5d0","avatarUrl":"/avatars/2288b96a9a0ae8f584768f54e098def1.svg","isPro":false,"fullname":"Jieyu Zhang","user":"jieyuz2","type":"user"},"summary":"Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.","upvotes":17,"discussionId":"698a94fc1b2dc6b37d61aedb","projectPage":"https://theory-of-space.github.io/","githubRepo":"https://github.com/mll-lab-nu/Theory-of-Space","githubRepoAddedBy":"user","ai_summary":"Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.","ai_keywords":["spatial embodied intelligence","multimodal foundation models","active exploration","spatial belief","cognitive mapping","spatial belief probing","Active-Passive Gap","belief inertia","vision-based models","text-based agents"],"githubStars":5},"publishedAt":"2026-02-04T14:06:40.000Z","title":"Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?","summary":"Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07055.png","numComments":1,"submittedBy":{"_id":"640131b08ba76abe4b71b5d0","avatarUrl":"/avatars/2288b96a9a0ae8f584768f54e098def1.svg","fullname":"Jieyu Zhang","name":"jieyuz2","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09022","authors":[{"_id":"698ab6ec1b2dc6b37d61b023","name":"Zehan Wang","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b024","name":"Tengfei Wang","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b025","name":"Haiyu Zhang","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b026","name":"Xuhui Zuo","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b027","name":"Junta Wu","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b028","name":"Haoyuan Wang","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b029","name":"Wenqiang Sun","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b02a","name":"Zhenwei Wang","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b02b","name":"Chenjie Cao","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b02c","name":"Hengshuang Zhao","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b02d","name":"Chunchao Guo","hidden":false},{"_id":"698ab6ec1b2dc6b37d61b02e","name":"Zhou Zhao","hidden":false}],"publishedAt":"2026-02-09T18:59:47.000Z","submittedOnDailyAt":"2026-02-10T02:11:53.182Z","title":"WorldCompass: Reinforcement Learning for Long-Horizon World Models","submittedOnDailyBy":{"_id":"6425761a175bd295228311a0","avatarUrl":"/avatars/dcd0d267445563d0616d5a31b5d754b7.svg","isPro":false,"fullname":"zehan wang","user":"sleetwang6","type":"user"},"summary":"This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.","upvotes":16,"discussionId":"698ab6ec1b2dc6b37d61b02f","projectPage":"https://3d-models.hunyuan.tencent.com/world/","ai_summary":"WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.","ai_keywords":["Reinforcement Learning","world models","video generation","rollout strategy","reward functions","reward-hacking","negative-aware fine-tuning","efficiency optimizations"],"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}},"publishedAt":"2026-02-09T13:59:47.000Z","title":"WorldCompass: Reinforcement Learning for Long-Horizon World Models","summary":"This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09022.png","numComments":1,"submittedBy":{"_id":"6425761a175bd295228311a0","avatarUrl":"/avatars/dcd0d267445563d0616d5a31b5d754b7.svg","fullname":"zehan wang","name":"sleetwang6","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07075","authors":[{"_id":"698a9b8d1b2dc6b37d61af1a","user":{"_id":"648c8f9eb8f4a3542b7f065b","avatarUrl":"/avatars/7320b2b940279755c1c53454fd028594.svg","isPro":false,"fullname":"Xinwu Ye","user":"XinwuYe","type":"user"},"name":"Xinwu Ye","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:51.472Z","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af1b","name":"Yicheng Mao","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af1c","name":"Jia Zhang","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af1d","user":{"_id":"682259cdf0cb2560fcc41f4e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BwG1kcULZ3BNVSYRKEi47.png","isPro":false,"fullname":"Yoyo Liu","user":"yoyoliuuu","type":"user"},"name":"Yimeng Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:48.819Z","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af1e","name":"Li Hao","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af1f","name":"Fang Wu","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af20","name":"Zhiwei Li","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af21","name":"Yuxuan Liao","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af22","name":"Zehong Wang","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af23","name":"Zhiyuan Liu","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af24","user":{"_id":"64e314ad24809d7fa0f20fbc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bHE0w_hjDFvU-Aul0_E7g.jpeg","isPro":false,"fullname":"Zhenfei Yin","user":"JeremyYin","type":"user"},"name":"Zhenfei Yin","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:53.788Z","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af25","name":"Li Yuan","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af26","name":"Philip Torr","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af27","name":"Huan Sun","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af28","name":"Xiangxiang Zeng","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af29","name":"Mengdi Wang","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af2a","name":"Le Cong","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af2b","name":"Shenghua Gao","hidden":false},{"_id":"698a9b8d1b2dc6b37d61af2c","name":"Xiangru Tang","hidden":false}],"publishedAt":"2026-02-06T01:28:27.000Z","submittedOnDailyAt":"2026-02-10T01:22:01.665Z","title":"LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning","submittedOnDailyBy":{"_id":"63357c608adfa81faf2ac180","avatarUrl":"/avatars/ae0314c644f882251baf59b9134fd36f.svg","isPro":false,"fullname":"Xiangru Tang","user":"RTT1","type":"user"},"summary":"Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.","upvotes":15,"discussionId":"698a9b8d1b2dc6b37d61af2d","githubRepo":"https://github.com/xinwuye/LatentChem","githubRepoAddedBy":"user","ai_summary":"LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.","ai_keywords":["chemical large language models","Chain-of-Thought","latent reasoning","continuous latent space","textual generation","multi-step reasoning","ChemCoTBench","inference speedup"],"githubStars":15},"publishedAt":"2026-02-05T20:28:27.000Z","title":"LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning","summary":"Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07075.png","numComments":1,"submittedBy":{"_id":"63357c608adfa81faf2ac180","avatarUrl":"/avatars/ae0314c644f882251baf59b9134fd36f.svg","fullname":"Xiangru Tang","name":"RTT1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.08658","authors":[{"_id":"698b0ddf1b2dc6b37d61b3bd","name":"Mingzi Cao","hidden":false},{"_id":"698b0ddf1b2dc6b37d61b3be","name":"Xingwei Tan","hidden":false},{"_id":"698b0ddf1b2dc6b37d61b3bf","name":"Mahmud Akhter","hidden":false},{"_id":"698b0ddf1b2dc6b37d61b3c0","name":"Marco Valentino","hidden":false},{"_id":"698b0ddf1b2dc6b37d61b3c1","name":"Maria Liakata","hidden":false},{"_id":"698b0ddf1b2dc6b37d61b3c2","name":"Xi Wang","hidden":false},{"_id":"698b0ddf1b2dc6b37d61b3c3","name":"Nikolaos Aletras","hidden":false}],"publishedAt":"2026-02-09T13:51:48.000Z","submittedOnDailyAt":"2026-02-10T09:25:38.803Z","title":"Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models","submittedOnDailyBy":{"_id":"643d0a4d8a55b2bbf4f2a90e","avatarUrl":"/avatars/9534aaf81cbf12f015c6826b682fdb84.svg","isPro":false,"fullname":"Xingwei Tan","user":"XingweiT","type":"user"},"summary":"Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to 14.60) across realistic tasks.","upvotes":12,"discussionId":"698b0ddf1b2dc6b37d61b3c4","githubRepo":"https://github.com/voalmciaf/FR-OOD","githubRepoAddedBy":"user","ai_summary":"Research investigates how fundamental reasoning paradigms influence large language model generalization through targeted training approaches and evaluation on real-world tasks.","ai_keywords":["Large Language Model","reasoning paradigms","fine-tuning","mixture-of-experts","out-of-domain tasks","generalizability"],"githubStars":0},"publishedAt":"2026-02-09T08:51:48.000Z","title":"Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models","summary":"Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to 14.60) across realistic tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08658.png","numComments":1,"submittedBy":{"_id":"643d0a4d8a55b2bbf4f2a90e","avatarUrl":"/avatars/9534aaf81cbf12f015c6826b682fdb84.svg","fullname":"Xingwei Tan","name":"XingweiT","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.06540","authors":[{"_id":"6989512bbeecc443208d2656","name":"Yishan Li","hidden":false},{"_id":"6989512bbeecc443208d2657","user":{"_id":"64f5abc2e8f27f20a067a596","avatarUrl":"/avatars/d0eac39488fac0c9c08d76109cabaa9f.svg","isPro":false,"fullname":"cwt","user":"yiye2023","type":"user"},"name":"Wentong Chen","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:47.521Z","hidden":false},{"_id":"6989512bbeecc443208d2658","name":"Yukun Yan","hidden":false},{"_id":"6989512bbeecc443208d2659","user":{"_id":"650be89d2158362826fa6a2d","avatarUrl":"/avatars/ede42a71fbbb72289c437926909703fd.svg","isPro":false,"fullname":"Li Mingwei","user":"Ming0614","type":"user"},"name":"Mingwei Li","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:07:42.488Z","hidden":false},{"_id":"6989512bbeecc443208d265a","name":"Sen Mei","hidden":false},{"_id":"6989512bbeecc443208d265b","name":"Xiaorong Wang","hidden":false},{"_id":"6989512bbeecc443208d265c","name":"Kunpeng Liu","hidden":false},{"_id":"6989512bbeecc443208d265d","name":"Xin Cong","hidden":false},{"_id":"6989512bbeecc443208d265e","name":"Shuo Wang","hidden":false},{"_id":"6989512bbeecc443208d265f","name":"Zhong Zhang","hidden":false},{"_id":"6989512bbeecc443208d2660","name":"Yaxi Lu","hidden":false},{"_id":"6989512bbeecc443208d2661","name":"Zhenghao Liu","hidden":false},{"_id":"6989512bbeecc443208d2662","name":"Yankai Lin","hidden":false},{"_id":"6989512bbeecc443208d2663","name":"Zhiyuan Liu","hidden":false},{"_id":"6989512bbeecc443208d2664","name":"Maosong Sun","hidden":false}],"publishedAt":"2026-02-06T09:45:04.000Z","submittedOnDailyAt":"2026-02-10T00:57:57.523Z","title":"AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research","submittedOnDailyBy":{"_id":"64f5abc2e8f27f20a067a596","avatarUrl":"/avatars/d0eac39488fac0c9c08d76109cabaa9f.svg","isPro":false,"fullname":"cwt","user":"yiye2023","type":"user"},"summary":"Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.","upvotes":12,"discussionId":"6989512bbeecc443208d2665","githubRepo":"https://github.com/OpenBMB/AgentCPM/tree/main/AgentCPM-Report","githubRepoAddedBy":"user","ai_summary":"AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.","ai_keywords":["Writing As Reasoning Policy","WARP","Evidence-Based Drafting","Reasoning-Driven Deepening","Multi-Stage Agentic Training","cold-start","atomic skill RL","holistic pipeline RL","deep research agent","insight-driven analysis","plan-then-write paradigm"],"githubStars":728,"organization":{"_id":"633fe81429b5a95f6e16e34a","name":"openbmb","fullname":"OpenBMB","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"}},"publishedAt":"2026-02-06T04:45:04.000Z","title":"AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research","summary":"Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06540.png","numComments":1,"submittedBy":{"_id":"64f5abc2e8f27f20a067a596","avatarUrl":"/avatars/d0eac39488fac0c9c08d76109cabaa9f.svg","fullname":"cwt","name":"yiye2023","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"633fe81429b5a95f6e16e34a","name":"openbmb","fullname":"OpenBMB","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06454","authors":[{"_id":"698a9d531b2dc6b37d61af3d","user":{"_id":"662672eaebdfec5cfdf1d034","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662672eaebdfec5cfdf1d034/RhsKly3KvbtPkDuVnEdWb.jpeg","isPro":false,"fullname":"Jiwon Song","user":"jiwonsong","type":"user"},"name":"Jiwon Song","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:44.201Z","hidden":false},{"_id":"698a9d531b2dc6b37d61af3e","name":"Yoongon Kim","hidden":false},{"_id":"698a9d531b2dc6b37d61af3f","name":"Jae-Joon Kim","hidden":false}],"publishedAt":"2026-02-06T07:35:01.000Z","submittedOnDailyAt":"2026-02-10T00:28:46.008Z","title":"RelayGen: Intra-Generation Model Switching for Efficient Reasoning","submittedOnDailyBy":{"_id":"662672eaebdfec5cfdf1d034","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662672eaebdfec5cfdf1d034/RhsKly3KvbtPkDuVnEdWb.jpeg","isPro":false,"fullname":"Jiwon Song","user":"jiwonsong","type":"user"},"summary":"Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.","upvotes":11,"discussionId":"698a9d531b2dc6b37d61af40","githubRepo":"https://github.com/jiwonsong-dev/RelayGen","githubRepoAddedBy":"user","ai_summary":"RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.","ai_keywords":["large reasoning models","multi-step reasoning trajectories","inference-time scaling","token probability margins","segment-level control","model switching","speculative decoding","end-to-end speedup"],"githubStars":2,"organization":{"_id":"698422913a080cd2873577a4","name":"SNU-VLSI","fullname":"Seoul National University VLSI Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662672eaebdfec5cfdf1d034/7kyeWE2-6lCuFC2PG3xhz.png"}},"publishedAt":"2026-02-06T02:35:01.000Z","title":"RelayGen: Intra-Generation Model Switching for Efficient Reasoning","summary":"Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06454.png","numComments":1,"submittedBy":{"_id":"662672eaebdfec5cfdf1d034","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662672eaebdfec5cfdf1d034/RhsKly3KvbtPkDuVnEdWb.jpeg","fullname":"Jiwon Song","name":"jiwonsong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"698422913a080cd2873577a4","name":"SNU-VLSI","fullname":"Seoul National University VLSI Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662672eaebdfec5cfdf1d034/7kyeWE2-6lCuFC2PG3xhz.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08808","authors":[{"_id":"698ac3981b2dc6b37d61b1a9","name":"Yapei Chang","hidden":false},{"_id":"698ac3981b2dc6b37d61b1aa","name":"Kyle Lo","hidden":false},{"_id":"698ac3981b2dc6b37d61b1ab","name":"Mohit Iyyer","hidden":false},{"_id":"698ac3981b2dc6b37d61b1ac","name":"Luca Soldaini","hidden":false}],"publishedAt":"2026-02-09T15:47:14.000Z","submittedOnDailyAt":"2026-02-10T03:05:33.584Z","title":"How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.","upvotes":6,"discussionId":"698ac3981b2dc6b37d61b1ad","githubRepo":"https://github.com/lilakk/how2everything","githubRepoAddedBy":"user","ai_summary":"A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.","ai_keywords":["goal-conditioned procedure generation","How2Mine","How2Bench","How2Score","LLM judge","distillation","reinforcement learning","pretraining","closed loop evaluation"],"githubStars":3,"organization":{"_id":"5e70f3648ce3c604d78fe132","name":"allenai","fullname":"Ai2","avatar":"https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}},"publishedAt":"2026-02-09T10:47:14.000Z","title":"How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs","summary":"Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08808.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"organization":{"_id":"5e70f3648ce3c604d78fe132","name":"allenai","fullname":"Ai2","avatar":"https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08236","authors":[{"_id":"698aa1cf1b2dc6b37d61af4c","name":"Shoubin Yu","hidden":false},{"_id":"698aa1cf1b2dc6b37d61af4d","name":"Yue Zhang","hidden":false},{"_id":"698aa1cf1b2dc6b37d61af4e","name":"Zun Wang","hidden":false},{"_id":"698aa1cf1b2dc6b37d61af4f","name":"Jaehong Yoon","hidden":false},{"_id":"698aa1cf1b2dc6b37d61af50","name":"Huaxiu Yao","hidden":false},{"_id":"698aa1cf1b2dc6b37d61af51","name":"Mingyu Ding","hidden":false},{"_id":"698aa1cf1b2dc6b37d61af52","name":"Mohit Bansal","hidden":false}],"publishedAt":"2026-02-09T03:21:48.000Z","submittedOnDailyAt":"2026-02-10T00:42:02.821Z","title":"When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning","submittedOnDailyBy":{"_id":"63ee8fb05f1300034de097fd","avatarUrl":"/avatars/bceedb88927d8633948266503c2dd0b1.svg","isPro":true,"fullname":"Yu","user":"Shoubin","type":"user"},"summary":"Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.","upvotes":6,"discussionId":"698aa1cf1b2dc6b37d61af53","projectPage":"https://adaptive-visual-tts.github.io/","githubRepo":"https://github.com/Yui010206/Adaptive-Visual-Imagination-Control/","githubRepoAddedBy":"user","ai_summary":"Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.","ai_keywords":["Multimodal Large Language Models","visual spatial reasoning","world models","visual imagination","test-time adaptation","embodied navigation","SAT","MMSI","R2R"],"githubStars":7,"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}},"publishedAt":"2026-02-08T22:21:48.000Z","title":"When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning","summary":"Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08236.png","numComments":1,"submittedBy":{"_id":"63ee8fb05f1300034de097fd","avatarUrl":"/avatars/bceedb88927d8633948266503c2dd0b1.svg","fullname":"Yu","name":"Shoubin","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07775","authors":[{"_id":"698ac1fc1b2dc6b37d61b183","name":"Haodong Li","hidden":false},{"_id":"698ac1fc1b2dc6b37d61b184","name":"Shaoteng Liu","hidden":false},{"_id":"698ac1fc1b2dc6b37d61b185","name":"Zhe Lin","hidden":false},{"_id":"698ac1fc1b2dc6b37d61b186","name":"Manmohan Chandraker","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/5LzL-wYavOE2BRuVcFIqL.mp4"],"publishedAt":"2026-02-08T02:16:02.000Z","submittedOnDailyAt":"2026-02-10T02:58:29.492Z","title":"Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/","upvotes":6,"discussionId":"698ac1fd1b2dc6b37d61b187","projectPage":"https://rolling-sink.github.io/","ai_summary":"Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.","ai_keywords":["autoregressive video diffusion models","train-test gap","self forcing","rolling sink","AR cache maintenance","long-horizon video synthesis","temporal consistency","visual fidelity"],"organization":{"_id":"637b318856db0404b7c5a0c2","name":"adobe-research","fullname":"Adobe Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"}},"publishedAt":"2026-02-07T21:16:02.000Z","title":"Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion","summary":"Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/5LzL-wYavOE2BRuVcFIqL.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07775.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"organization":{"_id":"637b318856db0404b7c5a0c2","name":"adobe-research","fullname":"Adobe Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06694","authors":[{"_id":"69894d2abeecc443208d2610","user":{"_id":"6670d2ec92412fd464eac919","avatarUrl":"/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg","isPro":false,"fullname":"Hyochan Chong","user":"d7chong","type":"user"},"name":"Hyochan Chong","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:50.005Z","hidden":false},{"_id":"69894d2abeecc443208d2611","user":{"_id":"67f25ca8cef233be93bec839","avatarUrl":"/avatars/9e0bf25d4d7b1ab2a0a59b1bf04b0d80.svg","isPro":false,"fullname":"Dongkyu Kim","user":"dongkyu-kim","type":"user"},"name":"Dongkyu Kim","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:07:47.203Z","hidden":false},{"_id":"69894d2abeecc443208d2612","name":"Changdong Kim","hidden":false},{"_id":"69894d2abeecc443208d2613","name":"Minseop Choi","hidden":false}],"publishedAt":"2026-02-06T13:26:44.000Z","submittedOnDailyAt":"2026-02-10T00:03:08.555Z","title":"NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models","submittedOnDailyBy":{"_id":"6670d2ec92412fd464eac919","avatarUrl":"/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg","isPro":false,"fullname":"Hyochan Chong","user":"d7chong","type":"user"},"summary":"Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.","upvotes":6,"discussionId":"69894d2bbeecc443208d2614","ai_summary":"NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.","ai_keywords":["post-training quantization","low-rank binary factorization","alternating direction method of multipliers","binary quantization","sub-1-bit compression","latent binary matrices","block reconstruction","model reconstruction"],"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"}},"publishedAt":"2026-02-06T08:26:44.000Z","title":"NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models","summary":"Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06694.png","numComments":2,"submittedBy":{"_id":"6670d2ec92412fd464eac919","avatarUrl":"/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg","fullname":"Hyochan Chong","name":"d7chong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"686df54910a52f2c2cf03c06","name":"SamsungResearch","fullname":"Samsung Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.07796","authors":[{"_id":"698ab61c1b2dc6b37d61b001","user":{"_id":"659d1b4507fb049e8c3dc342","avatarUrl":"/avatars/b942aadfbd717fc9569cfc97979e2507.svg","isPro":false,"fullname":"JIatong Li","user":"LiJT","type":"user"},"name":"Jiatong Li","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:12.052Z","hidden":false},{"_id":"698ab61c1b2dc6b37d61b002","user":{"_id":"672fc8ede7c89e44c9757259","avatarUrl":"/avatars/caa0d0de519ea96992c81328c89b3843.svg","isPro":false,"fullname":"Changdae Oh","user":"changdae","type":"user"},"name":"Changdae Oh","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:10.206Z","hidden":false},{"_id":"698ab61c1b2dc6b37d61b003","name":"Hyeong Kyu Choi","hidden":false},{"_id":"698ab61c1b2dc6b37d61b004","name":"Jindong Wang","hidden":false},{"_id":"698ab61c1b2dc6b37d61b005","name":"Sharon Li","hidden":false}],"publishedAt":"2026-02-08T03:23:22.000Z","submittedOnDailyAt":"2026-02-10T04:51:37.816Z","title":"Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents","submittedOnDailyBy":{"_id":"672fc8ede7c89e44c9757259","avatarUrl":"/avatars/caa0d0de519ea96992c81328c89b3843.svg","isPro":false,"fullname":"Changdae Oh","user":"changdae","type":"user"},"summary":"Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.","upvotes":5,"discussionId":"698ab61c1b2dc6b37d61b006","ai_summary":"Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.","ai_keywords":["large language models","reasoning","agent scenarios","information disclosure","performance degradation","transparent prompting"],"organization":{"_id":"61d090ec03bc10eb8e1c2970","name":"uw-madison","fullname":"University of Wisconsin - Madison","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}},"publishedAt":"2026-02-07T22:23:22.000Z","title":"Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents","summary":"Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07796.png","numComments":1,"submittedBy":{"_id":"672fc8ede7c89e44c9757259","avatarUrl":"/avatars/caa0d0de519ea96992c81328c89b3843.svg","fullname":"Changdae Oh","name":"changdae","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"61d090ec03bc10eb8e1c2970","name":"uw-madison","fullname":"University of Wisconsin - Madison","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08145","authors":[{"_id":"698ab9ea1b2dc6b37d61b0a9","name":"Xinyu Yang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0aa","name":"Junlin Han","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0ab","name":"Rishi Bommasani","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0ac","name":"Jinqi Luo","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0ad","name":"Wenjie Qu","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0ae","name":"Wangchunshu Zhou","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0af","name":"Adel Bibi","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b0","name":"Xiyao Wang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b1","name":"Jaehong Yoon","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b2","name":"Elias Stengel-Eskin","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b3","name":"Shengbang Tong","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b4","name":"Lingfeng Shen","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b5","name":"Rafael Rafailov","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b6","name":"Runjia Li","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b7","name":"Zhaoyang Wang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b8","name":"Yiyang Zhou","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0b9","name":"Chenhang Cui","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0ba","name":"Yu Wang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0bb","name":"Wenhao Zheng","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0bc","name":"Huichi Zhou","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0bd","name":"Jindong Gu","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0be","name":"Zhaorun Chen","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0bf","name":"Peng Xia","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c0","name":"Tony Lee","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c1","name":"Thomas Zollo","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c2","name":"Vikash Sehwag","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c3","name":"Jixuan Leng","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c4","name":"Jiuhai Chen","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c5","name":"Yuxin Wen","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c6","name":"Huan Zhang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c7","name":"Zhun Deng","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c8","name":"Linjun Zhang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0c9","name":"Pavel Izmailov","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0ca","name":"Pang Wei Koh","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0cb","name":"Yulia Tsvetkov","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0cc","name":"Andrew Wilson","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0cd","name":"Jiaheng Zhang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0ce","name":"James Zou","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0cf","name":"Cihang Xie","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d0","name":"Hao Wang","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d1","name":"Philip Torr","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d2","name":"Julian McAuley","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d3","name":"David Alvarez-Melis","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d4","name":"Florian Tramr","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d5","name":"Kaidi Xu","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d6","name":"Suman Jana","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d7","name":"Chris Callison-Burch","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d8","name":"Rene Vidal","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0d9","name":"Filippos Kokkinos","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0da","name":"Mohit Bansal","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0db","name":"Beidi Chen","hidden":false},{"_id":"698ab9ea1b2dc6b37d61b0dc","name":"Huaxiu Yao","hidden":false}],"publishedAt":"2026-02-04T17:25:03.000Z","submittedOnDailyAt":"2026-02-10T02:26:53.485Z","title":"Reliable and Responsible Foundation Models: A Comprehensive Survey","submittedOnDailyBy":{"_id":"6279a4f6812ee439d9c72d3f","avatarUrl":"/avatars/a35a5674d1168d345d9fc5018485283e.svg","isPro":false,"fullname":"Jinqi Luo","user":"peterljq","type":"user"},"summary":"Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.","upvotes":4,"discussionId":"698ab9ea1b2dc6b37d61b0dd","ai_summary":"Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.","ai_keywords":["Large Language Models","Multimodal Large Language Models","Text-to-Image Models","Image-Editing Models","Video Generative Models","Artificial Intelligence-Generated Content","hallucinations","alignment","AIGC detection","bias","fairness","security","privacy","uncertainty","explainability","distribution shift","model limitations"],"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"}},"publishedAt":"2026-02-04T12:25:03.000Z","title":"Reliable and Responsible Foundation Models: A Comprehensive Survey","summary":"Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08145.png","numComments":1,"submittedBy":{"_id":"6279a4f6812ee439d9c72d3f","avatarUrl":"/avatars/a35a5674d1168d345d9fc5018485283e.svg","fullname":"Jinqi Luo","name":"peterljq","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21363","authors":[{"_id":"69899c2abeecc443208d2798","user":{"_id":"65c0ae04b7db0ab09541beed","avatarUrl":"/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg","isPro":false,"fullname":"Weidong Huang","user":"Weidong-Huang","type":"user"},"name":"Weidong Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:31:38.475Z","hidden":false},{"_id":"69899c2abeecc443208d2799","name":"Zhehan Li","hidden":false},{"_id":"69899c2abeecc443208d279a","name":"Hangxin Liu","hidden":false},{"_id":"69899c2abeecc443208d279b","name":"Biao Hou","hidden":false},{"_id":"69899c2abeecc443208d279c","name":"Yao Su","hidden":false},{"_id":"69899c2abeecc443208d279d","name":"Jingwen Zhang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65c0ae04b7db0ab09541beed/yo-qXj6GMi-WuCBDvTBbK.mp4"],"publishedAt":"2026-01-29T07:43:24.000Z","submittedOnDailyAt":"2026-02-10T00:26:35.151Z","title":"Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control","submittedOnDailyBy":{"_id":"65c0ae04b7db0ab09541beed","avatarUrl":"/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg","isPro":false,"fullname":"Weidong Huang","user":"Weidong-Huang","type":"user"},"summary":"Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.","upvotes":4,"discussionId":"69899c2bbeecc443208d279e","projectPage":"https://lift-humanoid.github.io/","githubRepo":"https://github.com/bigai-ai/LIFT-humanoid","githubRepoAddedBy":"user","ai_summary":"Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.","ai_keywords":["Proximal Policy Optimization","Soft Actor-Critic","on-policy methods","off-policy RL","model-based RL","large-scale parallel simulation","zero-shot deployment","sample efficiency","large-batch update","Update-To-Data ratio","deterministic policy","stochastic exploration","physics-informed world model"],"githubStars":52},"publishedAt":"2026-01-29T02:43:24.000Z","title":"Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control","summary":"Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65c0ae04b7db0ab09541beed/yo-qXj6GMi-WuCBDvTBbK.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21363.png","numComments":3,"submittedBy":{"_id":"65c0ae04b7db0ab09541beed","avatarUrl":"/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg","fullname":"Weidong Huang","name":"Weidong-Huang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.09003","authors":[{"_id":"698b18981b2dc6b37d61b404","name":"Yudong Wang","hidden":false},{"_id":"698b18981b2dc6b37d61b405","name":"Zixuan Fu","hidden":false},{"_id":"698b18981b2dc6b37d61b406","name":"Hengyu Zhao","hidden":false},{"_id":"698b18981b2dc6b37d61b407","name":"Chen Zhao","hidden":false},{"_id":"698b18981b2dc6b37d61b408","name":"Chuyue Zhou","hidden":false},{"_id":"698b18981b2dc6b37d61b409","name":"Xinle Lin","hidden":false},{"_id":"698b18981b2dc6b37d61b40a","name":"Hongya Lyu","hidden":false},{"_id":"698b18981b2dc6b37d61b40b","name":"Shuaikang Xue","hidden":false},{"_id":"698b18981b2dc6b37d61b40c","name":"Yi Yi","hidden":false},{"_id":"698b18981b2dc6b37d61b40d","name":"Yingjiao Wang","hidden":false},{"_id":"698b18981b2dc6b37d61b40e","name":"Zhi Zheng","hidden":false},{"_id":"698b18981b2dc6b37d61b40f","name":"Yuzhou Zhang","hidden":false},{"_id":"698b18981b2dc6b37d61b410","name":"Jie Zhou","hidden":false},{"_id":"698b18981b2dc6b37d61b411","name":"Chaojun Xiao","hidden":false},{"_id":"698b18981b2dc6b37d61b412","name":"Xu Han","hidden":false},{"_id":"698b18981b2dc6b37d61b413","name":"Zhiyuan Liu","hidden":false},{"_id":"698b18981b2dc6b37d61b414","name":"Maosong Sun","hidden":false}],"publishedAt":"2026-02-09T18:47:51.000Z","submittedOnDailyAt":"2026-02-10T09:08:48.179Z","title":"Data Science and Technology Towards AGI Part I: Tiered Data Management","submittedOnDailyBy":{"_id":"63be286fb3b8c44f8cecc16f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63be286fb3b8c44f8cecc16f/1CIkfEKoTnBYdYDSuQ8AT.jpeg","isPro":false,"fullname":"Yudong Wang","user":"BigDong","type":"user"},"summary":"The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.","upvotes":3,"discussionId":"698b18981b2dc6b37d61b415","projectPage":"https://ultradata.openbmb.cn/","ai_summary":"Large language models are increasingly guiding data management processes through a tiered framework that optimizes data quality, cost, and training efficiency across different stages of model development.","ai_keywords":["large language models","data management","data quality","training efficiency","tiered data management","data curation","model-guided data refinement","pre-training","mid-training","alignment"],"organization":{"_id":"633fe81429b5a95f6e16e34a","name":"openbmb","fullname":"OpenBMB","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"}},"publishedAt":"2026-02-09T13:47:51.000Z","title":"Data Science and Technology Towards AGI Part I: Tiered Data Management","summary":"The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09003.png","numComments":1,"submittedBy":{"_id":"63be286fb3b8c44f8cecc16f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63be286fb3b8c44f8cecc16f/1CIkfEKoTnBYdYDSuQ8AT.jpeg","fullname":"Yudong Wang","name":"BigDong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"633fe81429b5a95f6e16e34a","name":"openbmb","fullname":"OpenBMB","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08961","authors":[{"_id":"698ab60e1b2dc6b37d61aff8","user":{"_id":"6697ac8427e4e21a3a92da27","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png","isPro":false,"fullname":"Ruijie Zhu","user":"RuijieZhu","type":"user"},"name":"Ruijie Zhu","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:13.882Z","hidden":false},{"_id":"698ab60e1b2dc6b37d61aff9","name":"Jiahao Lu","hidden":false},{"_id":"698ab60e1b2dc6b37d61affa","name":"Wenbo Hu","hidden":false},{"_id":"698ab60e1b2dc6b37d61affb","name":"Xiaoguang Han","hidden":false},{"_id":"698ab60e1b2dc6b37d61affc","name":"Jianfei Cai","hidden":false},{"_id":"698ab60e1b2dc6b37d61affd","name":"Ying Shan","hidden":false},{"_id":"698ab60e1b2dc6b37d61affe","name":"Chuanxia Zheng","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/rpQaSmVHqXA-1zkmqY0dK.qt"],"publishedAt":"2026-02-09T17:58:12.000Z","submittedOnDailyAt":"2026-02-10T03:56:29.177Z","title":"MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE","submittedOnDailyBy":{"_id":"6697ac8427e4e21a3a92da27","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png","isPro":false,"fullname":"Ruijie Zhu","user":"RuijieZhu","type":"user"},"summary":"We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page","upvotes":3,"discussionId":"698ab60e1b2dc6b37d61afff","projectPage":"https://ruijiezhu94.github.io/MotionCrafter_Page","githubRepo":"https://github.com/TencentARC/MotionCrafter","githubRepoAddedBy":"user","ai_summary":"MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.","ai_keywords":["video diffusion","4D geometry","dense motion estimation","3D point maps","3D scene flows","shared coordinate system","4D VAE","RGB VAE latents","data normalization","VAE training strategy","diffusion priors"],"githubStars":30,"organization":{"_id":"60e3f7f641ca131919975fe5","name":"TencentARC","fullname":"ARC Lab, Tencent PCG","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"}},"publishedAt":"2026-02-09T12:58:12.000Z","title":"MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE","summary":"We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/rpQaSmVHqXA-1zkmqY0dK.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08961.png","numComments":1,"submittedBy":{"_id":"6697ac8427e4e21a3a92da27","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png","fullname":"Ruijie Zhu","name":"RuijieZhu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"60e3f7f641ca131919975fe5","name":"TencentARC","fullname":"ARC Lab, Tencent PCG","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08829","authors":[{"_id":"698ab50a1b2dc6b37d61afdb","name":"Hao Peng","hidden":false},{"_id":"698ab50a1b2dc6b37d61afdc","name":"Yunjia Qi","hidden":false},{"_id":"698ab50a1b2dc6b37d61afdd","name":"Xiaozhi Wang","hidden":false},{"_id":"698ab50a1b2dc6b37d61afde","name":"Zijun Yao","hidden":false},{"_id":"698ab50a1b2dc6b37d61afdf","name":"Lei Hou","hidden":false},{"_id":"698ab50a1b2dc6b37d61afe0","name":"Juanzi Li","hidden":false}],"publishedAt":"2026-02-09T16:00:30.000Z","submittedOnDailyAt":"2026-02-10T02:25:20.733Z","title":"WildReward: Learning Reward Models from In-the-Wild Human Interactions","submittedOnDailyBy":{"_id":"625a5446f1063e7085d5178a","avatarUrl":"/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg","isPro":false,"fullname":"Hao Peng","user":"Wesleythu","type":"user"},"summary":"Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.","upvotes":3,"discussionId":"698ab50a1b2dc6b37d61afe1","ai_summary":"WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.","ai_keywords":["reward models","large language models","ordinal regression","in-the-wild interactions","user feedback","online DPO training"],"organization":{"_id":"64db4fc57266618e854318f4","name":"THU-KEG","fullname":"Knowledge Engineer Group @ Tsinghua University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/648c4b46e549be47af1aafcd/5atqdE9AUWvYAHm9FNkG_.png"}},"publishedAt":"2026-02-09T11:00:30.000Z","title":"WildReward: Learning Reward Models from In-the-Wild Human Interactions","summary":"Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08829.png","numComments":1,"submittedBy":{"_id":"625a5446f1063e7085d5178a","avatarUrl":"/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg","fullname":"Hao Peng","name":"Wesleythu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"64db4fc57266618e854318f4","name":"THU-KEG","fullname":"Knowledge Engineer Group @ Tsinghua University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/648c4b46e549be47af1aafcd/5atqdE9AUWvYAHm9FNkG_.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06445","authors":[{"_id":"6989adb6beecc443208d27de","user":{"_id":"65c0ae04b7db0ab09541beed","avatarUrl":"/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg","isPro":false,"fullname":"Weidong Huang","user":"Weidong-Huang","type":"user"},"name":"Weidong Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:31:28.409Z","hidden":false},{"_id":"6989adb6beecc443208d27df","name":"Jingwen Zhang","hidden":false},{"_id":"6989adb6beecc443208d27e0","name":"Jiongye Li","hidden":false},{"_id":"6989adb6beecc443208d27e1","name":"Shibowen Zhang","hidden":false},{"_id":"6989adb6beecc443208d27e2","name":"Jiayang Wu","hidden":false},{"_id":"6989adb6beecc443208d27e3","name":"Jiayi Wang","hidden":false},{"_id":"6989adb6beecc443208d27e4","name":"Hangxin Liu","hidden":false},{"_id":"6989adb6beecc443208d27e5","name":"Yaodong Yang","hidden":false},{"_id":"6989adb6beecc443208d27e6","name":"Yao Su","hidden":false}],"publishedAt":"2026-02-06T07:14:43.000Z","submittedOnDailyAt":"2026-02-10T00:27:21.524Z","title":"ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking","submittedOnDailyBy":{"_id":"65c0ae04b7db0ab09541beed","avatarUrl":"/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg","isPro":false,"fullname":"Weidong Huang","user":"Weidong-Huang","type":"user"},"summary":"Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.","upvotes":3,"discussionId":"6989adb6beecc443208d27e7","projectPage":"https://sites.google.com/view/eco-humanoid","githubRepo":"https://github.com/bigai-ai/ECO-humanoid","githubRepoAddedBy":"user","ai_summary":"Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.","ai_keywords":["model predictive control","reinforcement learning","constrained optimization","Lagrangian method","energy-constrained optimization","humanoid robotics","sim-to-sim transfer","sim-to-real transfer"],"githubStars":0},"publishedAt":"2026-02-06T02:14:43.000Z","title":"ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking","summary":"Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06445.png","numComments":1,"submittedBy":{"_id":"65c0ae04b7db0ab09541beed","avatarUrl":"/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg","fullname":"Weidong Huang","name":"Weidong-Huang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.07803","authors":[{"_id":"698acb2d1b2dc6b37d61b1f1","name":"Jiale Qian","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f2","name":"Hao Meng","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f3","name":"Tian Zheng","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f4","name":"Pengcheng Zhu","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f5","name":"Haopeng Lin","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f6","name":"Yuhang Dai","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f7","name":"Hanke Xie","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f8","name":"Wenxiao Cao","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1f9","name":"Ruixuan Shang","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1fa","name":"Jun Wu","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1fb","name":"Hongmei Liu","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1fc","name":"Hanlin Wen","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1fd","name":"Jian Zhao","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1fe","name":"Zhonglin Jiang","hidden":false},{"_id":"698acb2d1b2dc6b37d61b1ff","name":"Yong Chen","hidden":false},{"_id":"698acb2d1b2dc6b37d61b200","name":"Shunshun Yin","hidden":false},{"_id":"698acb2d1b2dc6b37d61b201","name":"Ming Tao","hidden":false},{"_id":"698acb2d1b2dc6b37d61b202","name":"Jianguo Wei","hidden":false},{"_id":"698acb2d1b2dc6b37d61b203","name":"Lei Xie","hidden":false},{"_id":"698acb2d1b2dc6b37d61b204","name":"Xinsheng Wang","hidden":false}],"publishedAt":"2026-02-08T03:51:23.000Z","submittedOnDailyAt":"2026-02-10T03:38:46.400Z","title":"SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis","submittedOnDailyBy":{"_id":"6564364649f816b798ac3b4e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6564364649f816b798ac3b4e/4cOQ-YtP4UQF94r9nel3B.jpeg","isPro":false,"fullname":"Xinsheng Wang","user":"Xinsheng-Wang","type":"user"},"summary":"While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.","upvotes":2,"discussionId":"698acb2e1b2dc6b37d61b205","ai_summary":"A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.","ai_keywords":["singing voice synthesis","symbolic musical scores","melodic representations","zero-shot generalization","speech synthesis","Mandarin Chinese","English","Cantonese"],"organization":{"_id":"68c932c483517d3b908445b1","name":"Soul-AILab","fullname":"Soul-AILab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6564364649f816b798ac3b4e/hdbm42BRO080btTwwQTD1.jpeg"}},"publishedAt":"2026-02-07T22:51:23.000Z","title":"SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis","summary":"While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07803.png","numComments":1,"submittedBy":{"_id":"6564364649f816b798ac3b4e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6564364649f816b798ac3b4e/4cOQ-YtP4UQF94r9nel3B.jpeg","fullname":"Xinsheng Wang","name":"Xinsheng-Wang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10,"isUserFollowing":false},"organization":{"_id":"68c932c483517d3b908445b1","name":"Soul-AILab","fullname":"Soul-AILab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6564364649f816b798ac3b4e/hdbm42BRO080btTwwQTD1.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.06942","authors":[{"_id":"6989c3f4beecc443208d2847","name":"Duygu Altinok","hidden":false}],"publishedAt":"2026-02-06T18:41:14.000Z","submittedOnDailyAt":"2026-02-10T10:49:11.092Z","title":"Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay","submittedOnDailyBy":{"_id":"635c4736177df3f16e99db63","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/635c4736177df3f16e99db63/wP8zIiCLNLyVEJCwPr51-.jpeg","isPro":true,"fullname":"Duygu Altinok","user":"BayanDuygu","type":"user"},"summary":"Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.","upvotes":2,"discussionId":"6989c3f4beecc443208d2848","ai_summary":"A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.","ai_keywords":["tokenization","morphologically rich languages","subword tokenization","WordPiece","morphology level","character-level tokenization","intrinsic diagnostics","extrinsic evaluation","NLI","STS","sentiment analysis","NER","POS","dependency parsing","morphology-aware diagnostic toolkit","F1 score","lemma atomicity","segmentation indices","CER","WER","continuation rates","affix-type coverage","token-level atomicity"],"organization":{"_id":"635e8862398ff343c4f60ff3","name":"turkish-nlp-suite","fullname":"Turkish NLP Suite","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1675973012506-635c4736177df3f16e99db63.png"}},"publishedAt":"2026-02-06T13:41:14.000Z","title":"Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay","summary":"Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06942.png","numComments":1,"submittedBy":{"_id":"635c4736177df3f16e99db63","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/635c4736177df3f16e99db63/wP8zIiCLNLyVEJCwPr51-.jpeg","fullname":"Duygu Altinok","name":"BayanDuygu","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":46,"isUserFollowing":false},"organization":{"_id":"635e8862398ff343c4f60ff3","name":"turkish-nlp-suite","fullname":"Turkish NLP Suite","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1675973012506-635c4736177df3f16e99db63.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06600","authors":[{"_id":"6989bbc2beecc443208d281f","user":{"_id":"65fd6e54d7ef9a5a419608da","avatarUrl":"/avatars/b7de1aad32faa5b3b68e4dd7825d4924.svg","isPro":false,"fullname":"Hao","user":"larry2210","type":"user"},"name":"Zhuoyuan Hao","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:31:21.469Z","hidden":false},{"_id":"6989bbc2beecc443208d2820","name":"Zhuo Li","hidden":false},{"_id":"6989bbc2beecc443208d2821","name":"Wu Li","hidden":false},{"_id":"6989bbc2beecc443208d2822","name":"Fangming Liu","hidden":false},{"_id":"6989bbc2beecc443208d2823","name":"Min Zhang","hidden":false},{"_id":"6989bbc2beecc443208d2824","name":"Jing Li","hidden":false}],"publishedAt":"2026-02-06T10:53:26.000Z","submittedOnDailyAt":"2026-02-10T08:36:31.312Z","title":"Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning","submittedOnDailyBy":{"_id":"65fd6e54d7ef9a5a419608da","avatarUrl":"/avatars/b7de1aad32faa5b3b68e4dd7825d4924.svg","isPro":false,"fullname":"Hao","user":"larry2210","type":"user"},"summary":"Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the spontaneous repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the Echo of Prompt (EOP), as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the Echo Likelihood Gap L as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop Echo-Distilled SFT (ED-SFT) to instill an ``echo-then-reason'' pattern through supervised finetuning, and Echoic Prompting (EP) to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an attention refocusing mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.","upvotes":2,"discussionId":"6989bbc3beecc443208d2825","githubRepo":"https://github.com/hhh2210/echoes-as-anchors","githubRepoAddedBy":"user","ai_summary":"Large reasoning models exhibit spontaneous question repetition patterns that can be formalized and leveraged to improve computational efficiency and accuracy through echo-aware training and prompting techniques.","ai_keywords":["large reasoning models","self-consistency","parallel thinking","thinking tokens","echo of prompt","echo removal","rejection-based conditioning","echo likelihood gap","echo-distilled sft","echoic prompting","attention refocusing","likelihood analysis","layer-wise attention"],"githubStars":1},"publishedAt":"2026-02-06T05:53:26.000Z","title":"Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning","summary":"Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the spontaneous repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the Echo of Prompt (EOP), as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the Echo Likelihood Gap L as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop Echo-Distilled SFT (ED-SFT) to instill an ``echo-then-reason'' pattern through supervised finetuning, and Echoic Prompting (EP) to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an attention refocusing mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06600.png","numComments":1,"submittedBy":{"_id":"65fd6e54d7ef9a5a419608da","avatarUrl":"/avatars/b7de1aad32faa5b3b68e4dd7825d4924.svg","fullname":"Hao","name":"larry2210","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.08818","authors":[{"_id":"698aef421b2dc6b37d61b315","name":"Annemette Brok Pirchert","hidden":false},{"_id":"698aef421b2dc6b37d61b316","user":{"_id":"65dee4eb2df2dd7ceecb5850","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65dee4eb2df2dd7ceecb5850/WZCx-1X-7944O-BX7h29L.jpeg","isPro":false,"fullname":"Jacob Nielsen","user":"JacobBITLABS","type":"user"},"name":"Jacob Nielsen","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:04:06.873Z","hidden":false},{"_id":"698aef421b2dc6b37d61b317","name":"Mogens Henrik From","hidden":false},{"_id":"698aef421b2dc6b37d61b318","name":"Lukas Galke Poech","hidden":false},{"_id":"698aef421b2dc6b37d61b319","name":"Peter Schneider-Kamp","hidden":false}],"publishedAt":"2026-02-09T15:54:29.000Z","submittedOnDailyAt":"2026-02-10T06:14:56.471Z","title":"FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models","submittedOnDailyBy":{"_id":"65dee4eb2df2dd7ceecb5850","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65dee4eb2df2dd7ceecb5850/WZCx-1X-7944O-BX7h29L.jpeg","isPro":false,"fullname":"Jacob Nielsen","user":"JacobBITLABS","type":"user"},"summary":"Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating 6 experts with ranks 2^0 to 2^{14} resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across 120 tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score 47.18) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score 45.46) at less than one third the parameters (10.75B for FlexMoRE vs. 33.27B for FlexOlmo). All code will be made available.","upvotes":1,"discussionId":"698aef421b2dc6b37d61b31a","ai_summary":"FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters.","ai_keywords":["mixture-of-experts architectures","low-rank adapters","rank-heterogenous experts","downstream task performance","regression analysis","parameter efficiency"],"organization":{"_id":"698adcd164acbfdb47438aba","name":"schneiderkamplab","fullname":"Schneider-Kamp Lab"}},"publishedAt":"2026-02-09T10:54:29.000Z","title":"FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models","summary":"Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating 6 experts with ranks 2^0 to 2^{14} resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across 120 tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score 47.18) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score 45.46) at less than one third the parameters (10.75B for FlexMoRE vs. 33.27B for FlexOlmo). All code will be made available.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08818.png","numComments":1,"submittedBy":{"_id":"65dee4eb2df2dd7ceecb5850","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65dee4eb2df2dd7ceecb5850/WZCx-1X-7944O-BX7h29L.jpeg","fullname":"Jacob Nielsen","name":"JacobBITLABS","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"698adcd164acbfdb47438aba","name":"schneiderkamplab","fullname":"Schneider-Kamp Lab"},"isAuthorParticipating":true},{"paper":{"id":"2602.07491","authors":[{"_id":"698af8e01b2dc6b37d61b34a","name":"Isabella A. Stewart","hidden":false},{"_id":"698af8e01b2dc6b37d61b34b","name":"Tarjei Paule Hage","hidden":false},{"_id":"698af8e01b2dc6b37d61b34c","name":"Yu-Chuan Hsu","hidden":false},{"_id":"698af8e01b2dc6b37d61b34d","user":{"_id":"623ce1c6b66fedf374859fe7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg","isPro":true,"fullname":"Markus Buehler","user":"mjbuehler","type":"user"},"name":"Markus J. Buehler","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:27:39.258Z","hidden":false}],"publishedAt":"2026-02-07T10:50:34.000Z","submittedOnDailyAt":"2026-02-10T06:53:23.570Z","title":"GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design","submittedOnDailyBy":{"_id":"623ce1c6b66fedf374859fe7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg","isPro":true,"fullname":"Markus Buehler","user":"mjbuehler","type":"user"},"summary":"Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.","upvotes":1,"discussionId":"698af8e11b2dc6b37d61b34e","ai_summary":"A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives.","ai_keywords":["large language models","multi-agent framework","knowledge graphs","materials science","problem decomposition","evidence retrieval","graph traversal","hypothesis generation","ablation studies","domain-spanning reasoning","sustainable substitutes","PFAS chemicals","tribological performance","thermal stability","chemical resistance","biocompatibility"],"organization":{"_id":"6552174a04d4294fa01d5375","name":"lamm-mit","fullname":"LAMM: MIT Laboratory for Atomistic and Molecular Mechanics","avatar":"https://cdn-uploads.huggingface.co/production/uploads/623ce1c6b66fedf374859fe7/Qw9Np_ESnnWWUm7HXleHR.jpeg"}},"publishedAt":"2026-02-07T05:50:34.000Z","title":"GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design","summary":"Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07491.png","numComments":1,"submittedBy":{"_id":"623ce1c6b66fedf374859fe7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg","fullname":"Markus Buehler","name":"mjbuehler","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":30,"isUserFollowing":false},"organization":{"_id":"6552174a04d4294fa01d5375","name":"lamm-mit","fullname":"LAMM: MIT Laboratory for Atomistic and Molecular Mechanics","avatar":"https://cdn-uploads.huggingface.co/production/uploads/623ce1c6b66fedf374859fe7/Qw9Np_ESnnWWUm7HXleHR.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.07150","authors":[{"_id":"698b02911b2dc6b37d61b36b","name":"Bjarni Haukur Bjarnason","hidden":false},{"_id":"698b02911b2dc6b37d61b36c","name":"Andr Silva","hidden":false},{"_id":"698b02911b2dc6b37d61b36d","name":"Martin Monperrus","hidden":false}],"publishedAt":"2026-02-06T19:49:13.000Z","submittedOnDailyAt":"2026-02-10T07:35:00.507Z","title":"On Randomness in Agentic Evals","submittedOnDailyBy":{"_id":"6403a2c7929304a3c80236cd","avatarUrl":"/avatars/82a2c90bf642ebdb45d820a088395ac4.svg","isPro":false,"fullname":"Andr Silva","user":"andre15silva","type":"user"},"summary":"Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.","upvotes":1,"discussionId":"698b02911b2dc6b37d61b36e","ai_summary":"Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.","ai_keywords":["pass@1","pass@k","pass^k","SWE-Bench-Verified","agentic trajectories","statistical power analysis","evaluation variance","solution strategies"],"organization":{"_id":"64be91abf8f28a19b0ee8c0e","name":"ASSERT-KTH","fullname":"ASSERT | Research group at KTH Royal Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6403a2c7929304a3c80236cd/D-KoLU_3IEJNDHVX-u67T.png"}},"publishedAt":"2026-02-06T14:49:13.000Z","title":"On Randomness in Agentic Evals","summary":"Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07150.png","numComments":1,"submittedBy":{"_id":"6403a2c7929304a3c80236cd","avatarUrl":"/avatars/82a2c90bf642ebdb45d820a088395ac4.svg","fullname":"Andr Silva","name":"andre15silva","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"64be91abf8f28a19b0ee8c0e","name":"ASSERT-KTH","fullname":"ASSERT | Research group at KTH Royal Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6403a2c7929304a3c80236cd/D-KoLU_3IEJNDHVX-u67T.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07120","authors":[{"_id":"698add0e1b2dc6b37d61b2b0","name":"Jacqueline He","hidden":false},{"_id":"698add0e1b2dc6b37d61b2b1","name":"Jonathan Hayase","hidden":false},{"_id":"698add0e1b2dc6b37d61b2b2","name":"Wen-tau Yih","hidden":false},{"_id":"698add0e1b2dc6b37d61b2b3","name":"Sewoong Oh","hidden":false},{"_id":"698add0e1b2dc6b37d61b2b4","name":"Luke Zettlemoyer","hidden":false},{"_id":"698add0e1b2dc6b37d61b2b5","name":"Pang Wei Koh","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/627c52e51b02d7b5bf671dca/lWKUhsTok_dDHB0foWJe3.gif"],"publishedAt":"2026-02-06T19:00:14.000Z","submittedOnDailyAt":"2026-02-10T16:01:28.435Z","title":"Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model","submittedOnDailyBy":{"_id":"627c52e51b02d7b5bf671dca","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666240522285-627c52e51b02d7b5bf671dca.png","isPro":false,"fullname":"Jacqueline He","user":"jacquelinehe","type":"user"},"summary":"Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored_{Byte} Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored_{Byte} Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.","upvotes":1,"discussionId":"698add0e1b2dc6b37d61b2b6","projectPage":"https://tinyurl.com/anchored-decoding-demo","githubRepo":"https://github.com/jacqueline-he/anchored-decoding","githubRepoAddedBy":"user","ai_summary":"Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility.","ai_keywords":["Anchored Decoding","language models","verbatim copying","safe LM","information budget","per-step constraints","Pareto frontier","TinyComma 1.8B","Anchored$_{\\mathrm{Byte}}$ Decoding","ByteSampler framework"],"githubStars":1,"organization":{"_id":"646d31f04e59b82995e17a8a","name":"uwnlp","fullname":"University of Washington NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61a8f52d89ea472e600a0a10/r5imF4kxbsc1l2F1lqz2V.png"}},"publishedAt":"2026-02-06T14:00:14.000Z","title":"Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model","summary":"Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored_{Byte} Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored_{Byte} Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/627c52e51b02d7b5bf671dca/lWKUhsTok_dDHB0foWJe3.gif"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07120.png","numComments":1,"submittedBy":{"_id":"627c52e51b02d7b5bf671dca","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666240522285-627c52e51b02d7b5bf671dca.png","fullname":"Jacqueline He","name":"jacquelinehe","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"646d31f04e59b82995e17a8a","name":"uwnlp","fullname":"University of Washington NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61a8f52d89ea472e600a0a10/r5imF4kxbsc1l2F1lqz2V.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07090","authors":[{"_id":"698ac44a1b2dc6b37d61b1af","name":"Yu-Che Tsai","hidden":false},{"_id":"698ac44a1b2dc6b37d61b1b0","name":"Hsiang Hsiao","hidden":false},{"_id":"698ac44a1b2dc6b37d61b1b1","name":"Kuan-Yu Chen","hidden":false},{"_id":"698ac44a1b2dc6b37d61b1b2","name":"Shou-De Lin","hidden":false}],"publishedAt":"2026-02-06T09:28:55.000Z","submittedOnDailyAt":"2026-02-10T03:11:37.804Z","title":"Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks","submittedOnDailyBy":{"_id":"64970396bf36c2cc700ed3cb","avatarUrl":"/avatars/dbbf82c204c34afe62f8f1e73e9d1818.svg","isPro":false,"fullname":"Yu-Che Tsai","user":"Roytsai27","type":"user"},"summary":"Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.","upvotes":1,"discussionId":"698ac44a1b2dc6b37d61b1b3","ai_summary":"SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.","ai_keywords":["text embeddings","differential privacy","embedding inversion attacks","concept-specific privacy protection","differentiable mask learning","Mahalanobis mechanism","elliptical noise","privacy-sensitive dimensions","downstream performance"]},"publishedAt":"2026-02-06T04:28:55.000Z","title":"Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks","summary":"Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07090.png","numComments":1,"submittedBy":{"_id":"64970396bf36c2cc700ed3cb","avatarUrl":"/avatars/dbbf82c204c34afe62f8f1e73e9d1818.svg","fullname":"Yu-Che Tsai","name":"Roytsai27","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07080","authors":[{"_id":"698a95ec1b2dc6b37d61aee3","name":"Yicheng He","hidden":false},{"_id":"698a95ec1b2dc6b37d61aee4","name":"Zheng Zhao","hidden":false},{"_id":"698a95ec1b2dc6b37d61aee5","name":"Zhou Kaiyu","hidden":false},{"_id":"698a95ec1b2dc6b37d61aee6","name":"Bryan Dai","hidden":false},{"_id":"698a95ec1b2dc6b37d61aee7","name":"Jie Fu","hidden":false},{"_id":"698a95ec1b2dc6b37d61aee8","name":"Yonghui Yang","hidden":false}],"publishedAt":"2026-02-06T03:49:15.000Z","submittedOnDailyAt":"2026-02-10T14:14:45.057Z","title":"CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs","submittedOnDailyBy":{"_id":"6621e88fa11ce46061d25a16","avatarUrl":"/avatars/4a9a965a2d0f33e2855d2909a3e162bc.svg","isPro":false,"fullname":"Yicheng He","user":"bruno888","type":"user"},"summary":"Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.","upvotes":1,"discussionId":"698a95ec1b2dc6b37d61aee9","ai_summary":"LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.","ai_keywords":["mechanistic interpretability","code verification","neural dynamics","algorithmic trajectory","line-level attribution graphs","residual flows","structural signatures","logical validity","computational circuits","topological features","causal interventions"],"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}},"publishedAt":"2026-02-05T22:49:15.000Z","title":"CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs","summary":"Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07080.png","numComments":1,"submittedBy":{"_id":"6621e88fa11ce46061d25a16","avatarUrl":"/avatars/4a9a965a2d0f33e2855d2909a3e162bc.svg","fullname":"Yicheng He","name":"bruno888","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.05929","authors":[{"_id":"698b3b891b2dc6b37d61b4cc","name":"Jian Chen","hidden":false},{"_id":"698b3b891b2dc6b37d61b4cd","name":"Zhuoran Wang","hidden":false},{"_id":"698b3b891b2dc6b37d61b4ce","name":"Jiayu Qin","hidden":false},{"_id":"698b3b891b2dc6b37d61b4cf","name":"Ming Li","hidden":false},{"_id":"698b3b891b2dc6b37d61b4d0","name":"Meng Wang","hidden":false},{"_id":"698b3b891b2dc6b37d61b4d1","name":"Changyou Chen","hidden":false},{"_id":"698b3b891b2dc6b37d61b4d2","name":"Yin Chen","hidden":false},{"_id":"698b3b891b2dc6b37d61b4d3","name":"Qizhen Weng","hidden":false},{"_id":"698b3b891b2dc6b37d61b4d4","name":"Yirui Liu","hidden":false}],"publishedAt":"2026-02-05T17:41:57.000Z","submittedOnDailyAt":"2026-02-10T11:39:33.212Z","title":"KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs","submittedOnDailyBy":{"_id":"64593836c16ecb4815e082ce","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64593836c16ecb4815e082ce/cHv6bOowt4N7n-dweNozO.jpeg","isPro":false,"fullname":"Jian Chen","user":"puar-playground","type":"user"},"summary":"Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.","upvotes":1,"discussionId":"698b3b8a1b2dc6b37d61b4d5","ai_summary":"KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.","ai_keywords":["KV-cache","SVD-based method","low-rank approximation","Frobenius norm","gradient-free","incremental evaluation","Normalized Effective Rank","compressibility","autoregressive decoding","GPU memory bandwidth","KV-cache compression","dataset-level evaluation","layer-wise evaluation"],"organization":{"_id":"65990dffc6457161ca82d1f1","name":"Tele-AI","fullname":"Tele-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64368160eef1f55654ab21dc/sLbwBxn-8iokBAxwk0vpB.png"}},"publishedAt":"2026-02-05T12:41:57.000Z","title":"KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs","summary":"Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05929.png","numComments":1,"submittedBy":{"_id":"64593836c16ecb4815e082ce","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64593836c16ecb4815e082ce/cHv6bOowt4N7n-dweNozO.jpeg","fullname":"Jian Chen","name":"puar-playground","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"65990dffc6457161ca82d1f1","name":"Tele-AI","fullname":"Tele-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64368160eef1f55654ab21dc/sLbwBxn-8iokBAxwk0vpB.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.05708","authors":[{"_id":"6989dffebeecc443208d28de","name":"Chuangtao Ma","hidden":false},{"_id":"6989dffebeecc443208d28df","name":"Zeyu Zhang","hidden":false},{"_id":"6989dffebeecc443208d28e0","name":"Arijit Khan","hidden":false},{"_id":"6989dffebeecc443208d28e1","name":"Sebastian Schelter","hidden":false},{"_id":"6989dffebeecc443208d28e2","name":"Paul Groth","hidden":false}],"publishedAt":"2026-02-05T14:33:00.000Z","submittedOnDailyAt":"2026-02-10T10:30:25.979Z","title":"Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration","submittedOnDailyBy":{"_id":"66c8b6e42400073af3219888","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c8b6e42400073af3219888/iZiwL_KwhNboAGg0RKUAy.jpeg","isPro":false,"fullname":"Chuangtao Ma","user":"ctma","type":"user"},"summary":"Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.","upvotes":1,"discussionId":"6989dfffbeecc443208d28e3","ai_summary":"CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.","ai_keywords":["retrieval-augmented generation","large-scale entity matching","blocking-based batch retrieval","generation overhead","entity matching","data integration"]},"publishedAt":"2026-02-05T09:33:00.000Z","title":"Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration","summary":"Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05708.png","numComments":1,"submittedBy":{"_id":"66c8b6e42400073af3219888","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c8b6e42400073af3219888/iZiwL_KwhNboAGg0RKUAy.jpeg","fullname":"Chuangtao Ma","name":"ctma","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07054","authors":[{"_id":"698ad51b1b2dc6b37d61b251","name":"Ashutosh Chaubey","hidden":false},{"_id":"698ad51b1b2dc6b37d61b252","name":"Jiacheng Pang","hidden":false},{"_id":"698ad51b1b2dc6b37d61b253","name":"Maksim Siniukov","hidden":false},{"_id":"698ad51b1b2dc6b37d61b254","name":"Mohammad Soleymani","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6541185dbd60d2bd193f7999/t1LVgPhBIAsDHQQOOF3_H.png"],"publishedAt":"2026-02-04T18:24:25.000Z","submittedOnDailyAt":"2026-02-10T04:34:30.828Z","title":"AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization","submittedOnDailyBy":{"_id":"6541185dbd60d2bd193f7999","avatarUrl":"/avatars/4ce4a0feff9bfbb87e9f40431718ba00.svg","isPro":false,"fullname":"Ashutosh Chaubey","user":"chaubeyG","type":"user"},"summary":"Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.","upvotes":1,"discussionId":"698ad51b1b2dc6b37d61b255","projectPage":"https://avere-iclr.github.io/","ai_summary":"A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.","ai_keywords":["multimodal large language models","preference optimization","cue-emotion associations","hallucinations","modality agreement","AVEm-DPO","spurious associations","text priors","audiovisual inputs","emotion-centric queries"],"organization":{"_id":"66a403d0dcb5bbc6e98bb7d0","name":"UniversityofSouthernCalifornia","fullname":"University of Southern California","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"}},"publishedAt":"2026-02-04T13:24:25.000Z","title":"AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization","summary":"Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6541185dbd60d2bd193f7999/t1LVgPhBIAsDHQQOOF3_H.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07054.png","numComments":1,"submittedBy":{"_id":"6541185dbd60d2bd193f7999","avatarUrl":"/avatars/4ce4a0feff9bfbb87e9f40431718ba00.svg","fullname":"Ashutosh Chaubey","name":"chaubeyG","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66a403d0dcb5bbc6e98bb7d0","name":"UniversityofSouthernCalifornia","fullname":"University of Southern California","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07040","authors":[{"_id":"698abed21b2dc6b37d61b167","name":"Emmett Bicker","hidden":false}],"publishedAt":"2026-02-03T19:01:23.000Z","submittedOnDailyAt":"2026-02-10T02:44:58.980Z","title":"Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.\n  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.\n  Aster is accessible via a web interface and API at asterlab.ai.","upvotes":1,"discussionId":"698abed21b2dc6b37d61b168","projectPage":"https://www.asterlab.ai/","ai_summary":"Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.","ai_keywords":["AI agent","autonomous scientific discovery","iterative program improvement","state-of-the-art performance","computational efficiency","web interface","API"]},"publishedAt":"2026-02-03T14:01:23.000Z","title":"Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods","summary":"We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.\n  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.\n  Aster is accessible via a web interface and API at asterlab.ai.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07040.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":230,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.02827","authors":[{"_id":"698af29c1b2dc6b37d61b322","user":{"_id":"67b63b7828019ee3d38ee8fb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/oqVtzM8kQ6ZokMP7Qj0us.png","isPro":false,"fullname":"roi pony","user":"roi-pony","type":"user"},"name":"Roi Pony","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:04:03.510Z","hidden":false},{"_id":"698af29c1b2dc6b37d61b323","user":{"_id":"67ab764de48a46f097ffebf2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/68zlE-pEQAJHv9CdaPSJz.png","isPro":false,"fullname":"Raz Goldfarb","user":"Adirazgold","type":"user"},"name":"Adi Raz","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:04:00.895Z","hidden":false},{"_id":"698af29c1b2dc6b37d61b324","name":"Oshri Naparstek","hidden":false},{"_id":"698af29c1b2dc6b37d61b325","name":"Idan Friedman","hidden":false},{"_id":"698af29c1b2dc6b37d61b326","name":"Udi Barzelay","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67b63b7828019ee3d38ee8fb/3TS-aZLkUlwQNQtKp05Nu.mp4","https://cdn-uploads.huggingface.co/production/uploads/67b63b7828019ee3d38ee8fb/kg25Dc7bFl5FY94-ck-xu.png"],"publishedAt":"2026-02-02T21:27:01.000Z","submittedOnDailyAt":"2026-02-10T16:47:56.238Z","title":"Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval","submittedOnDailyBy":{"_id":"67b63b7828019ee3d38ee8fb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/oqVtzM8kQ6ZokMP7Qj0us.png","isPro":false,"fullname":"roi pony","user":"roi-pony","type":"user"},"summary":"Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-K identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5times, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.","upvotes":1,"discussionId":"698af29c1b2dc6b37d61b327","projectPage":"https://roipony.github.io/ColBandit/","ai_summary":"Col-Bandit reduces computational costs in multi-vector late-interaction retrieval by adaptively pruning token-level interactions during query processing while maintaining ranking accuracy.","ai_keywords":["multi-vector late-interaction retrievers","ColBERT","MaxSim interactions","reranking","Top-K identification","finite-population","uncertainty-aware bounds","query-time pruning","sparse interaction matrix","dense late-interaction scoring"],"organization":{"_id":"6760ab6c5c9a8ea8370ab95b","name":"ibm-research","fullname":"IBM Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/npxapKcW-cXX3J2JBl2vY.png"}},"publishedAt":"2026-02-02T16:27:01.000Z","title":"Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval","summary":"Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-K identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5times, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67b63b7828019ee3d38ee8fb/3TS-aZLkUlwQNQtKp05Nu.mp4","https://cdn-uploads.huggingface.co/production/uploads/67b63b7828019ee3d38ee8fb/kg25Dc7bFl5FY94-ck-xu.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02827.png","numComments":1,"submittedBy":{"_id":"67b63b7828019ee3d38ee8fb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/oqVtzM8kQ6ZokMP7Qj0us.png","fullname":"roi pony","name":"roi-pony","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6760ab6c5c9a8ea8370ab95b","name":"ibm-research","fullname":"IBM Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/npxapKcW-cXX3J2JBl2vY.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08629","authors":[{"_id":"698b35cc1b2dc6b37d61b4aa","name":"Bo Peng","hidden":false},{"_id":"698b35cc1b2dc6b37d61b4ab","name":"Sirui Chen","hidden":false},{"_id":"698b35cc1b2dc6b37d61b4ac","name":"Jiaguo Tian","hidden":false},{"_id":"698b35cc1b2dc6b37d61b4ad","name":"Yu Qiao","hidden":false},{"_id":"698b35cc1b2dc6b37d61b4ae","name":"Chaochao Lu","hidden":false}],"publishedAt":"2026-02-09T13:21:32.000Z","submittedOnDailyAt":"2026-02-10T11:13:48.589Z","title":"CauScale: Neural Causal Discovery at Scale","submittedOnDailyBy":{"_id":"63a65b9161348381efe8f1c2","avatarUrl":"/avatars/0417475f8e0825cb6225d74c71fd727a.svg","isPro":false,"fullname":"Bo Peng (SII)","user":"MagicalChair","type":"user"},"summary":"Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.","upvotes":0,"discussionId":"698b35cc1b2dc6b37d61b4af","ai_summary":"CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.","ai_keywords":["causal discovery","neural architecture","data embeddings","tied attention weights","two-stream design","relational evidence","statistical graph priors","structural signals","mAP","inference speedups"],"organization":{"_id":"690305366de14cfa81f62935","name":"OpenCausaLab","fullname":"OpenCausaLab"}},"publishedAt":"2026-02-09T08:21:32.000Z","title":"CauScale: Neural Causal Discovery at Scale","summary":"Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08629.png","numComments":1,"submittedBy":{"_id":"63a65b9161348381efe8f1c2","avatarUrl":"/avatars/0417475f8e0825cb6225d74c71fd727a.svg","fullname":"Bo Peng (SII)","name":"MagicalChair","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"690305366de14cfa81f62935","name":"OpenCausaLab","fullname":"OpenCausaLab"},"isAuthorParticipating":false},{"paper":{"id":"2602.08004","authors":[{"_id":"698ab5161b2dc6b37d61afe3","name":"George Ling","hidden":false},{"_id":"698ab5161b2dc6b37d61afe4","name":"Shanshan Zhong","hidden":false},{"_id":"698ab5161b2dc6b37d61afe5","name":"Richard Huang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64311e8edd466752c73ab69a/ChnbbFpMVTbwvErxpR_dw.png"],"publishedAt":"2026-02-08T15:14:12.000Z","submittedOnDailyAt":"2026-02-10T12:40:33.383Z","title":"Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality","submittedOnDailyBy":{"_id":"64311e8edd466752c73ab69a","avatarUrl":"/avatars/34f559399e57534ee9883029c8b91ccf.svg","isPro":false,"fullname":"Shanshan Zhong","user":"zhongshsh","type":"user"},"summary":"Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.","upvotes":0,"discussionId":"698ab5161b2dc6b37d61afe6"},"publishedAt":"2026-02-08T10:14:12.000Z","title":"Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality","summary":"Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64311e8edd466752c73ab69a/ChnbbFpMVTbwvErxpR_dw.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08004.png","numComments":1,"submittedBy":{"_id":"64311e8edd466752c73ab69a","avatarUrl":"/avatars/34f559399e57534ee9883029c8b91ccf.svg","fullname":"Shanshan Zhong","name":"zhongshsh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07125","authors":[{"_id":"698ba5a76052d3bed96308ed","name":"Jianrui Zhang","hidden":false},{"_id":"698ba5a76052d3bed96308ee","name":"Anirudh Sundara Rajan","hidden":false},{"_id":"698ba5a76052d3bed96308ef","name":"Brandon Han","hidden":false},{"_id":"698ba5a76052d3bed96308f0","name":"Soochahn Lee","hidden":false},{"_id":"698ba5a76052d3bed96308f1","name":"Sukanta Ganguly","hidden":false},{"_id":"698ba5a76052d3bed96308f2","name":"Yong Jae Lee","hidden":false}],"publishedAt":"2026-02-06T19:01:54.000Z","submittedOnDailyAt":"2026-02-10T19:10:46.415Z","title":"Reasoning-Augmented Representations for Multimodal Retrieval","submittedOnDailyBy":{"_id":"6508b164abdde5290e5e4939","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6508b164abdde5290e5e4939/lQgAs3BHwCyI7Go1QA62m.jpeg","isPro":false,"fullname":"Harris Zhang","user":"HanSolo9682","type":"user"},"summary":"Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.","upvotes":0,"discussionId":"698ba5a76052d3bed96308f3","githubRepo":"https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval","githubRepoAddedBy":"user","ai_summary":"UMR systems face challenges with latent reasoning tasks, which the proposed framework addresses by decoupling reasoning from retrieval through enhanced visual and textual representations.","ai_keywords":["multimodal retrieval","embedding models","latent reasoning","Vision-Language Model","dense captioning","compositional constraints","retrieval constraints","distribution shift","semantically dense representations"],"githubStars":0},"publishedAt":"2026-02-06T14:01:54.000Z","title":"Reasoning-Augmented Representations for Multimodal Retrieval","summary":"Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07125.png","numComments":1,"submittedBy":{"_id":"6508b164abdde5290e5e4939","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6508b164abdde5290e5e4939/lQgAs3BHwCyI7Go1QA62m.jpeg","fullname":"Harris Zhang","name":"HanSolo9682","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.05946","authors":[{"_id":"698b39131b2dc6b37d61b4b1","name":"Rajdeep Haldar","hidden":false},{"_id":"698b39131b2dc6b37d61b4b2","name":"Lantao Mei","hidden":false},{"_id":"698b39131b2dc6b37d61b4b3","name":"Guang Lin","hidden":false},{"_id":"698b39131b2dc6b37d61b4b4","name":"Yue Xing","hidden":false},{"_id":"698b39131b2dc6b37d61b4b5","name":"Qifan Song","hidden":false}],"publishedAt":"2026-02-05T18:01:52.000Z","submittedOnDailyAt":"2026-02-10T11:28:18.329Z","title":"f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment","submittedOnDailyBy":{"_id":"66d651735d3541e40e2a735c","avatarUrl":"/avatars/2e7ae4a8f30708052f2d73b9ca818d56.svg","isPro":false,"fullname":"Rajdeep Haldar","user":"rhaldar97","type":"user"},"summary":"Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.","upvotes":0,"discussionId":"698b39131b2dc6b37d61b4b6","ai_summary":"Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.","ai_keywords":["Preference Alignment","f-divergence","variational representation","reinforcement learning","on-policy","hybrid policy optimization","LLM alignment","RLVR","Math Reasoning","Safety Alignment"]},"publishedAt":"2026-02-05T13:01:52.000Z","title":"f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment","summary":"Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning, and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05946.png","numComments":1,"submittedBy":{"_id":"66d651735d3541e40e2a735c","avatarUrl":"/avatars/2e7ae4a8f30708052f2d73b9ca818d56.svg","fullname":"Rajdeep Haldar","name":"rhaldar97","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false}]
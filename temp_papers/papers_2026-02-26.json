[{"paper":{"id":"2602.18283","authors":[{"_id":"699d32bf4e37ec6dfa1bc690","name":"Lei Xin","hidden":false},{"_id":"699d32bf4e37ec6dfa1bc691","name":"Yuhao Zheng","hidden":false},{"_id":"699d32bf4e37ec6dfa1bc692","name":"Ke Cheng","hidden":false},{"_id":"699d32bf4e37ec6dfa1bc693","user":{"_id":"652fc2605615e57807e3db19","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png","isPro":false,"fullname":"Changjiang Jiang","user":"arnodjiang","type":"user"},"name":"Changjiang Jiang","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:03:02.605Z","hidden":false},{"_id":"699d32bf4e37ec6dfa1bc694","name":"Zifan Zhang","hidden":false},{"_id":"699d32bf4e37ec6dfa1bc695","name":"Fanhu Zeng","hidden":false}],"publishedAt":"2026-02-20T15:11:40.000Z","submittedOnDailyAt":"2026-02-26T01:11:59.076Z","title":"HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation","submittedOnDailyBy":{"_id":"64107c7df52d7eb22e062956","avatarUrl":"/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg","isPro":false,"fullname":"Yuhao Zheng","user":"yhzheng1031","type":"user"},"summary":"Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.","upvotes":46,"discussionId":"699d32bf4e37ec6dfa1bc696","ai_summary":"HyTRec addresses the challenge of modeling long user behavior sequences by combining linear and softmax attention mechanisms with a temporal-aware delta network to balance efficiency and retrieval precision.","ai_keywords":["Hybrid Attention","linear attention","softmax attention","long-term stable preferences","short-term intent spikes","temporal-aware delta network","TADN","Hit Rate"],"organization":{"_id":"6350bdf559bfa9a85d42fea4","name":"WuhanUniversity","fullname":"Wuhan Univeristy","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}},"publishedAt":"2026-02-20T10:11:40.000Z","title":"HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation","summary":"Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18283.png","numComments":1,"submittedBy":{"_id":"64107c7df52d7eb22e062956","avatarUrl":"/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg","fullname":"Yuhao Zheng","name":"yhzheng1031","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6350bdf559bfa9a85d42fea4","name":"WuhanUniversity","fullname":"Wuhan Univeristy","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.17602","authors":[{"_id":"699fd9f8ebfce7fbcca91c00","user":{"_id":"65f836339e3737dc3040f3be","avatarUrl":"/avatars/70b479f58338b71192a05331dfa1bb15.svg","isPro":false,"fullname":"Hojung Jung","user":"cossmoss","type":"user"},"name":"Hojung Jung","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:01:39.873Z","hidden":false},{"_id":"699fd9f8ebfce7fbcca91c01","name":"Rodrigo Hormazabal","hidden":false},{"_id":"699fd9f8ebfce7fbcca91c02","name":"Jaehyeong Jo","hidden":false},{"_id":"699fd9f8ebfce7fbcca91c03","name":"Youngrok Park","hidden":false},{"_id":"699fd9f8ebfce7fbcca91c04","name":"Kyunggeun Roh","hidden":false},{"_id":"699fd9f8ebfce7fbcca91c05","name":"Se-Young Yun","hidden":false},{"_id":"699fd9f8ebfce7fbcca91c06","name":"Sehui Han","hidden":false},{"_id":"699fd9f8ebfce7fbcca91c07","name":"Dae-Woong Jeong","hidden":false}],"publishedAt":"2026-02-19T18:27:11.000Z","submittedOnDailyAt":"2026-02-26T08:10:14.563Z","title":"MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models","submittedOnDailyBy":{"_id":"65e5bd4568234ef5d6decadc","avatarUrl":"/avatars/c41095a946c0176b949c0b3566136c05.svg","isPro":false,"fullname":"Jaehyeong Jo","user":"harryjo97","type":"user"},"summary":"Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.","upvotes":41,"discussionId":"699fd9f9ebfce7fbcca91c08","ai_summary":"MolHIT presents a hierarchical discrete diffusion model for molecular graph generation that achieves superior chemical validity and property-guided synthesis compared to existing 1D and graph-based approaches.","ai_keywords":["diffusion models","molecular generation","graph diffusion models","chemical validity","hierarchical discrete diffusion model","chemical priors","atom encoding","MOSES dataset","multi-property guided generation","scaffold extension"],"organization":{"_id":"6475760c33192631bad2bb38","name":"kaist-ai","fullname":"KAIST AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}},"publishedAt":"2026-02-19T13:27:11.000Z","title":"MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models","summary":"Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17602.png","numComments":1,"submittedBy":{"_id":"65e5bd4568234ef5d6decadc","avatarUrl":"/avatars/c41095a946c0176b949c0b3566136c05.svg","fullname":"Jaehyeong Jo","name":"harryjo97","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"6475760c33192631bad2bb38","name":"kaist-ai","fullname":"KAIST AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.12160","authors":[{"_id":"698e9f3bcace060ff123ae16","user":{"_id":"67d50738fed7787297d737d6","avatarUrl":"/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg","isPro":false,"fullname":"xuguo","user":"XuGuo699","type":"user"},"name":"Xu Guo","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:03:53.038Z","hidden":false},{"_id":"698e9f3bcace060ff123ae17","name":"Fulong Ye","hidden":false},{"_id":"698e9f3bcace060ff123ae18","name":"Qichao Sun","hidden":false},{"_id":"698e9f3bcace060ff123ae19","name":"Liyang Chen","hidden":false},{"_id":"698e9f3bcace060ff123ae1a","name":"Bingchuan Li","hidden":false},{"_id":"698e9f3bcace060ff123ae1b","name":"Pengze Zhang","hidden":false},{"_id":"698e9f3bcace060ff123ae1c","name":"Jiawei Liu","hidden":false},{"_id":"698e9f3bcace060ff123ae1d","name":"Songtao Zhao","hidden":false},{"_id":"698e9f3bcace060ff123ae1e","name":"Qian He","hidden":false},{"_id":"698e9f3bcace060ff123ae1f","name":"Xiangwang Hou","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/oyLOvr4XR4tSXLrO7N76E.mp4"],"publishedAt":"2026-02-12T16:41:52.000Z","submittedOnDailyAt":"2026-02-26T01:31:25.512Z","title":"DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation","submittedOnDailyBy":{"_id":"67d50738fed7787297d737d6","avatarUrl":"/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg","isPro":false,"fullname":"xuguo","user":"XuGuo699","type":"user"},"summary":"Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.","upvotes":29,"discussionId":"698e9f3ccace060ff123ae20","projectPage":"https://guoxu1233.github.io/DreamID-Omni/","githubRepo":"https://github.com/Guoxu1233/DreamID-Omni","githubRepoAddedBy":"user","ai_summary":"DreamID-Omni is a unified framework for controllable human-centric audio-video generation that uses a symmetric conditional diffusion transformer with dual-level disentanglement and multi-task progressive training to achieve state-of-the-art performance.","ai_keywords":["conditional diffusion transformer","symmetric conditional injection scheme","dual-level disentanglement","synchronized RoPE","structured captions","multi-task progressive training","audio-video generation","reference-based audio-video generation","video editing","audio-driven video animation","identity-timbre binding","speaker confusion","diffusion models"],"githubStars":54,"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}},"publishedAt":"2026-02-12T11:41:52.000Z","title":"DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation","summary":"Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/oyLOvr4XR4tSXLrO7N76E.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12160.png","numComments":1,"submittedBy":{"_id":"67d50738fed7787297d737d6","avatarUrl":"/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg","fullname":"xuguo","name":"XuGuo699","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.21818","authors":[{"_id":"699fba56ebfce7fbcca91b1a","name":"Guibin Chen","hidden":false},{"_id":"699fba56ebfce7fbcca91b1b","name":"Dixuan Lin","hidden":false},{"_id":"699fba56ebfce7fbcca91b1c","name":"Jiangping Yang","hidden":false},{"_id":"699fba56ebfce7fbcca91b1d","name":"Youqiang Zhang","hidden":false},{"_id":"699fba56ebfce7fbcca91b1e","name":"Zhengcong Fei","hidden":false},{"_id":"699fba56ebfce7fbcca91b1f","name":"Debang Li","hidden":false},{"_id":"699fba56ebfce7fbcca91b20","name":"Sheng Chen","hidden":false},{"_id":"699fba56ebfce7fbcca91b21","name":"Chaofeng Ao","hidden":false},{"_id":"699fba56ebfce7fbcca91b22","name":"Nuo Pang","hidden":false},{"_id":"699fba56ebfce7fbcca91b23","name":"Yiming Wang","hidden":false},{"_id":"699fba56ebfce7fbcca91b24","name":"Yikun Dou","hidden":false},{"_id":"699fba56ebfce7fbcca91b25","name":"Zheng Chen","hidden":false},{"_id":"699fba56ebfce7fbcca91b26","name":"Mingyuan Fan","hidden":false},{"_id":"699fba56ebfce7fbcca91b27","name":"Tuanhui Li","hidden":false},{"_id":"699fba56ebfce7fbcca91b28","name":"Mingshan Chang","hidden":false},{"_id":"699fba56ebfce7fbcca91b29","name":"Hao Zhang","hidden":false},{"_id":"699fba56ebfce7fbcca91b2a","name":"Xiaopeng Sun","hidden":false},{"_id":"699fba56ebfce7fbcca91b2b","name":"Jingtao Xu","hidden":false},{"_id":"699fba56ebfce7fbcca91b2c","name":"Yuqiang Xie","hidden":false},{"_id":"699fba56ebfce7fbcca91b2d","name":"Jiahua Wang","hidden":false},{"_id":"699fba56ebfce7fbcca91b2e","name":"Zhiheng Xu","hidden":false},{"_id":"699fba56ebfce7fbcca91b2f","name":"Weiming Xiong","hidden":false},{"_id":"699fba56ebfce7fbcca91b30","name":"Yuzhe Jin","hidden":false},{"_id":"699fba56ebfce7fbcca91b31","name":"Baoxuan Gu","hidden":false},{"_id":"699fba56ebfce7fbcca91b32","name":"Binjie Mao","hidden":false},{"_id":"699fba56ebfce7fbcca91b33","name":"Yunjie Yu","hidden":false},{"_id":"699fba56ebfce7fbcca91b34","name":"Jujie He","hidden":false},{"_id":"699fba56ebfce7fbcca91b35","name":"Yuhao Feng","hidden":false},{"_id":"699fba56ebfce7fbcca91b36","name":"Shiwen Tu","hidden":false},{"_id":"699fba56ebfce7fbcca91b37","name":"Chaojie Wang","hidden":false},{"_id":"699fba56ebfce7fbcca91b38","name":"Rui Yan","hidden":false},{"_id":"699fba56ebfce7fbcca91b39","name":"Wei Shen","hidden":false},{"_id":"699fba56ebfce7fbcca91b3a","name":"Jingchen Wu","hidden":false},{"_id":"699fba56ebfce7fbcca91b3b","name":"Peng Zhao","hidden":false},{"_id":"699fba56ebfce7fbcca91b3c","name":"Xuanyue Zhong","hidden":false},{"_id":"699fba56ebfce7fbcca91b3d","name":"Zhuangzhuang Liu","hidden":false},{"_id":"699fba56ebfce7fbcca91b3e","name":"Kaifei Wang","hidden":false},{"_id":"699fba56ebfce7fbcca91b3f","name":"Fuxiang Zhang","hidden":false},{"_id":"699fba56ebfce7fbcca91b40","name":"Weikai Xu","hidden":false},{"_id":"699fba56ebfce7fbcca91b41","name":"Wenyan Liu","hidden":false},{"_id":"699fba56ebfce7fbcca91b42","name":"Binglu Zhang","hidden":false},{"_id":"699fba56ebfce7fbcca91b43","name":"Yu Shen","hidden":false},{"_id":"699fba56ebfce7fbcca91b44","name":"Tianhui Xiong","hidden":false},{"_id":"699fba56ebfce7fbcca91b45","name":"Bin Peng","hidden":false},{"_id":"699fba56ebfce7fbcca91b46","name":"Liang Zeng","hidden":false},{"_id":"699fba56ebfce7fbcca91b47","name":"Xuchen Song","hidden":false},{"_id":"699fba56ebfce7fbcca91b48","name":"Haoxiang Guo","hidden":false},{"_id":"699fba56ebfce7fbcca91b49","name":"Peiyu Wang","hidden":false},{"_id":"699fba56ebfce7fbcca91b4a","name":"Yahui Zhou","hidden":false}],"publishedAt":"2026-02-25T11:47:00.000Z","submittedOnDailyAt":"2026-02-26T00:43:38.210Z","title":"SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.","upvotes":19,"discussionId":"699fba56ebfce7fbcca91b4b","ai_summary":"SkyReels V4 is a unified multimodal video foundation model that generates, edits, and inpaints video and audio simultaneously using a dual-stream architecture with shared text encoding and efficient high-resolution processing.","ai_keywords":["Multimodal Diffusion Transformer","MMDiT","Multimodal Large Language Models","MMLM","video audio generation","video inpainting","video editing","channel concatenation formulation","joint generation","super-resolution","frame interpolation"],"organization":{"_id":"6522615d9334173c627b0efa","name":"Skywork","fullname":"Skywork","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"}},"publishedAt":"2026-02-25T06:47:00.000Z","title":"SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model","summary":"SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21818.png","numComments":4,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":240,"isUserFollowing":false},"organization":{"_id":"6522615d9334173c627b0efa","name":"Skywork","fullname":"Skywork","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.21534","authors":[{"_id":"699fbaf1ebfce7fbcca91b4d","name":"Xiaoxuan Wang","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b4e","name":"Han Zhang","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b4f","name":"Haixin Wang","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b50","name":"Yidan Shi","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b51","name":"Ruoyan Li","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b52","name":"Kaiqiao Han","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b53","name":"Chenyi Tong","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b54","name":"Haoran Deng","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b55","name":"Renliang Sun","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b56","name":"Alexander Taylor","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b57","name":"Yanqiao Zhu","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b58","name":"Jason Cong","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b59","name":"Yizhou Sun","hidden":false},{"_id":"699fbaf1ebfce7fbcca91b5a","name":"Wei Wang","hidden":false}],"publishedAt":"2026-02-25T03:43:34.000Z","submittedOnDailyAt":"2026-02-26T00:47:18.994Z","title":"ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning","submittedOnDailyBy":{"_id":"64ba5946c0f19c9025665a3c","avatarUrl":"/avatars/bb148094ce52f1f385d30968dc22e0e6.svg","isPro":false,"fullname":"Xiaoxuan Wang","user":"xw27","type":"user"},"summary":"Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.","upvotes":16,"discussionId":"699fbaf1ebfce7fbcca91b5b","githubRepo":"https://github.com/WillDreamer/ARL-Arena","githubRepoAddedBy":"user","ai_summary":"ARLArena framework analyzes training stability in agentic reinforcement learning and proposes SAMPO method for stable policy optimization across diverse tasks.","ai_keywords":["agentic reinforcement learning","policy gradient","training stability","policy optimization","ARLArena","SAMPO"],"githubStars":17,"organization":{"_id":"67784c39dac147922d8d09f0","name":"UCLA","fullname":"University of California, Los Angeles","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"}},"publishedAt":"2026-02-24T22:43:34.000Z","title":"ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning","summary":"Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21534.png","numComments":1,"submittedBy":{"_id":"64ba5946c0f19c9025665a3c","avatarUrl":"/avatars/bb148094ce52f1f385d30968dc22e0e6.svg","fullname":"Xiaoxuan Wang","name":"xw27","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67784c39dac147922d8d09f0","name":"UCLA","fullname":"University of California, Los Angeles","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.22190","authors":[{"_id":"699fc4f3ebfce7fbcca91bc5","name":"Rui Yang","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bc6","name":"Qianhui Wu","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bc7","name":"Zhaoyang Wang","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bc8","name":"Hanyang Chen","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bc9","name":"Ke Yang","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bca","name":"Hao Cheng","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bcb","name":"Huaxiu Yao","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bcc","name":"Baoling Peng","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bcd","name":"Huan Zhang","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bce","name":"Jianfeng Gao","hidden":false},{"_id":"699fc4f3ebfce7fbcca91bcf","name":"Tong Zhang","hidden":false}],"publishedAt":"2026-02-25T18:34:57.000Z","submittedOnDailyAt":"2026-02-26T02:17:51.385Z","title":"GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL","submittedOnDailyBy":{"_id":"64d45451c34a346181b130dd","avatarUrl":"/avatars/9bb8205b889337df5d321539c9b5d69d.svg","isPro":true,"fullname":"Rui Yang","user":"Ray2333","type":"user"},"summary":"Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.","upvotes":11,"discussionId":"699fc4f3ebfce7fbcca91bd0","projectPage":"https://gui-libra.github.io","githubRepo":"https://github.com/GUI-Libra/GUI-Libra","githubRepoAddedBy":"user","ai_summary":"GUI-Libra addresses limitations in open-source GUI agents through specialized training methods that improve reasoning-grounding alignment and reinforcement learning under partial verifiability, demonstrating enhanced task completion across web and mobile platforms.","ai_keywords":["GUI agents","action-aligned reasoning data","post-training pipelines","SFT","CoT reasoning","RLVR","partial verifiability","KL regularization","KL trust region","success-adaptive scaling","step-wise accuracy","end-to-end task completion","data curation","reasoning-grounding alignment"],"githubStars":8,"organization":{"_id":"68ef00955bff0d62c986e4f9","name":"UIUC-ScaleML","fullname":"UIUC ScaleML Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64cb1ad1667f4f80852f6050/ZUSgn9xQjfqDOHiI_-7Tw.png"}},"publishedAt":"2026-02-25T13:34:57.000Z","title":"GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL","summary":"Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22190.png","numComments":1,"submittedBy":{"_id":"64d45451c34a346181b130dd","avatarUrl":"/avatars/9bb8205b889337df5d321539c9b5d69d.svg","fullname":"Rui Yang","name":"Ray2333","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":15,"isUserFollowing":false},"organization":{"_id":"68ef00955bff0d62c986e4f9","name":"UIUC-ScaleML","fullname":"UIUC ScaleML Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64cb1ad1667f4f80852f6050/ZUSgn9xQjfqDOHiI_-7Tw.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.22208","authors":[{"_id":"699fb974ebfce7fbcca91b04","name":"Georgy Savva","hidden":false},{"_id":"699fb974ebfce7fbcca91b05","name":"Oscar Michel","hidden":false},{"_id":"699fb974ebfce7fbcca91b06","name":"Daohan Lu","hidden":false},{"_id":"699fb974ebfce7fbcca91b07","name":"Suppakit Waiwitlikhit","hidden":false},{"_id":"699fb974ebfce7fbcca91b08","name":"Timothy Meehan","hidden":false},{"_id":"699fb974ebfce7fbcca91b09","name":"Dhairya Mishra","hidden":false},{"_id":"699fb974ebfce7fbcca91b0a","name":"Srivats Poddar","hidden":false},{"_id":"699fb974ebfce7fbcca91b0b","name":"Jack Lu","hidden":false},{"_id":"699fb974ebfce7fbcca91b0c","name":"Saining Xie","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/nMpp08pwIVyS2FTX_4zWe.mp4"],"publishedAt":"2026-02-25T18:59:01.000Z","submittedOnDailyAt":"2026-02-26T00:40:52.562Z","title":"Solaris: Building a Multiplayer Video World Model in Minecraft","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.","upvotes":10,"discussionId":"699fb974ebfce7fbcca91b0d","projectPage":"https://solaris-wm.github.io/","githubRepo":"https://github.com/solaris-wm/solaris","githubRepoAddedBy":"user","ai_summary":"Solaris is a multiplayer video world model that simulates consistent multi-view observations through a novel data collection system and staged training approach.","ai_keywords":["video world models","multiplayer","multi-agent interactions","data collection","staged pipeline","bidirectional","causal","Self Forcing","checkpointed Self Forcing"],"githubStars":56},"publishedAt":"2026-02-25T13:59:01.000Z","title":"Solaris: Building a Multiplayer Video World Model in Minecraft","summary":"Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/nMpp08pwIVyS2FTX_4zWe.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22208.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":240,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.21461","authors":[{"_id":"699feb51ebfce7fbcca91c2e","user":{"_id":"63318b2349a9563915469f3b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg","isPro":false,"fullname":"Xiaoke Huang","user":"xk-huang","type":"user"},"name":"Xiaoke Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:01:35.636Z","hidden":false},{"_id":"699feb51ebfce7fbcca91c2f","name":"Bhavul Gauri","hidden":false},{"_id":"699feb51ebfce7fbcca91c30","name":"Kam Woh Ng","hidden":false},{"_id":"699feb51ebfce7fbcca91c31","name":"Tony Ng","hidden":false},{"_id":"699feb51ebfce7fbcca91c32","name":"Mengmeng Xu","hidden":false},{"_id":"699feb51ebfce7fbcca91c33","name":"Zhiheng Liu","hidden":false},{"_id":"699feb51ebfce7fbcca91c34","name":"Weiming Ren","hidden":false},{"_id":"699feb51ebfce7fbcca91c35","name":"Zhaochong An","hidden":false},{"_id":"699feb51ebfce7fbcca91c36","name":"Zijian Zhou","hidden":false},{"_id":"699feb51ebfce7fbcca91c37","name":"Haonan Qiu","hidden":false},{"_id":"699feb51ebfce7fbcca91c38","name":"Yuyin Zhou","hidden":false},{"_id":"699feb51ebfce7fbcca91c39","name":"Sen He","hidden":false},{"_id":"699feb51ebfce7fbcca91c3a","name":"Ziheng Wang","hidden":false},{"_id":"699feb51ebfce7fbcca91c3b","name":"Tao Xiang","hidden":false},{"_id":"699feb51ebfce7fbcca91c3c","name":"Xiao Han","hidden":false}],"publishedAt":"2026-02-25T00:27:23.000Z","submittedOnDailyAt":"2026-02-26T06:12:10.007Z","title":"VecGlypher: Unified Vector Glyph Generation with Language Models","submittedOnDailyBy":{"_id":"63318b2349a9563915469f3b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg","isPro":false,"fullname":"Xiaoke Huang","user":"xk-huang","type":"user"},"summary":"Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.","upvotes":9,"discussionId":"699feb52ebfce7fbcca91c3d","projectPage":"https://xk-huang.github.io/VecGlypher/","githubRepo":"https://github.com/xk-huang/VecGlypher","githubRepoAddedBy":"user","ai_summary":"VecGlypher is a multimodal language model that generates high-fidelity vector glyphs directly from text or image inputs, bypassing traditional raster-to-vector processes and enabling direct SVG path generation.","ai_keywords":["multimodal language model","vector glyphs","SVG path tokens","autoregressive generation","typography-aware data","two-stage training","coordinate normalization","path canonicalization","font creation"],"githubStars":1,"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}},"publishedAt":"2026-02-24T19:27:23.000Z","title":"VecGlypher: Unified Vector Glyph Generation with Language Models","summary":"Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21461.png","numComments":1,"submittedBy":{"_id":"63318b2349a9563915469f3b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg","fullname":"Xiaoke Huang","name":"xk-huang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.19163","authors":[{"_id":"699fb334ebfce7fbcca91ad0","name":"Kai Liu","hidden":false},{"_id":"699fb334ebfce7fbcca91ad1","name":"Yanhao Zheng","hidden":false},{"_id":"699fb334ebfce7fbcca91ad2","name":"Kai Wang","hidden":false},{"_id":"699fb334ebfce7fbcca91ad3","name":"Shengqiong Wu","hidden":false},{"_id":"699fb334ebfce7fbcca91ad4","name":"Rongjunchen Zhang","hidden":false},{"_id":"699fb334ebfce7fbcca91ad5","name":"Jiebo Luo","hidden":false},{"_id":"699fb334ebfce7fbcca91ad6","name":"Dimitrios Hatzinakos","hidden":false},{"_id":"699fb334ebfce7fbcca91ad7","name":"Ziwei Liu","hidden":false},{"_id":"699fb334ebfce7fbcca91ad8","name":"Hao Fei","hidden":false},{"_id":"699fb334ebfce7fbcca91ad9","name":"Tat-Seng Chua","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/BdSUgdHoEbTgTu6tHanG1.mp4","https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/bhNigK8NVUKgJ33WobH4i.jpeg","https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kggZ01w0SeC0sb3YHCq6m.jpeg","https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/fUrz6yQZoxYotNr_YL5E-.jpeg"],"publishedAt":"2026-02-22T12:44:28.000Z","submittedOnDailyAt":"2026-02-26T00:22:24.649Z","title":"JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation","submittedOnDailyBy":{"_id":"678bdcbe600666579235a1f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png","isPro":false,"fullname":"KAI LIU","user":"kkail8","type":"user"},"summary":"AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.","upvotes":9,"discussionId":"699fb335ebfce7fbcca91ada","projectPage":"https://javisverse.github.io/JavisDiT2-page/","githubRepo":"https://github.com/JavisVerse/JavisDiT","githubRepoAddedBy":"user","ai_summary":"JavisDiT++ presents a unified framework for joint audio-video generation using modality-specific mixture-of-experts, temporal-aligned RoPE, and audio-video direct preference optimization to achieve high-quality, synchronized multimedia synthesis.","ai_keywords":["joint audio-video generation","modality-specific mixture-of-experts","MS-MoE","temporal-aligned RoPE","TA-RoPE","audio-video direct preference optimization","AV-DPO","multimodal synthesis","synchronized generation","semantically aligned"],"githubStars":322,"organization":{"_id":"67adfac46083604e4b664e43","name":"JavisVerse","fullname":"JavisVerse","avatar":"https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"}},"publishedAt":"2026-02-22T07:44:28.000Z","title":"JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation","summary":"AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/BdSUgdHoEbTgTu6tHanG1.mp4","https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/bhNigK8NVUKgJ33WobH4i.jpeg","https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kggZ01w0SeC0sb3YHCq6m.jpeg","https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/fUrz6yQZoxYotNr_YL5E-.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19163.png","numComments":1,"submittedBy":{"_id":"678bdcbe600666579235a1f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png","fullname":"KAI LIU","name":"kkail8","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"67adfac46083604e4b664e43","name":"JavisVerse","fullname":"JavisVerse","avatar":"https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.15030","authors":[{"_id":"69966aaf1268a6b79e0d0253","name":"Kaiyu Yue","hidden":false},{"_id":"69966aaf1268a6b79e0d0254","name":"Menglin Jia","hidden":false},{"_id":"69966aaf1268a6b79e0d0255","name":"Ji Hou","hidden":false},{"_id":"69966aaf1268a6b79e0d0256","name":"Tom Goldstein","hidden":false}],"publishedAt":"2026-02-16T18:59:57.000Z","submittedOnDailyAt":"2026-02-26T04:22:36.951Z","title":"Image Generation with a Sphere Encoder","submittedOnDailyBy":{"_id":"640d0dbc8036cc2142273a83","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/640d0dbc8036cc2142273a83/cicTWJVqqvQv_DgDucWgY.jpeg","isPro":true,"fullname":"Kaiyu Yue","user":"kaiyuyue","type":"user"},"summary":"We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .","upvotes":9,"discussionId":"69966ab01268a6b79e0d0257","projectPage":"https://sphere-encoder.github.io","ai_summary":"The Sphere Encoder is an efficient generative model that produces images in a single forward pass by mapping images to a spherical latent space and decoding from random points on that sphere, achieving diffusion-like quality with significantly reduced inference costs.","ai_keywords":["sphere encoder","generative framework","spherical latent space","encoder-decoder architecture","image reconstruction losses","conditional generation","inference cost"]},"publishedAt":"2026-02-16T13:59:57.000Z","title":"Image Generation with a Sphere Encoder","summary":"We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15030.png","numComments":1,"submittedBy":{"_id":"640d0dbc8036cc2142273a83","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/640d0dbc8036cc2142273a83/cicTWJVqqvQv_DgDucWgY.jpeg","fullname":"Kaiyu Yue","name":"kaiyuyue","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.22010","authors":[{"_id":"699fb776ebfce7fbcca91adc","name":"Yue Su","hidden":false},{"_id":"699fb776ebfce7fbcca91add","name":"Sijin Chen","hidden":false},{"_id":"699fb776ebfce7fbcca91ade","name":"Haixin Shi","hidden":false},{"_id":"699fb776ebfce7fbcca91adf","name":"Mingyu Liu","hidden":false},{"_id":"699fb776ebfce7fbcca91ae0","name":"Zhengshen Zhang","hidden":false},{"_id":"699fb776ebfce7fbcca91ae1","name":"Ningyuan Huang","hidden":false},{"_id":"699fb776ebfce7fbcca91ae2","name":"Weiheng Zhong","hidden":false},{"_id":"699fb776ebfce7fbcca91ae3","name":"Zhengbang Zhu","hidden":false},{"_id":"699fb776ebfce7fbcca91ae4","name":"Yuxiao Liu","hidden":false},{"_id":"699fb776ebfce7fbcca91ae5","name":"Xihui Liu","hidden":false}],"publishedAt":"2026-02-25T15:27:09.000Z","submittedOnDailyAt":"2026-02-26T00:31:10.968Z","title":"World Guidance: World Modeling in Condition Space for Action Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/","upvotes":7,"discussionId":"699fb776ebfce7fbcca91ae6","projectPage":"https://selen-suyue.github.io/WoGNet/","ai_summary":"World Guidance framework enhances Vision-Language-Action models by mapping future observations into compact conditions for improved action generation and generalization.","ai_keywords":["Vision-Language-Action models","future observation modeling","action generation","world modeling","condition space","future prediction","human manipulation videos"],"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-02-25T10:27:09.000Z","title":"World Guidance: World Modeling in Condition Space for Action Generation","summary":"Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22010.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":240,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.21548","authors":[{"_id":"69a0230eebfce7fbcca91ce5","name":"Yongtong Wu","hidden":false},{"_id":"69a0230eebfce7fbcca91ce6","name":"Shaoyuan Chen","hidden":false},{"_id":"69a0230eebfce7fbcca91ce7","name":"Yinmin Zhong","hidden":false},{"_id":"69a0230eebfce7fbcca91ce8","name":"Rilin Huang","hidden":false},{"_id":"69a0230eebfce7fbcca91ce9","name":"Yixuan Tan","hidden":false},{"_id":"69a0230eebfce7fbcca91cea","name":"Wentao Zhang","hidden":false},{"_id":"69a0230eebfce7fbcca91ceb","name":"Liyue Zhang","hidden":false},{"_id":"69a0230eebfce7fbcca91cec","name":"Shangyan Zhou","hidden":false},{"_id":"69a0230eebfce7fbcca91ced","name":"Yuxuan Liu","hidden":false},{"_id":"69a0230eebfce7fbcca91cee","name":"Shunfeng Zhou","hidden":false},{"_id":"69a0230eebfce7fbcca91cef","name":"Mingxing Zhang","hidden":false},{"_id":"69a0230eebfce7fbcca91cf0","name":"Xin Jin","hidden":false},{"_id":"69a0230eebfce7fbcca91cf1","name":"Panpan Huang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5f17f0a0925b9863e28ad517/SiH9Ok7vPOEbD37rD9wL9.png"],"publishedAt":"2026-02-25T04:10:58.000Z","submittedOnDailyAt":"2026-02-26T08:29:39.899Z","title":"DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference","submittedOnDailyBy":{"_id":"5f17f0a0925b9863e28ad517","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg","isPro":true,"fullname":"Victor Mustar","user":"victor","type":"user"},"summary":"The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.\n  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.\n  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87times on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96times without violating SLO.","upvotes":7,"discussionId":"69a0230febfce7fbcca91cf2","ai_summary":"DualPath addresses KV-cache storage I/O bottlenecks in multi-turn LLM inference by introducing dual-path loading and dynamic load balancing across prefill and decode engines.","ai_keywords":["KV-Cache","prefill engines","decoding engines","RDMA","global scheduler","offline inference","online serving"],"organization":{"_id":"652faff917096ceb6bf53f3f","name":"deepseek-ai","fullname":"DeepSeek","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}},"publishedAt":"2026-02-24T23:10:58.000Z","title":"DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference","summary":"The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.\n  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.\n  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87times on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96times without violating SLO.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5f17f0a0925b9863e28ad517/SiH9Ok7vPOEbD37rD9wL9.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21548.png","numComments":1,"submittedBy":{"_id":"5f17f0a0925b9863e28ad517","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg","fullname":"Victor Mustar","name":"victor","type":"user","isPro":true,"isHf":true,"isHfAdmin":true,"isMod":false,"followerCount":5249,"isUserFollowing":false},"organization":{"_id":"652faff917096ceb6bf53f3f","name":"deepseek-ai","fullname":"DeepSeek","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.21778","authors":[{"_id":"699fcf85ebfce7fbcca91bdd","name":"Liangbing Zhao","hidden":false},{"_id":"699fcf85ebfce7fbcca91bde","name":"Le Zhuo","hidden":false},{"_id":"699fcf85ebfce7fbcca91bdf","name":"Sayak Paul","hidden":false},{"_id":"699fcf85ebfce7fbcca91be0","name":"Hongsheng Li","hidden":false},{"_id":"699fcf85ebfce7fbcca91be1","name":"Mohamed Elhoseiny","hidden":false}],"publishedAt":"2026-02-25T10:54:46.000Z","submittedOnDailyAt":"2026-02-26T10:03:40.878Z","title":"From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors","submittedOnDailyBy":{"_id":"5f7fbd813e94f16a85448745","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg","isPro":true,"fullname":"Sayak Paul","user":"sayakpaul","type":"user"},"summary":"Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.","upvotes":5,"discussionId":"699fcf85ebfce7fbcca91be2","projectPage":"https://liangbingzhao.github.io/statics2dynamics/","githubRepo":"https://github.com/liangbingzhao/PhysicEdit","githubRepoAddedBy":"user","ai_summary":"PhysicEdit enhances image editing by incorporating physical state transitions through a dual-thinking mechanism combining frozen vision-language models with learnable transition queries in a diffusion framework.","ai_keywords":["diffusion models","vision-language models","physical state transitions","textual-visual dual-thinking mechanism","transition queries","Qwen2.5-VL","PhysicEdit","PhysicTran38K","video-based dataset","causal dynamics","refraction","material deformation"],"githubStars":3},"publishedAt":"2026-02-25T05:54:46.000Z","title":"From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors","summary":"Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified. To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21778.png","numComments":1,"submittedBy":{"_id":"5f7fbd813e94f16a85448745","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg","fullname":"Sayak Paul","name":"sayakpaul","type":"user","isPro":true,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":859,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.20122","authors":[{"_id":"699e0a50dfbcf0b800aec9be","user":{"_id":"698aac36282aa3513435f270","avatarUrl":"/avatars/0c80739ff7ae481bd9561351c6584c32.svg","isPro":false,"fullname":"Lingwei Gu","user":"LingweiGu","type":"user"},"name":"Lingwei Gu","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:02:47.990Z","hidden":false},{"_id":"699e0a50dfbcf0b800aec9bf","user":{"_id":"663113ed3e584ce1d96b1d32","avatarUrl":"/avatars/28154c3c0ac3bb100dceed1c760d5fd3.svg","isPro":false,"fullname":"Nour Jedidi","user":"njedidi","type":"user"},"name":"Nour Jedidi","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:02:44.297Z","hidden":false},{"_id":"699e0a50dfbcf0b800aec9c0","name":"Jimmy Lin","hidden":false}],"publishedAt":"2026-02-23T18:37:49.000Z","submittedOnDailyAt":"2026-02-26T03:21:35.457Z","title":"NanoKnow: How to Know What Your Language Model Knows","submittedOnDailyBy":{"_id":"663113ed3e584ce1d96b1d32","avatarUrl":"/avatars/28154c3c0ac3bb100dceed1c760d5fd3.svg","isPro":false,"fullname":"Nour Jedidi","user":"njedidi","type":"user"},"summary":"How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.","upvotes":4,"discussionId":"699e0a50dfbcf0b800aec9c1","githubRepo":"https://github.com/castorini/NanoKnow","githubRepoAddedBy":"user","ai_summary":"NanoKnow benchmark enables analysis of knowledge sources in LLMs by partitioning questions based on pre-training data presence, revealing how parametric and external knowledge interact in model responses.","ai_keywords":["large language models","pre-training data","parametric knowledge","external knowledge","knowledge encoding","benchmark dataset","Natural Questions","SQuAD","nanochat"],"githubStars":6,"organization":{"_id":"5ec82896968f6028e0559f71","name":"castorini","fullname":"Castorini","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680844a9f4cd66b8c7de1cfd/2WgvmGFJch-Uuico9gXkB.jpeg"}},"publishedAt":"2026-02-23T13:37:49.000Z","title":"NanoKnow: How to Know What Your Language Model Knows","summary":"How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20122.png","numComments":1,"submittedBy":{"_id":"663113ed3e584ce1d96b1d32","avatarUrl":"/avatars/28154c3c0ac3bb100dceed1c760d5fd3.svg","fullname":"Nour Jedidi","name":"njedidi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"5ec82896968f6028e0559f71","name":"castorini","fullname":"Castorini","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680844a9f4cd66b8c7de1cfd/2WgvmGFJch-Uuico9gXkB.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.21472","authors":[{"_id":"699fbf82ebfce7fbcca91ba3","name":"Louis Bethune","hidden":false},{"_id":"699fbf82ebfce7fbcca91ba4","name":"Victor Turrisi","hidden":false},{"_id":"699fbf82ebfce7fbcca91ba5","name":"Bruno Kacper Mlodozeniec","hidden":false},{"_id":"699fbf82ebfce7fbcca91ba6","name":"Pau Rodriguez Lopez","hidden":false},{"_id":"699fbf82ebfce7fbcca91ba7","name":"Lokesh Boominathan","hidden":false},{"_id":"699fbf82ebfce7fbcca91ba8","name":"Nikhil Bhendawade","hidden":false},{"_id":"699fbf82ebfce7fbcca91ba9","name":"Amitis Shidani","hidden":false},{"_id":"699fbf82ebfce7fbcca91baa","name":"Joris Pelemans","hidden":false},{"_id":"699fbf82ebfce7fbcca91bab","name":"Theo X. Olausson","hidden":false},{"_id":"699fbf82ebfce7fbcca91bac","name":"Devon Hjelm","hidden":false},{"_id":"699fbf82ebfce7fbcca91bad","name":"Paul Dixon","hidden":false},{"_id":"699fbf82ebfce7fbcca91bae","name":"Joao Monteiro","hidden":false},{"_id":"699fbf82ebfce7fbcca91baf","name":"Pierre Ablin","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb0","name":"Vishnu Banna","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb1","name":"Arno Blaas","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb2","name":"Nick Henderson","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb3","name":"Kari Noriy","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb4","name":"Dan Busbridge","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb5","name":"Josh Susskind","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb6","name":"Marco Cuturi","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb7","name":"Irina Belousova","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb8","name":"Luca Zappella","hidden":false},{"_id":"699fbf82ebfce7fbcca91bb9","name":"Russ Webb","hidden":false},{"_id":"699fbf82ebfce7fbcca91bba","name":"Jason Ramapuram","hidden":false}],"publishedAt":"2026-02-25T01:02:11.000Z","submittedOnDailyAt":"2026-02-26T01:05:30.906Z","title":"The Design Space of Tri-Modal Masked Diffusion Models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities.","upvotes":3,"discussionId":"699fbf82ebfce7fbcca91bbb","ai_summary":"A large-scale study of tri-modal discrete diffusion models demonstrates improved performance across text, image, and speech generation tasks through systematic analysis of scaling laws and optimized inference methods.","ai_keywords":["discrete diffusion models","autoregressive language models","unimodal model","bimodal generation","tri-modal masked diffusion model","multimodal scaling laws","modality mixing ratios","noise schedules","stochastic differential equation","SDE-based reparameterization","batch-size effects","gradient variance","stochastic optimization","text-to-image","text-to-speech"],"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"}},"publishedAt":"2026-02-24T20:02:11.000Z","title":"The Design Space of Tri-Modal Masked Diffusion Models","summary":"Discrete diffusion models have emerged as strong alternatives to autoregressive language models, with recent work initializing and fine-tuning a base unimodal model for bimodal generation. Diverging from previous approaches, we introduce the first tri-modal masked diffusion model pretrained from scratch on text, image-text, and audio-text data. We systematically analyze multimodal scaling laws, modality mixing ratios, noise schedules, and batch-size effects, and we provide optimized inference sampling defaults. Our batch-size analysis yields a novel stochastic differential equation (SDE)-based reparameterization that eliminates the need for tuning the optimal batch size as reported in recent work. This reparameterization decouples the physical batch size, often chosen based on compute constraints (GPU saturation, FLOP efficiency, wall-clock time), from the logical batch size, chosen to balance gradient variance during stochastic optimization. Finally, we pretrain a preliminary 3B-parameter tri-modal model on 6.4T tokens, demonstrating the capabilities of a unified design and achieving strong results in text generation, text-to-image tasks, and text-to-speech tasks. Our work represents the largest-scale systematic open study of multimodal discrete diffusion models conducted to date, providing insights into scaling behaviors across multiple modalities.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21472.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":240,"isUserFollowing":false},"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.18993","authors":[{"_id":"699febcaebfce7fbcca91c3f","user":{"_id":"664206ea24555ed542a0bc47","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/664206ea24555ed542a0bc47/C9eFlGdsuZSUZHBtu6rLZ.jpeg","isPro":false,"fullname":"jiwoo chung","user":"wldn0202","type":"user"},"name":"Jiwoo Chung","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:01:33.083Z","hidden":false},{"_id":"699febcaebfce7fbcca91c40","name":"Sangeek Hyun","hidden":false},{"_id":"699febcaebfce7fbcca91c41","name":"MinKyu Lee","hidden":false},{"_id":"699febcaebfce7fbcca91c42","name":"Byeongju Han","hidden":false},{"_id":"699febcaebfce7fbcca91c43","name":"Geonho Cha","hidden":false},{"_id":"699febcaebfce7fbcca91c44","name":"Dongyoon Wee","hidden":false},{"_id":"699febcaebfce7fbcca91c45","name":"Youngjun Hong","hidden":false},{"_id":"699febcaebfce7fbcca91c46","name":"Jae-Pil Heo","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/664206ea24555ed542a0bc47/cz4r_NiByJ7h5JW2WyDwv.mp4","https://cdn-uploads.huggingface.co/production/uploads/664206ea24555ed542a0bc47/Ub6Srlu1_LSRqPfTDdMyB.mp4"],"publishedAt":"2026-02-22T00:48:03.000Z","submittedOnDailyAt":"2026-02-26T04:25:02.457Z","title":"SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models","submittedOnDailyBy":{"_id":"664206ea24555ed542a0bc47","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/664206ea24555ed542a0bc47/C9eFlGdsuZSUZHBtu6rLZ.jpeg","isPro":false,"fullname":"jiwoo chung","user":"wldn0202","type":"user"},"summary":"Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.","upvotes":3,"discussionId":"699febcaebfce7fbcca91c47","projectPage":"https://jiwoogit.github.io/SeaCache/","githubRepo":"https://github.com/jiwoogit/SeaCache","githubRepoAddedBy":"user","ai_summary":"Spectral-Evolution-Aware Cache (SeaCache) improves diffusion model inference speed by using spectrally aligned representations to optimize intermediate output reuse, achieving better latency-quality trade-offs than previous methods.","ai_keywords":["diffusion models","denoising process","caching strategies","feature distances","spectral evolution","spectrally aligned representation","Spectral-Evolution-Aware filter","redundancy estimation","dynamic schedules","latency-quality trade-offs"],"githubStars":9},"publishedAt":"2026-02-21T19:48:03.000Z","title":"SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models","summary":"Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/664206ea24555ed542a0bc47/cz4r_NiByJ7h5JW2WyDwv.mp4","https://cdn-uploads.huggingface.co/production/uploads/664206ea24555ed542a0bc47/Ub6Srlu1_LSRqPfTDdMyB.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18993.png","numComments":1,"submittedBy":{"_id":"664206ea24555ed542a0bc47","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/664206ea24555ed542a0bc47/C9eFlGdsuZSUZHBtu6rLZ.jpeg","fullname":"jiwoo chung","name":"wldn0202","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.21456","authors":[{"_id":"69a04708fe0f78cfa89ffc5b","name":"Chuan Meng","hidden":false},{"_id":"69a04708fe0f78cfa89ffc5c","name":"Litu Ou","hidden":false},{"_id":"69a04708fe0f78cfa89ffc5d","name":"Sean MacAvaney","hidden":false},{"_id":"69a04708fe0f78cfa89ffc5e","name":"Jeff Dalton","hidden":false}],"publishedAt":"2026-02-25T00:18:07.000Z","submittedOnDailyAt":"2026-02-26T20:57:37.920Z","title":"Revisiting Text Ranking in Deep Research","submittedOnDailyBy":{"_id":"622f2feea32d46b4be9ed8c4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NDeZQZQK5U-9m10yQwDVf.png","isPro":false,"fullname":"Litu Ou","user":"learn3r","type":"user"},"summary":"Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch.","upvotes":2,"discussionId":"69a04708fe0f78cfa89ffc5f","githubRepo":"https://github.com/ChuanMeng/text-ranking-in-deep-research","githubRepoAddedBy":"user","ai_summary":"Research on deep research tasks reveals that web search APIs' opacity hinders understanding of text ranking methods, which are evaluated across retrieval units, pipeline configurations, and query characteristics using a fixed-corpus dataset with multiple agents, retrievers, and re-rankers.","ai_keywords":["large language model","web search APIs","iterative search queries","external evidence","text ranking methods","retrieval units","pipeline configurations","retrievers","re-rankers","query characteristics","BrowseComp-Plus","lexical retrieval","multi-vector retrievers","passage-level units","document length normalization","re-ranking","agent-issued queries","natural-language questions"],"githubStars":0,"organization":{"_id":"6719f883fbeafdcc1c1c6631","name":"UniversityOfEdinburgh","fullname":"University of Edinburgh","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6719f716fd2d124648d68f9f/MromQvIfXMeww78iaAWWK.png"}},"publishedAt":"2026-02-24T19:18:07.000Z","title":"Revisiting Text Ranking in Deep Research","summary":"Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21456.png","numComments":1,"submittedBy":{"_id":"622f2feea32d46b4be9ed8c4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NDeZQZQK5U-9m10yQwDVf.png","fullname":"Litu Ou","name":"learn3r","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"6719f883fbeafdcc1c1c6631","name":"UniversityOfEdinburgh","fullname":"University of Edinburgh","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6719f716fd2d124648d68f9f/MromQvIfXMeww78iaAWWK.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14878","authors":[{"_id":"699e517bdfbcf0b800aeca57","name":"Mohammed Mehedi Hasan","hidden":false},{"_id":"699e517bdfbcf0b800aeca58","user":{"_id":"62b4f3b7464e664268bf4e85","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg","isPro":false,"fullname":"Leo","user":"hao-li","type":"user"},"name":"Hao Li","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:30:08.258Z","hidden":false},{"_id":"699e517bdfbcf0b800aeca59","name":"Gopi Krishnan Rajbahadur","hidden":false},{"_id":"699e517bdfbcf0b800aeca5a","name":"Bram Adams","hidden":false},{"_id":"699e517bdfbcf0b800aeca5b","name":"Ahmed E. Hassan","hidden":false}],"publishedAt":"2026-02-16T16:10:11.000Z","submittedOnDailyAt":"2026-02-26T00:39:58.966Z","title":"Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions","submittedOnDailyBy":{"_id":"62b4f3b7464e664268bf4e85","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg","isPro":false,"fullname":"Leo","user":"hao-li","type":"user"},"summary":"The Model Context Protocol (MCP) introduces a standard specification that defines how Foundation Model (FM)-based agents should interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.\n  Hence, we examine 856 tools spread across 103 MCP servers empirically, assess their description quality, and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, and then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These results indicate that achieving performance gains is not straightforward; while execution cost can act as a trade-off, execution context can also impact. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.","upvotes":2,"discussionId":"699e517cdfbcf0b800aeca5c","ai_summary":"Foundation model agents rely on natural language tool descriptions for effective interaction with external systems, but poor description quality significantly impacts performance and efficiency.","ai_keywords":["Foundation Model","tool descriptions","agent performance","description quality","execution steps","task success rates","partial goal completion","execution cost","behavioral reliability","token overhead","context window"]},"publishedAt":"2026-02-16T11:10:11.000Z","title":"Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions","summary":"The Model Context Protocol (MCP) introduces a standard specification that defines how Foundation Model (FM)-based agents should interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.\n  Hence, we examine 856 tools spread across 103 MCP servers empirically, assess their description quality, and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, and then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These results indicate that achieving performance gains is not straightforward; while execution cost can act as a trade-off, execution context can also impact. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14878.png","numComments":1,"submittedBy":{"_id":"62b4f3b7464e664268bf4e85","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62b4f3b7464e664268bf4e85/atFIscmB37aur0a1zuQ2o.jpeg","fullname":"Leo","name":"hao-li","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.22144","authors":[{"_id":"699fc58debfce7fbcca91bd2","name":"Lingfeng Ren","hidden":false},{"_id":"699fc58debfce7fbcca91bd3","name":"Weihao Yu","hidden":false},{"_id":"699fc58debfce7fbcca91bd4","name":"Runpeng Yu","hidden":false},{"_id":"699fc58debfce7fbcca91bd5","name":"Xinchao Wang","hidden":false}],"publishedAt":"2026-02-25T17:50:41.000Z","submittedOnDailyAt":"2026-02-26T01:32:45.219Z","title":"NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors","submittedOnDailyBy":{"_id":"5df833bdda6d0311fd3d5403","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png","isPro":false,"fullname":"Weihao Yu","user":"whyu","type":"user"},"summary":"Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.","upvotes":1,"discussionId":"699fc58debfce7fbcca91bd6","githubRepo":"https://github.com/lingfengren/NoLan","githubRepoAddedBy":"user","ai_summary":"Object hallucinations in LVLMs are primarily caused by language decoder priors, leading to the development of a training-free framework that suppresses these priors to reduce hallucinations.","ai_keywords":["Large Vision-Language Models","object hallucination","vision encoder","language decoder","multimodal inputs","text-only inputs","NoLanguage-Hallucination Decoding","NoLan"],"githubStars":3},"publishedAt":"2026-02-25T12:50:41.000Z","title":"NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors","summary":"Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22144.png","numComments":1,"submittedBy":{"_id":"5df833bdda6d0311fd3d5403","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5df833bdda6d0311fd3d5403/62OtGJEQXdOuhV9yCd4HS.png","fullname":"Weihao Yu","name":"whyu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":17,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.21835","authors":[{"_id":"699fba49ebfce7fbcca91b0f","name":"Jianhui Wei","hidden":false},{"_id":"699fba49ebfce7fbcca91b10","name":"Xiaotian Zhang","hidden":false},{"_id":"699fba49ebfce7fbcca91b11","name":"Yichen Li","hidden":false},{"_id":"699fba49ebfce7fbcca91b12","name":"Yuan Wang","hidden":false},{"_id":"699fba49ebfce7fbcca91b13","name":"Yan Zhang","hidden":false},{"_id":"699fba49ebfce7fbcca91b14","name":"Ziyi Chen","hidden":false},{"_id":"699fba49ebfce7fbcca91b15","name":"Zhihang Tang","hidden":false},{"_id":"699fba49ebfce7fbcca91b16","name":"Wei Xu","hidden":false},{"_id":"699fba49ebfce7fbcca91b17","name":"Zuozhu Liu","hidden":false}],"publishedAt":"2026-02-25T12:08:53.000Z","submittedOnDailyAt":"2026-02-26T00:45:50.850Z","title":"UniVBench: Towards Unified Evaluation for Video Foundation Models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.","upvotes":1,"discussionId":"699fba49ebfce7fbcca91b18","githubRepo":"https://github.com/JianhuiWei7/UniVBench","githubRepoAddedBy":"user","ai_summary":"UniVBench introduces a comprehensive benchmark for evaluating video foundation models across multiple capabilities including understanding, generation, editing, and reconstruction using high-quality, diverse video content and a unified evaluation system.","ai_keywords":["video foundation models","video understanding","video generation","video editing","video reconstruction","UniVBench","UniV-Eval","multimodal systems","instruction following","agentic evaluation system"],"githubStars":6},"publishedAt":"2026-02-25T07:08:53.000Z","title":"UniVBench: Towards Unified Evaluation for Video Foundation Models","summary":"Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21835.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":240,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.21374","authors":[{"_id":"69a0bbe8d192e06f5829cbf6","name":"Mohammadreza Ghaffarzadeh-Esfahani","hidden":false},{"_id":"69a0bbe8d192e06f5829cbf7","name":"Nahid Yousefian","hidden":false},{"_id":"69a0bbe8d192e06f5829cbf8","name":"Ebrahim Heidari-Farsani","hidden":false},{"_id":"69a0bbe8d192e06f5829cbf9","name":"Ali Akbar Omidvarian","hidden":false},{"_id":"69a0bbe8d192e06f5829cbfa","name":"Sepehr Ghahraei","hidden":false},{"_id":"69a0bbe8d192e06f5829cbfb","name":"Atena Farangi","hidden":false},{"_id":"69a0bbe8d192e06f5829cbfc","name":"AmirBahador Boroumand","hidden":false}],"publishedAt":"2026-02-24T21:10:29.000Z","submittedOnDailyAt":"2026-02-26T19:08:35.051Z","title":"Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages","submittedOnDailyBy":{"_id":"645d63c0ce72244df7b36be8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/645d63c0ce72244df7b36be8/09vhYAzgv1svwvQM4eIE9.jpeg","isPro":false,"fullname":"MoRezaGH","user":"Moreza009","type":"user"},"summary":"Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.","upvotes":1,"discussionId":"69a0bbe9d192e06f5829cbfd","githubRepo":"https://github.com/mohammad-gh009/Small-language-models-on-clinical-data-extraction","githubRepoAddedBy":"user","ai_summary":"A two-step pipeline using translation and small language models demonstrates effective clinical feature extraction from Persian medical transcripts, with larger models showing better performance and bilingual approaches improving sensitivity.","ai_keywords":["few-shot prompting","macro-averaged F1-score","Matthews Correlation Coefficient","sensitivity","specificity","clinical feature extraction","open-source small language models","translation model","multilingual clinical NLP","privacy-preserving"],"githubStars":0},"publishedAt":"2026-02-24T16:10:29.000Z","title":"Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages","summary":"Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21374.png","numComments":1,"submittedBy":{"_id":"645d63c0ce72244df7b36be8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/645d63c0ce72244df7b36be8/09vhYAzgv1svwvQM4eIE9.jpeg","fullname":"MoRezaGH","name":"Moreza009","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.20857","authors":[{"_id":"699f69bfebfce7fbcca91a51","user":{"_id":"699f67b199fe2283790c968b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/699f67b199fe2283790c968b/HcFbIfr5ShNLdQITW_wl3.jpeg","isPro":false,"fullname":"Teymur Aghayev","user":"teymuraghayev76","type":"user"},"name":"Teymur Aghayev","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:02:31.414Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/699f67b199fe2283790c968b/f7NaV_1yx_hLtryeLE_We.jpeg","https://cdn-uploads.huggingface.co/production/uploads/699f67b199fe2283790c968b/ql9wQZ9wxmw2Po_EsO4Kb.jpeg"],"publishedAt":"2026-02-24T12:58:21.000Z","submittedOnDailyAt":"2026-02-26T07:47:05.995Z","title":"Functional Continuous Decomposition","submittedOnDailyBy":{"_id":"699f67b199fe2283790c968b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/699f67b199fe2283790c968b/HcFbIfr5ShNLdQITW_wl3.jpeg","isPro":false,"fullname":"Teymur Aghayev","user":"teymuraghayev76","type":"user"},"summary":"The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to C^1 continuous fitting, FCD transforms raw time-series data into M modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN.","upvotes":1,"discussionId":"699f69c0ebfce7fbcca91a52","projectPage":"http://arxiv.org/abs/2602.20857","githubRepo":"https://github.com/Tima-a/fcd","githubRepoAddedBy":"user","ai_summary":"Functional Continuous Decomposition enables parametric, continuous optimization of time-series data with guaranteed continuity for capturing local and global patterns, enhancing machine learning model performance through improved feature extraction.","ai_keywords":["Functional Continuous Decomposition","Levenberg-Marquardt optimization","C^1 continuous fitting","time-series data","Convolutional Neural Network","feature extraction","parametric optimization"],"githubStars":1,"organization":{"_id":"66c871cdc5ef68cf6db71ae6","name":"VilniusTech","fullname":"Vilniaus Gedimino technikos universitetas","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66c8700dd33e34fbc2897983/9M-4YzoIy27fgdjTuZcWr.png"}},"publishedAt":"2026-02-24T07:58:21.000Z","title":"Functional Continuous Decomposition","summary":"The analysis of non-stationary time-series data requires insight into its local and global patterns with physical interpretability. However, traditional smoothing algorithms, such as B-splines, Savitzky-Golay filtering, and Empirical Mode Decomposition (EMD), lack the ability to perform parametric optimization with guaranteed continuity. In this paper, we propose Functional Continuous Decomposition (FCD), a JAX-accelerated framework that performs parametric, continuous optimization on a wide range of mathematical functions. By using Levenberg-Marquardt optimization to achieve up to C^1 continuous fitting, FCD transforms raw time-series data into M modes that capture different temporal patterns from short-term to long-term trends. Applications of FCD include physics, medicine, financial analysis, and machine learning, where it is commonly used for the analysis of signal temporal patterns, optimized parameters, derivatives, and integrals of decomposition. Furthermore, FCD can be applied for physical analysis and feature extraction with an average SRMSE of 0.735 per segment and a speed of 0.47s on full decomposition of 1,000 points. Finally, we demonstrate that a Convolutional Neural Network (CNN) enhanced with FCD features, such as optimized function values, parameters, and derivatives, achieved 16.8% faster convergence and 2.5% higher accuracy over a standard CNN.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/699f67b199fe2283790c968b/f7NaV_1yx_hLtryeLE_We.jpeg","https://cdn-uploads.huggingface.co/production/uploads/699f67b199fe2283790c968b/ql9wQZ9wxmw2Po_EsO4Kb.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20857.png","numComments":1,"submittedBy":{"_id":"699f67b199fe2283790c968b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/699f67b199fe2283790c968b/HcFbIfr5ShNLdQITW_wl3.jpeg","fullname":"Teymur Aghayev","name":"teymuraghayev76","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"66c871cdc5ef68cf6db71ae6","name":"VilniusTech","fullname":"Vilniaus Gedimino technikos universitetas","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66c8700dd33e34fbc2897983/9M-4YzoIy27fgdjTuZcWr.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.20273","authors":[{"_id":"699f6620ebfce7fbcca91a4b","user":{"_id":"6240c0c9d35be2c16ccfbe41","avatarUrl":"/avatars/d30d6c029a89939e7802f590480c5492.svg","isPro":false,"fullname":"Josh Ying","user":"zfying","type":"user"},"name":"Zhuofan Josh Ying","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:02:35.839Z","hidden":false},{"_id":"699f6620ebfce7fbcca91a4c","name":"Shauli Ravfogel","hidden":false},{"_id":"699f6620ebfce7fbcca91a4d","name":"Nikolaus Kriegeskorte","hidden":false},{"_id":"699f6620ebfce7fbcca91a4e","name":"Peter Hase","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6240c0c9d35be2c16ccfbe41/x-Owy_bGMoft8_orKTIc3.png"],"publishedAt":"2026-02-23T19:01:31.000Z","submittedOnDailyAt":"2026-02-26T13:00:49.898Z","title":"The Truthfulness Spectrum Hypothesis","submittedOnDailyBy":{"_id":"6240c0c9d35be2c16ccfbe41","avatarUrl":"/avatars/d30d6c029a89939e7802f590480c5492.svg","isPro":false,"fullname":"Josh Ying","user":"zfying","type":"user"},"summary":"Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.","upvotes":1,"discussionId":"699f6620ebfce7fbcca91a4f","githubRepo":"https://github.com/zfying/truth_spec","githubRepoAddedBy":"user","ai_summary":"Large language models contain truth directions ranging from domain-general to domain-specific in their representational space, with linear probes showing varying generalization capabilities and causal interventions revealing differential effectiveness of these directions.","ai_keywords":["linear probes","domain-general","domain-specific","truthfulness spectrum hypothesis","Mahalanobis cosine similarity","concept-erasure methods","causal interventions","representational space","cross-domain generalization"],"githubStars":0,"organization":{"_id":"63f68badb607296857bb2441","name":"columbia","fullname":"Columbia University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"}},"publishedAt":"2026-02-23T14:01:31.000Z","title":"The Truthfulness Spectrum Hypothesis","summary":"Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6240c0c9d35be2c16ccfbe41/x-Owy_bGMoft8_orKTIc3.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20273.png","numComments":1,"submittedBy":{"_id":"6240c0c9d35be2c16ccfbe41","avatarUrl":"/avatars/d30d6c029a89939e7802f590480c5492.svg","fullname":"Josh Ying","name":"zfying","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"63f68badb607296857bb2441","name":"columbia","fullname":"Columbia University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.19594","authors":[{"_id":"69a05014fe0f78cfa89ffc73","name":"Ayush Nangia","hidden":false},{"_id":"69a05014fe0f78cfa89ffc74","name":"Shikhar Mishra","hidden":false},{"_id":"69a05014fe0f78cfa89ffc75","name":"Aman Gokrani","hidden":false},{"_id":"69a05014fe0f78cfa89ffc76","name":"Paras Chopra","hidden":false}],"publishedAt":"2026-02-23T08:37:53.000Z","submittedOnDailyAt":"2026-02-26T11:23:04.587Z","title":"ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?","submittedOnDailyBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","isPro":false,"fullname":"Paras Chopra","user":"paraslossfunk","type":"user"},"summary":"We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.","upvotes":1,"discussionId":"69a05015fe0f78cfa89ffc77","ai_summary":"ISO-Bench evaluates coding agents on real-world LLM inference optimization tasks from popular serving frameworks, using combined execution and LLM-based metrics to assess performance.","ai_keywords":["coding agents","LLM serving frameworks","vLLM","SGLang","benchmark","optimization patch","runtime-based metrics","LLM-based metrics","codebase","bottleneck identification"],"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"}},"publishedAt":"2026-02-23T03:37:53.000Z","title":"ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?","summary":"We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19594.png","numComments":1,"submittedBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","fullname":"Paras Chopra","name":"paraslossfunk","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.18527","authors":[{"_id":"699fbcddebfce7fbcca91b64","user":{"_id":"660bb388aeb2c22d9bd3894a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/660bb388aeb2c22d9bd3894a/zSmagaNp6yXovj8R8-beT.jpeg","isPro":false,"fullname":"Jason Liu","user":"liuzhan22","type":"user"},"name":"Zhan Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-26T10:01:54.438Z","hidden":false},{"_id":"699fbcddebfce7fbcca91b65","name":"Changli Tang","hidden":false},{"_id":"699fbcddebfce7fbcca91b66","name":"Yuxin Wang","hidden":false},{"_id":"699fbcddebfce7fbcca91b67","name":"Zhiyuan Zhu","hidden":false},{"_id":"699fbcddebfce7fbcca91b68","name":"Youjun Chen","hidden":false},{"_id":"699fbcddebfce7fbcca91b69","name":"Yiwen Shao","hidden":false},{"_id":"699fbcddebfce7fbcca91b6a","name":"Tianzi Wang","hidden":false},{"_id":"699fbcddebfce7fbcca91b6b","name":"Lei Ke","hidden":false},{"_id":"699fbcddebfce7fbcca91b6c","name":"Zengrui Jin","hidden":false},{"_id":"699fbcddebfce7fbcca91b6d","name":"Chao Zhang","hidden":false}],"publishedAt":"2026-02-20T04:06:07.000Z","submittedOnDailyAt":"2026-02-26T00:56:24.242Z","title":"JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments","submittedOnDailyBy":{"_id":"660bb388aeb2c22d9bd3894a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/660bb388aeb2c22d9bd3894a/zSmagaNp6yXovj8R8-beT.jpeg","isPro":false,"fullname":"Jason Liu","user":"liuzhan22","type":"user"},"summary":"Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.","upvotes":1,"discussionId":"699fbcdeebfce7fbcca91b6e","ai_summary":"JAEGER extends audio-visual large language models to 3D space by integrating RGB-D observations and multi-channel audio to improve spatial reasoning and source localization.","ai_keywords":["audio-visual large language models","3D space","RGB-D observations","multi-channel first-order ambisonics","neural intensity vector","direction-of-arrival estimation","spatial grounding","spatial reasoning","SpatialSceneQA","instruction-tuning"],"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-02-19T23:06:07.000Z","title":"JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments","summary":"Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18527.png","numComments":1,"submittedBy":{"_id":"660bb388aeb2c22d9bd3894a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/660bb388aeb2c22d9bd3894a/zSmagaNp6yXovj8R8-beT.jpeg","fullname":"Jason Liu","name":"liuzhan22","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":true},{"paper":{"id":"2602.16729","authors":[{"_id":"69a090bdd192e06f5829cbab","name":"Shahriar Golchin","hidden":false},{"_id":"69a090bdd192e06f5829cbac","name":"Marc Wetter","hidden":false}],"publishedAt":"2026-02-17T18:29:22.000Z","submittedOnDailyAt":"2026-02-26T17:30:12.582Z","title":"Intent Laundering: AI Safety Datasets Are Not What They Seem","submittedOnDailyBy":{"_id":"6263102f3662a749162d67e6","avatarUrl":"/avatars/51c36acbc4d3b76632273ccad5ca8d5c.svg","isPro":false,"fullname":"Shahriar Golchin","user":"shahriargolchin","type":"user"},"summary":"We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated \"reasonably safe\" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave.","upvotes":1,"discussionId":"69a090bdd192e06f5829cbad","ai_summary":"AI safety datasets rely heavily on triggering cues that do not accurately represent real-world adversarial behavior, making models appear safe when they are actually vulnerable once these cues are removed.","ai_keywords":["AI safety datasets","adversarial attacks","triggering cues","intent laundering","jailbreaking techniques","black-box access","model safety evaluation"],"organization":{"_id":"63291d28cc0f0ca1420f5b15","name":"Labelbox","fullname":"Labelbox, Inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630dece078e89813403db421/SezxCyNgOeQ60EosiRdGf.png"}},"publishedAt":"2026-02-17T13:29:22.000Z","title":"Intent Laundering: AI Safety Datasets Are Not What They Seem","summary":"We systematically evaluate the quality of widely used AI safety datasets from two perspectives: in isolation and in practice. In isolation, we examine how well these datasets reflect real-world adversarial attacks based on three key properties: being driven by ulterior intent, well-crafted, and out-of-distribution. We find that these datasets overrely on \"triggering cues\": words or phrases with overt negative/sensitive connotations that are intended to trigger safety mechanisms explicitly, which is unrealistic compared to real-world attacks. In practice, we evaluate whether these datasets genuinely measure safety risks or merely provoke refusals through triggering cues. To explore this, we introduce \"intent laundering\": a procedure that abstracts away triggering cues from adversarial attacks (data points) while strictly preserving their malicious intent and all relevant details. Our results indicate that current AI safety datasets fail to faithfully represent real-world adversarial behavior due to their overreliance on triggering cues. Once these cues are removed, all previously evaluated \"reasonably safe\" models become unsafe, including Gemini 3 Pro and Claude Sonnet 3.7. Moreover, when intent laundering is adapted as a jailbreaking technique, it consistently achieves high attack success rates, ranging from 90% to over 98%, under fully black-box access. Overall, our findings expose a significant disconnect between how model safety is evaluated by existing datasets and how real-world adversaries behave.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16729.png","numComments":1,"submittedBy":{"_id":"6263102f3662a749162d67e6","avatarUrl":"/avatars/51c36acbc4d3b76632273ccad5ca8d5c.svg","fullname":"Shahriar Golchin","name":"shahriargolchin","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"63291d28cc0f0ca1420f5b15","name":"Labelbox","fullname":"Labelbox, Inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630dece078e89813403db421/SezxCyNgOeQ60EosiRdGf.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.19004","authors":[{"_id":"699fc5baebfce7fbcca91bd8","name":"Duc Duy Nguyen","hidden":false},{"_id":"699fc5baebfce7fbcca91bd9","name":"Tat-Jun Chin","hidden":false},{"_id":"699fc5baebfce7fbcca91bda","name":"Minh Hoai","hidden":false}],"publishedAt":"2026-02-22T01:54:29.000Z","submittedOnDailyAt":"2026-02-26T01:33:54.991Z","title":"MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment","submittedOnDailyBy":{"_id":"699d825e43bedffd9e114396","avatarUrl":"/avatars/13d9e45303fc234de33eaf7ddddea9dc.svg","isPro":false,"fullname":"Nguyen Duy Duc","user":"duyddwcs123","type":"user"},"summary":"We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind.","upvotes":0,"discussionId":"699fc5baebfce7fbcca91bdb","githubRepo":"https://github.com/bbvisual/MoBind","githubRepoAddedBy":"user","ai_summary":"MoBind learns joint representations between IMU signals and 2D pose sequences through hierarchical contrastive learning to achieve cross-modal retrieval, temporal synchronization, and action recognition with fine-grained alignment.","ai_keywords":["hierarchical contrastive learning","cross-modal retrieval","temporal synchronization","subject localization","body-part localization","action recognition","IMU signals","2D pose sequences","token-level temporal segments","body-part trajectories","global motion aggregation","fine-grained temporal alignment"],"githubStars":2,"organization":{"_id":"64e0ade31ff253fc8d8a724f","name":"AdelaideUniversity","fullname":"Adelaide University"}},"publishedAt":"2026-02-21T20:54:29.000Z","title":"MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment","summary":"We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19004.png","numComments":1,"submittedBy":{"_id":"699d825e43bedffd9e114396","avatarUrl":"/avatars/13d9e45303fc234de33eaf7ddddea9dc.svg","fullname":"Nguyen Duy Duc","name":"duyddwcs123","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"64e0ade31ff253fc8d8a724f","name":"AdelaideUniversity","fullname":"Adelaide University"},"isAuthorParticipating":false},{"paper":{"id":"2602.18964","authors":[{"_id":"699dab1eb767f258d0bf1ac9","user":{"_id":"6426ef119a7e77d65466b9e8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6426ef119a7e77d65466b9e8/-U0eihhdm5_McRrF8U7Yr.jpeg","isPro":false,"fullname":"Toheeb Jimoh","user":"toheebadura","type":"user"},"name":"Toheeb Aduramomi Jimoh","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:31:03.799Z","hidden":false},{"_id":"699dab1eb767f258d0bf1aca","name":"Tabea De Wille","hidden":false},{"_id":"699dab1eb767f258d0bf1acb","name":"Nikola S. Nikolov","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6426ef119a7e77d65466b9e8/cwf3v2_lfI4ZD6zHyL_fV.png"],"publishedAt":"2026-02-21T22:10:18.000Z","submittedOnDailyAt":"2026-02-26T08:25:06.719Z","title":"Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language","submittedOnDailyBy":{"_id":"6426ef119a7e77d65466b9e8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6426ef119a7e77d65466b9e8/-U0eihhdm5_McRrF8U7Yr.jpeg","isPro":false,"fullname":"Toheeb Jimoh","user":"toheebadura","type":"user"},"summary":"Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present Yor-Sarc, the first gold-standard dataset for sarcasm detection in Yorb, a tonal Niger-Congo language spoken by over 50 million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorb sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' = 0.7660; pairwise Cohen's = 0.6732--0.8743), with 83.3% unanimous consensus. One annotator pair achieved almost perfect agreement (= 0.8743; 93.8% raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining 16.7% majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarchttps://github.com/toheebadura/yor-sarc is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.","upvotes":0,"discussionId":"699dab1eb767f258d0bf1acc","projectPage":"https://arxiv.org/abs/2602.18964","githubRepo":"https://github.com/toheebadura/yor-sarc","githubRepoAddedBy":"user","ai_summary":"A new gold-standard dataset for sarcasm detection in Yorb is introduced with high inter-annotator agreement and soft labels for uncertainty-aware modeling.","ai_keywords":["sarcasm detection","Yorb","annotation protocol","inter-annotator agreement","soft labels","uncertainty-aware modeling"],"githubStars":0,"organization":{"_id":"66c9a276cc47b8e6e9d9b337","name":"ULimerick","fullname":"University of Limerick","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66c9a18acc47b8e6e9d955a1/T93bzhawHOVeUjK_D3bXV.png"}},"publishedAt":"2026-02-21T17:10:18.000Z","title":"Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language","summary":"Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present Yor-Sarc, the first gold-standard dataset for sarcasm detection in Yorb, a tonal Niger-Congo language spoken by over 50 million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorb sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' = 0.7660; pairwise Cohen's = 0.6732--0.8743), with 83.3% unanimous consensus. One annotator pair achieved almost perfect agreement (= 0.8743; 93.8% raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining 16.7% majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarchttps://github.com/toheebadura/yor-sarc is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6426ef119a7e77d65466b9e8/cwf3v2_lfI4ZD6zHyL_fV.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18964.png","numComments":1,"submittedBy":{"_id":"6426ef119a7e77d65466b9e8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6426ef119a7e77d65466b9e8/-U0eihhdm5_McRrF8U7Yr.jpeg","fullname":"Toheeb Jimoh","name":"toheebadura","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66c9a276cc47b8e6e9d9b337","name":"ULimerick","fullname":"University of Limerick","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66c9a18acc47b8e6e9d955a1/T93bzhawHOVeUjK_D3bXV.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.18589","authors":[{"_id":"699ea59bdfbcf0b800aecc30","user":{"_id":"699ea5299b93bc6afbfb57f2","avatarUrl":"/avatars/0474c80792ad69fa17329f8df5b66da1.svg","isPro":false,"fullname":"Jiayang Shi","user":"jiayangshi","type":"user"},"name":"Jiayang Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-25T17:28:40.210Z","hidden":false},{"_id":"699ea59bdfbcf0b800aecc31","name":"Daniel M. Pelt","hidden":false},{"_id":"699ea59bdfbcf0b800aecc32","name":"K. Joost Batenburg","hidden":false}],"publishedAt":"2026-02-20T19:54:47.000Z","submittedOnDailyAt":"2026-02-26T10:17:10.223Z","title":"DM4CT: Benchmarking Diffusion Models for Computed Tomography Reconstruction","submittedOnDailyBy":{"_id":"699ea5299b93bc6afbfb57f2","avatarUrl":"/avatars/0474c80792ad69fa17329f8df5b66da1.svg","isPro":false,"fullname":"Jiayang Shi","user":"jiayangshi","type":"user"},"summary":"Diffusion models have recently emerged as powerful priors for solving inverse problems. While computed tomography (CT) is theoretically a linear inverse problem, it poses many practical challenges. These include correlated noise, artifact structures, reliance on system geometry, and misaligned value ranges, which make the direct application of diffusion models more difficult than in domains like natural image generation. To systematically evaluate how diffusion models perform in this context and compare them with established reconstruction methods, we introduce DM4CT, a comprehensive benchmark for CT reconstruction. DM4CT includes datasets from both medical and industrial domains with sparse-view and noisy configurations. To explore the challenges of deploying diffusion models in practice, we additionally acquire a high-resolution CT dataset at a high-energy synchrotron facility and evaluate all methods under real experimental conditions. We benchmark ten recent diffusion-based methods alongside seven strong baselines, including model-based, unsupervised, and supervised approaches. Our analysis provides detailed insights into the behavior, strengths, and limitations of diffusion models for CT reconstruction. The real-world dataset is publicly available at zenodo.org/records/15420527, and the codebase is open-sourced at github.com/DM4CT/DM4CT.","upvotes":0,"discussionId":"699ea59bdfbcf0b800aecc33","projectPage":"https://dm4ct.github.io/DM4CT/","githubRepo":"https://github.com/DM4CT/DM4CT","githubRepoAddedBy":"user","ai_summary":"Diffusion models are evaluated for computed tomography reconstruction through a comprehensive benchmark addressing practical challenges like correlated noise and artifact structures.","ai_keywords":["diffusion models","computed tomography","inverse problems","sparse-view","noisy configurations","CT reconstruction","DM4CT","synchrotron facility","model-based approaches","unsupervised approaches","supervised approaches"],"githubStars":13},"publishedAt":"2026-02-20T14:54:47.000Z","title":"DM4CT: Benchmarking Diffusion Models for Computed Tomography Reconstruction","summary":"Diffusion models have recently emerged as powerful priors for solving inverse problems. While computed tomography (CT) is theoretically a linear inverse problem, it poses many practical challenges. These include correlated noise, artifact structures, reliance on system geometry, and misaligned value ranges, which make the direct application of diffusion models more difficult than in domains like natural image generation. To systematically evaluate how diffusion models perform in this context and compare them with established reconstruction methods, we introduce DM4CT, a comprehensive benchmark for CT reconstruction. DM4CT includes datasets from both medical and industrial domains with sparse-view and noisy configurations. To explore the challenges of deploying diffusion models in practice, we additionally acquire a high-resolution CT dataset at a high-energy synchrotron facility and evaluate all methods under real experimental conditions. We benchmark ten recent diffusion-based methods alongside seven strong baselines, including model-based, unsupervised, and supervised approaches. Our analysis provides detailed insights into the behavior, strengths, and limitations of diffusion models for CT reconstruction. The real-world dataset is publicly available at zenodo.org/records/15420527, and the codebase is open-sourced at github.com/DM4CT/DM4CT.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18589.png","numComments":1,"submittedBy":{"_id":"699ea5299b93bc6afbfb57f2","avatarUrl":"/avatars/0474c80792ad69fa17329f8df5b66da1.svg","fullname":"Jiayang Shi","name":"jiayangshi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true}]
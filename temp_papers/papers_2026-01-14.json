[{"paper":{"id":"2601.06789","authors":[{"_id":"69671036c5e371f6b235d143","user":{"_id":"692881094c3f4293dfe29e3d","avatarUrl":"/avatars/bddfaae8041a45498d46ef65ba17c920.svg","isPro":false,"fullname":"qihao wang","user":"jimson991","type":"user"},"name":"Qihao Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:48:07.079Z","hidden":false},{"_id":"69671036c5e371f6b235d144","user":{"_id":"64b74fca17570fdff9b2aded","avatarUrl":"/avatars/8b3519a7011af52dadc87ffef700c77c.svg","isPro":false,"fullname":"Ziming Cheng","user":"cadche","type":"user"},"name":"Ziming Cheng","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:48:13.370Z","hidden":false},{"_id":"69671036c5e371f6b235d145","user":{"_id":"6513ee3c9af40a65586b43f5","avatarUrl":"/avatars/815ed3876cefa12b25bf955edcbf71a3.svg","isPro":false,"fullname":"shuo zhang","user":"shuozhang","type":"user"},"name":"Shuo Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:48:18.640Z","hidden":false},{"_id":"69671036c5e371f6b235d146","name":"Fan Liu","hidden":false},{"_id":"69671036c5e371f6b235d147","name":"Rui Xu","hidden":false},{"_id":"69671036c5e371f6b235d148","name":"Heng Lian","hidden":false},{"_id":"69671036c5e371f6b235d149","user":{"_id":"65bb3c545a5dbabc818e9044","avatarUrl":"/avatars/4c239557bd5e33179cbf4f3a440bbf33.svg","isPro":false,"fullname":"Kunyi Wang","user":"KunyiWang","type":"user"},"name":"Kunyi Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:48:25.411Z","hidden":false},{"_id":"69671036c5e371f6b235d14a","user":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"name":"Xiaoming Yu","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:48:47.145Z","hidden":false},{"_id":"69671036c5e371f6b235d14b","name":"Jianghao Yin","hidden":false},{"_id":"69671036c5e371f6b235d14c","name":"Sen Hu","hidden":false},{"_id":"69671036c5e371f6b235d14d","name":"Yue Hu","hidden":false},{"_id":"69671036c5e371f6b235d14e","user":{"_id":"64803e5dc57f629056c601f1","avatarUrl":"/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg","isPro":false,"fullname":"Shaolei Zhang","user":"zhangshaolei","type":"user"},"name":"Shaolei Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:49:13.491Z","hidden":false},{"_id":"69671036c5e371f6b235d14f","name":"Yanbing Liu","hidden":false},{"_id":"69671036c5e371f6b235d150","user":{"_id":"6874f7f0f8e67e9b5714adf2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png","isPro":false,"fullname":"RongHao Chen","user":"SuPA4ki","type":"user"},"name":"Ronghao Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:49:02.800Z","hidden":false},{"_id":"69671036c5e371f6b235d151","user":{"_id":"6603d56ab4344a2b07cd6d21","avatarUrl":"/avatars/1569bb60166532317c85e80da722ba1c.svg","isPro":false,"fullname":"Huacan Wang","user":"Huacan-Wang","type":"user"},"name":"Huacan Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:48:57.426Z","hidden":false}],"publishedAt":"2026-01-11T06:41:26.000Z","submittedOnDailyAt":"2026-01-14T01:40:39.607Z","title":"MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences","submittedOnDailyBy":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"summary":"While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.","upvotes":59,"discussionId":"69671036c5e371f6b235d152","githubRepo":"https://github.com/QuantaAlpha/MemGovern","githubRepoAddedBy":"user","ai_summary":"MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.","ai_keywords":["autonomous software engineering","SWE agents","closed-world limitation","open-world experience","GitHub","experience governance","experience cards","agentic experience search","SWE-bench Verified"],"githubStars":19},"publishedAt":"2026-01-11T01:41:26.000Z","title":"MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences","summary":"While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a \"closed-world\" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06789.png","numComments":1,"submittedBy":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","fullname":"Yu_xm","name":"Yu2020","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.07022","authors":[{"_id":"6966474587c71000b5a910d2","user":{"_id":"65446c938737c799e9ad6f83","avatarUrl":"/avatars/6ade251e01442b14cbf8cd7888358fd1.svg","isPro":false,"fullname":"Sungrae Park","user":"sungrae-park","type":"user"},"name":"Sungrae Park","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:41:08.385Z","hidden":false},{"_id":"6966474587c71000b5a910d3","name":"Sanghoon Kim","hidden":false},{"_id":"6966474587c71000b5a910d4","name":"Jungho Cho","hidden":false},{"_id":"6966474587c71000b5a910d5","name":"Gyoungjin Gim","hidden":false},{"_id":"6966474587c71000b5a910d6","name":"Dawoon Jung","hidden":false},{"_id":"6966474587c71000b5a910d7","name":"Mikyoung Cha","hidden":false},{"_id":"6966474587c71000b5a910d8","name":"Eunhae Choo","hidden":false},{"_id":"6966474587c71000b5a910d9","name":"Taekgyu Hong","hidden":false},{"_id":"6966474587c71000b5a910da","name":"Minbyul Jeong","hidden":false},{"_id":"6966474587c71000b5a910db","name":"SeHwan Joo","hidden":false},{"_id":"6966474587c71000b5a910dc","name":"Minsoo Khang","hidden":false},{"_id":"6966474587c71000b5a910dd","name":"Eunwon Kim","hidden":false},{"_id":"6966474587c71000b5a910de","name":"Minjeong Kim","hidden":false},{"_id":"6966474587c71000b5a910df","name":"Sujeong Kim","hidden":false},{"_id":"6966474587c71000b5a910e0","name":"Yunsu Kim","hidden":false},{"_id":"6966474587c71000b5a910e1","name":"Hyeonju Lee","hidden":false},{"_id":"6966474587c71000b5a910e2","name":"Seunghyun Lee","hidden":false},{"_id":"6966474587c71000b5a910e3","name":"Sukyung Lee","hidden":false},{"_id":"6966474587c71000b5a910e4","name":"Siyoung Park","hidden":false},{"_id":"6966474587c71000b5a910e5","name":"Gyungin Shin","hidden":false},{"_id":"6966474587c71000b5a910e6","user":{"_id":"64f04fa29a957782e2224dea","avatarUrl":"/avatars/db853c30ceb59ddabc9a83dc25845690.svg","isPro":false,"fullname":"Inseo Song","user":"SSON9","type":"user"},"name":"Inseo Song","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:52:53.501Z","hidden":false},{"_id":"6966474587c71000b5a910e7","name":"Wonho Song","hidden":false},{"_id":"6966474587c71000b5a910e8","name":"Seonghoon Yang","hidden":false},{"_id":"6966474587c71000b5a910e9","user":{"_id":"66e0d4bf290df82f137de44c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66e0d4bf290df82f137de44c/RJ43RxY56_OvZmauF88Tw.jpeg","isPro":false,"fullname":"Kyle Yi","user":"younatics","type":"user"},"name":"Seungyoun Yi","status":"claimed_verified","statusLastChangedAt":"2026-01-14T12:42:43.424Z","hidden":false},{"_id":"6966474587c71000b5a910ea","name":"Sanghoon Yoon","hidden":false},{"_id":"6966474587c71000b5a910eb","name":"Jeonghyun Ko","hidden":false},{"_id":"6966474587c71000b5a910ec","name":"Seyoung Song","hidden":false},{"_id":"6966474587c71000b5a910ed","name":"Keunwoo Choi","hidden":false},{"_id":"6966474587c71000b5a910ee","name":"Hwalsuk Lee","hidden":false},{"_id":"6966474587c71000b5a910ef","name":"Sunghun Kim","hidden":false},{"_id":"6966474587c71000b5a910f0","name":"Du-Seong Chang","hidden":false},{"_id":"6966474587c71000b5a910f1","name":"Kyunghyun Cho","hidden":false},{"_id":"6966474587c71000b5a910f2","name":"Junsuk Choe","hidden":false},{"_id":"6966474587c71000b5a910f3","name":"Hwaran Lee","hidden":false},{"_id":"6966474587c71000b5a910f4","name":"Jae-Gil Lee","hidden":false},{"_id":"6966474587c71000b5a910f5","name":"KyungTae Lim","hidden":false},{"_id":"6966474587c71000b5a910f6","name":"Alice Oh","hidden":false}],"publishedAt":"2026-01-11T18:33:09.000Z","submittedOnDailyAt":"2026-01-14T02:22:03.363Z","title":"Solar Open Technical Report","submittedOnDailyBy":{"_id":"64587be872b60ae7a3817858","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png","isPro":false,"fullname":"Minbyul Jeong","user":"Minbyul","type":"user"},"summary":"We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.","upvotes":50,"discussionId":"6966474587c71000b5a910f7","ai_summary":"Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.","ai_keywords":["Mixture-of-Experts","language model","underserved languages","data synthesis","progressive curriculum","reinforcement learning","SnapPO","domain-specific data","quality thresholds","composition optimization"],"organization":{"_id":"62940d125d1c94a62e838db2","name":"upstage","fullname":"upstage","avatar":"https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}},"publishedAt":"2026-01-11T13:33:09.000Z","title":"Solar Open Technical Report","summary":"We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07022.png","numComments":1,"submittedBy":{"_id":"64587be872b60ae7a3817858","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png","fullname":"Minbyul Jeong","name":"Minbyul","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"62940d125d1c94a62e838db2","name":"upstage","fullname":"upstage","avatar":"https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.04745","authors":[{"_id":"6964724e138cc47cbd765325","user":{"_id":"68e4ba9bb3738c567535654e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68e4ba9bb3738c567535654e/DmkMgEaKb3N3bnbJYk1cC.png","isPro":false,"fullname":"wu","user":"realty2333","type":"user"},"name":"Tingyu Wu","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:53:39.763Z","hidden":false},{"_id":"6964724e138cc47cbd765326","user":{"_id":"692d850486aa9dfeebcf10b5","avatarUrl":"/avatars/6f7782844275f3eec7d8466fab787923.svg","isPro":false,"fullname":"Zhisheng Chen","user":"Zhisheng888","type":"user"},"name":"Zhisheng Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:49:43.502Z","hidden":false},{"_id":"6964724e138cc47cbd765327","name":"Ziyan Weng","hidden":false},{"_id":"6964724e138cc47cbd765328","user":{"_id":"6776ce2f10eb0715dbb89df6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GH7VYlzgdrUEDlQfW60Ez.png","isPro":false,"fullname":"Shuhe Wangv2","user":"Super-shuhe-v2","type":"user"},"name":"Shuhe Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:49:52.050Z","hidden":false},{"_id":"6964724e138cc47cbd765329","user":{"_id":"6411c9c71d87842eedc5ad23","avatarUrl":"/avatars/b8a06aeafbbf7272a831534c2307d65e.svg","isPro":false,"fullname":"Chenglong Li","user":"ChenglongLi","type":"user"},"name":"Chenglong Li","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:49:57.344Z","hidden":false},{"_id":"6964724e138cc47cbd76532a","name":"Shuo Zhang","hidden":false},{"_id":"6964724e138cc47cbd76532b","name":"Sen Hu","hidden":false},{"_id":"6964724e138cc47cbd76532c","name":"Silin Wu","hidden":false},{"_id":"6964724e138cc47cbd76532d","user":{"_id":"68f287f2faba6f123f8a3b3c","avatarUrl":"/avatars/58a34b0f45bb34d74f86a638eff7dc94.svg","isPro":false,"fullname":"Qizhen Lan","user":"lanqz7766","type":"user"},"name":"Qizhen Lan","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:50:03.306Z","hidden":false},{"_id":"6964724e138cc47cbd76532e","user":{"_id":"6603d56ab4344a2b07cd6d21","avatarUrl":"/avatars/1569bb60166532317c85e80da722ba1c.svg","isPro":false,"fullname":"Huacan Wang","user":"Huacan-Wang","type":"user"},"name":"Huacan Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:50:08.549Z","hidden":false},{"_id":"6964724e138cc47cbd76532f","user":{"_id":"6874f7f0f8e67e9b5714adf2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/g2bltqJmCR7MY3zEaQHr6.png","isPro":false,"fullname":"RongHao Chen","user":"SuPA4ki","type":"user"},"name":"Ronghao Chen","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:50:13.928Z","hidden":false}],"publishedAt":"2026-01-08T09:11:33.000Z","submittedOnDailyAt":"2026-01-14T05:31:09.992Z","title":"KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions","submittedOnDailyBy":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"summary":"Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.","upvotes":46,"discussionId":"6964724f138cc47cbd765330","githubRepo":"https://github.com/QuantaAlpha/KnowMeBench","githubRepoAddedBy":"auto","ai_summary":"Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.","ai_keywords":["memory benchmarks","autobiographical narratives","retrieval-augmented systems","factual recall","subjective state attribution","principle-level reasoning"],"githubStars":83},"publishedAt":"2026-01-08T04:11:33.000Z","title":"KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions","summary":"Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04745.png","numComments":1,"submittedBy":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","fullname":"Yu_xm","name":"Yu2020","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.08225","authors":[{"_id":"6967202cc5e371f6b235d1cc","name":"Jungho Cho","hidden":false},{"_id":"6967202cc5e371f6b235d1cd","name":"Minbyul Jeong","hidden":false},{"_id":"6967202cc5e371f6b235d1ce","user":{"_id":"65446c938737c799e9ad6f83","avatarUrl":"/avatars/6ade251e01442b14cbf8cd7888358fd1.svg","isPro":false,"fullname":"Sungrae Park","user":"sungrae-park","type":"user"},"name":"Sungrae Park","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:49:29.339Z","hidden":false}],"publishedAt":"2026-01-13T05:14:09.000Z","submittedOnDailyAt":"2026-01-14T02:19:55.431Z","title":"User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale","submittedOnDailyBy":{"_id":"64587be872b60ae7a3817858","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png","isPro":false,"fullname":"Minbyul Jeong","user":"Minbyul","type":"user"},"summary":"The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.","upvotes":40,"discussionId":"6967202dc5e371f6b235d1cf","ai_summary":"Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.","ai_keywords":["large reasoning models","multi-turn dialogue generation","task-oriented simulation","user simulator","behavioral rules","turn-by-turn feedback","automated task generation","high-density dataset","human-agent interaction"],"organization":{"_id":"62940d125d1c94a62e838db2","name":"upstage","fullname":"upstage","avatar":"https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"}},"publishedAt":"2026-01-13T00:14:09.000Z","title":"User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale","summary":"The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08225.png","numComments":2,"submittedBy":{"_id":"64587be872b60ae7a3817858","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png","fullname":"Minbyul Jeong","name":"Minbyul","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"62940d125d1c94a62e838db2","name":"upstage","fullname":"upstage","avatar":"https://cdn-uploads.huggingface.co/production/uploads/649144feeb13c70f7671c603/bUxWC5jKltd-MyrrCNCv5.png"},"isAuthorParticipating":false},{"paper":{"id":"2512.24965","authors":[{"_id":"6958946b832867f253525a3a","name":"Siyuan Hu","hidden":false},{"_id":"6958946b832867f253525a3b","user":{"_id":"64440be5af034cdfd69ca3a7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg","isPro":false,"fullname":"Qinghong (Kevin) Lin","user":"KevinQHLin","type":"user"},"name":"Kevin Qinghong Lin","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:53:47.889Z","hidden":false},{"_id":"6958946b832867f253525a3c","user":{"_id":"661ab3da2b14565c7acccf5c","avatarUrl":"/avatars/fa4fc03664803e02aede4d4c3d50b393.svg","isPro":false,"fullname":"Mike Zheng Shou","user":"AnalMom","type":"user"},"name":"Mike Zheng Shou","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:51:00.387Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/_OdaniV8mAw_Y2nQ85G_I.mp4"],"publishedAt":"2025-12-31T16:51:14.000Z","submittedOnDailyAt":"2026-01-14T02:46:22.550Z","title":"ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands","submittedOnDailyBy":{"_id":"64440be5af034cdfd69ca3a7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg","isPro":false,"fullname":"Qinghong (Kevin) Lin","user":"KevinQHLin","type":"user"},"summary":"Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-π, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.","upvotes":34,"discussionId":"6958946b832867f253525a3f","projectPage":"https://showlab.github.io/showui-pi","githubRepo":"https://github.com/showlab/showui-pi","githubRepoAddedBy":"auto","githubStars":18,"organization":{"_id":"63a553c4ce5763e06f78669c","name":"showlab","fullname":"Show Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"}},"publishedAt":"2025-12-31T11:51:14.000Z","title":"ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands","summary":"Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-π, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/_OdaniV8mAw_Y2nQ85G_I.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24965.png","numComments":1,"submittedBy":{"_id":"64440be5af034cdfd69ca3a7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg","fullname":"Qinghong (Kevin) Lin","name":"KevinQHLin","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":41,"isUserFollowing":false},"organization":{"_id":"63a553c4ce5763e06f78669c","name":"showlab","fullname":"Show Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1671779505215-63a55320ce5763e06f78519c.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.08079","authors":[{"_id":"6966fc3cc5e371f6b235d02e","user":{"_id":"64a627232944e255ef574dda","avatarUrl":"/avatars/4c2fd5bf922013fe691c6a3e3fa138a2.svg","isPro":false,"fullname":"Hongjin Qian","user":"TommyChien","type":"user"},"name":"Hongjin Qian","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:50:24.206Z","hidden":false},{"_id":"6966fc3cc5e371f6b235d02f","name":"Zhao Cao","hidden":false},{"_id":"6966fc3cc5e371f6b235d030","user":{"_id":"64a38c590111d5ff6c3d5f2b","avatarUrl":"/avatars/ef13dc7ce243819bc0da9b04e778b432.svg","isPro":false,"fullname":"zhengliu","user":"lz1001","type":"user"},"name":"Zheng Liu","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:50:47.540Z","hidden":false}],"publishedAt":"2026-01-12T23:44:59.000Z","submittedOnDailyAt":"2026-01-14T08:49:23.887Z","title":"MemoBrain: Executive Memory as an Agentic Brain for Reasoning","submittedOnDailyBy":{"_id":"64a38c590111d5ff6c3d5f2b","avatarUrl":"/avatars/ef13dc7ce243819bc0da9b04e778b432.svg","isPro":false,"fullname":"zhengliu","user":"lz1001","type":"user"},"summary":"Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.\n  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.\n  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.","upvotes":31,"discussionId":"6966fc3cc5e371f6b235d031","githubRepo":"https://github.com/qhjqhj00/MemoBrain","githubRepoAddedBy":"user","ai_summary":"Memory management in tool-augmented agents is crucial for maintaining coherent, goal-directed reasoning over extended tasks, requiring explicit mechanisms to track and organize reasoning steps within constrained contexts.","ai_keywords":["tool-augmented agents","long-horizon reasoning","working context","executive memory model","dependency-aware memory","reasoning steps","cognitive control","reasoning trajectories","context budget","memory pruning","sub-trajectory folding"],"githubStars":39},"publishedAt":"2026-01-12T18:44:59.000Z","title":"MemoBrain: Executive Memory as an Agentic Brain for Reasoning","summary":"Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.\n  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.\n  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08079.png","numComments":1,"submittedBy":{"_id":"64a38c590111d5ff6c3d5f2b","avatarUrl":"/avatars/ef13dc7ce243819bc0da9b04e778b432.svg","fullname":"zhengliu","name":"lz1001","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":13,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.06487","authors":[{"_id":"69660763fc8c4ecc02c7fab6","user":{"_id":"66259e3f9277b825c8e11168","avatarUrl":"/avatars/078ab932f6837f502851b75bc719ab2c.svg","isPro":false,"fullname":"Zhang","user":"QiangZhang","type":"user"},"name":"Qiang Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:53:23.892Z","hidden":false},{"_id":"69660763fc8c4ecc02c7fab7","user":{"_id":"61454d930aac1efe3e8d842e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61454d930aac1efe3e8d842e/sQELHgJP6c_547OrT8JZJ.jpeg","isPro":false,"fullname":"boli","user":"bcol","type":"user"},"name":"Boli Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-13T15:45:22.457Z","hidden":false},{"_id":"69660763fc8c4ecc02c7fab8","name":"Fanrui Zhang","hidden":false},{"_id":"69660763fc8c4ecc02c7fab9","name":"Ruixue Ding","hidden":false},{"_id":"69660763fc8c4ecc02c7faba","name":"Shihang Wang","hidden":false},{"_id":"69660763fc8c4ecc02c7fabb","user":{"_id":"657429d833e5a4bf5b278615","avatarUrl":"/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg","isPro":false,"fullname":"QiuchenWang","user":"Qiuchen-Wang","type":"user"},"name":"Qiuchen Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:52:59.514Z","hidden":false},{"_id":"69660763fc8c4ecc02c7fabc","name":"Yinfeng Huang","hidden":false},{"_id":"69660763fc8c4ecc02c7fabd","user":{"_id":"623437bd4f78e3acb8bd14bd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623437bd4f78e3acb8bd14bd/6ZIRpE9We4TiMtzDBtPFS.jpeg","isPro":false,"fullname":"Haonan Zhang","user":"haonanzhang","type":"user"},"name":"Haonan Zhang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:53:47.932Z","hidden":false},{"_id":"69660763fc8c4ecc02c7fabe","name":"Rongxiang Zhu","hidden":false},{"_id":"69660763fc8c4ecc02c7fabf","name":"Pengyong Wang","hidden":false},{"_id":"69660763fc8c4ecc02c7fac0","name":"Ailin Ren","hidden":false},{"_id":"69660763fc8c4ecc02c7fac1","name":"Xin Li","hidden":false},{"_id":"69660763fc8c4ecc02c7fac2","name":"Pengjun Xie","hidden":false},{"_id":"69660763fc8c4ecc02c7fac3","name":"Jiawei Liu","hidden":false},{"_id":"69660763fc8c4ecc02c7fac4","name":"Ning Guo","hidden":false},{"_id":"69660763fc8c4ecc02c7fac5","name":"Jingren Zhou","hidden":false},{"_id":"69660763fc8c4ecc02c7fac6","name":"Zheng-Jun Zha","hidden":false}],"publishedAt":"2026-01-10T08:43:07.000Z","submittedOnDailyAt":"2026-01-14T01:12:42.456Z","title":"ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking","submittedOnDailyBy":{"_id":"657429d833e5a4bf5b278615","avatarUrl":"/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg","isPro":false,"fullname":"QiuchenWang","user":"Qiuchen-Wang","type":"user"},"summary":"Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.","upvotes":30,"discussionId":"69660764fc8c4ecc02c7fac7","projectPage":"https://tongyi-agent.github.io/blog/arenarl/","githubRepo":"https://github.com/Alibaba-NLP/qqr","githubRepoAddedBy":"user","ai_summary":"Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.","ai_keywords":["reinforcement learning","large language model agents","reward models","scalar scores","discrimination collapse","intra-group relative ranking","pairwise evaluation","multi-level rubrics","adversarial arena","tournament-based ranking","advantage estimation","efficiency","precision","benchmarks","SFT","RL training","multi-dimensional evaluation"],"githubStars":49,"organization":{"_id":"661f98de142a51d630dbbcc4","name":"Alibaba-NLP","fullname":"Alibaba-NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"}},"publishedAt":"2026-01-10T03:43:07.000Z","title":"ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking","summary":"Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06487.png","numComments":1,"submittedBy":{"_id":"657429d833e5a4bf5b278615","avatarUrl":"/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg","fullname":"QiuchenWang","name":"Qiuchen-Wang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"661f98de142a51d630dbbcc4","name":"Alibaba-NLP","fullname":"Alibaba-NLP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/dfd_EcIfylvu3sdc2WMqX.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.08584","authors":[{"_id":"6967031ec5e371f6b235d068","name":"Alexander H. Liu","hidden":false},{"_id":"6967031ec5e371f6b235d069","name":"Kartik Khandelwal","hidden":false},{"_id":"6967031ec5e371f6b235d06a","user":{"_id":"627bf27cf19c5eb46d54cea8","avatarUrl":"/avatars/b8ca0b4e841858c1d234671187234f56.svg","isPro":false,"fullname":"Sandeep Subramanian","user":"MaximumEntropy","type":"user"},"name":"Sandeep Subramanian","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:51:19.735Z","hidden":false},{"_id":"6967031ec5e371f6b235d06b","user":{"_id":"686e4868213b9b0627a4d4a0","avatarUrl":"/avatars/e3ec63c0944dacafca8a3eaff84da061.svg","isPro":false,"fullname":"Victor Jouault","user":"victorjouault","type":"user"},"name":"Victor Jouault","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:51:25.262Z","hidden":false},{"_id":"6967031ec5e371f6b235d06c","name":"Abhinav Rastogi","hidden":false},{"_id":"6967031ec5e371f6b235d06d","user":{"_id":"64d5010b050438e3b92247ad","avatarUrl":"/avatars/c527bba4869617b468bd8b90a5d77d7f.svg","isPro":false,"fullname":"adrien sade","user":"sade-adrien","type":"user"},"name":"Adrien Sadé","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:51:34.426Z","hidden":false},{"_id":"6967031ec5e371f6b235d06e","user":{"_id":"6835a7b06cf009790a12a724","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8VcjSbqPctm8NU8bTExf4.png","isPro":false,"fullname":"Alan Jeffares","user":"alanjeffares","type":"user"},"name":"Alan Jeffares","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:51:42.957Z","hidden":false},{"_id":"6967031ec5e371f6b235d06f","name":"Albert Jiang","hidden":false},{"_id":"6967031ec5e371f6b235d070","name":"Alexandre Cahill","hidden":false},{"_id":"6967031ec5e371f6b235d071","name":"Alexandre Gavaudan","hidden":false},{"_id":"6967031ec5e371f6b235d072","user":{"_id":"6391c7456176fbc67b9ecd2c","avatarUrl":"/avatars/4aa9be94d8284118b8018362abf11f01.svg","isPro":false,"fullname":"Alexandre Sablayrolles","user":"alexsablay","type":"user"},"name":"Alexandre Sablayrolles","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:52:00.082Z","hidden":false},{"_id":"6967031ec5e371f6b235d073","name":"Amélie Héliou","hidden":false},{"_id":"6967031ec5e371f6b235d074","user":{"_id":"63c0eeeb3bdc86f8108e22e7","avatarUrl":"/avatars/0aab18e9d3c1f7221434ad1f4de530b1.svg","isPro":false,"fullname":"Amos You","user":"amosyou","type":"user"},"name":"Amos You","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:52:10.792Z","hidden":false},{"_id":"6967031ec5e371f6b235d075","user":{"_id":"62cdbd1ba9be5c19555f7ffc","avatarUrl":"/avatars/d7d60a8628127f62b276bc1815938ea9.svg","isPro":false,"fullname":"Andy Ehrenberg","user":"andyehrenberg","type":"user"},"name":"Andy Ehrenberg","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:52:17.987Z","hidden":false},{"_id":"6967031ec5e371f6b235d076","name":"Andy Lo","hidden":false},{"_id":"6967031ec5e371f6b235d077","name":"Anton Eliseev","hidden":false},{"_id":"6967031ec5e371f6b235d078","user":{"_id":"66e184e86048d62cd8fb4e52","avatarUrl":"/avatars/bc633262648dd5335171e5372552d67d.svg","isPro":false,"fullname":"Antonia Calvi","user":"NinaCalvi","type":"user"},"name":"Antonia Calvi","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:52:31.745Z","hidden":false},{"_id":"6967031ec5e371f6b235d079","user":{"_id":"630df38664f1f8d0c76c01fd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1661858687827-noauth.jpeg","isPro":false,"fullname":"Avinash Sooriyarachchi","user":"AviSoori1x","type":"user"},"name":"Avinash Sooriyarachchi","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:52:37.953Z","hidden":false},{"_id":"6967031ec5e371f6b235d07a","user":{"_id":"64c1b9ab203c836866dcb1dc","avatarUrl":"/avatars/3f400cbabcf0b5ed71761f98a4ce858a.svg","isPro":false,"fullname":"Baptiste Bout","user":"baptiste-bt","type":"user"},"name":"Baptiste Bout","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:52:44.142Z","hidden":false},{"_id":"6967031ec5e371f6b235d07b","name":"Baptiste Rozière","hidden":false},{"_id":"6967031ec5e371f6b235d07c","name":"Baudouin De Monicault","hidden":false},{"_id":"6967031ec5e371f6b235d07d","user":{"_id":"6818899e66f9244540972090","avatarUrl":"/avatars/95a79dceedbf91760ef5ec5be92bc7b3.svg","isPro":false,"fullname":"Clemence Lanfranchi","user":"ClemenceMistral","type":"user"},"name":"Clémence Lanfranchi","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:53:01.383Z","hidden":false},{"_id":"6967031ec5e371f6b235d07e","user":{"_id":"6368fd9c5774ddb7e8effa3e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6368fd9c5774ddb7e8effa3e/d2BcekRfTUX-X5VDC_wbi.jpeg","isPro":false,"fullname":"Corentin Barreau","user":"CorentinBarreau","type":"user"},"name":"Corentin Barreau","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:53:08.522Z","hidden":false},{"_id":"6967031ec5e371f6b235d07f","name":"Cyprien Courtot","hidden":false},{"_id":"6967031ec5e371f6b235d080","name":"Daniele Grattarola","hidden":false},{"_id":"6967031ec5e371f6b235d081","name":"Darius Dabert","hidden":false},{"_id":"6967031ec5e371f6b235d082","name":"Diego de las Casas","hidden":false},{"_id":"6967031ec5e371f6b235d083","name":"Elliot Chane-Sane","hidden":false},{"_id":"6967031ec5e371f6b235d084","name":"Faruk Ahmed","hidden":false},{"_id":"6967031ec5e371f6b235d085","name":"Gabrielle Berrada","hidden":false},{"_id":"6967031ec5e371f6b235d086","name":"Gaëtan Ecrepont","hidden":false},{"_id":"6967031ec5e371f6b235d087","name":"Gauthier Guinet","hidden":false},{"_id":"6967031ec5e371f6b235d088","name":"Georgii Novikov","hidden":false},{"_id":"6967031ec5e371f6b235d089","name":"Guillaume Kunsch","hidden":false},{"_id":"6967031ec5e371f6b235d08a","name":"Guillaume Lample","hidden":false},{"_id":"6967031ec5e371f6b235d08b","name":"Guillaume Martin","hidden":false},{"_id":"6967031ec5e371f6b235d08c","name":"Gunshi Gupta","hidden":false},{"_id":"6967031ec5e371f6b235d08d","name":"Jan Ludziejewski","hidden":false},{"_id":"6967031ec5e371f6b235d08e","name":"Jason Rute","hidden":false},{"_id":"6967031ec5e371f6b235d08f","name":"Joachim Studnia","hidden":false},{"_id":"6967031ec5e371f6b235d090","name":"Jonas Amar","hidden":false},{"_id":"6967031ec5e371f6b235d091","name":"Joséphine Delas","hidden":false},{"_id":"6967031ec5e371f6b235d092","name":"Josselin Somerville Roberts","hidden":false},{"_id":"6967031ec5e371f6b235d093","name":"Karmesh Yadav","hidden":false},{"_id":"6967031ec5e371f6b235d094","name":"Khyathi Chandu","hidden":false},{"_id":"6967031ec5e371f6b235d095","name":"Kush Jain","hidden":false},{"_id":"6967031ec5e371f6b235d096","name":"Laurence Aitchison","hidden":false},{"_id":"6967031ec5e371f6b235d097","name":"Laurent Fainsin","hidden":false},{"_id":"6967031ec5e371f6b235d098","name":"Léonard Blier","hidden":false},{"_id":"6967031ec5e371f6b235d099","name":"Lingxiao Zhao","hidden":false},{"_id":"6967031ec5e371f6b235d09a","name":"Louis Martin","hidden":false},{"_id":"6967031ec5e371f6b235d09b","name":"Lucile Saulnier","hidden":false},{"_id":"6967031ec5e371f6b235d09c","name":"Luyu Gao","hidden":false},{"_id":"6967031ec5e371f6b235d09d","name":"Maarten Buyl","hidden":false},{"_id":"6967031ec5e371f6b235d09e","name":"Margaret Jennings","hidden":false},{"_id":"6967031ec5e371f6b235d09f","name":"Marie Pellat","hidden":false},{"_id":"6967031ec5e371f6b235d0a0","name":"Mark Prins","hidden":false},{"_id":"6967031ec5e371f6b235d0a1","name":"Mathieu Poirée","hidden":false},{"_id":"6967031ec5e371f6b235d0a2","name":"Mathilde Guillaumin","hidden":false},{"_id":"6967031ec5e371f6b235d0a3","name":"Matthieu Dinot","hidden":false},{"_id":"6967031ec5e371f6b235d0a4","name":"Matthieu Futeral","hidden":false},{"_id":"6967031ec5e371f6b235d0a5","name":"Maxime Darrin","hidden":false},{"_id":"6967031ec5e371f6b235d0a6","name":"Maximilian Augustin","hidden":false},{"_id":"6967031ec5e371f6b235d0a7","name":"Mia Chiquier","hidden":false},{"_id":"6967031ec5e371f6b235d0a8","name":"Michel Schimpf","hidden":false},{"_id":"6967031ec5e371f6b235d0a9","name":"Nathan Grinsztajn","hidden":false},{"_id":"6967031ec5e371f6b235d0aa","name":"Neha Gupta","hidden":false},{"_id":"6967031ec5e371f6b235d0ab","name":"Nikhil Raghuraman","hidden":false},{"_id":"6967031ec5e371f6b235d0ac","name":"Olivier Bousquet","hidden":false},{"_id":"6967031ec5e371f6b235d0ad","name":"Olivier Duchenne","hidden":false},{"_id":"6967031ec5e371f6b235d0ae","name":"Patricia Wang","hidden":false},{"_id":"6967031ec5e371f6b235d0af","name":"Patrick von Platen","hidden":false},{"_id":"6967031ec5e371f6b235d0b0","name":"Paul Jacob","hidden":false},{"_id":"6967031ec5e371f6b235d0b1","name":"Paul Wambergue","hidden":false},{"_id":"6967031ec5e371f6b235d0b2","name":"Paula Kurylowicz","hidden":false},{"_id":"6967031ec5e371f6b235d0b3","name":"Pavankumar Reddy Muddireddy","hidden":false},{"_id":"6967031ec5e371f6b235d0b4","name":"Philomène Chagniot","hidden":false},{"_id":"6967031ec5e371f6b235d0b5","name":"Pierre Stock","hidden":false},{"_id":"6967031ec5e371f6b235d0b6","name":"Pravesh Agrawal","hidden":false},{"_id":"6967031ec5e371f6b235d0b7","name":"Quentin Torroba","hidden":false},{"_id":"6967031ec5e371f6b235d0b8","name":"Romain Sauvestre","hidden":false},{"_id":"6967031ec5e371f6b235d0b9","name":"Roman Soletskyi","hidden":false},{"_id":"6967031ec5e371f6b235d0ba","name":"Rupert Menneer","hidden":false},{"_id":"6967031ec5e371f6b235d0bb","name":"Sagar Vaze","hidden":false},{"_id":"6967031ec5e371f6b235d0bc","name":"Samuel Barry","hidden":false},{"_id":"6967031ec5e371f6b235d0bd","name":"Sanchit Gandhi","hidden":false},{"_id":"6967031ec5e371f6b235d0be","name":"Siddhant Waghjale","hidden":false},{"_id":"6967031ec5e371f6b235d0bf","name":"Siddharth Gandhi","hidden":false},{"_id":"6967031ec5e371f6b235d0c0","name":"Soham Ghosh","hidden":false},{"_id":"6967031ec5e371f6b235d0c1","name":"Srijan Mishra","hidden":false},{"_id":"6967031ec5e371f6b235d0c2","name":"Sumukh Aithal","hidden":false},{"_id":"6967031ec5e371f6b235d0c3","name":"Szymon Antoniak","hidden":false},{"_id":"6967031ec5e371f6b235d0c4","name":"Teven Le Scao","hidden":false},{"_id":"6967031ec5e371f6b235d0c5","name":"Théo Cachet","hidden":false},{"_id":"6967031ec5e371f6b235d0c6","name":"Theo Simon Sorg","hidden":false},{"_id":"6967031ec5e371f6b235d0c7","name":"Thibaut Lavril","hidden":false},{"_id":"6967031ec5e371f6b235d0c8","name":"Thiziri Nait Saada","hidden":false},{"_id":"6967031ec5e371f6b235d0c9","name":"Thomas Chabal","hidden":false},{"_id":"6967031ec5e371f6b235d0ca","name":"Thomas Foubert","hidden":false},{"_id":"6967031ec5e371f6b235d0cb","name":"Thomas Robert","hidden":false},{"_id":"6967031ec5e371f6b235d0cc","name":"Thomas Wang","hidden":false},{"_id":"6967031ec5e371f6b235d0cd","name":"Tim Lawson","hidden":false},{"_id":"6967031ec5e371f6b235d0ce","name":"Tom Bewley","hidden":false},{"_id":"6967031ec5e371f6b235d0cf","name":"Tom Bewley","hidden":false},{"_id":"6967031ec5e371f6b235d0d0","name":"Tom Edwards","hidden":false},{"_id":"6967031ec5e371f6b235d0d1","name":"Umar Jamil","hidden":false},{"_id":"6967031ec5e371f6b235d0d2","name":"Umberto Tomasini","hidden":false},{"_id":"6967031ec5e371f6b235d0d3","name":"Valeriia Nemychnikova","hidden":false},{"_id":"6967031ec5e371f6b235d0d4","name":"Van Phung","hidden":false},{"_id":"6967031ec5e371f6b235d0d5","name":"Vincent Maladière","hidden":false},{"_id":"6967031ec5e371f6b235d0d6","name":"Virgile Richard","hidden":false},{"_id":"6967031ec5e371f6b235d0d7","name":"Wassim Bouaziz","hidden":false},{"_id":"6967031ec5e371f6b235d0d8","name":"Wen-Ding Li","hidden":false},{"_id":"6967031ec5e371f6b235d0d9","name":"William Marshall","hidden":false},{"_id":"6967031ec5e371f6b235d0da","name":"Xinghui Li","hidden":false},{"_id":"6967031ec5e371f6b235d0db","name":"Xinyu Yang","hidden":false},{"_id":"6967031ec5e371f6b235d0dc","name":"Yassine El Ouahidi","hidden":false},{"_id":"6967031ec5e371f6b235d0dd","name":"Yihan Wang","hidden":false},{"_id":"6967031ec5e371f6b235d0de","name":"Yunhao Tang","hidden":false},{"_id":"6967031ec5e371f6b235d0df","name":"Zaccharie Ramzi","hidden":false}],"publishedAt":"2026-01-13T14:06:03.000Z","submittedOnDailyAt":"2026-01-14T00:14:55.855Z","title":"Ministral 3","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.","upvotes":28,"discussionId":"6967031ec5e371f6b235d0e0","ai_summary":"The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.","ai_keywords":["parameter-efficient dense language models","cascade distillation","iterative pruning","continued training","distillation technique","instruction finetuned","reasoning model","image understanding capabilities"],"organization":{"_id":"64edf4004f42c35eea1b1632","name":"mistralai","fullname":"Mistral AI_","avatar":"https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"}},"publishedAt":"2026-01-13T09:06:03.000Z","title":"Ministral 3","summary":"We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08584.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":208,"isUserFollowing":false},"organization":{"_id":"64edf4004f42c35eea1b1632","name":"mistralai","fullname":"Mistral AI_","avatar":"https://cdn-uploads.huggingface.co/production/uploads/634c17653d11eaedd88b314d/9OgyfKstSZtbmsmuG8MbU.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08831","authors":[{"_id":"6967aa5cc5e371f6b235d2d8","name":"Yang-Che Sun","hidden":false},{"_id":"6967aa5cc5e371f6b235d2d9","name":"Cheng Sun","hidden":false},{"_id":"6967aa5cc5e371f6b235d2da","name":"Chin-Yang Lin","hidden":false},{"_id":"6967aa5cc5e371f6b235d2db","name":"Fu-En Yang","hidden":false},{"_id":"6967aa5cc5e371f6b235d2dc","name":"Min-Hung Chen","hidden":false},{"_id":"6967aa5cc5e371f6b235d2dd","name":"Yen-Yu Lin","hidden":false},{"_id":"6967aa5cc5e371f6b235d2de","name":"Yu-Lun Liu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/qT8bYUbrv5gN2DdSwhj8x.mp4"],"publishedAt":"2026-01-13T18:59:54.000Z","submittedOnDailyAt":"2026-01-14T12:09:27.723Z","title":"3AM: Segment Anything with Geometric Consistency in Videos","submittedOnDailyBy":{"_id":"6459d5da3b6fafd9664807ab","avatarUrl":"/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg","isPro":false,"fullname":"Yu-Lun Liu","user":"yulunliu","type":"user"},"summary":"Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/","upvotes":21,"discussionId":"6967aa5dc5e371f6b235d2df","projectPage":"https://jayisaking.github.io/3AM-Page/","ai_summary":"3AM enhances video object segmentation by integrating 3D-aware features from MUSt3R into SAM2, achieving improved viewpoint consistency with only RGB input at inference.","ai_keywords":["SAM2","MUSt3R","3D-aware features","Feature Merger","multi-level features","geometric correspondence","spatial position","visual similarity","field-of-view aware sampling","wide-baseline motion","IoU","Positive IoU"]},"publishedAt":"2026-01-13T13:59:54.000Z","title":"3AM: Segment Anything with Geometric Consistency in Videos","summary":"Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/qT8bYUbrv5gN2DdSwhj8x.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08831.png","numComments":1,"submittedBy":{"_id":"6459d5da3b6fafd9664807ab","avatarUrl":"/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg","fullname":"Yu-Lun Liu","name":"yulunliu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.07264","authors":[{"_id":"69671055c5e371f6b235d154","user":{"_id":"65b8909c89eb3dfbe8d26780","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg","isPro":false,"fullname":"Weihao XUAN","user":"weihao1115","type":"user"},"name":"Weihao Xuan","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:51:23.840Z","hidden":false},{"_id":"69671055c5e371f6b235d155","name":"Qingcheng Zeng","hidden":false},{"_id":"69671055c5e371f6b235d156","name":"Heli Qi","hidden":false},{"_id":"69671055c5e371f6b235d157","user":{"_id":"67c8af4885b7a86d7515b77c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4bRu0MKg6voGX5fyqninn.png","isPro":false,"fullname":"Lorenzo Xiao","user":"lrzneedresearch","type":"user"},"name":"Yunze Xiao","status":"claimed_verified","statusLastChangedAt":"2026-01-14T12:47:43.795Z","hidden":false},{"_id":"69671055c5e371f6b235d158","user":{"_id":"65aaf7bb494b2faa870f04d2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5twYdL2f-2h3Dx0BhC7HW.png","isPro":false,"fullname":"JJ Wang","user":"junjuewang","type":"user"},"name":"Junjue Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:54:17.992Z","hidden":false},{"_id":"69671055c5e371f6b235d159","user":{"_id":"635c8bafe3737b9e4e29406a","avatarUrl":"/avatars/884e8e086fe374512ed6956ab59038af.svg","isPro":false,"fullname":"naoto yoko","user":"Naotoyokoyama","type":"user"},"name":"Naoto Yokoya","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:54:24.392Z","hidden":false}],"publishedAt":"2026-01-12T07:10:35.000Z","submittedOnDailyAt":"2026-01-14T01:18:38.163Z","title":"The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents","submittedOnDailyBy":{"_id":"63bc77661374e3ef9135735f","avatarUrl":"/avatars/94b04545ed9d30bfe58691672a0b5618.svg","isPro":false,"fullname":"Qingcheng Zeng","user":"qcz","type":"user"},"summary":"Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.","upvotes":21,"discussionId":"69671055c5e371f6b235d15a","ai_summary":"Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.","ai_keywords":["large language models","tool-integrated agents","calibration","reinforcement learning","reward design","verbalized calibration","evidence tools","verification tools"]},"publishedAt":"2026-01-12T02:10:35.000Z","title":"The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents","summary":"Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07264.png","numComments":1,"submittedBy":{"_id":"63bc77661374e3ef9135735f","avatarUrl":"/avatars/94b04545ed9d30bfe58691672a0b5618.svg","fullname":"Qingcheng Zeng","name":"qcz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.08670","authors":[{"_id":"69679f39c5e371f6b235d2ca","user":{"_id":"62c88e75a5ac2974c0a5c8ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666106981514-62c88e75a5ac2974c0a5c8ea.jpeg","isPro":false,"fullname":"Giulio Corallo","user":"giulio98","type":"user"},"name":"Giulio Corallo","status":"claimed_verified","statusLastChangedAt":"2026-01-14T14:21:42.003Z","hidden":false},{"_id":"69679f39c5e371f6b235d2cb","name":"Paolo Papotti","hidden":false}],"publishedAt":"2026-01-13T15:46:59.000Z","submittedOnDailyAt":"2026-01-14T11:21:59.741Z","title":"Parallel Context-of-Experts Decoding for Retrieval Augmented Generation","submittedOnDailyBy":{"_id":"62c88e75a5ac2974c0a5c8ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666106981514-62c88e75a5ac2974c0a5c8ea.jpeg","isPro":false,"fullname":"Giulio Corallo","user":"giulio98","type":"user"},"summary":"Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.","upvotes":17,"discussionId":"69679f3ac5e371f6b235d2cc","ai_summary":"Parallel Context-of-Experts Decoding enables multi-document reasoning in retrieval-augmented generation by treating retrieved documents as isolated experts and synchronizing predictions through a retrieval-aware contrastive decoding rule.","ai_keywords":["retrieval augmented generation","attention mechanism","decoding","retrieval-aware contrastive decoding","expert models","cross-document reasoning","prefill bottlenecks","document KV caches"],"organization":{"_id":"6152dcdfecf3ca6ab820e328","name":"SAP","fullname":"SAP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67f7a0ad07b08a2b3c3ac94e/lIw9a3y-z_5RGc7i64YPu.png"}},"publishedAt":"2026-01-13T10:46:59.000Z","title":"Parallel Context-of-Experts Decoding for Retrieval Augmented Generation","summary":"Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08670.png","numComments":1,"submittedBy":{"_id":"62c88e75a5ac2974c0a5c8ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666106981514-62c88e75a5ac2974c0a5c8ea.jpeg","fullname":"Giulio Corallo","name":"giulio98","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"6152dcdfecf3ca6ab820e328","name":"SAP","fullname":"SAP","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67f7a0ad07b08a2b3c3ac94e/lIw9a3y-z_5RGc7i64YPu.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.08620","authors":[{"_id":"69676b6fc5e371f6b235d275","name":"António Loison","hidden":false},{"_id":"69676b6fc5e371f6b235d276","name":"Quentin Macé","hidden":false},{"_id":"69676b6fc5e371f6b235d277","user":{"_id":"66e16a677c2eb2da5109fb5c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66e16a677c2eb2da5109fb5c/jPkg3g4mtCYbm8ci-ymtX.png","isPro":false,"fullname":"Antoine EDY","user":"antoineedy","type":"user"},"name":"Antoine Edy","status":"claimed_verified","statusLastChangedAt":"2026-01-14T14:21:43.867Z","hidden":false},{"_id":"69676b6fc5e371f6b235d278","name":"Victor Xing","hidden":false},{"_id":"69676b6fc5e371f6b235d279","name":"Tom Balough","hidden":false},{"_id":"69676b6fc5e371f6b235d27a","name":"Gabriel Moreira","hidden":false},{"_id":"69676b6fc5e371f6b235d27b","name":"Bo Liu","hidden":false},{"_id":"69676b6fc5e371f6b235d27c","name":"Manuel Faysse","hidden":false},{"_id":"69676b6fc5e371f6b235d27d","name":"Céline Hudelot","hidden":false},{"_id":"69676b6fc5e371f6b235d27e","name":"Gautier Viaud","hidden":false}],"publishedAt":"2026-01-13T15:00:33.000Z","submittedOnDailyAt":"2026-01-14T12:06:32.312Z","title":"ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios","submittedOnDailyBy":{"_id":"661e945eebe3616a1b09e279","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/661e945eebe3616a1b09e279/U3DL1BNouUpcusCKAPZm0.jpeg","isPro":false,"fullname":"Quentin Macé","user":"QuentinJG","type":"user"},"summary":"Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.","upvotes":7,"discussionId":"69676b6fc5e371f6b235d27f","ai_summary":"ViDoRe v3 is a multimodal retrieval-augmented generation benchmark with diverse document types and languages, revealing that visual retrieval and late interaction models improve performance while current systems still face challenges with non-textual elements and complex queries.","ai_keywords":["Retrieval-Augmented Generation","multimodal RAG","visual retrieval","late-interaction models","textual reranking","hybrid models","visual grounding","document corpora","multi-type queries","human annotation"],"organization":{"_id":"66461aecbaca00c4a2bd4150","name":"vidore","fullname":"Vidore","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66e16a677c2eb2da5109fb5c/bYfczmI-GQ-EEGbMuZUbS.png"}},"publishedAt":"2026-01-13T10:00:33.000Z","title":"ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios","summary":"Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08620.png","numComments":1,"submittedBy":{"_id":"661e945eebe3616a1b09e279","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/661e945eebe3616a1b09e279/U3DL1BNouUpcusCKAPZm0.jpeg","fullname":"Quentin Macé","name":"QuentinJG","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":29,"isUserFollowing":false},"organization":{"_id":"66461aecbaca00c4a2bd4150","name":"vidore","fullname":"Vidore","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66e16a677c2eb2da5109fb5c/bYfczmI-GQ-EEGbMuZUbS.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08303","authors":[{"_id":"696708d3c5e371f6b235d10c","name":"Dongting Hu","hidden":false},{"_id":"696708d3c5e371f6b235d10d","name":"Aarush Gupta","hidden":false},{"_id":"696708d3c5e371f6b235d10e","name":"Magzhan Gabidolla","hidden":false},{"_id":"696708d3c5e371f6b235d10f","name":"Arpit Sahni","hidden":false},{"_id":"696708d3c5e371f6b235d110","name":"Huseyin Coskun","hidden":false},{"_id":"696708d3c5e371f6b235d111","name":"Yanyu Li","hidden":false},{"_id":"696708d3c5e371f6b235d112","name":"Yerlan Idelbayev","hidden":false},{"_id":"696708d3c5e371f6b235d113","name":"Ahsan Mahmood","hidden":false},{"_id":"696708d3c5e371f6b235d114","name":"Aleksei Lebedev","hidden":false},{"_id":"696708d3c5e371f6b235d115","name":"Dishani Lahiri","hidden":false},{"_id":"696708d3c5e371f6b235d116","name":"Anujraaj Goyal","hidden":false},{"_id":"696708d3c5e371f6b235d117","name":"Ju Hu","hidden":false},{"_id":"696708d3c5e371f6b235d118","name":"Mingming Gong","hidden":false},{"_id":"696708d3c5e371f6b235d119","name":"Sergey Tulyakov","hidden":false},{"_id":"696708d3c5e371f6b235d11a","name":"Anil Kag","hidden":false}],"publishedAt":"2026-01-13T07:46:46.000Z","submittedOnDailyAt":"2026-01-14T00:39:31.241Z","title":"SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.","upvotes":7,"discussionId":"696708d3c5e371f6b235d11b","ai_summary":"An efficient diffusion transformer framework for mobile and edge devices that maintains high-generation quality while reducing computational costs through compact architecture, elastic training, and knowledge-guided distillation.","ai_keywords":["diffusion transformers","DiTs","sparse attention mechanism","elastic training","supernetwork","knowledge-guided distribution matching distillation","step-distillation pipeline","teacher models","high-fidelity generation","low-latency generation"],"organization":{"_id":"668450a2c1cbe5e008ac6515","name":"Snapchat","fullname":"Snapchat Inc.","avatar":"https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"}},"publishedAt":"2026-01-13T02:46:46.000Z","title":"SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices","summary":"Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08303.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":208,"isUserFollowing":false},"organization":{"_id":"668450a2c1cbe5e008ac6515","name":"Snapchat","fullname":"Snapchat Inc.","avatar":"https://cdn-uploads.huggingface.co/production/uploads/648ca58a39d2584ee47efef6/plasFy052q2795odYb6NO.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.08828","authors":[{"_id":"696707d0c5e371f6b235d0fc","user":{"_id":"613940c0905b1938233881e3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png","isPro":false,"fullname":"Xindi Wu","user":"xindiw","type":"user"},"name":"Xindi Wu","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:54:39.179Z","hidden":false},{"_id":"696707d0c5e371f6b235d0fd","user":{"_id":"6802b9b830403e82449e31f6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9SlqIc-_PmUwgEAMDefHb.png","isPro":false,"fullname":"Despoina Paschalidou","user":"dpaschalidou","type":"user"},"name":"Despoina Paschalidou","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:54:45.681Z","hidden":false},{"_id":"696707d0c5e371f6b235d0fe","name":"Jun Gao","hidden":false},{"_id":"696707d0c5e371f6b235d0ff","user":{"_id":"6744c6ec6ec99a37d4ba9235","avatarUrl":"/avatars/a5384b63bb615192f6fa157c6ea89e92.svg","isPro":false,"fullname":"Antonio","user":"Antoniotorralbaborruel","type":"user"},"name":"Antonio Torralba","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:54:55.870Z","hidden":false},{"_id":"696707d0c5e371f6b235d100","name":"Laura Leal-Taixé","hidden":false},{"_id":"696707d0c5e371f6b235d101","name":"Olga Russakovsky","hidden":false},{"_id":"696707d0c5e371f6b235d102","name":"Sanja Fidler","hidden":false},{"_id":"696707d0c5e371f6b235d103","user":{"_id":"631b7370bf1351ed2bd0abdc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/631b7370bf1351ed2bd0abdc/p0ZRMjgp5mt3sT5OhdHsp.png","isPro":false,"fullname":"Jonathan Lorraine","user":"lorraine2","type":"user"},"name":"Jonathan Lorraine","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:55:12.587Z","hidden":false}],"publishedAt":"2026-01-13T18:59:09.000Z","submittedOnDailyAt":"2026-01-14T00:35:02.653Z","title":"Motion Attribution for Video Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.","upvotes":6,"discussionId":"696707d0c5e371f6b235d104","projectPage":"https://research.nvidia.com/labs/sil/projects/MOTIVE/","ai_summary":"Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.","ai_keywords":["video generation models","data attribution","gradient-based","motion-centric","temporal dynamics","motion-weighted loss masks","fine-tuning","text-to-video models","VBench","motion smoothness","dynamic degree"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-01-13T13:59:09.000Z","title":"Motion Attribution for Video Generation","summary":"Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08828.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":208,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08665","authors":[{"_id":"69670f41c5e371f6b235d124","user":{"_id":"66a4e3f1dcf448e5a7a91e6b","avatarUrl":"/avatars/94e047d2390298c70e0e330e4987a3e1.svg","isPro":false,"fullname":"Shaoan Wang","user":"wsakobe","type":"user"},"name":"Shaoan Wang","status":"admin_assigned","statusLastChangedAt":"2026-01-14T12:55:24.003Z","hidden":false},{"_id":"69670f41c5e371f6b235d125","name":"Yuanfei Luo","hidden":false},{"_id":"69670f41c5e371f6b235d126","name":"Xingyu Chen","hidden":false},{"_id":"69670f41c5e371f6b235d127","name":"Aocheng Luo","hidden":false},{"_id":"69670f41c5e371f6b235d128","name":"Dongyue Li","hidden":false},{"_id":"69670f41c5e371f6b235d129","name":"Chang Liu","hidden":false},{"_id":"69670f41c5e371f6b235d12a","name":"Sheng Chen","hidden":false},{"_id":"69670f41c5e371f6b235d12b","name":"Yangang Zhang","hidden":false},{"_id":"69670f41c5e371f6b235d12c","name":"Junzhi Yu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/2jcT5yoG-xs87FKOi4SnB.mp4"],"publishedAt":"2026-01-13T15:43:43.000Z","submittedOnDailyAt":"2026-01-14T01:06:58.251Z","title":"VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.","upvotes":6,"discussionId":"69670f41c5e371f6b235d12d","projectPage":"https://wsakobe.github.io/VLingNav-web/","ai_summary":"VLingNav enhances embodied navigation through linguistic-driven cognition with adaptive reasoning and visual-assisted memory, achieving state-of-the-art performance and zero-shot transfer to real robots.","ai_keywords":["VLA models","embodied navigation","large VLMs","dual-process theory","chain-of-thought mechanism","visual-assisted linguistic memory","cross-modal semantic memory","Nav-AdaCoT-2.9M","online expert-guided reinforcement learning","zero-shot transfer"],"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-01-13T10:43:43.000Z","title":"VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory","summary":"VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/2jcT5yoG-xs87FKOi4SnB.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08665.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":208,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08587","authors":[{"_id":"696705cec5e371f6b235d0f4","name":"Zhengbo Xu","hidden":false},{"_id":"696705cec5e371f6b235d0f5","user":{"_id":"641ab8b0a5f876fe30c1ff87","avatarUrl":"/avatars/a95544619ef05814d58e2b2b8581f223.svg","isPro":false,"fullname":"jiema","user":"unrealMJ","type":"user"},"name":"Jie Ma","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:51:30.285Z","hidden":false},{"_id":"696705cec5e371f6b235d0f6","name":"Ziheng Wang","hidden":false},{"_id":"696705cec5e371f6b235d0f7","name":"Zhan Peng","hidden":false},{"_id":"696705cec5e371f6b235d0f8","user":{"_id":"6455afeabda0fbba412d4922","avatarUrl":"/avatars/367731ce1c71d1e19ff415a52ae4067d.svg","isPro":false,"fullname":"Jun Liang","user":"utopiar","type":"user"},"name":"Jun Liang","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:51:28.253Z","hidden":false},{"_id":"696705cec5e371f6b235d0f9","name":"Jing Li","hidden":false}],"publishedAt":"2026-01-13T14:10:34.000Z","submittedOnDailyAt":"2026-01-14T00:27:48.852Z","title":"End-to-End Video Character Replacement without Structural Guidance","submittedOnDailyBy":{"_id":"640d3eaa3623f6a56dde856d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg","isPro":true,"fullname":"vansin","user":"vansin","type":"user"},"summary":"Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha","upvotes":6,"discussionId":"696705cfc5e371f6b235d0fa","projectPage":"https://orange-3dv-team.github.io/MoCha/","githubRepo":"https://github.com/Orange-3DV-Team/MoCha","githubRepoAddedBy":"user","ai_summary":"MoCha enables controllable video character replacement using a single frame mask through condition-aware RoPE and a comprehensive data construction pipeline with specialized datasets.","ai_keywords":["controllable video character replacement","reconstruction-based paradigm","per-frame segmentation masks","structural guidance","facial identity","condition-aware RoPE","RL-based post-training","data construction pipeline","high-fidelity rendered dataset","expression-driven dataset","augmented dataset"],"githubStars":550,"organization":{"_id":"69670c472ad4f2c0e892575c","name":"Orange-Team","fullname":"Orange Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6455afeabda0fbba412d4922/Sy7zZn0kb-Q1SCSUwOn-9.png"}},"publishedAt":"2026-01-13T09:10:34.000Z","title":"End-to-End Video Character Replacement without Structural Guidance","summary":"Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08587.png","numComments":1,"submittedBy":{"_id":"640d3eaa3623f6a56dde856d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg","fullname":"vansin","name":"vansin","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":36,"isUserFollowing":false},"organization":{"_id":"69670c472ad4f2c0e892575c","name":"Orange-Team","fullname":"Orange Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6455afeabda0fbba412d4922/Sy7zZn0kb-Q1SCSUwOn-9.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08468","authors":[{"_id":"69670378c5e371f6b235d0e2","name":"Jiangshan Duo","hidden":false},{"_id":"69670378c5e371f6b235d0e3","name":"Hanyu Li","hidden":false},{"_id":"69670378c5e371f6b235d0e4","name":"Hailin Zhang","hidden":false},{"_id":"69670378c5e371f6b235d0e5","name":"Yudong Wang","hidden":false},{"_id":"69670378c5e371f6b235d0e6","name":"Sujian Li","hidden":false},{"_id":"69670378c5e371f6b235d0e7","name":"Liang Zhao","hidden":false}],"publishedAt":"2026-01-13T11:47:42.000Z","submittedOnDailyAt":"2026-01-14T00:16:30.844Z","title":"JudgeRLVR: Judge First, Generate Second for Efficient Reasoning","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.","upvotes":5,"discussionId":"69670378c5e371f6b235d0e8","ai_summary":"Reinforcement learning with verifiable rewards is enhanced through a judge-then-generate paradigm that improves both efficiency and accuracy in mathematical problem-solving.","ai_keywords":["reinforcement learning","verifiable rewards","large language models","discriminative capability","judge-then-generate paradigm","fine-tuning","vanilla RLVR","Qwen3-30B-A3B","mathematical problem-solving"]},"publishedAt":"2026-01-13T06:47:42.000Z","title":"JudgeRLVR: Judge First, Generate Second for Efficient Reasoning","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08468.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":208,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.06786","authors":[{"_id":"6967833bc5e371f6b235d29d","name":"Jewon Yeom","hidden":false},{"_id":"6967833bc5e371f6b235d29e","name":"Jaewon Sok","hidden":false},{"_id":"6967833bc5e371f6b235d29f","name":"Seonghyeon Park","hidden":false},{"_id":"6967833bc5e371f6b235d2a0","name":"Jeongjae Park","hidden":false},{"_id":"6967833bc5e371f6b235d2a1","name":"Taesup Kim","hidden":false}],"publishedAt":"2026-01-11T06:21:13.000Z","submittedOnDailyAt":"2026-01-14T09:21:47.145Z","title":"EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs","submittedOnDailyBy":{"_id":"646323796577948b3c62017e","avatarUrl":"/avatars/94c6a33bdf180e5c9e0cbfbc157c4311.svg","isPro":false,"fullname":"Je Won YEOM","user":"Scuttie","type":"user"},"summary":"Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.","upvotes":5,"discussionId":"6967833bc5e371f6b235d2a2","ai_summary":"Training large language models for improved reasoning while maintaining calibration through epistemic learning reduces inference compute requirements and enhances performance on mathematical and coding tasks.","ai_keywords":["large language models","self-training","model-generated data","reasoning abilities","calibration","model collapse","epistemic learning","epistemically-calibrated reasoning","supervised fine-tuning","self-evaluation signals","Pareto-superiority","mathematical reasoning","code generation","inference compute"],"organization":{"_id":"66d54dc8033492801db2bf5a","name":"SeoulNatlUniv","fullname":"Seoul National University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"}},"publishedAt":"2026-01-11T01:21:13.000Z","title":"EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs","summary":"Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06786.png","numComments":1,"submittedBy":{"_id":"646323796577948b3c62017e","avatarUrl":"/avatars/94c6a33bdf180e5c9e0cbfbc157c4311.svg","fullname":"Je Won YEOM","name":"Scuttie","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"66d54dc8033492801db2bf5a","name":"SeoulNatlUniv","fullname":"Seoul National University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.08321","authors":[{"_id":"6967daf0c5e371f6b235d328","name":"Lichen Ma","hidden":false},{"_id":"6967daf0c5e371f6b235d329","name":"Xiaolong Fu","hidden":false},{"_id":"6967daf0c5e371f6b235d32a","name":"Gaojing Zhou","hidden":false},{"_id":"6967daf0c5e371f6b235d32b","name":"Zipeng Guo","hidden":false},{"_id":"6967daf0c5e371f6b235d32c","name":"Ting Zhu","hidden":false},{"_id":"6967daf0c5e371f6b235d32d","name":"Yichun Liu","hidden":false},{"_id":"6967daf0c5e371f6b235d32e","name":"Yu Shi","hidden":false},{"_id":"6967daf0c5e371f6b235d32f","name":"Jason Li","hidden":false},{"_id":"6967daf0c5e371f6b235d330","name":"Junshi Huang","hidden":false}],"publishedAt":"2026-01-13T08:18:49.000Z","submittedOnDailyAt":"2026-01-14T15:36:05.786Z","title":"UM-Text: A Unified Multimodal Model for Image Understanding","submittedOnDailyBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","isPro":true,"fullname":"AK","user":"akhaliq","type":"user"},"summary":"With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.","upvotes":4,"discussionId":"6967daf0c5e371f6b235d331","ai_summary":"A unified multimodal model for visual text editing that understands natural language instructions and maintains stylistic consistency with reference images through visual language modeling and contextual embedding combination.","ai_keywords":["visual language model","visual text editing","natural language instructions","stylistic consistency","UM-Encoder","regional consistency loss","three-stage training strategy","UM-DATA-200K"]},"publishedAt":"2026-01-13T03:18:49.000Z","title":"UM-Text: A Unified Multimodal Model for Image Understanding","summary":"With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08321.png","numComments":1,"submittedBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","fullname":"AK","name":"akhaliq","type":"user","isPro":true,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":9097,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.07290","authors":[{"_id":"69660cecfc8c4ecc02c7fae4","user":{"_id":"657d11ee1ede8a7bb79cc47d","avatarUrl":"/avatars/b357fbd92885a26c34ededda637c6384.svg","isPro":false,"fullname":"JPShi","user":"JPShi","type":"user"},"name":"Jiapeng Shi","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:52:57.701Z","hidden":false},{"_id":"69660cecfc8c4ecc02c7fae5","name":"Junke Wang","hidden":false},{"_id":"69660cecfc8c4ecc02c7fae6","name":"Zuyao You","hidden":false},{"_id":"69660cecfc8c4ecc02c7fae7","name":"Bo He","hidden":false},{"_id":"69660cecfc8c4ecc02c7fae8","name":"Zuxuan Wu","hidden":false}],"publishedAt":"2026-01-12T07:51:37.000Z","submittedOnDailyAt":"2026-01-14T13:46:47.002Z","title":"VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding","submittedOnDailyBy":{"_id":"657d11ee1ede8a7bb79cc47d","avatarUrl":"/avatars/b357fbd92885a26c34ededda637c6384.svg","isPro":false,"fullname":"JPShi","user":"JPShi","type":"user"},"summary":"This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.","upvotes":4,"discussionId":"69660cecfc8c4ecc02c7fae9","githubRepo":"https://github.com/JPShi12/VideoLoom","githubRepoAddedBy":"user","ai_summary":"VideoLoom is a unified video large language model that achieves state-of-the-art performance in spatial-temporal video understanding through a specialized dataset and benchmark.","ai_keywords":["Video LLM","spatial-temporal understanding","LoomData-8.7k","temporally grounded captions","spatially localized captions","ReVOS","referring video object segmentation","Charades-STA","temporal grounding","LoomBench"],"githubStars":12,"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}},"publishedAt":"2026-01-12T02:51:37.000Z","title":"VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding","summary":"This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07290.png","numComments":1,"submittedBy":{"_id":"657d11ee1ede8a7bb79cc47d","avatarUrl":"/avatars/b357fbd92885a26c34ededda637c6384.svg","fullname":"JPShi","name":"JPShi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.04582","authors":[{"_id":"69670c16c5e371f6b235d11d","name":"Mizanur Rahman","hidden":false},{"_id":"69670c16c5e371f6b235d11e","name":"Mohammed Saidul Islam","hidden":false},{"_id":"69670c16c5e371f6b235d11f","name":"Md Tahmid Rahman Laskar","hidden":false},{"_id":"69670c16c5e371f6b235d120","name":"Shafiq Joty","hidden":false},{"_id":"69670c16c5e371f6b235d121","name":"Enamul Hoque","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64fb380148411fc78972acab/63_NquH1f0T9wDKJ4Pska.png"],"publishedAt":"2026-01-08T04:29:07.000Z","submittedOnDailyAt":"2026-01-14T00:56:54.754Z","title":"Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization","submittedOnDailyBy":{"_id":"64fb380148411fc78972acab","avatarUrl":"/avatars/e3b1fb54dea4d5e293acf79857a835a1.svg","isPro":false,"fullname":"Mizanur","user":"mizanurr","type":"user"},"summary":"Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.","upvotes":2,"discussionId":"69670c16c5e371f6b235d122","ai_summary":"A reinforcement learning framework for text-to-visualization generation that improves chart quality and code execution by optimizing multiple objectives using post-execution feedback.","ai_keywords":["Text2Vis","reinforcement learning","Group Relative Policy Optimization","multi-objective reward","code execution","visualization quality","post-execution feedback","Qwen2.5"],"organization":{"_id":"671048d8de5ba28c43b7d815","name":"York-reem1100","fullname":"York University ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6710486b243baf4c58f17a1b/NjIJAOU6CEijEtKN0uidL.png"}},"publishedAt":"2026-01-07T23:29:07.000Z","title":"Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization","summary":"Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64fb380148411fc78972acab/63_NquH1f0T9wDKJ4Pska.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04582.png","numComments":1,"submittedBy":{"_id":"64fb380148411fc78972acab","avatarUrl":"/avatars/e3b1fb54dea4d5e293acf79857a835a1.svg","fullname":"Mizanur","name":"mizanurr","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"671048d8de5ba28c43b7d815","name":"York-reem1100","fullname":"York University ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6710486b243baf4c58f17a1b/NjIJAOU6CEijEtKN0uidL.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.07632","authors":[{"_id":"6966ab01c5e371f6b235cf6e","user":{"_id":"66bb6f5a7686e47d84f111d1","avatarUrl":"/avatars/8f4ddca4ef47e07389a93ed126d8ffb6.svg","isPro":false,"fullname":"Zhankai Ye","user":"zy22b","type":"user"},"name":"Zhankai Ye","status":"claimed_verified","statusLastChangedAt":"2026-01-14T09:52:38.382Z","hidden":false},{"_id":"6966ab01c5e371f6b235cf6f","name":"Bofan Li","hidden":false},{"_id":"6966ab01c5e371f6b235cf70","name":"Yukai Jin","hidden":false},{"_id":"6966ab01c5e371f6b235cf71","name":"Shuoqiu Li","hidden":false},{"_id":"6966ab01c5e371f6b235cf72","name":"Wei Wang","hidden":false},{"_id":"6966ab01c5e371f6b235cf73","name":"Yanfu Zhang","hidden":false},{"_id":"6966ab01c5e371f6b235cf74","name":"Shangqian Gao","hidden":false},{"_id":"6966ab01c5e371f6b235cf75","name":"Xin Liu","hidden":false}],"publishedAt":"2026-01-12T15:14:29.000Z","submittedOnDailyAt":"2026-01-14T18:58:55.475Z","title":"GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models","submittedOnDailyBy":{"_id":"66bb6f5a7686e47d84f111d1","avatarUrl":"/avatars/8f4ddca4ef47e07389a93ed126d8ffb6.svg","isPro":false,"fullname":"Zhankai Ye","user":"zy22b","type":"user"},"summary":"Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.","upvotes":1,"discussionId":"6966ab01c5e371f6b235cf76","githubRepo":"https://github.com/JYe16/GeoMotionGPT","githubRepoAddedBy":"user","ai_summary":"A novel framework for motion understanding and motion-language reasoning that aligns motion token geometry with language model embeddings through orthogonal constraints and sparse projection techniques.","ai_keywords":["Large Language Models","motion tokenization","semantic embedding learning","motion codebook","Gumbel-Softmax","sparse projection","orthonormal regularization","decoder-only quantizer","motion-language reasoning","geometric alignment"],"githubStars":1,"organization":{"_id":"6495f573ba0bfbb2d028c910","name":"FloridaStateUniversity","fullname":"Florida State University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6495f507b68b87c87b5a9e61/6fR9wVuKXcj8ixSDkGRxA.png"}},"publishedAt":"2026-01-12T10:14:29.000Z","title":"GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models","summary":"Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07632.png","numComments":1,"submittedBy":{"_id":"66bb6f5a7686e47d84f111d1","avatarUrl":"/avatars/8f4ddca4ef47e07389a93ed126d8ffb6.svg","fullname":"Zhankai Ye","name":"zy22b","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6495f573ba0bfbb2d028c910","name":"FloridaStateUniversity","fullname":"Florida State University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6495f507b68b87c87b5a9e61/6fR9wVuKXcj8ixSDkGRxA.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.02669","authors":[{"_id":"696742c7c5e371f6b235d1e7","name":"Hongzhan Lin","hidden":false},{"_id":"696742c7c5e371f6b235d1e8","name":"Zixin Chen","hidden":false},{"_id":"696742c7c5e371f6b235d1e9","name":"Zhiqi Shen","hidden":false},{"_id":"696742c7c5e371f6b235d1ea","name":"Ziyang Luo","hidden":false},{"_id":"696742c7c5e371f6b235d1eb","name":"Zhen Ye","hidden":false},{"_id":"696742c7c5e371f6b235d1ec","name":"Jing Ma","hidden":false},{"_id":"696742c7c5e371f6b235d1ed","name":"Tat-Seng Chua","hidden":false},{"_id":"696742c7c5e371f6b235d1ee","name":"Guandong Xu","hidden":false}],"publishedAt":"2026-01-06T02:51:56.000Z","submittedOnDailyAt":"2026-01-14T04:58:02.826Z","title":"Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking","submittedOnDailyBy":{"_id":"6499466c7d1edf7cb612a9a6","avatarUrl":"/avatars/c2e18594aa0879db8226f2a04496fb0b.svg","isPro":false,"fullname":"Hongzhan Lin","user":"danielhzlin","type":"user"},"summary":"Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.","upvotes":1,"discussionId":"696742c7c5e371f6b235d1ef","ai_summary":"FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.","ai_keywords":["fact-checking workflow","claim extraction","evidence retrieval","LLM-driven fact-checking process","tool-augmented interactions","justification-based verdict prediction","arena-styled judgment mechanism","consolidated reference guidelines","pairwise comparisons","claim-evolution module","semantically controlled claims","factual robustness","end-to-end fact-checking competence","holistic evaluation","factual reasoning","safety-critical applications"]},"publishedAt":"2026-01-05T21:51:56.000Z","title":"Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking","summary":"Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02669.png","numComments":1,"submittedBy":{"_id":"6499466c7d1edf7cb612a9a6","avatarUrl":"/avatars/c2e18594aa0879db8226f2a04496fb0b.svg","fullname":"Hongzhan Lin","name":"danielhzlin","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false}]
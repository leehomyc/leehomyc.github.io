[{"paper":{"id":"2602.23152","authors":[{"_id":"69a106faa13deaa449448917","name":"Jingxuan Wei","hidden":false},{"_id":"69a106faa13deaa449448918","user":{"_id":"640f7083208821a59b74c757","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678735253848-640f7083208821a59b74c757.jpeg","isPro":false,"fullname":"Siyuan Li","user":"Lupin1998","type":"user"},"name":"Siyuan Li","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:47.703Z","hidden":false},{"_id":"69a106faa13deaa449448919","name":"Yuhang Xu","hidden":false},{"_id":"69a106faa13deaa44944891a","name":"Zheng Sun","hidden":false},{"_id":"69a106faa13deaa44944891b","name":"Junjie Jiang","hidden":false},{"_id":"69a106faa13deaa44944891c","name":"Hexuan Jin","hidden":false},{"_id":"69a106faa13deaa44944891d","name":"Caijun Jia","hidden":false},{"_id":"69a106faa13deaa44944891e","name":"Honghao He","hidden":false},{"_id":"69a106faa13deaa44944891f","name":"Xinglong Xu","hidden":false},{"_id":"69a106faa13deaa449448920","name":"Xi bai","hidden":false},{"_id":"69a106faa13deaa449448921","name":"Chang Yu","hidden":false},{"_id":"69a106faa13deaa449448922","name":"Yumou Liu","hidden":false},{"_id":"69a106faa13deaa449448923","name":"Junnan Zhu","hidden":false},{"_id":"69a106faa13deaa449448924","user":{"_id":"64ef522242da8d2a897d62da","avatarUrl":"/avatars/03611010d247da66696ac8976d4d3ed3.svg","isPro":false,"fullname":"xuanhe zhou","user":"zhouxh19","type":"user"},"name":"Xuanhe Zhou","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:45.120Z","hidden":false},{"_id":"69a106faa13deaa449448925","name":"Jintao Chen","hidden":false},{"_id":"69a106faa13deaa449448926","name":"Xiaobin Hu","hidden":false},{"_id":"69a106faa13deaa449448927","name":"Shancheng Pang","hidden":false},{"_id":"69a106faa13deaa449448928","name":"Bihui Yu","hidden":false},{"_id":"69a106faa13deaa449448929","name":"Ran He","hidden":false},{"_id":"69a106faa13deaa44944892a","name":"Zhen Lei","hidden":false},{"_id":"69a106faa13deaa44944892b","name":"Stan Z. Li","hidden":false},{"_id":"69a106faa13deaa44944892c","name":"Conghui He","hidden":false},{"_id":"69a106faa13deaa44944892d","name":"Shuicheng Yan","hidden":false},{"_id":"69a106faa13deaa44944892e","user":{"_id":"64be296a46cc3cdfbb057f7e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg","isPro":false,"fullname":"Cheng Tan","user":"chengtan9907","type":"user"},"name":"Cheng Tan","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:37.726Z","hidden":false}],"publishedAt":"2026-02-26T16:15:55.000Z","submittedOnDailyAt":"2026-02-27T00:24:05.560Z","title":"The Trinity of Consistency as a Defining Principle for General World Models","submittedOnDailyBy":{"_id":"64be296a46cc3cdfbb057f7e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg","isPro":false,"fullname":"Cheng Tan","user":"chengtan9907","type":"user"},"summary":"The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.","upvotes":159,"discussionId":"69a106faa13deaa44944892f","projectPage":"https://openraiser.github.io/CoW-Bench/","githubRepo":"https://github.com/openraiser/awesome-world-model-evolution","githubRepoAddedBy":"user","ai_summary":"World Models require three consistency principles—modal, spatial, and temporal—for general artificial intelligence, with a proposed benchmark evaluating multimodal learning systems.","ai_keywords":["World Models","video generation models","Unified Multimodal Model","multimodal learning","multi-frame reasoning","CoW-Bench","modal consistency","spatial consistency","temporal consistency"],"githubStars":15,"organization":{"_id":"66ce9d1f5e180b9b9c8e6f31","name":"opendatalab","fullname":"OpenDataLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}},"publishedAt":"2026-02-26T11:15:55.000Z","title":"The Trinity of Consistency as a Defining Principle for General World Models","summary":"The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23152.png","numComments":3,"submittedBy":{"_id":"64be296a46cc3cdfbb057f7e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg","fullname":"Cheng Tan","name":"chengtan9907","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"66ce9d1f5e180b9b9c8e6f31","name":"opendatalab","fullname":"OpenDataLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.22859","authors":[{"_id":"69a0ffd0a13deaa4494488fb","user":{"_id":"66a4c04555677524a0c8047b","avatarUrl":"/avatars/0913eadc4e33eab9ac705e1ad4df9fc9.svg","isPro":false,"fullname":"Hongrui Jia","user":"hongruijia","type":"user"},"name":"Hongrui Jia","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:41:05.608Z","hidden":false},{"_id":"69a0ffd0a13deaa4494488fc","name":"Chaoya Jiang","hidden":false},{"_id":"69a0ffd0a13deaa4494488fd","name":"Shikun Zhang","hidden":false},{"_id":"69a0ffd0a13deaa4494488fe","name":"Wei Ye","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/61b9976efd429dff1ed3bd44/IMYdZMOvbDFl55vSivuiA.jpeg"],"publishedAt":"2026-02-26T10:53:57.000Z","submittedOnDailyAt":"2026-02-27T04:28:11.559Z","title":"From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models","submittedOnDailyBy":{"_id":"61b9976efd429dff1ed3bd44","avatarUrl":"/avatars/5cdaa04e970e1e3dcfbb38ba89c0660a.svg","isPro":false,"fullname":"jiangchaoya","user":"jcy","type":"user"},"summary":"As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.","upvotes":142,"discussionId":"69a0ffd0a13deaa4494488ff","githubRepo":"https://github.com/hongruijia/DPE","githubRepoAddedBy":"user","ai_summary":"Diagnostic-driven Progressive Evolution enables continuous improvement of large multimodal models through iterative diagnosis and targeted data generation guided by identified weaknesses.","ai_keywords":["Large Multimodal Models","reinforcement learning","diagnostic-driven progressive evolution","continual learning","multimodal data","quality control","targeted reinforcement"],"githubStars":27},"publishedAt":"2026-02-26T05:53:57.000Z","title":"From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models","summary":"As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/61b9976efd429dff1ed3bd44/IMYdZMOvbDFl55vSivuiA.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22859.png","numComments":2,"submittedBy":{"_id":"61b9976efd429dff1ed3bd44","avatarUrl":"/avatars/5cdaa04e970e1e3dcfbb38ba89c0660a.svg","fullname":"jiangchaoya","name":"jcy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.22638","authors":[{"_id":"69a13c51a13deaa449448a3f","name":"Zhiheng Song","hidden":false},{"_id":"69a13c51a13deaa449448a40","name":"Jingshuai Zhang","hidden":false},{"_id":"69a13c51a13deaa449448a41","name":"Chuan Qin","hidden":false},{"_id":"69a13c51a13deaa449448a42","name":"Chao Wang","hidden":false},{"_id":"69a13c51a13deaa449448a43","name":"Chao Chen","hidden":false},{"_id":"69a13c51a13deaa449448a44","name":"Longfei Xu","hidden":false},{"_id":"69a13c51a13deaa449448a45","name":"Kaikui Liu","hidden":false},{"_id":"69a13c51a13deaa449448a46","name":"Xiangxiang Chu","hidden":false},{"_id":"69a13c51a13deaa449448a47","name":"Hengshu Zhu","hidden":false}],"publishedAt":"2026-02-26T05:39:38.000Z","submittedOnDailyAt":"2026-02-27T09:31:41.352Z","title":"MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios","submittedOnDailyBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","isPro":false,"fullname":"xiaochonglinghu","user":"xiaochonglinghu","type":"user"},"summary":"Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .","upvotes":86,"discussionId":"69a13c52a13deaa449448a48","githubRepo":"https://github.com/AMAP-ML/MobilityBench","githubRepoAddedBy":"user","ai_summary":"MobileBench is a scalable benchmark for evaluating LLM-based route-planning agents in real-world scenarios, featuring anonymized user queries and a deterministic sandbox for reproducible testing.","ai_keywords":["route-planning agents","large language models","MobilityBench","API-replay sandbox","deterministic environment","multi-dimensional evaluation","outcome validity","instruction understanding","planning","tool use","efficiency","real-world mobility scenarios"],"githubStars":94,"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}},"publishedAt":"2026-02-26T00:39:38.000Z","title":"MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios","summary":"Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making. However, systematic evaluation in real-world mobility settings is hindered by diverse routing demands, non-deterministic mapping services, and limited reproducibility. In this study, we introduce MobilityBench, a scalable benchmark for evaluating LLM-based route-planning agents in real-world mobility scenarios. MobilityBench is constructed from large-scale, anonymized real user queries collected from Amap and covers a broad spectrum of route-planning intents across multiple cities worldwide. To enable reproducible, end-to-end evaluation, we design a deterministic API-replay sandbox that eliminates environmental variance from live services. We further propose a multi-dimensional evaluation protocol centered on outcome validity, complemented by assessments of instruction understanding, planning, tool use, and efficiency. Using MobilityBench, we evaluate multiple LLM-based route-planning agents across diverse real-world mobility scenarios and provide an in-depth analysis of their behaviors and performance. Our findings reveal that current models perform competently on Basic information retrieval and Route Planning tasks, yet struggle considerably with Preference-Constrained Route Planning, underscoring significant room for improvement in personalized mobility applications. We publicly release the benchmark data, evaluation toolkit, and documentation at https://github.com/AMAP-ML/MobilityBench .","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22638.png","numComments":2,"submittedBy":{"_id":"66d255e3947594430c723ff6","avatarUrl":"/avatars/c56e4792332a01bf34085a75ee64916e.svg","fullname":"xiaochonglinghu","name":"xiaochonglinghu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"64488b334988ee01f2a8d856","name":"alibaba-inc","fullname":"alibaba-inc","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.22897","authors":[{"_id":"69a0fda9a13deaa4494488e3","name":"Xiaoxi Li","hidden":false},{"_id":"69a0fda9a13deaa4494488e4","user":{"_id":"63db16330cc3bc12bc0b6f8f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63db16330cc3bc12bc0b6f8f/ld0JQIfX1SBlDVDOmw9VT.jpeg","isPro":false,"fullname":"Wenxiang Jiao","user":"wxjiao","type":"user"},"name":"Wenxiang Jiao","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:41:37.304Z","hidden":false},{"_id":"69a0fda9a13deaa4494488e5","name":"Jiarui Jin","hidden":false},{"_id":"69a0fda9a13deaa4494488e6","name":"Shijian Wang","hidden":false},{"_id":"69a0fda9a13deaa4494488e7","user":{"_id":"61cd4b833dd34ba1985e0753","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png","isPro":false,"fullname":"KABI","user":"dongguanting","type":"user"},"name":"Guanting Dong","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:41:09.826Z","hidden":false},{"_id":"69a0fda9a13deaa4494488e8","name":"Jiajie Jin","hidden":false},{"_id":"69a0fda9a13deaa4494488e9","name":"Hao Wang","hidden":false},{"_id":"69a0fda9a13deaa4494488ea","name":"Yinuo Wang","hidden":false},{"_id":"69a0fda9a13deaa4494488eb","name":"Ji-Rong Wen","hidden":false},{"_id":"69a0fda9a13deaa4494488ec","name":"Yuan Lu","hidden":false},{"_id":"69a0fda9a13deaa4494488ed","name":"Zhicheng Dou","hidden":false}],"publishedAt":"2026-02-26T11:35:04.000Z","submittedOnDailyAt":"2026-02-27T00:29:04.184Z","title":"OmniGAIA: Towards Native Omni-Modal AI Agents","submittedOnDailyBy":{"_id":"66e03eace17fb5ff054b7686","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66e03eace17fb5ff054b7686/PpSV0Qo5lwTyxIZMp57xq.jpeg","isPro":false,"fullname":"Xiaoxi Li","user":"lixiaoxi45","type":"user"},"summary":"Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.","upvotes":46,"discussionId":"69a0fda9a13deaa4494488ee","githubRepo":"https://github.com/RUC-NLPIR/OmniGAIA","githubRepoAddedBy":"user","ai_summary":"OmniGAIA benchmark evaluates multi-modal agents on complex reasoning tasks across video, audio, and image modalities, while OmniAtlas agent improves tool-use capabilities through hindsight-guided tree exploration and OmniDPO fine-tuning.","ai_keywords":["multi-modal LLMs","omni-modal perception","cross-modal reasoning","tool-integrated reasoning","hindsight-guided tree exploration","OmniDPO"],"githubStars":34},"publishedAt":"2026-02-26T06:35:04.000Z","title":"OmniGAIA: Towards Native Omni-Modal AI Agents","summary":"Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22897.png","numComments":2,"submittedBy":{"_id":"66e03eace17fb5ff054b7686","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66e03eace17fb5ff054b7686/PpSV0Qo5lwTyxIZMp57xq.jpeg","fullname":"Xiaoxi Li","name":"lixiaoxi45","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":21,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.22766","authors":[{"_id":"69a112ffa13deaa44944896f","name":"You Li","hidden":false},{"_id":"69a112ffa13deaa449448970","name":"Chi Chen","hidden":false},{"_id":"69a112ffa13deaa449448971","name":"Yanghao Li","hidden":false},{"_id":"69a112ffa13deaa449448972","name":"Fanhu Zeng","hidden":false},{"_id":"69a112ffa13deaa449448973","name":"Kaiyu Huang","hidden":false},{"_id":"69a112ffa13deaa449448974","name":"Jinan Xu","hidden":false},{"_id":"69a112ffa13deaa449448975","name":"Maosong Sun","hidden":false}],"publishedAt":"2026-02-26T08:56:23.000Z","submittedOnDailyAt":"2026-02-27T01:16:22.929Z","title":"Imagination Helps Visual Reasoning, But Not Yet in Latent Space","submittedOnDailyBy":{"_id":"654f3e104c8874c64d43aafa","avatarUrl":"/avatars/00de263f98a81c52cdb321fb11b16c06.svg","isPro":false,"fullname":"You Li","user":"Michael4933","type":"user"},"summary":"Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.","upvotes":31,"discussionId":"69a112ffa13deaa449448976","githubRepo":"https://github.com/AI9Stars/CapImagine","githubRepoAddedBy":"user","ai_summary":"Research reveals that latent visual reasoning in multimodal models suffers from input-latent and latent-answer disconnects, leading to the proposal of CapImagine, a text-based approach that outperforms complex latent-space methods.","ai_keywords":["Multimodal Large Language Models","causal mediation analysis","latent tokens","visual reasoning","input-latent disconnect","latent-answer disconnect","CapImagine"],"githubStars":10,"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-02-26T03:56:23.000Z","title":"Imagination Helps Visual Reasoning, But Not Yet in Latent Space","summary":"Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22766.png","numComments":2,"submittedBy":{"_id":"654f3e104c8874c64d43aafa","avatarUrl":"/avatars/00de263f98a81c52cdb321fb11b16c06.svg","fullname":"You Li","name":"Michael4933","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":false},{"paper":{"id":"2602.23008","authors":[{"_id":"69a11540a13deaa449448995","user":{"_id":"66228a37f5c285535cc9cc83","avatarUrl":"/avatars/0982d6553a9c9001ebdca2878bcfff34.svg","isPro":false,"fullname":"Zeyuan Liu","user":"ZeyuanLiu","type":"user"},"name":"Zeyuan Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:23.614Z","hidden":false},{"_id":"69a11540a13deaa449448996","user":{"_id":"63e48f6d9db5da2dc1f6288e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676046878664-63e48f6d9db5da2dc1f6288e.png","isPro":false,"fullname":"JeonghyeKim","user":"beanie00","type":"user"},"name":"Jeonghye Kim","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:18.705Z","hidden":false},{"_id":"69a11540a13deaa449448997","user":{"_id":"66a1f912345b3106f47ce860","avatarUrl":"/avatars/40177299c64e16703e7bfe83de0810be.svg","isPro":false,"fullname":"Xufang Luo","user":"daixufang","type":"user"},"name":"Xufang Luo","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:15.195Z","hidden":false},{"_id":"69a11540a13deaa449448998","name":"Dongsheng Li","hidden":false},{"_id":"69a11540a13deaa449448999","name":"Yuqing Yang","hidden":false}],"publishedAt":"2026-02-26T13:50:57.000Z","submittedOnDailyAt":"2026-02-27T01:31:44.571Z","title":"Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization","submittedOnDailyBy":{"_id":"63e48f6d9db5da2dc1f6288e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676046878664-63e48f6d9db5da2dc1f6288e.png","isPro":false,"fullname":"JeonghyeKim","user":"beanie00","type":"user"},"summary":"Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.","upvotes":26,"discussionId":"69a11540a13deaa44944899a","ai_summary":"EMPO² is a hybrid reinforcement learning framework that enhances exploration for large language model agents by integrating memory mechanisms with on- and off-policy updates, demonstrating improved performance and adaptability in complex environments.","ai_keywords":["reinforcement learning","large language model agents","exploration","memory augmentation","on-policy updates","off-policy updates","ScienceWorld","WebShop"],"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}},"publishedAt":"2026-02-26T08:50:57.000Z","title":"Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization","summary":"Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23008.png","numComments":2,"submittedBy":{"_id":"63e48f6d9db5da2dc1f6288e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676046878664-63e48f6d9db5da2dc1f6288e.png","fullname":"JeonghyeKim","name":"beanie00","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.23258","authors":[{"_id":"69a14dd7a13deaa449448a7e","name":"Yutong Wang","hidden":false},{"_id":"69a14dd7a13deaa449448a7f","name":"Siyuan Xiong","hidden":false},{"_id":"69a14dd7a13deaa449448a80","name":"Xuebo Liu","hidden":false},{"_id":"69a14dd7a13deaa449448a81","name":"Wenkang Zhou","hidden":false},{"_id":"69a14dd7a13deaa449448a82","name":"Liang Ding","hidden":false},{"_id":"69a14dd7a13deaa449448a83","name":"Miao Zhang","hidden":false},{"_id":"69a14dd7a13deaa449448a84","name":"Min Zhang","hidden":false}],"publishedAt":"2026-02-26T17:31:43.000Z","submittedOnDailyAt":"2026-02-27T05:27:41.765Z","title":"AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning","submittedOnDailyBy":{"_id":"652fbf9e75f5f7a84e10cb2e","avatarUrl":"/avatars/73e7af465dbb57826a52e49c3e72f55f.svg","isPro":false,"fullname":"Xuebo Liu","user":"SunbowLiu","type":"user"},"summary":"While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.","upvotes":23,"discussionId":"69a14dd7a13deaa449448a85","githubRepo":"https://github.com/TonySY2/AgentDropoutV2","githubRepoAddedBy":"user","ai_summary":"AgentDropoutV2 is a test-time framework that dynamically optimizes multi-agent system information flow through error correction and pruning mechanisms without requiring retraining.","ai_keywords":["multi-agent systems","test-time rectify-or-reject pruning","retrieval-augmented rectifier","failure-driven indicator pool","distilled failure patterns","error propagation","fallback strategy","context-aware indicators"],"githubStars":14,"organization":{"_id":"670819b38c9c6f598f37d86f","name":"HarbinInstituteofTechnologyHIT","fullname":"Harbin Institute of Technology"}},"publishedAt":"2026-02-26T12:31:43.000Z","title":"AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning","summary":"While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23258.png","numComments":2,"submittedBy":{"_id":"652fbf9e75f5f7a84e10cb2e","avatarUrl":"/avatars/73e7af465dbb57826a52e49c3e72f55f.svg","fullname":"Xuebo Liu","name":"SunbowLiu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"670819b38c9c6f598f37d86f","name":"HarbinInstituteofTechnologyHIT","fullname":"Harbin Institute of Technology"},"isAuthorParticipating":false},{"paper":{"id":"2602.23363","authors":[{"_id":"69a12e58a13deaa4494489fc","user":{"_id":"62e23c7f555a866437a53cd0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62e23c7f555a866437a53cd0/UaAsYZQXuwb4NSG5WnvdG.jpeg","isPro":false,"fullname":"Sahal Shaji","user":"sahalshajim","type":"user"},"name":"Sahal Shaji Mullappilly","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:39:40.482Z","hidden":false},{"_id":"69a12e58a13deaa4494489fd","user":{"_id":"650289dbc130d99814b34dc5","avatarUrl":"/avatars/ff0cf5add144cd79c41a255f41f34efb.svg","isPro":false,"fullname":"K Mohammed Irfan","user":"k-m-irfan","type":"user"},"name":"Mohammed Irfan Kurpath","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:39:38.185Z","hidden":false},{"_id":"69a12e58a13deaa4494489fe","name":"Omair Mohamed","hidden":false},{"_id":"69a12e58a13deaa4494489ff","name":"Mohamed Zidan","hidden":false},{"_id":"69a12e58a13deaa449448a00","name":"Fahad Khan","hidden":false},{"_id":"69a12e58a13deaa449448a01","name":"Salman Khan","hidden":false},{"_id":"69a12e58a13deaa449448a02","name":"Rao Anwer","hidden":false},{"_id":"69a12e58a13deaa449448a03","name":"Hisham Cholakkal","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/62e23c7f555a866437a53cd0/WddQgtwWa0OeMcmCOnun1.qt"],"publishedAt":"2026-02-26T18:59:46.000Z","submittedOnDailyAt":"2026-02-27T05:40:54.483Z","title":"MediX-R1: Open Ended Medical Reinforcement Learning","submittedOnDailyBy":{"_id":"62e23c7f555a866437a53cd0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62e23c7f555a866437a53cd0/UaAsYZQXuwb4NSG5WnvdG.jpeg","isPro":false,"fullname":"Sahal Shaji","user":"sahalshajim","type":"user"},"summary":"We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only sim51K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com","upvotes":14,"discussionId":"69a12e58a13deaa449448a04","projectPage":"https://medix.cvmbzuai.com/","githubRepo":"https://github.com/mbzuai-oryx/MediX-R1","githubRepoAddedBy":"user","ai_summary":"MediX-R1 presents an open-ended reinforcement learning framework for medical multimodal large language models that uses diverse reward signals and LLM-based evaluation to improve clinical reasoning beyond multiple-choice formats.","ai_keywords":["Reinforcement Learning","vision-language backbone","Group Based RL","LLM-based accuracy reward","medical embedding-based semantic reward","lightweight format reward","lightweight modality reward","Reference-based LLM-as-judge","medical reasoning","multimodal large language models"],"githubStars":13,"organization":{"_id":"61fb9e24dc607a42af5f193f","name":"MBZUAI","fullname":"Mohamed Bin Zayed University of Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}},"publishedAt":"2026-02-26T13:59:46.000Z","title":"MediX-R1: Open Ended Medical Reinforcement Learning","summary":"We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only sim51K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/62e23c7f555a866437a53cd0/WddQgtwWa0OeMcmCOnun1.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23363.png","numComments":1,"submittedBy":{"_id":"62e23c7f555a866437a53cd0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62e23c7f555a866437a53cd0/UaAsYZQXuwb4NSG5WnvdG.jpeg","fullname":"Sahal Shaji","name":"sahalshajim","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"61fb9e24dc607a42af5f193f","name":"MBZUAI","fullname":"Mohamed Bin Zayed University of Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.22675","authors":[{"_id":"69a14feba13deaa449448a87","name":"Qianben Chen","hidden":false},{"_id":"69a14feba13deaa449448a88","name":"Tianrui Qin","hidden":false},{"_id":"69a14feba13deaa449448a89","name":"King Zhu","hidden":false},{"_id":"69a14feba13deaa449448a8a","name":"Qiexiang Wang","hidden":false},{"_id":"69a14feba13deaa449448a8b","name":"Chengjun Yu","hidden":false},{"_id":"69a14feba13deaa449448a8c","name":"Shu Xu","hidden":false},{"_id":"69a14feba13deaa449448a8d","name":"Jiaqi Wu","hidden":false},{"_id":"69a14feba13deaa449448a8e","name":"Jiayu Zhang","hidden":false},{"_id":"69a14feba13deaa449448a8f","name":"Xinpeng Liu","hidden":false},{"_id":"69a14feba13deaa449448a90","name":"Xin Gui","hidden":false},{"_id":"69a14feba13deaa449448a91","name":"Jingyi Cao","hidden":false},{"_id":"69a14feba13deaa449448a92","name":"Piaohong Wang","hidden":false},{"_id":"69a14feba13deaa449448a93","user":{"_id":"657c1f7e688f1a0f7ecfe264","avatarUrl":"/avatars/265afcb7b0eeddbcf66ec4cdd4920dd3.svg","isPro":false,"fullname":"Dingfeng Shi","user":"hugteste","type":"user"},"name":"Dingfeng Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:39:18.806Z","hidden":false},{"_id":"69a14feba13deaa449448a94","name":"He Zhu","hidden":false},{"_id":"69a14feba13deaa449448a95","name":"Tiannan Wang","hidden":false},{"_id":"69a14feba13deaa449448a96","name":"Yuqing Wang","hidden":false},{"_id":"69a14feba13deaa449448a97","name":"Maojia Song","hidden":false},{"_id":"69a14feba13deaa449448a98","name":"Tianyu Zheng","hidden":false},{"_id":"69a14feba13deaa449448a99","name":"Ge Zhang","hidden":false},{"_id":"69a14feba13deaa449448a9a","name":"Jian Yang","hidden":false},{"_id":"69a14feba13deaa449448a9b","name":"Jiaheng Liu","hidden":false},{"_id":"69a14feba13deaa449448a9c","user":{"_id":"6417d9ea8f689506e7148417","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg","isPro":false,"fullname":"minghao","user":"Liam-Liu","type":"user"},"name":"Minghao Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:39:21.379Z","hidden":false},{"_id":"69a14feba13deaa449448a9d","name":"Yuchen Eleanor Jiang","hidden":false},{"_id":"69a14feba13deaa449448a9e","name":"Wangchunshu Zhou","hidden":false}],"publishedAt":"2026-02-26T06:46:41.000Z","submittedOnDailyAt":"2026-02-27T05:37:12.477Z","title":"Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization","submittedOnDailyBy":{"_id":"64be3a8b86e7fb5b8a7ec8a9","avatarUrl":"/avatars/58280c82f643a5a8073623eff33fefb2.svg","isPro":false,"fullname":"Chen","user":"Qianben","type":"user"},"summary":"Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose Search More, Think Less (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\\%), GAIA (75.7\\%), Xbench (82.0\\%), and DeepResearch Bench (45.9\\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\\%, while improving accuracy.","upvotes":14,"discussionId":"69a14feba13deaa449448a9f","githubRepo":"https://github.com/OPPO-PersonalAI/SMTL","githubRepoAddedBy":"user","ai_summary":"A deep learning framework called SMTL improves efficient long-horizon agentic search by replacing sequential reasoning with parallel evidence acquisition, achieving state-of-the-art performance across multiple research benchmarks while reducing reasoning steps by 70.7%.","ai_keywords":["deep research agents","reasoning depth","inference cost","search-intensive scenarios","generalization","agentic search","parallel evidence acquisition","context management","supervised fine-tuning","reinforcement learning","BrowseComp","GAIA","Xbench","DeepResearch Bench"],"githubStars":0,"organization":{"_id":"67177eecd0fad5b4ccc09461","name":"OPPOer","fullname":"OPPO","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"}},"publishedAt":"2026-02-26T01:46:41.000Z","title":"Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization","summary":"Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose Search More, Think Less (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6\\%), GAIA (75.7\\%), Xbench (82.0\\%), and DeepResearch Bench (45.9\\%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7\\%, while improving accuracy.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22675.png","numComments":2,"submittedBy":{"_id":"64be3a8b86e7fb5b8a7ec8a9","avatarUrl":"/avatars/58280c82f643a5a8073623eff33fefb2.svg","fullname":"Chen","name":"Qianben","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"67177eecd0fad5b4ccc09461","name":"OPPOer","fullname":"OPPO","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66e24afddce93c7249b418c0/gQ-XFJehEyAH12zhbeR8Z.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.23361","authors":[{"_id":"69a11a51a13deaa4494489bb","user":{"_id":"692da82ab884a1ba56270557","avatarUrl":"/avatars/e98409da85032ad2092b77f9fceee784.svg","isPro":false,"fullname":"Sven Elflein","user":"sven-el","type":"user"},"name":"Sven Elflein","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:00.776Z","hidden":false},{"_id":"69a11a51a13deaa4494489bc","name":"Ruilong Li","hidden":false},{"_id":"69a11a51a13deaa4494489bd","name":"Sérgio Agostinho","hidden":false},{"_id":"69a11a51a13deaa4494489be","name":"Zan Gojcic","hidden":false},{"_id":"69a11a51a13deaa4494489bf","name":"Laura Leal-Taixé","hidden":false},{"_id":"69a11a51a13deaa4494489c0","name":"Qunjie Zhou","hidden":false},{"_id":"69a11a51a13deaa4494489c1","name":"Aljosa Osep","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/692da82ab884a1ba56270557/MfAsycfptzzlmO0_Y7TYp.gif"],"publishedAt":"2026-02-26T18:59:33.000Z","submittedOnDailyAt":"2026-02-27T16:09:16.796Z","title":"VGG-T^3: Offline Feed-Forward 3D Reconstruction at Scale","submittedOnDailyBy":{"_id":"692da82ab884a1ba56270557","avatarUrl":"/avatars/e98409da85032ad2092b77f9fceee784.svg","isPro":false,"fullname":"Sven Elflein","user":"sven-el","type":"user"},"summary":"We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T^3 (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a 1k image collection in just 54 seconds, achieving a 11.6times speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.","upvotes":11,"discussionId":"69a11a51a13deaa4494489c2","projectPage":"https://research.nvidia.com/labs/dvl/projects/vgg-ttt/","ai_summary":"VGG-T³ addresses scalability issues in 3D reconstruction by transforming variable-length key-value representations into fixed-size MLPs through test-time training, enabling linear scaling with input views and achieving significant speedup over traditional softmax attention methods.","ai_keywords":["3D reconstruction","feed-forward methods","computational requirements","memory requirements","Key-Value space representation","scene geometry","Multi-Layer Perceptron","test-time training","Visual Geometry Grounded Test Time Training","softmax attention","point map reconstruction","visual localization"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-26T13:59:33.000Z","title":"VGG-T^3: Offline Feed-Forward 3D Reconstruction at Scale","summary":"We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T^3 (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a 1k image collection in just 54 seconds, achieving a 11.6times speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/692da82ab884a1ba56270557/MfAsycfptzzlmO0_Y7TYp.gif"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23361.png","numComments":1,"submittedBy":{"_id":"692da82ab884a1ba56270557","avatarUrl":"/avatars/e98409da85032ad2092b77f9fceee784.svg","fullname":"Sven Elflein","name":"sven-el","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.21760","authors":[{"_id":"69a126bfa13deaa4494489d7","user":{"_id":"6682d1f32200824e2de95646","avatarUrl":"/avatars/0c5207cbe46fa43a00085b92bafd8eec.svg","isPro":false,"fullname":"Euisoo","user":"jyssys","type":"user"},"name":"Euisoo Jung","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:39:52.100Z","hidden":false},{"_id":"69a126bfa13deaa4494489d8","name":"Byunghyun Kim","hidden":false},{"_id":"69a126bfa13deaa4494489d9","name":"Hyunjin Kim","hidden":false},{"_id":"69a126bfa13deaa4494489da","name":"Seonghye Cho","hidden":false},{"_id":"69a126bfa13deaa4494489db","name":"Jae-Gil Lee","hidden":false}],"publishedAt":"2026-02-25T10:23:07.000Z","submittedOnDailyAt":"2026-02-27T02:40:46.617Z","title":"Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling","submittedOnDailyBy":{"_id":"6682429bb1f69d3cd0104389","avatarUrl":"/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg","isPro":false,"fullname":"Jaehyun Park","user":"Cabbalett","type":"user"},"summary":"Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves 2.31times and 2.07times latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.","upvotes":10,"discussionId":"69a126bfa13deaa4494489dc","githubRepo":"https://github.com/kaist-dmlab/Hybridiff","githubRepoAddedBy":"user","ai_summary":"A hybrid parallelism framework for diffusion models that combines condition-based partitioning and adaptive pipeline scheduling to reduce inference latency while maintaining image quality across different architectures.","ai_keywords":["diffusion models","data parallel strategy","condition-based partitioning","pipeline scheduling","adaptive parallelism switching","denoising paths","U-Net-based diffusion models","DiT-based flow-matching architectures","inference latency","image quality"],"githubStars":4},"publishedAt":"2026-02-25T05:23:07.000Z","title":"Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling","summary":"Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves 2.31times and 2.07times latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21760.png","numComments":1,"submittedBy":{"_id":"6682429bb1f69d3cd0104389","avatarUrl":"/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg","fullname":"Jaehyun Park","name":"Cabbalett","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.23205","authors":[{"_id":"69a16d76a13deaa449448acd","user":{"_id":"6437a813fac5ea753f1c72d2","avatarUrl":"/avatars/69e60e60497e404149a1dad46649dad4.svg","isPro":false,"fullname":"wenjia Wang","user":"WenjiaWang","type":"user"},"name":"Wenjia Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:39:01.959Z","hidden":false},{"_id":"69a16d76a13deaa449448ace","name":"Liang Pan","hidden":false},{"_id":"69a16d76a13deaa449448acf","name":"Huaijin Pi","hidden":false},{"_id":"69a16d76a13deaa449448ad0","name":"Yuke Lou","hidden":false},{"_id":"69a16d76a13deaa449448ad1","name":"Xuqian Ren","hidden":false},{"_id":"69a16d76a13deaa449448ad2","name":"Yifan Wu","hidden":false},{"_id":"69a16d76a13deaa449448ad3","name":"Zhouyingcheng Liao","hidden":false},{"_id":"69a16d76a13deaa449448ad4","name":"Lei Yang","hidden":false},{"_id":"69a16d76a13deaa449448ad5","name":"Rishabh Dabral","hidden":false},{"_id":"69a16d76a13deaa449448ad6","name":"Christian Theobalt","hidden":false},{"_id":"69a16d76a13deaa449448ad7","name":"Taku Komura","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6437a813fac5ea753f1c72d2/snYDOeIkL9KrqdyJV7Tjh.mp4"],"publishedAt":"2026-02-26T16:53:41.000Z","submittedOnDailyAt":"2026-02-27T07:43:43.271Z","title":"EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents","submittedOnDailyBy":{"_id":"6437a813fac5ea753f1c72d2","avatarUrl":"/avatars/69e60e60497e404149a1dad46649dad4.svg","isPro":false,"fullname":"wenjia Wang","user":"WenjiaWang","type":"user"},"summary":"Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.","upvotes":8,"discussionId":"69a16d77a13deaa449448ad8","projectPage":"https://wenjiawang0312.github.io/projects/embodmocap/","githubRepo":"https://github.com/WenjiaWang0312/EmbodMocap","githubRepoAddedBy":"user","ai_summary":"A portable dual-iPhone system enables metric-scale human-scene reconstruction and supports embodied AI tasks including physics-based animation and robot motion control.","ai_keywords":["embodied agents","RGB-D sequences","metric world coordinate frame","monocular human-scene-reconstruction","physics-based character animation","sim-to-real RL","humanoid robot"],"githubStars":45},"publishedAt":"2026-02-26T11:53:41.000Z","title":"EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents","summary":"Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6437a813fac5ea753f1c72d2/snYDOeIkL9KrqdyJV7Tjh.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23205.png","numComments":1,"submittedBy":{"_id":"6437a813fac5ea753f1c72d2","avatarUrl":"/avatars/69e60e60497e404149a1dad46649dad4.svg","fullname":"wenjia Wang","name":"WenjiaWang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.17594","authors":[{"_id":"699da375b767f258d0bf1a54","name":"Lance Ying","hidden":false},{"_id":"699da375b767f258d0bf1a55","name":"Ryan Truong","hidden":false},{"_id":"699da375b767f258d0bf1a56","name":"Prafull Sharma","hidden":false},{"_id":"699da375b767f258d0bf1a57","name":"Kaiya Ivy Zhao","hidden":false},{"_id":"699da375b767f258d0bf1a58","name":"Nathan Cloos","hidden":false},{"_id":"699da375b767f258d0bf1a59","name":"Kelsey R. Allen","hidden":false},{"_id":"699da375b767f258d0bf1a5a","name":"Thomas L. Griffiths","hidden":false},{"_id":"699da375b767f258d0bf1a5b","name":"Katherine M. Collins","hidden":false},{"_id":"699da375b767f258d0bf1a5c","name":"José Hernández-Orallo","hidden":false},{"_id":"699da375b767f258d0bf1a5d","name":"Phillip Isola","hidden":false},{"_id":"699da375b767f258d0bf1a5e","name":"Samuel J. Gershman","hidden":false},{"_id":"699da375b767f258d0bf1a5f","name":"Joshua B. Tenenbaum","hidden":false}],"publishedAt":"2026-02-19T18:17:25.000Z","submittedOnDailyAt":"2026-02-27T02:33:35.695Z","title":"AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games","submittedOnDailyBy":{"_id":"66bb93a53397ce19d80af9ac","avatarUrl":"/avatars/030b4b509973c6d61dbe2e6f1a2a6d34.svg","isPro":false,"fullname":"Lance Ying","user":"lcying","type":"user"},"summary":"Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play all conceivable human games, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.","upvotes":7,"discussionId":"699da375b767f258d0bf1a60","projectPage":"https://aigamestore.org","ai_summary":"AI systems were evaluated across a diverse set of human-designed games to assess general intelligence, revealing significant gaps in performance compared to human players, particularly in complex cognitive tasks.","ai_keywords":["human-like general intelligence","general game playing","human games","Multiverse of Human Games","AI GameStore","vision-language models","world-model learning","memory","planning"],"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}},"publishedAt":"2026-02-19T13:17:25.000Z","title":"AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games","summary":"Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play all conceivable human games, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17594.png","numComments":2,"submittedBy":{"_id":"66bb93a53397ce19d80af9ac","avatarUrl":"/avatars/030b4b509973c6d61dbe2e6f1a2a6d34.svg","fullname":"Lance Ying","name":"lcying","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.22594","authors":[{"_id":"69a10abaa13deaa44944895d","name":"Qing Yu","hidden":false},{"_id":"69a10abaa13deaa44944895e","name":"Akihisa Watanabe","hidden":false},{"_id":"69a10abaa13deaa44944895f","name":"Kent Fujiwara","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cE21YhRnaLWjMplc3cQfe.mp4"],"publishedAt":"2026-02-26T03:58:25.000Z","submittedOnDailyAt":"2026-02-27T00:40:13.183Z","title":"Causal Motion Diffusion Models for Autoregressive Motion Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.","upvotes":5,"discussionId":"69a10abaa13deaa449448960","projectPage":"https://yu1ut.com/CMDM-HP/","ai_summary":"Causal Motion Diffusion Models introduce a unified framework for autoregressive motion generation using a causal diffusion transformer in a semantically aligned latent space, enabling fast, high-quality text-to-motion synthesis with improved temporal smoothness.","ai_keywords":["motion diffusion models","causal diffusion transformer","latent space","Motion-Language-Aligned Causal VAE","autoregressive motion generation","causal diffusion forcing","temporally ordered denoising","frame-wise sampling schedule","causal uncertainty","text-to-motion generation","streaming synthesis","long-horizon motion generation"]},"publishedAt":"2026-02-25T22:58:25.000Z","title":"Causal Motion Diffusion Models for Autoregressive Motion Generation","summary":"Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cE21YhRnaLWjMplc3cQfe.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22594.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":242,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.23339","authors":[{"_id":"69a1712ea06f8b911c5d9b5e","name":"Tilemachos Aravanis","hidden":false},{"_id":"69a1712ea06f8b911c5d9b5f","user":{"_id":"66a3ae59f33ff23e1c027ccd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66a3ae59f33ff23e1c027ccd/tZzpESNPnmhty62xhHszF.jpeg","isPro":true,"fullname":"Vladan Stojnic","user":"stojnvla","type":"user"},"name":"Vladan Stojnić","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:38:56.416Z","hidden":false},{"_id":"69a1712ea06f8b911c5d9b60","user":{"_id":"626a9b5205fe1cb65720e00e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/626a9b5205fe1cb65720e00e/hyWcWn_8jVZsu1Yc5Z0R8.png","isPro":false,"fullname":"Bill Psomas","user":"billpsomas","type":"user"},"name":"Bill Psomas","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:38:59.565Z","hidden":false},{"_id":"69a1712ea06f8b911c5d9b61","name":"Nikos Komodakis","hidden":false},{"_id":"69a1712ea06f8b911c5d9b62","name":"Giorgos Tolias","hidden":false}],"publishedAt":"2026-02-26T18:45:33.000Z","submittedOnDailyAt":"2026-02-27T11:59:10.291Z","title":"Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?","submittedOnDailyBy":{"_id":"626a9b5205fe1cb65720e00e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/626a9b5205fe1cb65720e00e/hyWcWn_8jVZsu1Yc5Z0R8.png","isPro":false,"fullname":"Bill Psomas","user":"billpsomas","type":"user"},"summary":"Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.","upvotes":4,"discussionId":"69a1712ea06f8b911c5d9b63","githubRepo":"https://github.com/TilemahosAravanis/Retrieve-and-Segment","githubRepoAddedBy":"user","ai_summary":"Retrieval-augmented test-time adaptation with learned fusion of textual and visual features bridges the performance gap between zero-shot and supervised open-vocabulary segmentation.","ai_keywords":["open-vocabulary segmentation","vision-language models","zero-shot recognition","pixel-level prediction","textual prompts","support set","test-time adapter","per-image classifier","learned fusion","modality synergy","fine-grained segmentation","personalized segmentation"],"githubStars":2,"organization":{"_id":"673c7340531b2bab1ee383e6","name":"vrg-prague","fullname":"Visual Recognition Group FEE CTU in Prague","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d148d4f60fb7773f0a341a/bAV_SGHM_uHJrNh9ARjiJ.png"}},"publishedAt":"2026-02-26T13:45:33.000Z","title":"Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?","summary":"Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23339.png","numComments":1,"submittedBy":{"_id":"626a9b5205fe1cb65720e00e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/626a9b5205fe1cb65720e00e/hyWcWn_8jVZsu1Yc5Z0R8.png","fullname":"Bill Psomas","name":"billpsomas","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"673c7340531b2bab1ee383e6","name":"vrg-prague","fullname":"Visual Recognition Group FEE CTU in Prague","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d148d4f60fb7773f0a341a/bAV_SGHM_uHJrNh9ARjiJ.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.23058","authors":[{"_id":"69a108a6a13deaa449448946","user":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"name":"Zeyu Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:40:35.576Z","hidden":false},{"_id":"69a108a6a13deaa449448947","name":"Danning Li","hidden":false},{"_id":"69a108a6a13deaa449448948","name":"Ian Reid","hidden":false},{"_id":"69a108a6a13deaa449448949","name":"Richard Hartley","hidden":false}],"publishedAt":"2026-02-26T14:42:53.000Z","submittedOnDailyAt":"2026-02-27T00:30:33.142Z","title":"GeoWorld: Geometric World Models","submittedOnDailyBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"summary":"Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.","upvotes":3,"discussionId":"69a108a7a13deaa44944894a","projectPage":"https://steve-zeyu-zhang.github.io/GeoWorld","ai_summary":"GeoWorld addresses limitations in energy-based predictive world models by utilizing hyperbolic geometry to preserve latent state structures and improve long-horizon prediction performance.","ai_keywords":["energy-based predictive world models","latent energy landscapes","Euclidean space","geometric structure","hierarchical relations","Hyperbolic JEPA","hyperbolic manifolds","geometric reinforcement learning","multi-step planning","long-horizon prediction"]},"publishedAt":"2026-02-26T09:42:53.000Z","title":"GeoWorld: Geometric World Models","summary":"Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23058.png","numComments":3,"submittedBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","fullname":"Zeyu Zhang","name":"SteveZeyuZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.22953","authors":[{"_id":"69a1c790bc2d66e68888eff6","name":"Elron Bandel","hidden":false},{"_id":"69a1c790bc2d66e68888eff7","name":"Asaf Yehudai","hidden":false},{"_id":"69a1c790bc2d66e68888eff8","name":"Lilach Eden","hidden":false},{"_id":"69a1c790bc2d66e68888eff9","name":"Yehoshua Sagron","hidden":false},{"_id":"69a1c790bc2d66e68888effa","name":"Yotam Perlitz","hidden":false},{"_id":"69a1c790bc2d66e68888effb","name":"Elad Venezian","hidden":false},{"_id":"69a1c790bc2d66e68888effc","name":"Natalia Razinkov","hidden":false},{"_id":"69a1c790bc2d66e68888effd","name":"Natan Ergas","hidden":false},{"_id":"69a1c790bc2d66e68888effe","name":"Shlomit Shachor Ifergan","hidden":false},{"_id":"69a1c790bc2d66e68888efff","name":"Segev Shlomov","hidden":false},{"_id":"69a1c790bc2d66e68888f000","name":"Michal Jacovi","hidden":false},{"_id":"69a1c790bc2d66e68888f001","name":"Leshem Choshen","hidden":false},{"_id":"69a1c790bc2d66e68888f002","name":"Liat Ein-Dor","hidden":false},{"_id":"69a1c790bc2d66e68888f003","name":"Yoav Katz","hidden":false},{"_id":"69a1c790bc2d66e68888f004","name":"Michal Shmueli-Scheuer","hidden":false}],"publishedAt":"2026-02-26T12:48:02.000Z","submittedOnDailyAt":"2026-02-27T14:10:03.649Z","title":"General Agent Evaluation","submittedOnDailyBy":{"_id":"61bf40824b4300d0fb0acf59","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1644224872623-61bf40824b4300d0fb0acf59.jpeg","isPro":false,"fullname":"Leshem Choshen","user":"borgr","type":"user"},"summary":"The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.","upvotes":3,"discussionId":"69a1c790bc2d66e68888f005","projectPage":"https://www.exgentic.ai","githubRepo":"https://github.com/Exgentic/exgentic","githubRepoAddedBy":"user","ai_summary":"General-purpose agents remain underdeveloped despite promising implementations, necessitating systematic evaluation frameworks and benchmarks to assess their true versatility across diverse environments.","ai_keywords":["general-purpose agents","agentic benchmarks","unified protocol","Exgentic framework","Open General Agent Leaderboard"],"githubStars":5,"organization":{"_id":"6760ab6c5c9a8ea8370ab95b","name":"ibm-research","fullname":"IBM Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/npxapKcW-cXX3J2JBl2vY.png"}},"publishedAt":"2026-02-26T07:48:02.000Z","title":"General Agent Evaluation","summary":"The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22953.png","numComments":1,"submittedBy":{"_id":"61bf40824b4300d0fb0acf59","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1644224872623-61bf40824b4300d0fb0acf59.jpeg","fullname":"Leshem Choshen","name":"borgr","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":24,"isUserFollowing":false},"organization":{"_id":"6760ab6c5c9a8ea8370ab95b","name":"ibm-research","fullname":"IBM Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/637bfdf60dc13843b468ac20/npxapKcW-cXX3J2JBl2vY.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.22437","authors":[{"_id":"69a108daa13deaa44944894c","name":"Zezhou Wang","hidden":false},{"_id":"69a108daa13deaa44944894d","name":"Youjie Li","hidden":false},{"_id":"69a108daa13deaa44944894e","name":"Zhiqi Lin","hidden":false},{"_id":"69a108daa13deaa44944894f","name":"Jiacheng Yang","hidden":false},{"_id":"69a108daa13deaa449448950","name":"Cong Xie","hidden":false},{"_id":"69a108daa13deaa449448951","name":"Guanyu Feng","hidden":false},{"_id":"69a108daa13deaa449448952","name":"Zheng Zhong","hidden":false},{"_id":"69a108daa13deaa449448953","name":"Ziyue Huang","hidden":false},{"_id":"69a108daa13deaa449448954","name":"Hongyu Zhu","hidden":false},{"_id":"69a108daa13deaa449448955","name":"Zhi Zhang","hidden":false},{"_id":"69a108daa13deaa449448956","name":"Yanghua Peng","hidden":false},{"_id":"69a108daa13deaa449448957","name":"Xin Liu","hidden":false}],"publishedAt":"2026-02-25T21:55:43.000Z","submittedOnDailyAt":"2026-02-27T00:30:57.506Z","title":"veScale-FSDP: Flexible and High-Performance FSDP at Scale","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.","upvotes":3,"discussionId":"69a108dba13deaa449448958","ai_summary":"veScale-FSDP introduces a redesigned fully sharded data parallel system with flexible sharding and structure-aware planning to improve scalability and efficiency for large-scale model training.","ai_keywords":["Fully Sharded Data Parallel","ZeRO","block-wise quantized training","non-element-wise optimizers","Shampoo","Muon","RaggedShard","structure-aware planning","communication efficiency","memory efficiency","scalability"]},"publishedAt":"2026-02-25T16:55:43.000Z","title":"veScale-FSDP: Flexible and High-Performance FSDP at Scale","summary":"Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22437.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":242,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.22045","authors":[{"_id":"69a1891fa06f8b911c5d9baf","user":{"_id":"66bdf184ea2de7decd9c71f5","avatarUrl":"/avatars/6dc0d9bfb6589d97ff400092017ce804.svg","isPro":false,"fullname":"Walter Hernandez Cruz","user":"walterhernandez","type":"user"},"name":"Walter Hernandez Cruz","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:38:53.972Z","hidden":false},{"_id":"69a1891fa06f8b911c5d9bb0","user":{"_id":"64b63f8ad57e02621dc93c8b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b63f8ad57e02621dc93c8b/B9FG2ybHRhwFTE98mFGvu.jpeg","isPro":false,"fullname":"Peter Devine","user":"ptrdvn","type":"user"},"name":"Peter Devine","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:38:51.286Z","hidden":false},{"_id":"69a1891fa06f8b911c5d9bb1","name":"Nikhil Vadgama","hidden":false},{"_id":"69a1891fa06f8b911c5d9bb2","name":"Paolo Tasca","hidden":false},{"_id":"69a1891fa06f8b911c5d9bb3","user":{"_id":"6755b290c3af453d7bcba5b7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fb9aLNQS4xssyKC3hIUp3.png","isPro":false,"fullname":"JX","user":"xujiahuayz","type":"user"},"name":"Jiahua Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:38:48.341Z","hidden":false}],"publishedAt":"2026-02-25T15:53:41.000Z","submittedOnDailyAt":"2026-02-27T09:54:01.709Z","title":"DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain","submittedOnDailyBy":{"_id":"66bdf184ea2de7decd9c71f5","avatarUrl":"/avatars/6dc0d9bfb6589d97ff400092017ce804.svg","isPro":false,"fullname":"Walter Hernandez Cruz","user":"walterhernandez","type":"user"},"summary":"We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.\n  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.\n  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.","upvotes":2,"discussionId":"69a1891fa06f8b911c5d9bb4","githubRepo":"https://github.com/dlt-science/DLT-Corpus","githubRepoAddedBy":"user","ai_summary":"The DLT-Corpus dataset, containing 2.98 billion tokens from diverse sources, enables analysis of technology emergence patterns and market-innovation correlations in the distributed ledger technology sector.","ai_keywords":["domain-specific text collection","Distributed Ledger Technology","NLP resources","technology transfer patterns","Named Entity Recognition","domain-adapted model","LedgerBERT"],"githubStars":0,"organization":{"_id":"66bdf19bb6c31ea43e9bcaaf","name":"ExponentialScience","fullname":"Exponential Science","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66bdf184ea2de7decd9c71f5/47Rr1CYEQ2fX1GVXEduRG.png"}},"publishedAt":"2026-02-25T10:53:41.000Z","title":"DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain","summary":"We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.\n  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.\n  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22045.png","numComments":1,"submittedBy":{"_id":"66bdf184ea2de7decd9c71f5","avatarUrl":"/avatars/6dc0d9bfb6589d97ff400092017ce804.svg","fullname":"Walter Hernandez Cruz","name":"walterhernandez","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"66bdf19bb6c31ea43e9bcaaf","name":"ExponentialScience","fullname":"Exponential Science","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66bdf184ea2de7decd9c71f5/47Rr1CYEQ2fX1GVXEduRG.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.20423","authors":[{"_id":"699e64a9dfbcf0b800aecaa9","user":{"_id":"67d33a3b1d5d1fd92c3cfce4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d33a3b1d5d1fd92c3cfce4/1f-hKaMMPBgMUbVXy3an-.jpeg","isPro":false,"fullname":"Taha Koleilat","user":"TahaKoleilat","type":"user"},"name":"Taha Koleilat","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:44:13.587Z","hidden":false},{"_id":"699e64a9dfbcf0b800aecaaa","name":"Hojat Asgariandehkordi","hidden":false},{"_id":"699e64a9dfbcf0b800aecaab","name":"Omid Nejati Manzari","hidden":false},{"_id":"699e64a9dfbcf0b800aecaac","name":"Berardino Barile","hidden":false},{"_id":"699e64a9dfbcf0b800aecaad","name":"Yiming Xiao","hidden":false},{"_id":"699e64a9dfbcf0b800aecaae","name":"Hassan Rivaz","hidden":false}],"publishedAt":"2026-02-23T23:46:05.000Z","submittedOnDailyAt":"2026-02-27T15:05:58.159Z","title":"MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation","submittedOnDailyBy":{"_id":"67d33a3b1d5d1fd92c3cfce4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d33a3b1d5d1fd92c3cfce4/1f-hKaMMPBgMUbVXy3an-.jpeg","isPro":false,"fullname":"Taha Koleilat","user":"TahaKoleilat","type":"user"},"summary":"Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.","upvotes":2,"discussionId":"699e64aadfbcf0b800aecaaf","projectPage":"https://tahakoleilat.github.io/MedCLIPSeg/","githubRepo":"https://github.com/HealthX-Lab/MedCLIPSeg","githubRepoAddedBy":"user","ai_summary":"MedCLIPSeg adapts CLIP for medical image segmentation by leveraging patch-level embeddings and probabilistic attention to achieve data-efficient, uncertain-aware segmentation with interpretability.","ai_keywords":["vision-language models","CLIP","medical image segmentation","probabilistic cross-modal attention","patch-level embeddings","soft patch-level contrastive loss","uncertainty-aware segmentation","domain generalizability"],"githubStars":2},"publishedAt":"2026-02-23T18:46:05.000Z","title":"MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation","summary":"Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20423.png","numComments":1,"submittedBy":{"_id":"67d33a3b1d5d1fd92c3cfce4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67d33a3b1d5d1fd92c3cfce4/1f-hKaMMPBgMUbVXy3an-.jpeg","fullname":"Taha Koleilat","name":"TahaKoleilat","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.20332","authors":[{"_id":"69a1a3277b76d967d086a6d4","name":"Nicole Cho","hidden":false},{"_id":"69a1a3277b76d967d086a6d5","name":"William Watson","hidden":false},{"_id":"69a1a3277b76d967d086a6d6","name":"Alec Koppel","hidden":false},{"_id":"69a1a3277b76d967d086a6d7","name":"Sumitra Ganesh","hidden":false},{"_id":"69a1a3277b76d967d086a6d8","name":"Manuela Veloso","hidden":false}],"publishedAt":"2026-02-23T20:28:48.000Z","submittedOnDailyAt":"2026-02-27T11:29:42.215Z","title":"No One Size Fits All: QueryBandits for Hallucination Mitigation","submittedOnDailyBy":{"_id":"6630a380b6d15534202ab83f","avatarUrl":"/avatars/69af9465544b668c65ad4d9866fbfd06.svg","isPro":false,"fullname":"Nicole Cho","user":"NicoleCho","type":"user"},"summary":"Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.","upvotes":2,"discussionId":"69a1a3287b76d967d086a6d9","projectPage":"https://arxiv.org/abs/2602.20332","ai_summary":"A contextual bandit framework named QueryBandits is introduced to adaptively select optimal query-rewrite strategies for reducing hallucinations in large language models, demonstrating superior performance over static policies and enabling deployment with closed-source models.","ai_keywords":["contextual bandit","Thompson Sampling","query-rewrite strategy","hallucinations","large language models","model-agnostic","empirical validation","calibrated reward function","semantic features","cumulative regret","forward-pass mechanisms"]},"publishedAt":"2026-02-23T15:28:48.000Z","title":"No One Size Fits All: QueryBandits for Hallucination Mitigation","summary":"Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20332.png","numComments":1,"submittedBy":{"_id":"6630a380b6d15534202ab83f","avatarUrl":"/avatars/69af9465544b668c65ad4d9866fbfd06.svg","fullname":"Nicole Cho","name":"NicoleCho","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.20300","authors":[{"_id":"69a1a2dc7b76d967d086a6ce","name":"William Watson","hidden":false},{"_id":"69a1a2dc7b76d967d086a6cf","name":"Nicole Cho","hidden":false},{"_id":"69a1a2dc7b76d967d086a6d0","name":"Sumitra Ganesh","hidden":false},{"_id":"69a1a2dc7b76d967d086a6d1","name":"Manuela Veloso","hidden":false}],"publishedAt":"2026-02-23T19:30:08.000Z","submittedOnDailyAt":"2026-02-27T11:28:35.891Z","title":"What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance","submittedOnDailyBy":{"_id":"6630a380b6d15534202ab83f","avatarUrl":"/avatars/69af9465544b668c65ad4d9866fbfd06.svg","isPro":false,"fullname":"Nicole Cho","user":"NicoleCho","type":"user"},"summary":"Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.","upvotes":2,"discussionId":"69a1a2dc7b76d967d086a6d2","projectPage":"https://arxiv.org/abs/2602.20300","ai_summary":"Analysis of 369,837 real-world queries reveals that specific linguistic features correlate with hallucination likelihood in large language models, identifying a risk landscape for query design.","ai_keywords":["hallucination","large language model","query feature vector","clause complexity","lexical rarity","anaphora","negation","answerability","intention grounding"]},"publishedAt":"2026-02-23T14:30:08.000Z","title":"What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance","summary":"Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20300.png","numComments":1,"submittedBy":{"_id":"6630a380b6d15534202ab83f","avatarUrl":"/avatars/69af9465544b668c65ad4d9866fbfd06.svg","fullname":"Nicole Cho","name":"NicoleCho","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.23259","authors":[{"_id":"69a10708a13deaa449448931","name":"Jiangxin Sun","hidden":false},{"_id":"69a10708a13deaa449448932","name":"Feng Xue","hidden":false},{"_id":"69a10708a13deaa449448933","name":"Teng Long","hidden":false},{"_id":"69a10708a13deaa449448934","name":"Chang Liu","hidden":false},{"_id":"69a10708a13deaa449448935","name":"Jian-Fang Hu","hidden":false},{"_id":"69a10708a13deaa449448936","name":"Wei-Shi Zheng","hidden":false},{"_id":"69a10708a13deaa449448937","name":"Nicu Sebe","hidden":false}],"publishedAt":"2026-02-26T17:32:30.000Z","submittedOnDailyAt":"2026-02-27T00:23:05.206Z","title":"Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.","upvotes":1,"discussionId":"69a10708a13deaa449448938","ai_summary":"A risk-aware framework for autonomous driving that uses world modeling and risk evaluation to generalize beyond expert demonstrations without requiring explicit expert supervision.","ai_keywords":["imitation learning","end-to-end autonomous driving","world model","predictive control","risk-aware","expert demonstrations","generalization","self-evaluation distillation","action proposal network"]},"publishedAt":"2026-02-26T12:32:30.000Z","title":"Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving","summary":"With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23259.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":242,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.23165","authors":[{"_id":"69a10783a13deaa44944893a","name":"Yichen Peng","hidden":false},{"_id":"69a10783a13deaa44944893b","name":"Jyun-Ting Song","hidden":false},{"_id":"69a10783a13deaa44944893c","name":"Siyeol Jung","hidden":false},{"_id":"69a10783a13deaa44944893d","name":"Ruofan Liu","hidden":false},{"_id":"69a10783a13deaa44944893e","name":"Haiyang Liu","hidden":false},{"_id":"69a10783a13deaa44944893f","name":"Xuangeng Chu","hidden":false},{"_id":"69a10783a13deaa449448940","name":"Ruicong Liu","hidden":false},{"_id":"69a10783a13deaa449448941","name":"Erwin Wu","hidden":false},{"_id":"69a10783a13deaa449448942","name":"Hideki Koike","hidden":false},{"_id":"69a10783a13deaa449448943","name":"Kris Kitani","hidden":false}],"publishedAt":"2026-02-26T16:30:07.000Z","submittedOnDailyAt":"2026-02-27T00:25:02.702Z","title":"DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.","upvotes":1,"discussionId":"69a10783a13deaa449448944","ai_summary":"DyaDiT is a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals by capturing interaction dynamics between two speakers.","ai_keywords":["diffusion transformer","dyadic audio signals","motion generation","social context","motion dictionary","conversational partner gestures","Seamless Interaction Dataset"]},"publishedAt":"2026-02-26T11:30:07.000Z","title":"DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation","summary":"Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.23165.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":242,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.21420","authors":[{"_id":"69a1deddbc2d66e68888f05b","name":"Yuanda Xu","hidden":false},{"_id":"69a1deddbc2d66e68888f05c","name":"Hejian Sang","hidden":false},{"_id":"69a1deddbc2d66e68888f05d","name":"Zhengze Zhou","hidden":false},{"_id":"69a1deddbc2d66e68888f05e","name":"Ran He","hidden":false},{"_id":"69a1deddbc2d66e68888f05f","name":"Zhipeng Wang","hidden":false}],"publishedAt":"2026-02-24T22:46:43.000Z","submittedOnDailyAt":"2026-02-27T15:45:53.907Z","title":"Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning","submittedOnDailyBy":{"_id":"646af200ca17a49700e94aa1","avatarUrl":"/avatars/fdee8313785f592ee11b1c879f3df775.svg","isPro":false,"fullname":"Hejian Sang","user":"pb09204048","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.","upvotes":1,"discussionId":"69a1deddbc2d66e68888f060","ai_summary":"Reinforcement learning with verifiable rewards suffers from reduced reasoning diversity due to uniform error penalization, which is addressed by a confidence-aware asymmetric error penalty method that dynamically modulates advantages based on rollout confidence.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","Large Language Models","Pass@1 accuracy","sharpened sampling","reasoning boundary","generation diversity","error penalization","overconfident errors","advantage normalization","confidence shift metric","selective regularizer","GRPO","DAPO","VERL framework","MATH-500","AIME 2025"],"organization":{"_id":"6697e878d8b5b78e6e7485b7","name":"LinkedIn","fullname":"LinkedIn","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6697e834fd52271e0b9ce8d8/VSBDJkmYgk4-LeXgTKThN.png"}},"publishedAt":"2026-02-24T17:46:43.000Z","title":"Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling, they simultaneously narrow the model's reasoning boundary and reduce generation diversity. We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric, c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors, plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO-Math-17K dataset using GRPO and DAPO within the VERL framework. Evaluated on MATH-500 and AIME 2025, ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21420.png","numComments":1,"submittedBy":{"_id":"646af200ca17a49700e94aa1","avatarUrl":"/avatars/fdee8313785f592ee11b1c879f3df775.svg","fullname":"Hejian Sang","name":"pb09204048","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"6697e878d8b5b78e6e7485b7","name":"LinkedIn","fullname":"LinkedIn","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6697e834fd52271e0b9ce8d8/VSBDJkmYgk4-LeXgTKThN.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.18253","authors":[{"_id":"699d7984c37da633d74f492c","user":{"_id":"621ff334fa5492893dc03d82","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/621ff334fa5492893dc03d82/EAIr-l3O4OeM10f1boLux.jpeg","isPro":false,"fullname":"Xabier de Zuazo","user":"zuazo","type":"user"},"name":"Xabier de Zuazo","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:44:27.030Z","hidden":false},{"_id":"699d7984c37da633d74f492d","name":"Vincenzo Verbeni","hidden":false},{"_id":"699d7984c37da633d74f492e","name":"Eva Navas","hidden":false},{"_id":"699d7984c37da633d74f492f","name":"Ibon Saratxaga","hidden":false},{"_id":"699d7984c37da633d74f4930","name":"Mathieu Bourguignon","hidden":false},{"_id":"699d7984c37da633d74f4931","name":"Nicola Molinaro","hidden":false}],"publishedAt":"2026-02-20T14:39:50.000Z","submittedOnDailyAt":"2026-02-27T11:26:29.591Z","title":"MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data","submittedOnDailyBy":{"_id":"621ff334fa5492893dc03d82","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/621ff334fa5492893dc03d82/EAIr-l3O4OeM10f1boLux.jpeg","isPro":false,"fullname":"Xabier de Zuazo","user":"zuazo","type":"user"},"summary":"Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.","upvotes":1,"discussionId":"699d7984c37da633d74f4932","githubRepo":"https://github.com/hitz-zentroa/meg-phone-decoding","githubRepoAddedBy":"user","ai_summary":"Transfer learning enables efficient MEG-based speech decoding from perception to production tasks using a Conformer model with minimal fine-tuning data.","ai_keywords":["Conformer-based model","transfer learning","cross-task decoding","MEG-based speech models","pre-training","fine-tuning"],"githubStars":4,"organization":{"_id":"62443ab0799d7b023c67827a","name":"HiTZ","fullname":"HiTZ zentroa","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1648638583865-5e60d95ab6e14f17937667c1.png"}},"publishedAt":"2026-02-20T09:39:50.000Z","title":"MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data","summary":"Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18253.png","numComments":1,"submittedBy":{"_id":"621ff334fa5492893dc03d82","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/621ff334fa5492893dc03d82/EAIr-l3O4OeM10f1boLux.jpeg","fullname":"Xabier de Zuazo","name":"zuazo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"62443ab0799d7b023c67827a","name":"HiTZ","fullname":"HiTZ zentroa","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1648638583865-5e60d95ab6e14f17937667c1.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.22479","authors":[{"_id":"69a11d56a13deaa4494489c4","user":{"_id":"632292053007fcbf2932da22","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg","isPro":true,"fullname":"Afshin Khadangi","user":"akhadangi","type":"user"},"name":"Afshin Khadangi","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:39:57.016Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632292053007fcbf2932da22/Z9KzRRdSIBeYCllOVhqJg.png"],"publishedAt":"2026-02-25T23:38:16.000Z","submittedOnDailyAt":"2026-02-27T02:37:42.712Z","title":"Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns","submittedOnDailyBy":{"_id":"632292053007fcbf2932da22","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg","isPro":true,"fullname":"Afshin Khadangi","user":"akhadangi","type":"user"},"summary":"Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC^{2} (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC^{2} combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC^{2} improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.","upvotes":0,"discussionId":"69a11d56a13deaa4494489c5","projectPage":"https://trc2lm.github.io","ai_summary":"TRC² addresses continual learning challenges in language models through a sparse, chunk-parallel architectural design that enables rapid adaptation without catastrophic forgetting.","ai_keywords":["continual learning","catastrophic forgetting","online updates","stability-plasticity tradeoff","decoder-only backbone","sparse thalamic routing","cortical columns","modulation","prediction","memory","feedback","corrective pathway","chunk-parallel","language modeling","continual learning benchmarks"]},"publishedAt":"2026-02-25T18:38:16.000Z","title":"Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns","summary":"Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC^{2} (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC^{2} combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC^{2} improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632292053007fcbf2932da22/Z9KzRRdSIBeYCllOVhqJg.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22479.png","numComments":1,"submittedBy":{"_id":"632292053007fcbf2932da22","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/632292053007fcbf2932da22/Cf4w-OunyQzHAJhyZsHTf.jpeg","fullname":"Afshin Khadangi","name":"akhadangi","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.20981","authors":[{"_id":"69a0ef99a13deaa4494488a1","name":"Christian Simon","hidden":false},{"_id":"69a0ef99a13deaa4494488a2","name":"Masato Ishii","hidden":false},{"_id":"69a0ef99a13deaa4494488a3","name":"Wei-Yao Wang","hidden":false},{"_id":"69a0ef99a13deaa4494488a4","name":"Koichi Saito","hidden":false},{"_id":"69a0ef99a13deaa4494488a5","name":"Akio Hayakawa","hidden":false},{"_id":"69a0ef99a13deaa4494488a6","name":"Dongseok Shim","hidden":false},{"_id":"69a0ef99a13deaa4494488a7","name":"Zhi Zhong","hidden":false},{"_id":"69a0ef99a13deaa4494488a8","name":"Shuyang Cui","hidden":false},{"_id":"69a0ef99a13deaa4494488a9","name":"Shusuke Takahashi","hidden":false},{"_id":"69a0ef99a13deaa4494488aa","user":{"_id":"6650773ca6acfdd2aba7d486","avatarUrl":"/avatars/d297886ea60dbff98a043caf825820ed.svg","isPro":false,"fullname":"Takashi Shibuya","user":"TakashiShibuyaSony","type":"user"},"name":"Takashi Shibuya","status":"claimed_verified","statusLastChangedAt":"2026-02-27T16:41:40.194Z","hidden":false},{"_id":"69a0ef99a13deaa4494488ab","name":"Yuki Mitsufuji","hidden":false}],"publishedAt":"2026-02-24T15:01:39.000Z","submittedOnDailyAt":"2026-02-27T08:18:36.943Z","title":"Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models","submittedOnDailyBy":{"_id":"63394c4b20c058d8e235a133","avatarUrl":"/avatars/c6203765918ff588bfcf857d1d5adecc.svg","isPro":false,"fullname":"christian simon","user":"cssen","type":"user"},"summary":"Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.","upvotes":0,"discussionId":"69a0ef99a13deaa4494488ac","ai_summary":"MMHNet enables long-form audio generation from video by integrating hierarchical methods and non-causal Mamba, achieving superior performance over existing video-to-audio approaches.","ai_keywords":["multimodal hierarchical networks","MMHNet","video-to-audio generation","long-form audio generation","non-causal Mamba","hierarchical method"],"organization":{"_id":"6304f161c2f4f2d4929d52d7","name":"Sony","fullname":"Sony","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/zpgaCDJEHxA8kYNDtMKnv.jpeg"}},"publishedAt":"2026-02-24T10:01:39.000Z","title":"Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models","summary":"Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20981.png","numComments":1,"submittedBy":{"_id":"63394c4b20c058d8e235a133","avatarUrl":"/avatars/c6203765918ff588bfcf857d1d5adecc.svg","fullname":"christian simon","name":"cssen","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6304f161c2f4f2d4929d52d7","name":"Sony","fullname":"Sony","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/zpgaCDJEHxA8kYNDtMKnv.jpeg"},"isAuthorParticipating":false}]
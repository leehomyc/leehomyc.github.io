[{"paper":{"id":"2602.01785","authors":[{"_id":"69818a88ce18b18628096389","user":{"_id":"645b0c3ec35da9c7afd95421","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg","isPro":false,"fullname":"Yuling","user":"YerbaPage","type":"user"},"name":"Yuling Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-03T10:03:06.748Z","hidden":false},{"_id":"69818a88ce18b1862809638a","user":{"_id":"68354b3e65397cd063da14e4","avatarUrl":"/avatars/1b85fc942b41f58dac8e72bd12dd8e55.svg","isPro":false,"fullname":"Chaoxiang Xie","user":"bailynlove","type":"user"},"name":"Chaoxiang Xie","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:31:45.296Z","hidden":false},{"_id":"69818a88ce18b1862809638b","name":"Zhensu Sun","hidden":false},{"_id":"69818a88ce18b1862809638c","name":"Yeheng Chen","hidden":false},{"_id":"69818a88ce18b1862809638d","name":"Chenxu Zhang","hidden":false},{"_id":"69818a88ce18b1862809638e","name":"Longfei Yun","hidden":false},{"_id":"69818a88ce18b1862809638f","name":"Chengcheng Wan","hidden":false},{"_id":"69818a88ce18b18628096390","name":"Hongyu Zhang","hidden":false},{"_id":"69818a88ce18b18628096391","name":"David Lo","hidden":false},{"_id":"69818a88ce18b18628096392","name":"Xiaodong Gu","hidden":false}],"publishedAt":"2026-02-02T08:10:21.000Z","submittedOnDailyAt":"2026-02-04T00:15:44.767Z","title":"CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding","submittedOnDailyBy":{"_id":"645b0c3ec35da9c7afd95421","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg","isPro":false,"fullname":"Yuling","user":"YerbaPage","type":"user"},"summary":"Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.","upvotes":81,"discussionId":"69818a89ce18b18628096393","ai_summary":"Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.","ai_keywords":["Multimodal LLMs","source code understanding","token compression","visual cues","syntax highlighting","code completion","clone detection","image modality","visual compression","computational efficiency"],"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}},"publishedAt":"2026-02-02T03:10:21.000Z","title":"CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding","summary":"Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01785.png","numComments":2,"submittedBy":{"_id":"645b0c3ec35da9c7afd95421","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg","fullname":"Yuling","name":"YerbaPage","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":97,"isUserFollowing":false},"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.03786","authors":[{"_id":"6982c1c69084cb4f0ecb574b","user":{"_id":"68a435cc22fdf7356962ccb9","avatarUrl":"/avatars/467f4732ade5f47b42433ff354acdeef.svg","isPro":false,"fullname":"jianhao ruan","user":"Aurorra1123","type":"user"},"name":"Jianhao Ruan","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:23.320Z","hidden":false},{"_id":"6982c1c69084cb4f0ecb574c","name":"Zhihao Xu","hidden":false},{"_id":"6982c1c69084cb4f0ecb574d","name":"Yiran Peng","hidden":false},{"_id":"6982c1c69084cb4f0ecb574e","name":"Fashen Ren","hidden":false},{"_id":"6982c1c69084cb4f0ecb574f","name":"Zhaoyang Yu","hidden":false},{"_id":"6982c1c69084cb4f0ecb5750","name":"Xinbing Liang","hidden":false},{"_id":"6982c1c69084cb4f0ecb5751","name":"Jinyu Xiang","hidden":false},{"_id":"6982c1c69084cb4f0ecb5752","name":"Bang Liu","hidden":false},{"_id":"6982c1c69084cb4f0ecb5753","name":"Chenglin Wu","hidden":false},{"_id":"6982c1c69084cb4f0ecb5754","name":"Yuyu Luo","hidden":false},{"_id":"6982c1c69084cb4f0ecb5755","user":{"_id":"65f40e83653c231cbaf7defe","avatarUrl":"/avatars/afa5ce72324112739e539865c9aee26b.svg","isPro":false,"fullname":"Jiayi Zhang","user":"didiforhugface","type":"user"},"name":"Jiayi Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:20.999Z","hidden":false}],"publishedAt":"2026-02-03T17:46:16.000Z","submittedOnDailyAt":"2026-02-04T02:34:02.843Z","title":"AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration","submittedOnDailyBy":{"_id":"68a435cc22fdf7356962ccb9","avatarUrl":"/avatars/467f4732ade5f47b42433ff354acdeef.svg","isPro":false,"fullname":"jianhao ruan","user":"Aurorra1123","type":"user"},"summary":"Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra","upvotes":66,"discussionId":"6982c1c69084cb4f0ecb5756","ai_summary":"AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.","ai_keywords":["language agents","sub-agent-as-tools paradigm","multi-turn task solving","agent abstraction","task automation","framework-agnostic","agent orchestration","automatic agent creation","Pareto-efficient","GAIA","SWE-Bench","Terminal-Bench"]},"publishedAt":"2026-02-03T12:46:16.000Z","title":"AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration","summary":"Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03786.png","numComments":1,"submittedBy":{"_id":"68a435cc22fdf7356962ccb9","avatarUrl":"/avatars/467f4732ade5f47b42433ff354acdeef.svg","fullname":"jianhao ruan","name":"Aurorra1123","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.02103","authors":[{"_id":"6981ab8dce18b1862809643a","name":"Liyan Xu","hidden":false},{"_id":"6981ab8dce18b1862809643b","name":"Mo Yu","hidden":false},{"_id":"6981ab8dce18b1862809643c","name":"Fandong Meng","hidden":false},{"_id":"6981ab8dce18b1862809643d","name":"Jie Zhou","hidden":false}],"publishedAt":"2026-02-02T13:46:56.000Z","submittedOnDailyAt":"2026-02-04T01:21:43.596Z","title":"No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs","submittedOnDailyBy":{"_id":"650f0fac11f3210cf7a8a849","avatarUrl":"/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg","isPro":false,"fullname":"Liyan Xu","user":"lxucs","type":"user"},"summary":"This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.","upvotes":57,"discussionId":"6981ab8dce18b1862809643e","ai_summary":"Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.","ai_keywords":["Chain-of-Thought","Large Language Models","latent planning","hidden states","Tele-Lens","multi-step reasoning","uncertainty estimation","CoT dynamics"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-02T08:46:56.000Z","title":"No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs","summary":"This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02103.png","numComments":1,"submittedBy":{"_id":"650f0fac11f3210cf7a8a849","avatarUrl":"/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg","fullname":"Liyan Xu","name":"lxucs","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02660","authors":[{"_id":"6982c09b9084cb4f0ecb5724","name":"Jiefeng Chen","hidden":false},{"_id":"6982c09b9084cb4f0ecb5725","name":"Bhavana Dalvi Mishra","hidden":false},{"_id":"6982c09b9084cb4f0ecb5726","name":"Jaehyun Nam","hidden":false},{"_id":"6982c09b9084cb4f0ecb5727","name":"Rui Meng","hidden":false},{"_id":"6982c09b9084cb4f0ecb5728","name":"Tomas Pfister","hidden":false},{"_id":"6982c09b9084cb4f0ecb5729","name":"Jinsung Yoon","hidden":false}],"publishedAt":"2026-02-02T19:00:03.000Z","submittedOnDailyAt":"2026-02-04T01:14:39.432Z","title":"MARS: Modular Agent with Reflective Search for Automated AI Research","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.","upvotes":47,"discussionId":"6982c09c9084cb4f0ecb572a","ai_summary":"MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.","ai_keywords":["Monte Carlo Tree Search","MCTS","modular construction","comparative reflective memory","cross-branch transfer"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-02T14:00:03.000Z","title":"MARS: Modular Agent with Reflective Search for Automated AI Research","summary":"Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02660.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03796","authors":[{"_id":"6982ba459084cb4f0ecb56b0","name":"Zhixue Fang","hidden":false},{"_id":"6982ba459084cb4f0ecb56b1","name":"Xu He","hidden":false},{"_id":"6982ba459084cb4f0ecb56b2","name":"Songlin Tang","hidden":false},{"_id":"6982ba459084cb4f0ecb56b3","name":"Haoxian Zhang","hidden":false},{"_id":"6982ba459084cb4f0ecb56b4","name":"Qingfeng Li","hidden":false},{"_id":"6982ba459084cb4f0ecb56b5","name":"Xiaoqiang Liu","hidden":false},{"_id":"6982ba459084cb4f0ecb56b6","name":"Pengfei Wan","hidden":false},{"_id":"6982ba459084cb4f0ecb56b7","name":"Kun Gai","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66a356f2f7352f4ffbb4e74a/eYfmS5epN6LuK7w-itRi8.mp4"],"publishedAt":"2026-02-03T17:59:09.000Z","submittedOnDailyAt":"2026-02-04T01:18:19.937Z","title":"3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation","submittedOnDailyBy":{"_id":"66a356f2f7352f4ffbb4e74a","avatarUrl":"/avatars/b905cc8719eb31b29e8a94717219a79b.svg","isPro":false,"fullname":"He","user":"Phoebux","type":"user"},"summary":"Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.","upvotes":43,"discussionId":"6982ba459084cb4f0ecb56b8","ai_summary":"3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.","ai_keywords":["motion encoder","video generator","motion tokens","cross-attention","view-rich supervision","geometric supervision","SMPL","camera control","motion fidelity","visual quality"],"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}},"publishedAt":"2026-02-03T12:59:09.000Z","title":"3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation","summary":"Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66a356f2f7352f4ffbb4e74a/eYfmS5epN6LuK7w-itRi8.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03796.png","numComments":5,"submittedBy":{"_id":"66a356f2f7352f4ffbb4e74a","avatarUrl":"/avatars/b905cc8719eb31b29e8a94717219a79b.svg","fullname":"He","name":"Phoebux","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.02619","authors":[{"_id":"6982cefe9084cb4f0ecb57a9","user":{"_id":"66d01e4401f2a6b4cd93ad87","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png","isPro":false,"fullname":"Mohan Jiang (SII)","user":"mhjiang0408","type":"user"},"name":"Mohan Jiang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:27:54.679Z","hidden":false},{"_id":"6982cefe9084cb4f0ecb57aa","name":"Dayuan Fu","hidden":false},{"_id":"6982cefe9084cb4f0ecb57ab","name":"Junhao Shi","hidden":false},{"_id":"6982cefe9084cb4f0ecb57ac","user":{"_id":"62dce08bb2c60f29c3d0a5da","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62dce08bb2c60f29c3d0a5da/dFRFamdOyPbR1OxQC_qOV.png","isPro":false,"fullname":"Ji Zeng","user":"stargazerzj","type":"user"},"name":"Ji Zeng","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:27:51.423Z","hidden":false},{"_id":"6982cefe9084cb4f0ecb57ad","name":"Weiye Si","hidden":false},{"_id":"6982cefe9084cb4f0ecb57ae","user":{"_id":"668e476520e499a0786ea56e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/668e476520e499a0786ea56e/lnvd1_UWW9o9ddrR6ehwR.png","isPro":false,"fullname":"Keyu Li (SII)","user":"weizhihao1","type":"user"},"name":"Keyu Li","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:27:49.277Z","hidden":false},{"_id":"6982cefe9084cb4f0ecb57af","name":"Xuefeng Li","hidden":false},{"_id":"6982cefe9084cb4f0ecb57b0","name":"Yang Xiao","hidden":false},{"_id":"6982cefe9084cb4f0ecb57b1","name":"Wenjie Li","hidden":false},{"_id":"6982cefe9084cb4f0ecb57b2","name":"Dequan Wang","hidden":false},{"_id":"6982cefe9084cb4f0ecb57b3","name":"Pengfei Liu","hidden":false}],"publishedAt":"2026-02-02T13:23:39.000Z","submittedOnDailyAt":"2026-02-04T02:17:05.727Z","title":"daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently","submittedOnDailyBy":{"_id":"66d01e4401f2a6b4cd93ad87","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png","isPro":false,"fullname":"Mohan Jiang (SII)","user":"mhjiang0408","type":"user"},"summary":"While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...","upvotes":43,"discussionId":"6982cefe9084cb4f0ecb57b4","githubRepo":"https://github.com/GAIR-NLP/daVinci-Agency","githubRepoAddedBy":"user","ai_summary":"Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix histories.","ai_keywords":["Large Language Models","long-horizon agentic workflows","training data","long-dependency structures","cross-stage evolutionary dynamics","data synthesis","Pull Request sequences","task decomposition","functional coherence","bug-fix histories","daVinci-Agency","continuous commits","unified functional objectives","verifiable refinement","causal dependencies","iterative refinements","goal-directed behavior","project-level task modeling","Toolathlon"],"githubStars":25,"organization":{"_id":"630bc2d186b8b9904c33ce1b","name":"GAIR","fullname":"SII - GAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}},"publishedAt":"2026-02-02T08:23:39.000Z","title":"daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently","summary":"While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02619.png","numComments":1,"submittedBy":{"_id":"66d01e4401f2a6b4cd93ad87","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png","fullname":"Mohan Jiang (SII)","name":"mhjiang0408","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"630bc2d186b8b9904c33ce1b","name":"GAIR","fullname":"SII - GAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.01630","authors":[{"_id":"6982b3dd9084cb4f0ecb564b","user":{"_id":"6671214c92412fd4640714eb","avatarUrl":"/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg","isPro":false,"fullname":"bohan zeng","user":"zbhpku","type":"user"},"name":"Bohan Zeng","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:47.439Z","hidden":false},{"_id":"6982b3dd9084cb4f0ecb564c","user":{"_id":"6708920aeae29d1cd41a703b","avatarUrl":"/avatars/922427a86523b0aa810412fd2d75f88e.svg","isPro":false,"fullname":"kaixin zhu","user":"czkk566","type":"user"},"name":"Kaixin Zhu","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:49.804Z","hidden":false},{"_id":"6982b3dd9084cb4f0ecb564d","name":"Daili Hua","hidden":false},{"_id":"6982b3dd9084cb4f0ecb564e","name":"Bozhou Li","hidden":false},{"_id":"6982b3dd9084cb4f0ecb564f","name":"Chengzhuo Tong","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5650","user":{"_id":"65e71ef39cf349af2940b317","avatarUrl":"/avatars/fc1cd8d3510946fc947d67b16b51834b.svg","isPro":false,"fullname":"Yuran Wang","user":"Ryann829","type":"user"},"name":"Yuran Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:54.435Z","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5651","name":"Xinyi Huang","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5652","user":{"_id":"674e77fa59a127e4eacf5dba","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/674e77fa59a127e4eacf5dba/W7qr94Buvvaio8zhKrEha.jpeg","isPro":false,"fullname":"Yifan Dai","user":"Moonwines","type":"user"},"name":"Yifan Dai","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:52.281Z","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5653","name":"Zixiang Zhang","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5654","name":"Yifan Yang","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5655","name":"Zhou Liu","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5656","name":"Hao Liang","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5657","name":"Xiaochen Ma","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5658","name":"Ruichuan An","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5659","name":"Tianyi Bai","hidden":false},{"_id":"6982b3dd9084cb4f0ecb565a","name":"Hongcheng Gao","hidden":false},{"_id":"6982b3dd9084cb4f0ecb565b","name":"Junbo Niu","hidden":false},{"_id":"6982b3dd9084cb4f0ecb565c","name":"Yang Shi","hidden":false},{"_id":"6982b3dd9084cb4f0ecb565d","name":"Xinlong Chen","hidden":false},{"_id":"6982b3dd9084cb4f0ecb565e","name":"Yue Ding","hidden":false},{"_id":"6982b3dd9084cb4f0ecb565f","name":"Minglei Shi","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5660","name":"Kai Zeng","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5661","name":"Yiwen Tang","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5662","name":"Yuanxing Zhang","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5663","name":"Pengfei Wan","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5664","name":"Xintao Wang","hidden":false},{"_id":"6982b3dd9084cb4f0ecb5665","name":"Wentao Zhang","hidden":false}],"publishedAt":"2026-02-02T04:42:44.000Z","submittedOnDailyAt":"2026-02-04T00:21:41.173Z","title":"Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks","submittedOnDailyBy":{"_id":"6671214c92412fd4640714eb","avatarUrl":"/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg","isPro":false,"fullname":"bohan zeng","user":"zbhpku","type":"user"},"summary":"World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.","upvotes":41,"discussionId":"6982b3de9084cb4f0ecb5666","ai_summary":"Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.","ai_keywords":["world models","physical dynamics","environment interaction","visual prediction","3D estimation","symbol grounding","unified framework","normative framework","spatial representation"],"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}},"publishedAt":"2026-02-01T23:42:44.000Z","title":"Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks","summary":"World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01630.png","numComments":2,"submittedBy":{"_id":"6671214c92412fd4640714eb","avatarUrl":"/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg","fullname":"bohan zeng","name":"zbhpku","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"662c559b322afcbae51b3c8b","name":"KlingTeam","fullname":"Kling Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.03048","authors":[{"_id":"6982c1039084cb4f0ecb572c","user":{"_id":"6708edcae69f6e30a816af9f","avatarUrl":"/avatars/c4daa9b0cb2f4bb2a7db0e78b22034cb.svg","isPro":false,"fullname":"Yao","user":"distant-yuan","type":"user"},"name":"Zhiyuan Yao","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:25.743Z","hidden":false},{"_id":"6982c1039084cb4f0ecb572d","name":"Yi-Kai Zhang","hidden":false},{"_id":"6982c1039084cb4f0ecb572e","name":"Yuxin Chen","hidden":false},{"_id":"6982c1039084cb4f0ecb572f","name":"Yueqing Sun","hidden":false},{"_id":"6982c1039084cb4f0ecb5730","name":"Zishan Xu","hidden":false},{"_id":"6982c1039084cb4f0ecb5731","name":"Yu Yang","hidden":false},{"_id":"6982c1039084cb4f0ecb5732","name":"Tianhao Hu","hidden":false},{"_id":"6982c1039084cb4f0ecb5733","name":"Qi Gu","hidden":false},{"_id":"6982c1039084cb4f0ecb5734","name":"Hui Su","hidden":false},{"_id":"6982c1039084cb4f0ecb5735","name":"Xunliang Cai","hidden":false}],"publishedAt":"2026-02-03T03:14:36.000Z","submittedOnDailyAt":"2026-02-04T01:16:22.296Z","title":"CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.","upvotes":32,"discussionId":"6982c1049084cb4f0ecb5736","ai_summary":"CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","GRPO","rollout budget","adaptive methods","Capability-Oriented Value function","heap-based greedy strategy","exploration and exploitation","LLM post-training efficiency"],"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}},"publishedAt":"2026-02-02T22:14:36.000Z","title":"CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03048.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03139","authors":[{"_id":"6982d8449084cb4f0ecb580d","name":"Tianhe Wu","hidden":false},{"_id":"6982d8449084cb4f0ecb580e","name":"Ruibin Li","hidden":false},{"_id":"6982d8449084cb4f0ecb580f","name":"Lei Zhang","hidden":false},{"_id":"6982d8449084cb4f0ecb5810","name":"Kede Ma","hidden":false}],"publishedAt":"2026-02-03T05:45:25.000Z","submittedOnDailyAt":"2026-02-04T02:57:31.839Z","title":"Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis","submittedOnDailyBy":{"_id":"655de51982afda0fc479fb91","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/655de51982afda0fc479fb91/-t9RLNEBAESO0niQGHoss.png","isPro":false,"fullname":"Tianhe Wu","user":"TianheWu","type":"user"},"summary":"Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.","upvotes":31,"discussionId":"6982d8449084cb4f0ecb5811","githubRepo":"https://github.com/Multimedia-Analytics-Laboratory/dpdmd","githubRepoAddedBy":"user","ai_summary":"A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.","ai_keywords":["distribution matching distillation","mode collapse","reverse-KL formulation","v-prediction","text-to-image generation","sample diversity","quality refinement"],"githubStars":43,"organization":{"_id":"660f70a49760d0856d246a35","name":"CityU-HongKong","fullname":"City University of Hong Kong","avatar":"https://cdn-uploads.huggingface.co/production/uploads/660f6d7951c7a7d619e75393/BXxWP_bnnPM4OLq6pGpFc.png"}},"publishedAt":"2026-02-03T00:45:25.000Z","title":"Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis","summary":"Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03139.png","numComments":2,"submittedBy":{"_id":"655de51982afda0fc479fb91","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/655de51982afda0fc479fb91/-t9RLNEBAESO0niQGHoss.png","fullname":"Tianhe Wu","name":"TianheWu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"660f70a49760d0856d246a35","name":"CityU-HongKong","fullname":"City University of Hong Kong","avatar":"https://cdn-uploads.huggingface.co/production/uploads/660f6d7951c7a7d619e75393/BXxWP_bnnPM4OLq6pGpFc.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03419","authors":[{"_id":"6982d38d9084cb4f0ecb57df","name":"Shuang Sun","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e0","name":"Huatong Song","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e1","name":"Lisheng Huang","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e2","name":"Jinhao Jiang","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e3","name":"Ran Le","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e4","name":"Zhihao Lv","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e5","name":"Zongchao Chen","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e6","name":"Yiwen Hu","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e7","name":"Wenyang Luo","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e8","name":"Wayne Xin Zhao","hidden":false},{"_id":"6982d38d9084cb4f0ecb57e9","name":"Yang Song","hidden":false},{"_id":"6982d38d9084cb4f0ecb57ea","name":"Hongteng Xu","hidden":false},{"_id":"6982d38d9084cb4f0ecb57eb","name":"Tao Zhang","hidden":false},{"_id":"6982d38d9084cb4f0ecb57ec","name":"Ji-Rong Wen","hidden":false}],"publishedAt":"2026-02-03T11:44:39.000Z","submittedOnDailyAt":"2026-02-04T02:52:59.734Z","title":"SWE-World: Building Software Engineering Agents in Docker-Free Environments","submittedOnDailyBy":{"_id":"61b8405b516a20acdf3b85ff","avatarUrl":"/avatars/3d2eae7c163a80b73260087b05a4230b.svg","isPro":false,"fullname":"Jinhao Jiang","user":"Boru","type":"user"},"summary":"Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World","upvotes":29,"discussionId":"6982d38d9084cb4f0ecb57ed","ai_summary":"A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.","ai_keywords":["large language models","software engineering agents","Docker-free framework","surrogate models","agent-environment interaction","test-time scaling","SWE-bench","Qwen2.5-Coder-32B"],"organization":{"_id":"6704ef33935b1a7c59795566","name":"RUC-AIBOX","fullname":"RUC-AIBOX","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"}},"publishedAt":"2026-02-03T06:44:39.000Z","title":"SWE-World: Building Software Engineering Agents in Docker-Free Environments","summary":"Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03419.png","numComments":1,"submittedBy":{"_id":"61b8405b516a20acdf3b85ff","avatarUrl":"/avatars/3d2eae7c163a80b73260087b05a4230b.svg","fullname":"Jinhao Jiang","name":"Boru","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6704ef33935b1a7c59795566","name":"RUC-AIBOX","fullname":"RUC-AIBOX","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03411","authors":[{"_id":"6982d3ec9084cb4f0ecb57ef","name":"Huatong Song","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f0","name":"Lisheng Huang","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f1","name":"Shuang Sun","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f2","name":"Jinhao Jiang","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f3","name":"Ran Le","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f4","user":{"_id":"649e6761f9134a06ed1e0cea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg","isPro":false,"fullname":"Daixuan Cheng","user":"daixuancheng","type":"user"},"name":"Daixuan Cheng","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:27:42.035Z","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f5","name":"Guoxin Chen","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f6","name":"Yiwen Hu","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f7","name":"Zongchao Chen","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f8","name":"Wayne Xin Zhao","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57f9","name":"Yang Song","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57fa","name":"Tao Zhang","hidden":false},{"_id":"6982d3ec9084cb4f0ecb57fb","name":"Ji-Rong Wen","hidden":false}],"publishedAt":"2026-02-03T11:38:48.000Z","submittedOnDailyAt":"2026-02-04T02:51:43.510Z","title":"SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training","submittedOnDailyBy":{"_id":"61b8405b516a20acdf3b85ff","avatarUrl":"/avatars/3d2eae7c163a80b73260087b05a4230b.svg","isPro":false,"fullname":"Jinhao Jiang","user":"Boru","type":"user"},"summary":"In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.","upvotes":27,"discussionId":"6982d3ec9084cb4f0ecb57fc","ai_summary":"SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.","ai_keywords":["post-training framework","teacher-trajectory synthesis","data curation","long-horizon SFT","RL with real execution feedback","inference framework design","SWE-bench Verified","resolve rate","test-time scaling","LLM-based environment feedback"],"organization":{"_id":"6704ef33935b1a7c59795566","name":"RUC-AIBOX","fullname":"RUC-AIBOX","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"}},"publishedAt":"2026-02-03T06:38:48.000Z","title":"SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training","summary":"In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03411.png","numComments":1,"submittedBy":{"_id":"61b8405b516a20acdf3b85ff","avatarUrl":"/avatars/3d2eae7c163a80b73260087b05a4230b.svg","fullname":"Jinhao Jiang","name":"Boru","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6704ef33935b1a7c59795566","name":"RUC-AIBOX","fullname":"RUC-AIBOX","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61b8405b516a20acdf3b85ff/Q3_mJHjNqZYfArFl1ZpAL.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03845","authors":[{"_id":"6982bc249084cb4f0ecb56da","name":"Tong Zheng","hidden":false},{"_id":"6982bc249084cb4f0ecb56db","user":{"_id":"62ea79dd01ed9b0e8f61ccd3","avatarUrl":"/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg","isPro":false,"fullname":"Chengsong Huang","user":"ChengsongHuang","type":"user"},"name":"Chengsong Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:28.107Z","hidden":false},{"_id":"6982bc249084cb4f0ecb56dc","user":{"_id":"65037565da2d88e201f63b7a","avatarUrl":"/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg","isPro":true,"fullname":"Runpeng Dai","user":"Leo-Dai","type":"user"},"name":"Runpeng Dai","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:30.438Z","hidden":false},{"_id":"6982bc249084cb4f0ecb56dd","name":"Yun He","hidden":false},{"_id":"6982bc249084cb4f0ecb56de","name":"Rui Liu","hidden":false},{"_id":"6982bc249084cb4f0ecb56df","name":"Xin Ni","hidden":false},{"_id":"6982bc249084cb4f0ecb56e0","name":"Huiwen Bao","hidden":false},{"_id":"6982bc249084cb4f0ecb56e1","name":"Kaishen Wang","hidden":false},{"_id":"6982bc249084cb4f0ecb56e2","name":"Hongtu Zhu","hidden":false},{"_id":"6982bc249084cb4f0ecb56e3","name":"Jiaxin Huang","hidden":false},{"_id":"6982bc249084cb4f0ecb56e4","name":"Furong Huang","hidden":false},{"_id":"6982bc249084cb4f0ecb56e5","name":"Heng Huang","hidden":false}],"publishedAt":"2026-02-03T18:59:41.000Z","submittedOnDailyAt":"2026-02-04T00:57:09.105Z","title":"Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing","submittedOnDailyBy":{"_id":"6623ea65b642e29cdf90a1b4","avatarUrl":"/avatars/e32e90574c1162b2be87ed78604e3e4d.svg","isPro":true,"fullname":"TongZheng","user":"TongZheng1999","type":"user"},"summary":"Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy.","upvotes":21,"discussionId":"6982bc259084cb4f0ecb56e6","projectPage":"https://huggingface.co/spaces/EfficientReasoning/efficient_reasoning_online_judgement","githubRepo":"https://github.com/zhengkid/Parallel-Probe","githubRepoAddedBy":"user","ai_summary":"Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.","ai_keywords":["parallel thinking","width-depth dynamics","intermediate answers","consensus-based early stopping","deviation-based branch pruning","Pareto frontier","test-time scaling","majority voting"],"githubStars":10,"organization":{"_id":"68b3c3bbc375e05b059370b2","name":"UMCP","fullname":"University of Maryland College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"}},"publishedAt":"2026-02-03T13:59:41.000Z","title":"Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing","summary":"Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03845.png","numComments":2,"submittedBy":{"_id":"6623ea65b642e29cdf90a1b4","avatarUrl":"/avatars/e32e90574c1162b2be87ed78604e3e4d.svg","fullname":"TongZheng","name":"TongZheng1999","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"68b3c3bbc375e05b059370b2","name":"UMCP","fullname":"University of Maryland College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.03619","authors":[{"_id":"6982b7fc9084cb4f0ecb5670","name":"Changze Lv","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5671","name":"Jie Zhou","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5672","name":"Wentao Zhao","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5673","name":"Jingwen Xu","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5674","name":"Zisu Huang","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5675","name":"Muzhao Tian","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5676","name":"Shihan Dou","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5677","name":"Tao Gui","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5678","name":"Le Tian","hidden":false},{"_id":"6982b7fc9084cb4f0ecb5679","name":"Xiao Zhou","hidden":false},{"_id":"6982b7fc9084cb4f0ecb567a","name":"Xiaoqing Zheng","hidden":false},{"_id":"6982b7fc9084cb4f0ecb567b","name":"Xuanjing Huang","hidden":false},{"_id":"6982b7fc9084cb4f0ecb567c","name":"Jie Zhou","hidden":false}],"publishedAt":"2026-02-03T15:09:56.000Z","submittedOnDailyAt":"2026-02-04T00:38:31.588Z","title":"Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation","submittedOnDailyBy":{"_id":"60efa4da8432bc401cd0abc6","avatarUrl":"/avatars/3c8d8db9aaa5bd5fd1f870ac0a6b655a.svg","isPro":false,"fullname":"Changze Lv","user":"fdu-lcz","type":"user"},"summary":"Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.","upvotes":21,"discussionId":"6982b7fc9084cb4f0ecb567d","ai_summary":"DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.","ai_keywords":["reinforcement learning","hybrid reward","human preference supervision","LLM-based rubric evaluation","Multi-agent Markov-state","DeepResearch bench","open-source baselines","closed-source models"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-03T10:09:56.000Z","title":"Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation","summary":"Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03619.png","numComments":1,"submittedBy":{"_id":"60efa4da8432bc401cd0abc6","avatarUrl":"/avatars/3c8d8db9aaa5bd5fd1f870ac0a6b655a.svg","fullname":"Changze Lv","name":"fdu-lcz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02444","authors":[{"_id":"69835a0237bca54cd0587d6f","user":{"_id":"67277f6a8baabd079b017d5f","avatarUrl":"/avatars/58fd8d2be0f953e22e332923cbf69c41.svg","isPro":false,"fullname":"Tyler Skow","user":"tskow21","type":"user"},"name":"Tyler Skow","status":"claimed_verified","statusLastChangedAt":"2026-02-04T19:03:29.499Z","hidden":false},{"_id":"69835a0237bca54cd0587d70","name":"Alexander Martin","hidden":false},{"_id":"69835a0237bca54cd0587d71","name":"Benjamin Van Durme","hidden":false},{"_id":"69835a0237bca54cd0587d72","name":"Rama Chellappa","hidden":false},{"_id":"69835a0237bca54cd0587d73","name":"Reno Kriz","hidden":false}],"publishedAt":"2026-02-02T18:40:37.000Z","submittedOnDailyAt":"2026-02-04T12:12:29.056Z","title":"RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval","submittedOnDailyBy":{"_id":"644ffb154f731658826945c8","avatarUrl":"/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg","isPro":false,"fullname":"Alex Martin","user":"alexmartin1722","type":"user"},"summary":"Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.","upvotes":16,"discussionId":"69835a0237bca54cd0587d74","githubRepo":"https://github.com/tskow99/RANKVIDEO-Reasoning-Reranker","githubRepoAddedBy":"user","ai_summary":"RANKVIDEO is a reasoning-based video retrieval system that improves upon traditional two-stage frameworks through explicit query-video pair analysis and a multi-objective training approach.","ai_keywords":["reranking","video retrieval","reasoning-based reranker","curriculum training","supervised fine-tuning","pointwise","pairwise","teacher confidence distillation","data synthesis pipeline","MultiVENT 2.0 benchmark"],"githubStars":1,"organization":{"_id":"643568fef81a16e74361e659","name":"hltcoe","fullname":"JHU Human Language Technology Center of Excellence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"}},"publishedAt":"2026-02-02T13:40:37.000Z","title":"RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval","summary":"Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02444.png","numComments":1,"submittedBy":{"_id":"644ffb154f731658826945c8","avatarUrl":"/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg","fullname":"Alex Martin","name":"alexmartin1722","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"643568fef81a16e74361e659","name":"hltcoe","fullname":"JHU Human Language Technology Center of Excellence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02380","authors":[{"_id":"6981774fce18b186280960be","user":{"_id":"654c6845bac6e6e49895a5b5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png","isPro":false,"fullname":"SII-Yibin Wang","user":"CodeGoat24","type":"user"},"name":"Yibin Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:33:05.113Z","hidden":false},{"_id":"6981774fce18b186280960bf","name":"Yuhang Zang","hidden":false},{"_id":"6981774fce18b186280960c0","name":"Feng Han","hidden":false},{"_id":"6981774fce18b186280960c1","name":"Jiazi Bu","hidden":false},{"_id":"6981774fce18b186280960c2","name":"Yujie Zhou","hidden":false},{"_id":"6981774fce18b186280960c3","name":"Cheng Jin","hidden":false},{"_id":"6981774fce18b186280960c4","name":"Jiaqi Wang","hidden":false}],"publishedAt":"2026-02-02T17:44:21.000Z","submittedOnDailyAt":"2026-02-04T00:14:14.041Z","title":"Unified Personalized Reward Model for Vision Generation","submittedOnDailyBy":{"_id":"654c6845bac6e6e49895a5b5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png","isPro":false,"fullname":"SII-Yibin Wang","user":"CodeGoat24","type":"user"},"summary":"Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.","upvotes":16,"discussionId":"6981774fce18b186280960c5","projectPage":"https://codegoat24.github.io/UnifiedReward/flex","githubRepo":"https://github.com/CodeGoat24/UnifiedReward","githubRepoAddedBy":"user","ai_summary":"UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.","ai_keywords":["multimodal reward models","visual generation","Bradley-Terry-style preference modeling","generative VLMs","reinforcement learning","preference optimization","direct preference optimization","structured reasoning traces","semantic intent","visual evidence","hierarchical assessment","flexible reasoning","context-adaptive reasoning","GRPO framework","image synthesis","video synthesis"],"githubStars":691,"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}},"publishedAt":"2026-02-02T12:44:21.000Z","title":"Unified Personalized Reward Model for Vision Generation","summary":"Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02380.png","numComments":1,"submittedBy":{"_id":"654c6845bac6e6e49895a5b5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png","fullname":"SII-Yibin Wang","name":"CodeGoat24","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":23,"isUserFollowing":false},"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.02636","authors":[{"_id":"6982bb119084cb4f0ecb56c4","user":{"_id":"616648c84c0937d31946f21b","avatarUrl":"/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg","isPro":false,"fullname":"Ziyang","user":"hzy","type":"user"},"name":"Ziyang Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:35.018Z","hidden":false},{"_id":"6982bb119084cb4f0ecb56c5","name":"Haolin Ren","hidden":false},{"_id":"6982bb119084cb4f0ecb56c6","name":"Xiaowei Yuan","hidden":false},{"_id":"6982bb119084cb4f0ecb56c7","user":{"_id":"64060b49a577649430bf6974","avatarUrl":"/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg","isPro":false,"fullname":"Jiawei Wang","user":"Jarvis1111","type":"user"},"name":"Jiawei Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:32.866Z","hidden":false},{"_id":"6982bb119084cb4f0ecb56c8","name":"Zhongtao Jiang","hidden":false},{"_id":"6982bb119084cb4f0ecb56c9","name":"Kun Xu","hidden":false},{"_id":"6982bb119084cb4f0ecb56ca","name":"Shizhu He","hidden":false},{"_id":"6982bb119084cb4f0ecb56cb","name":"Jun Zhao","hidden":false},{"_id":"6982bb119084cb4f0ecb56cc","name":"Kang Liu","hidden":false}],"publishedAt":"2026-02-02T18:32:48.000Z","submittedOnDailyAt":"2026-02-04T00:51:45.323Z","title":"WideSeek: Advancing Wide Research via Multi-Agent Scaling","submittedOnDailyBy":{"_id":"616648c84c0937d31946f21b","avatarUrl":"/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg","isPro":false,"fullname":"Ziyang","user":"hzy","type":"user"},"summary":"Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.","upvotes":12,"discussionId":"6982bb119084cb4f0ecb56cd","projectPage":"https://wideseek-ai.github.io/","githubRepo":"https://github.com/hzy312/WideSeek","githubRepoAddedBy":"user","ai_summary":"Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.","ai_keywords":["Wide Research","Deep Research","search intelligence","WideSeekBench","GBIS","multi-phase data pipeline","dynamic hierarchical multi-agent architecture","parallel sub-agents","unified training framework","multi-agent trajectories","end-to-end RL"],"githubStars":3},"publishedAt":"2026-02-02T13:32:48.000Z","title":"WideSeek: Advancing Wide Research via Multi-Agent Scaling","summary":"Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02636.png","numComments":3,"submittedBy":{"_id":"616648c84c0937d31946f21b","avatarUrl":"/avatars/7ca27de5c5116c91ff1db61ba6277ed5.svg","fullname":"Ziyang","name":"hzy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.21244","authors":[{"_id":"698171fbce18b18628096024","user":{"_id":"64bb937d8496ee0fb6cac9aa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64bb937d8496ee0fb6cac9aa/oFkKNxaMrd3wAciwP4Lu5.png","isPro":false,"fullname":"YijuGuo","user":"YijuGuo","type":"user"},"name":"Yiju Guo","status":"claimed_verified","statusLastChangedAt":"2026-02-03T10:07:34.331Z","hidden":false},{"_id":"698171fbce18b18628096025","name":"Tianyi Hu","hidden":false},{"_id":"698171fbce18b18628096026","name":"Zexu Sun","hidden":false},{"_id":"698171fbce18b18628096027","name":"Yankai Lin","hidden":false}],"publishedAt":"2026-01-29T04:08:24.000Z","submittedOnDailyAt":"2026-02-04T00:43:01.380Z","title":"Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification","submittedOnDailyBy":{"_id":"64bb937d8496ee0fb6cac9aa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64bb937d8496ee0fb6cac9aa/oFkKNxaMrd3wAciwP4Lu5.png","isPro":false,"fullname":"YijuGuo","user":"YijuGuo","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.","upvotes":12,"discussionId":"698171fbce18b18628096028","ai_summary":"LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","exploration","rollout budget","sampling success","policy optimization","interference tokens","prompt tokens","GRPO","LENS"],"organization":{"_id":"626a6d6b4909b521e1f59ce5","name":"baidu","fullname":"BAIDU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64f187a2cc1c03340ac30498/TYYUxK8xD1AxExFMWqbZD.png"}},"publishedAt":"2026-01-28T23:08:24.000Z","title":"Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21244.png","numComments":2,"submittedBy":{"_id":"64bb937d8496ee0fb6cac9aa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64bb937d8496ee0fb6cac9aa/oFkKNxaMrd3wAciwP4Lu5.png","fullname":"YijuGuo","name":"YijuGuo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"organization":{"_id":"626a6d6b4909b521e1f59ce5","name":"baidu","fullname":"BAIDU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64f187a2cc1c03340ac30498/TYYUxK8xD1AxExFMWqbZD.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.03086","authors":[{"_id":"6983585c37bca54cd0587d64","name":"Jiayao Mai","hidden":false},{"_id":"6983585c37bca54cd0587d65","name":"Bangyan Liao","hidden":false},{"_id":"6983585c37bca54cd0587d66","name":"Zhenjun Zhao","hidden":false},{"_id":"6983585c37bca54cd0587d67","name":"Yingping Zeng","hidden":false},{"_id":"6983585c37bca54cd0587d68","name":"Haoang Li","hidden":false},{"_id":"6983585c37bca54cd0587d69","name":"Javier Civera","hidden":false},{"_id":"6983585c37bca54cd0587d6a","name":"Tailin Wu","hidden":false},{"_id":"6983585c37bca54cd0587d6b","name":"Yi Zhou","hidden":false},{"_id":"6983585c37bca54cd0587d6c","name":"Peidong Liu","hidden":false}],"publishedAt":"2026-02-03T04:19:48.000Z","submittedOnDailyAt":"2026-02-04T12:04:11.416Z","title":"Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning","submittedOnDailyBy":{"_id":"684afb68f144221f28256461","avatarUrl":"/avatars/48c3d76057f78e1ca4abb2b121a2d089.svg","isPro":false,"fullname":"Zhenjun Zhao","user":"rickyeric","type":"user"},"summary":"The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.","upvotes":11,"discussionId":"6983585c37bca54cd0587d6d","ai_summary":"Neural Predictor-Corrector framework unifies homotopy methods across multiple domains and outperforms classical approaches through learned policies and amortized training.","ai_keywords":["predictor-corrector","reinforcement learning","sequential decision-making","amortized training","neural solver","homotopy paradigm","optimization","sampling"]},"publishedAt":"2026-02-02T23:19:48.000Z","title":"Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning","summary":"The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03086.png","numComments":1,"submittedBy":{"_id":"684afb68f144221f28256461","avatarUrl":"/avatars/48c3d76057f78e1ca4abb2b121a2d089.svg","fullname":"Zhenjun Zhao","name":"rickyeric","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.01362","authors":[{"_id":"69829ac79084cb4f0ecb55e4","user":{"_id":"65aa76b1cb5b4fb08ecb087c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65aa76b1cb5b4fb08ecb087c/0_x6Zb-H36XfugcsQ80Zh.jpeg","isPro":false,"fullname":"Liu Yue","user":"Mzero17","type":"user"},"name":"Yue Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:29:22.643Z","hidden":false},{"_id":"69829ac79084cb4f0ecb55e5","user":{"_id":"635b6b3590204c509f9755bd","avatarUrl":"/avatars/94398dd17ca1a2e74dc8d775abf636a7.svg","isPro":false,"fullname":"zhaoyuzhong","user":"callsys","type":"user"},"name":"Yuzhong Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:29:19.801Z","hidden":false},{"_id":"69829ac79084cb4f0ecb55e6","name":"Zheyong Xie","hidden":false},{"_id":"69829ac79084cb4f0ecb55e7","name":"Qixiang Ye","hidden":false},{"_id":"69829ac79084cb4f0ecb55e8","name":"Jianbin Jiao","hidden":false},{"_id":"69829ac79084cb4f0ecb55e9","name":"Yao Hu","hidden":false},{"_id":"69829ac79084cb4f0ecb55ea","name":"Shaosheng Cao","hidden":false},{"_id":"69829ac79084cb4f0ecb55eb","name":"Yunfan Liu","hidden":false}],"publishedAt":"2026-02-01T18:00:35.000Z","submittedOnDailyAt":"2026-02-04T01:16:01.435Z","title":"Balancing Understanding and Generation in Discrete Diffusion Models","submittedOnDailyBy":{"_id":"635b6b3590204c509f9755bd","avatarUrl":"/avatars/94398dd17ca1a2e74dc8d775abf636a7.svg","isPro":false,"fullname":"zhaoyuzhong","user":"callsys","type":"user"},"summary":"In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM","upvotes":11,"discussionId":"69829ac89084cb4f0ecb55ec","githubRepo":"https://github.com/MzeroMiko/XDLM","githubRepoAddedBy":"user","ai_summary":"XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.","ai_keywords":["Masked Diffusion Language Models","Uniform-noise Diffusion Language Models","stationary noise kernel","Pareto frontier","posterior probabilities","algebraic simplification","large language model","FID","MBPP","training dynamics"],"githubStars":7},"publishedAt":"2026-02-01T13:00:35.000Z","title":"Balancing Understanding and Generation in Discrete Diffusion Models","summary":"In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01362.png","numComments":1,"submittedBy":{"_id":"635b6b3590204c509f9755bd","avatarUrl":"/avatars/94398dd17ca1a2e74dc8d775abf636a7.svg","fullname":"zhaoyuzhong","name":"callsys","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.03798","authors":[{"_id":"6982e3479084cb4f0ecb5864","name":"Zimu Lu","hidden":false},{"_id":"6982e3479084cb4f0ecb5865","name":"Houxing Ren","hidden":false},{"_id":"6982e3479084cb4f0ecb5866","name":"Yunqiao Yang","hidden":false},{"_id":"6982e3479084cb4f0ecb5867","user":{"_id":"64d592c28767727dffa1f002","avatarUrl":"/avatars/fe38bcac944a2742dc12c624e62d24ef.svg","isPro":false,"fullname":"WangKe","user":"scikkk","type":"user"},"name":"Ke Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:27:24.737Z","hidden":false},{"_id":"6982e3479084cb4f0ecb5868","name":"Zhuofan Zong","hidden":false},{"_id":"6982e3479084cb4f0ecb5869","name":"Mingjie Zhan","hidden":false},{"_id":"6982e3479084cb4f0ecb586a","name":"Hongsheng Li","hidden":false}],"publishedAt":"2026-02-03T18:01:34.000Z","submittedOnDailyAt":"2026-02-04T03:49:19.922Z","title":"FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation","submittedOnDailyBy":{"_id":"64b0bfef2f2f9c345b87e673","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg","isPro":false,"fullname":"Zimu Lu","user":"luzimu","type":"user"},"summary":"Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.","upvotes":9,"discussionId":"6982e3479084cb4f0ecb586b","githubRepo":"https://github.com/mnluzimu/FullStack-Agent","githubRepoAddedBy":"user","ai_summary":"A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.","ai_keywords":["multi-agent framework","code editing","codebase navigation","bug localization","data-scaling","self-improving method","back-translation","backbone LLM","comprehensive benchmark","frontend","backend","database functionalities"],"githubStars":1},"publishedAt":"2026-02-03T13:01:34.000Z","title":"FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation","summary":"Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03798.png","numComments":1,"submittedBy":{"_id":"64b0bfef2f2f9c345b87e673","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg","fullname":"Zimu Lu","name":"luzimu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.03216","authors":[{"_id":"6982e8cd9084cb4f0ecb5880","user":{"_id":"639ffbc6beb95d698de9640d","avatarUrl":"/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg","isPro":false,"fullname":"Dongwon Jo","user":"dongwonjo","type":"user"},"name":"Dongwon Jo","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:27:22.258Z","hidden":false},{"_id":"6982e8cd9084cb4f0ecb5881","name":"Beomseok Kang","hidden":false},{"_id":"6982e8cd9084cb4f0ecb5882","name":"Jiwon Song","hidden":false},{"_id":"6982e8cd9084cb4f0ecb5883","name":"Jae-Joon Kim","hidden":false}],"publishedAt":"2026-02-03T07:31:14.000Z","submittedOnDailyAt":"2026-02-04T04:14:17.125Z","title":"Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection","submittedOnDailyBy":{"_id":"639ffbc6beb95d698de9640d","avatarUrl":"/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg","isPro":false,"fullname":"Dongwon Jo","user":"dongwonjo","type":"user"},"summary":"The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.","upvotes":9,"discussionId":"6982e8ce9084cb4f0ecb5884","ai_summary":"Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.","ai_keywords":["attention","token-level sparsification","QKV","Flash Attention","attention speedup","long-context inference","token selection","sparse attention"]},"publishedAt":"2026-02-03T02:31:14.000Z","title":"Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection","summary":"The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03216.png","numComments":2,"submittedBy":{"_id":"639ffbc6beb95d698de9640d","avatarUrl":"/avatars/7ef1aaadd5b378d00e17dc548e42cb7e.svg","fullname":"Dongwon Jo","name":"dongwonjo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.03747","authors":[{"_id":"6982cb3f9084cb4f0ecb578f","user":{"_id":"68961e2a83f79bb28f78baa3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bZmicB1lDqFjIVbpsg_8c.png","isPro":false,"fullname":"junchao-cuhk","user":"junchao-cuhk","type":"user"},"name":"Junchao Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:01.292Z","hidden":false},{"_id":"6982cb3f9084cb4f0ecb5790","name":"Ziyang Ye","hidden":false},{"_id":"6982cb3f9084cb4f0ecb5791","name":"Xinting Hu","hidden":false},{"_id":"6982cb3f9084cb4f0ecb5792","name":"Tianyu He","hidden":false},{"_id":"6982cb3f9084cb4f0ecb5793","name":"Guiyu Zhang","hidden":false},{"_id":"6982cb3f9084cb4f0ecb5794","name":"Shaoshuai Shi","hidden":false},{"_id":"6982cb3f9084cb4f0ecb5795","name":"Jiang Bian","hidden":false},{"_id":"6982cb3f9084cb4f0ecb5796","name":"Li Jiang","hidden":false}],"publishedAt":"2026-02-03T17:10:03.000Z","submittedOnDailyAt":"2026-02-04T12:20:49.591Z","title":"LIVE: Long-horizon Interactive Video World Modeling","submittedOnDailyBy":{"_id":"68961e2a83f79bb28f78baa3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bZmicB1lDqFjIVbpsg_8c.png","isPro":false,"fullname":"junchao-cuhk","user":"junchao-cuhk","type":"user"},"summary":"Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.","upvotes":8,"discussionId":"6982cb409084cb4f0ecb5797","projectPage":"https://junchao-cs.github.io/LIVE-demo/","githubRepo":"https://github.com/Junchao-cs/LIVE","githubRepoAddedBy":"user","ai_summary":"LIVE is a long-horizon video world model that uses cycle-consistency and diffusion loss to control error accumulation during extended video generation.","ai_keywords":["video world models","autoregressive models","error accumulation","cycle-consistency objective","diffusion loss","forward rollout","reverse generation","progressive training curriculum"],"githubStars":2,"organization":{"_id":"68151d0f51add3813f3f7d1b","name":"MicrosoftResearch","fullname":"Microsoft Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}},"publishedAt":"2026-02-03T12:10:03.000Z","title":"LIVE: Long-horizon Interactive Video World Modeling","summary":"Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03747.png","numComments":1,"submittedBy":{"_id":"68961e2a83f79bb28f78baa3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bZmicB1lDqFjIVbpsg_8c.png","fullname":"junchao-cuhk","name":"junchao-cuhk","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"68151d0f51add3813f3f7d1b","name":"MicrosoftResearch","fullname":"Microsoft Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.03709","authors":[{"_id":"69830ec29084cb4f0ecb5943","name":"Vynska Amalia Permadi","hidden":false},{"_id":"69830ec29084cb4f0ecb5944","user":{"_id":"643d0a4d8a55b2bbf4f2a90e","avatarUrl":"/avatars/9534aaf81cbf12f015c6826b682fdb84.svg","isPro":false,"fullname":"Xingwei Tan","user":"XingweiT","type":"user"},"name":"Xingwei Tan","status":"claimed_verified","statusLastChangedAt":"2026-02-04T09:51:04.996Z","hidden":false},{"_id":"69830ec29084cb4f0ecb5945","name":"Nafise Sadat Moosavi","hidden":false},{"_id":"69830ec29084cb4f0ecb5946","name":"Nikos Aletras","hidden":false}],"publishedAt":"2026-02-03T16:32:00.000Z","submittedOnDailyAt":"2026-02-04T07:09:43.139Z","title":"No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding","submittedOnDailyBy":{"_id":"643d0a4d8a55b2bbf4f2a90e","avatarUrl":"/avatars/9534aaf81cbf12f015c6826b682fdb84.svg","isPro":false,"fullname":"Xingwei Tan","user":"XingweiT","type":"user"},"summary":"Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.","upvotes":8,"discussionId":"69830ec29084cb4f0ecb5947","ai_summary":"Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains.","ai_keywords":["question answering","large language models","cultural understanding","multi-hop reasoning","dataset creation","validation pipeline","expert review","LLM-as-a-judge filtering"]},"publishedAt":"2026-02-03T11:32:00.000Z","title":"No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding","summary":"Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03709.png","numComments":1,"submittedBy":{"_id":"643d0a4d8a55b2bbf4f2a90e","avatarUrl":"/avatars/9534aaf81cbf12f015c6826b682fdb84.svg","fullname":"Xingwei Tan","name":"XingweiT","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.02676","authors":[{"_id":"6982b0439084cb4f0ecb562e","user":{"_id":"66b5b0ceef3f3fc987957343","avatarUrl":"/avatars/915de4d13789cd50f0cb61a9bf5fce80.svg","isPro":false,"fullname":"Xintong Zhang","user":"xintongzhang","type":"user"},"name":"Xintong Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:29:02.801Z","hidden":false},{"_id":"6982b0439084cb4f0ecb562f","name":"Xiaowen Zhang","hidden":false},{"_id":"6982b0439084cb4f0ecb5630","name":"Jongrong Wu","hidden":false},{"_id":"6982b0439084cb4f0ecb5631","name":"Zhi Gao","hidden":false},{"_id":"6982b0439084cb4f0ecb5632","name":"Shilin Yan","hidden":false},{"_id":"6982b0439084cb4f0ecb5633","name":"Zhenxin Diao","hidden":false},{"_id":"6982b0439084cb4f0ecb5634","name":"Kunpeng Gao","hidden":false},{"_id":"6982b0439084cb4f0ecb5635","name":"Xuanyan Chen","hidden":false},{"_id":"6982b0439084cb4f0ecb5636","name":"Yuwei Wu","hidden":false},{"_id":"6982b0439084cb4f0ecb5637","name":"Yunde Jia","hidden":false},{"_id":"6982b0439084cb4f0ecb5638","name":"Qing Li","hidden":false}],"publishedAt":"2026-02-02T19:00:27.000Z","submittedOnDailyAt":"2026-02-04T00:16:03.144Z","title":"AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process","submittedOnDailyBy":{"_id":"66b5b0ceef3f3fc987957343","avatarUrl":"/avatars/915de4d13789cd50f0cb61a9bf5fce80.svg","isPro":false,"fullname":"Xintong Zhang","user":"xintongzhang","type":"user"},"summary":"Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.","upvotes":8,"discussionId":"6982b0449084cb4f0ecb5639","projectPage":"https://adaptmmbench.github.io/","githubRepo":"https://github.com/xtong-zhang/AdaptMMBench","githubRepoAddedBy":"user","ai_summary":"AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.","ai_keywords":["Vision-Language Models","adaptive multimodal reasoning","Matthews Correlation Coefficient","task difficulty","model capacity","reasoning mode selection","multi-dimensional process evaluation","key step coverage","tool effectiveness","computational efficiency"],"githubStars":3},"publishedAt":"2026-02-02T14:00:27.000Z","title":"AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process","summary":"Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02676.png","numComments":1,"submittedBy":{"_id":"66b5b0ceef3f3fc987957343","avatarUrl":"/avatars/915de4d13789cd50f0cb61a9bf5fce80.svg","fullname":"Xintong Zhang","name":"xintongzhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.00747","authors":[{"_id":"6981faf447987be58cdb7d13","name":"Shengrui Li","hidden":false},{"_id":"6981faf447987be58cdb7d14","user":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","isPro":false,"fullname":"Fei Zhao","user":"Hiiamein","type":"user"},"name":"Fei Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:31:37.433Z","hidden":false},{"_id":"6981faf447987be58cdb7d15","name":"Kaiyan Zhao","hidden":false},{"_id":"6981faf447987be58cdb7d16","name":"Jieying Ye","hidden":false},{"_id":"6981faf447987be58cdb7d17","name":"Haifeng Liu","hidden":false},{"_id":"6981faf447987be58cdb7d18","name":"Fangcheng Shi","hidden":false},{"_id":"6981faf447987be58cdb7d19","name":"Zheyong Xie","hidden":false},{"_id":"6981faf447987be58cdb7d1a","name":"Yao Hu","hidden":false},{"_id":"6981faf447987be58cdb7d1b","name":"Shaosheng Cao","hidden":false}],"publishedAt":"2026-01-31T14:27:46.000Z","submittedOnDailyAt":"2026-02-04T01:22:02.364Z","title":"Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training","submittedOnDailyBy":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","isPro":false,"fullname":"Fei Zhao","user":"Hiiamein","type":"user"},"summary":"Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.","upvotes":8,"discussionId":"6981faf547987be58cdb7d1c","ai_summary":"DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.","ai_keywords":["Large Language Model","pre-training","data mixture","model merging","weighted model merging","proxy experiments","training costs","search trials","benchmark performance"]},"publishedAt":"2026-01-31T09:27:46.000Z","title":"Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training","summary":"Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00747.png","numComments":1,"submittedBy":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","fullname":"Fei Zhao","name":"Hiiamein","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.03677","authors":[{"_id":"6983275f9084cb4f0ecb596f","name":"Yu Zhang","hidden":false},{"_id":"6983275f9084cb4f0ecb5970","name":"Mufan Xu","hidden":false},{"_id":"6983275f9084cb4f0ecb5971","name":"Xuefeng Bai","hidden":false},{"_id":"6983275f9084cb4f0ecb5972","name":"Kehai chen","hidden":false},{"_id":"6983275f9084cb4f0ecb5973","name":"Pengfei Zhang","hidden":false},{"_id":"6983275f9084cb4f0ecb5974","name":"Yang Xiang","hidden":false},{"_id":"6983275f9084cb4f0ecb5975","name":"Min Zhang","hidden":false}],"publishedAt":"2026-02-03T15:59:24.000Z","submittedOnDailyAt":"2026-02-04T08:34:08.709Z","title":"Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration","submittedOnDailyBy":{"_id":"65fe9599d74aed6c3e6d93a2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65fe9599d74aed6c3e6d93a2/WxPDKPJ6-p0_xu0dzOYXw.jpeg","isPro":false,"fullname":"Yu Zhang","user":"271754echo","type":"user"},"summary":"Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere 5% of these critical heads can decrease the modality-following ratio by 60% through blocking, or increase it by 60% through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.","upvotes":5,"discussionId":"6983275f9084cb4f0ecb5976","ai_summary":"Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent.","ai_keywords":["multimodal large language models","instruction tokens","modality arbitration","attention layers","multimodal cues","semantic inertia","attention heads","causal interventions","information flow lens"]},"publishedAt":"2026-02-03T10:59:24.000Z","title":"Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration","summary":"Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere 5% of these critical heads can decrease the modality-following ratio by 60% through blocking, or increase it by 60% through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03677.png","numComments":1,"submittedBy":{"_id":"65fe9599d74aed6c3e6d93a2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65fe9599d74aed6c3e6d93a2/WxPDKPJ6-p0_xu0dzOYXw.jpeg","fullname":"Yu Zhang","name":"271754echo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.03647","authors":[{"_id":"6982bfb79084cb4f0ecb570c","name":"Bowei He","hidden":false},{"_id":"6982bfb79084cb4f0ecb570d","name":"Minda Hu","hidden":false},{"_id":"6982bfb79084cb4f0ecb570e","name":"Zenan Xu","hidden":false},{"_id":"6982bfb79084cb4f0ecb570f","name":"Hongru Wang","hidden":false},{"_id":"6982bfb79084cb4f0ecb5710","name":"Licheng Zong","hidden":false},{"_id":"6982bfb79084cb4f0ecb5711","name":"Yankai Chen","hidden":false},{"_id":"6982bfb79084cb4f0ecb5712","name":"Chen Ma","hidden":false},{"_id":"6982bfb79084cb4f0ecb5713","name":"Xue Liu","hidden":false},{"_id":"6982bfb79084cb4f0ecb5714","name":"Pluto Zhou","hidden":false},{"_id":"6982bfb79084cb4f0ecb5715","name":"Irwin King","hidden":false}],"publishedAt":"2026-02-03T15:32:09.000Z","submittedOnDailyAt":"2026-02-04T01:11:03.810Z","title":"Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.","upvotes":5,"discussionId":"6982bfb79084cb4f0ecb5716","ai_summary":"Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.","ai_keywords":["reinforcement learning","search-integrated reasoning","Actor-Refiner collaboration","multi-scale credit assignment","trajectory-level rewards","dense process reward","hybrid reward design","cut-and-regenerate mechanism","smoothed mixture policy","general QA","multi-hop QA"],"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}},"publishedAt":"2026-02-03T10:32:09.000Z","title":"Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration","summary":"Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03647.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.01053","authors":[{"_id":"6982ebf29084cb4f0ecb5886","name":"Hyesung Jeon","hidden":false},{"_id":"6982ebf29084cb4f0ecb5887","name":"Hyeongju Ha","hidden":false},{"_id":"6982ebf29084cb4f0ecb5888","name":"Jae-Joon Kim","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6400208acafc9d549863af59/N05YjVPfc0vOTPHqLIfyt.png"],"publishedAt":"2026-02-01T06:36:46.000Z","submittedOnDailyAt":"2026-02-04T04:22:49.354Z","title":"LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents","submittedOnDailyBy":{"_id":"6400208acafc9d549863af59","avatarUrl":"/avatars/6c383c810a038ce61e803f1d75132471.svg","isPro":false,"fullname":"Hyesung Jeon","user":"hjeon2k","type":"user"},"summary":"Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.","upvotes":5,"discussionId":"6982ebf39084cb4f0ecb5889","githubRepo":"https://github.com/hjeon2k/LRAgent","githubRepoAddedBy":"user","ai_summary":"LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.","ai_keywords":["multi-LoRA","KV cache sharing","LoRA weights","pretrained backbone","Flash-LoRA-Attention","low-rank cache","shared-$A$ multi-LoRA","attention computation"],"githubStars":1,"organization":{"_id":"6304ac9f5d136debcecba4fd","name":"snu","fullname":"Seoul National University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1661250691168-6304abe86dbbb80f1636b20b.jpeg"}},"publishedAt":"2026-02-01T01:36:46.000Z","title":"LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents","summary":"Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6400208acafc9d549863af59/N05YjVPfc0vOTPHqLIfyt.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01053.png","numComments":1,"submittedBy":{"_id":"6400208acafc9d549863af59","avatarUrl":"/avatars/6c383c810a038ce61e803f1d75132471.svg","fullname":"Hyesung Jeon","name":"hjeon2k","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6304ac9f5d136debcecba4fd","name":"snu","fullname":"Seoul National University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1661250691168-6304abe86dbbb80f1636b20b.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.00359","authors":[{"_id":"6981762dce18b1862809606c","user":{"_id":"65f8ae0f6c02ff2f6d772f7e","avatarUrl":"/avatars/cb3798f4a7f55f928ed2f5ead0407d36.svg","isPro":false,"fullname":"Minhua Lin","user":"ventr1c","type":"user"},"name":"Minhua Lin","status":"claimed_verified","statusLastChangedAt":"2026-02-03T13:59:41.819Z","hidden":false},{"_id":"6981762dce18b1862809606d","name":"Hanqing Lu","hidden":false},{"_id":"6981762dce18b1862809606e","name":"Zhan Shi","hidden":false},{"_id":"6981762dce18b1862809606f","name":"Bing He","hidden":false},{"_id":"6981762dce18b18628096070","name":"Rui Mao","hidden":false},{"_id":"6981762dce18b18628096071","name":"Zhiwei Zhang","hidden":false},{"_id":"6981762dce18b18628096072","name":"Zongyu Wu","hidden":false},{"_id":"6981762dce18b18628096073","name":"Xianfeng Tang","hidden":false},{"_id":"6981762dce18b18628096074","name":"Hui Liu","hidden":false},{"_id":"6981762dce18b18628096075","name":"Zhenwei Dai","hidden":false},{"_id":"6981762dce18b18628096076","name":"Xiang Zhang","hidden":false},{"_id":"6981762dce18b18628096077","name":"Suhang Wang","hidden":false},{"_id":"6981762dce18b18628096078","name":"Benoit Dumoulin","hidden":false},{"_id":"6981762dce18b18628096079","name":"Jian Pei","hidden":false}],"publishedAt":"2026-01-30T22:15:58.000Z","submittedOnDailyAt":"2026-02-04T15:45:57.045Z","title":"Position: Agentic Evolution is the Path to Evolving LLMs","submittedOnDailyBy":{"_id":"65f8ae0f6c02ff2f6d772f7e","avatarUrl":"/avatars/cb3798f4a7f55f928ed2f5ead0407d36.svg","isPro":false,"fullname":"Minhua Lin","user":"ventr1c","type":"user"},"summary":"As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.","upvotes":5,"discussionId":"6981762ece18b1862809607a","githubRepo":"https://github.com/ventr1c/agentic-evoluiton","githubRepoAddedBy":"user","ai_summary":"Large language models face limitations in adapting to changing real-world environments, necessitating a new approach called agentic evolution that treats deployment-time improvement as a goal-directed optimization process.","ai_keywords":["Large Language Models","deployment-time adaptation","parametric fine-tuning","heuristic memory accumulation","agentic evolution","A-Evolve","evolution-scaling hypothesis"],"githubStars":1},"publishedAt":"2026-01-30T17:15:58.000Z","title":"Position: Agentic Evolution is the Path to Evolving LLMs","summary":"As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00359.png","numComments":1,"submittedBy":{"_id":"65f8ae0f6c02ff2f6d772f7e","avatarUrl":"/avatars/cb3798f4a7f55f928ed2f5ead0407d36.svg","fullname":"Minhua Lin","name":"ventr1c","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.02537","authors":[{"_id":"6982bdf29084cb4f0ecb56f7","name":"Runjie Zhou","hidden":false},{"_id":"6982bdf29084cb4f0ecb56f8","name":"Youbo Shao","hidden":false},{"_id":"6982bdf29084cb4f0ecb56f9","name":"Haoyu Lu","hidden":false},{"_id":"6982bdf29084cb4f0ecb56fa","name":"Bowei Xing","hidden":false},{"_id":"6982bdf29084cb4f0ecb56fb","name":"Tongtong Bai","hidden":false},{"_id":"6982bdf29084cb4f0ecb56fc","name":"Yujie Chen","hidden":false},{"_id":"6982bdf29084cb4f0ecb56fd","name":"Jie Zhao","hidden":false},{"_id":"6982bdf29084cb4f0ecb56fe","name":"Lin Sui","hidden":false},{"_id":"6982bdf29084cb4f0ecb56ff","name":"Haotian Yao","hidden":false},{"_id":"6982bdf29084cb4f0ecb5700","name":"Zijia Zhao","hidden":false},{"_id":"6982bdf29084cb4f0ecb5701","name":"Hao Yang","hidden":false},{"_id":"6982bdf29084cb4f0ecb5702","name":"Haoning Wu","hidden":false},{"_id":"6982bdf29084cb4f0ecb5703","name":"Zaida Zhou","hidden":false},{"_id":"6982bdf29084cb4f0ecb5704","name":"Jinguo Zhu","hidden":false},{"_id":"6982bdf29084cb4f0ecb5705","name":"Zhiqi Huang","hidden":false},{"_id":"6982bdf29084cb4f0ecb5706","name":"Yiping Bao","hidden":false},{"_id":"6982bdf29084cb4f0ecb5707","name":"Yangyang Liu","hidden":false},{"_id":"6982bdf29084cb4f0ecb5708","name":"Y. Charles","hidden":false},{"_id":"6982bdf29084cb4f0ecb5709","name":"Xinyu Zhou","hidden":false}],"publishedAt":"2026-01-28T11:27:22.000Z","submittedOnDailyAt":"2026-02-04T01:03:09.845Z","title":"WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure \"what the model memorizes.\" The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.","upvotes":5,"discussionId":"6982bdf29084cb4f0ecb570a","ai_summary":"WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.","ai_keywords":["Multimodal Large Language Models","visual world knowledge","visual knowledge retrieval","reasoning","atomic visual world knowledge","visual factuality","encyclopedic breadth","hallucination rates"],"organization":{"_id":"6425a114812813f8f4a9b02c","name":"moonshotai","fullname":"Moonshot AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}},"publishedAt":"2026-01-28T06:27:22.000Z","title":"WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models","summary":"We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure \"what the model memorizes.\" The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02537.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"organization":{"_id":"6425a114812813f8f4a9b02c","name":"moonshotai","fullname":"Moonshot AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.02905","authors":[{"_id":"6982c7529084cb4f0ecb576d","user":{"_id":"64a833d2f152bba4b550c913","avatarUrl":"/avatars/cff37a427c01c6b6691f588481d96416.svg","isPro":false,"fullname":"Zhen Wang","user":"yzabc007","type":"user"},"name":"Zhen Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:14.277Z","hidden":false},{"_id":"6982c7529084cb4f0ecb576e","name":"Fan Bai","hidden":false},{"_id":"6982c7529084cb4f0ecb576f","name":"Zhongyan Luo","hidden":false},{"_id":"6982c7529084cb4f0ecb5770","name":"Jinyan Su","hidden":false},{"_id":"6982c7529084cb4f0ecb5771","user":{"_id":"616745f893f6837511d9ae25","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/616745f893f6837511d9ae25/H0JIGDL5VOu29zVd4KiKB.jpeg","isPro":false,"fullname":"Kaiser Sun","user":"KaiserWhoLearns","type":"user"},"name":"Kaiser Sun","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:09.745Z","hidden":false},{"_id":"6982c7529084cb4f0ecb5772","name":"Xinle Yu","hidden":false},{"_id":"6982c7529084cb4f0ecb5773","name":"Jieyuan Liu","hidden":false},{"_id":"6982c7529084cb4f0ecb5774","name":"Kun Zhou","hidden":false},{"_id":"6982c7529084cb4f0ecb5775","name":"Claire Cardie","hidden":false},{"_id":"6982c7529084cb4f0ecb5776","name":"Mark Dredze","hidden":false},{"_id":"6982c7529084cb4f0ecb5777","name":"Eric P. Xing","hidden":false},{"_id":"6982c7529084cb4f0ecb5778","name":"Zhiting Hu","hidden":false}],"publishedAt":"2026-02-02T23:21:13.000Z","submittedOnDailyAt":"2026-02-04T20:50:22.883Z","title":"FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights","submittedOnDailyBy":{"_id":"64a833d2f152bba4b550c913","avatarUrl":"/avatars/cff37a427c01c6b6691f588481d96416.svg","isPro":false,"fullname":"Zhen Wang","user":"yzabc007","type":"user"},"summary":"Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.","upvotes":4,"discussionId":"6982c7529084cb4f0ecb5779","projectPage":"https://firebench.github.io/","ai_summary":"Researchers developed FIRE-Bench, a comprehensive evaluation framework that challenges autonomous agents to rediscover established scientific findings through complete research cycles involving hypothesis generation, experimentation, coding, and evidence-based conclusion drawing.","ai_keywords":["large language models","autonomous agents","scientific discovery","research outputs","machine learning research","high-impact research","empirical evidence","experimental design","evidence-based reasoning","agent systems","frontier LLMs","research question"],"organization":{"_id":"697e87d12cc19315a8497001","name":"UCSanDiego","fullname":"University of California at San Diego","avatar":"https://cdn-uploads.huggingface.co/production/uploads/697e8687c00f332cf492d29e/KUQpvngxP4r9oBSDZwIwZ.png"}},"publishedAt":"2026-02-02T18:21:13.000Z","title":"FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights","summary":"Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02905.png","numComments":1,"submittedBy":{"_id":"64a833d2f152bba4b550c913","avatarUrl":"/avatars/cff37a427c01c6b6691f588481d96416.svg","fullname":"Zhen Wang","name":"yzabc007","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"697e87d12cc19315a8497001","name":"UCSanDiego","fullname":"University of California at San Diego","avatar":"https://cdn-uploads.huggingface.co/production/uploads/697e8687c00f332cf492d29e/KUQpvngxP4r9oBSDZwIwZ.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.01753","authors":[{"_id":"69816da3ce18b18628095fc1","user":{"_id":"67067633351e0c16a5c27497","avatarUrl":"/avatars/356aa3431198c8931b820a714bcfb19d.svg","isPro":false,"fullname":"Shenghao Fu","user":"fushh7","type":"user"},"name":"Shenghao Fu","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:33:16.203Z","hidden":false},{"_id":"69816da3ce18b18628095fc2","name":"Yukun Su","hidden":false},{"_id":"69816da3ce18b18628095fc3","name":"Fengyun Rao","hidden":false},{"_id":"69816da3ce18b18628095fc4","name":"Jing Lyu","hidden":false},{"_id":"69816da3ce18b18628095fc5","name":"Xiaohua Xie","hidden":false},{"_id":"69816da3ce18b18628095fc6","name":"Wei-Shi Zheng","hidden":false}],"publishedAt":"2026-02-02T07:38:45.000Z","submittedOnDailyAt":"2026-02-04T00:29:01.727Z","title":"ObjEmbed: Towards Universal Multimodal Object Embeddings","submittedOnDailyBy":{"_id":"67067633351e0c16a5c27497","avatarUrl":"/avatars/356aa3431198c8931b820a714bcfb19d.svg","isPro":false,"fullname":"Shenghao Fu","user":"fushh7","type":"user"},"summary":"Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.","upvotes":4,"discussionId":"69816da3ce18b18628095fc7","githubRepo":"https://github.com/WeChatCV/ObjEmbed","githubRepoAddedBy":"user","ai_summary":"ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.","ai_keywords":["multimodal embedding models","visual grounding","local image retrieval","global image retrieval","object embedding","IoU embedding","semantic matching","localization quality","semantic discrimination"],"githubStars":13},"publishedAt":"2026-02-02T02:38:45.000Z","title":"ObjEmbed: Towards Universal Multimodal Object Embeddings","summary":"Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01753.png","numComments":1,"submittedBy":{"_id":"67067633351e0c16a5c27497","avatarUrl":"/avatars/356aa3431198c8931b820a714bcfb19d.svg","fullname":"Shenghao Fu","name":"fushh7","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.19103","authors":[{"_id":"69797705df44b75fa47e473d","name":"Linshan Wu","hidden":false},{"_id":"69797705df44b75fa47e473e","name":"Jiaxin Zhuang","hidden":false},{"_id":"69797705df44b75fa47e473f","name":"Hao Chen","hidden":false}],"publishedAt":"2026-01-27T02:10:34.000Z","submittedOnDailyAt":"2026-02-04T00:07:54.729Z","title":"Glance and Focus Reinforcement for Pan-cancer Screening","submittedOnDailyBy":{"_id":"65d86ea2685624d5f206d7ec","avatarUrl":"/avatars/b9bc4c398d5def393bc782e9a7c5e302.svg","isPro":false,"fullname":"Linshan Wu","user":"Luffy503","type":"user"},"summary":"Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).","upvotes":4,"discussionId":"69797705df44b75fa47e4740","githubRepo":"https://github.com/Luffy03/GF-Screen","githubRepoAddedBy":"user","ai_summary":"A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.","ai_keywords":["reinforcement learning","glance model","focus model","segmentation","non-differentiable","group relative learning","pan-cancer screening","CT scans","foreground-background imbalance","false positives"],"githubStars":26,"organization":{"_id":"6609f50bf4ab651901ae4541","name":"hongkongust","fullname":"The Hong Kong University of Science and Technology"}},"publishedAt":"2026-01-26T21:10:34.000Z","title":"Glance and Focus Reinforcement for Pan-cancer Screening","summary":"Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19103.png","numComments":1,"submittedBy":{"_id":"65d86ea2685624d5f206d7ec","avatarUrl":"/avatars/b9bc4c398d5def393bc782e9a7c5e302.svg","fullname":"Linshan Wu","name":"Luffy503","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"6609f50bf4ab651901ae4541","name":"hongkongust","fullname":"The Hong Kong University of Science and Technology"},"isAuthorParticipating":false},{"paper":{"id":"2602.03806","authors":[{"_id":"6982bcd19084cb4f0ecb56e8","name":"Ziru Chen","hidden":false},{"_id":"6982bcd19084cb4f0ecb56e9","name":"Dongdong Chen","hidden":false},{"_id":"6982bcd19084cb4f0ecb56ea","name":"Ruinan Jin","hidden":false},{"_id":"6982bcd19084cb4f0ecb56eb","name":"Yingbin Liang","hidden":false},{"_id":"6982bcd19084cb4f0ecb56ec","name":"Yujia Xie","hidden":false},{"_id":"6982bcd19084cb4f0ecb56ed","name":"Huan Sun","hidden":false}],"publishedAt":"2026-02-03T18:08:41.000Z","submittedOnDailyAt":"2026-02-04T02:23:34.387Z","title":"Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation","submittedOnDailyBy":{"_id":"6494bbbb185ae221545a0ef6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6494bbbb185ae221545a0ef6/oA6HXlD0KnNfttYf2f_qD.jpeg","isPro":false,"fullname":"Ziru Chen","user":"ronch99","type":"user"},"summary":"Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.","upvotes":3,"discussionId":"6982bcd29084cb4f0ecb56ee","githubRepo":"https://github.com/OSU-NLP-Group/cobalt","githubRepoAddedBy":"user","ai_summary":"Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.","ai_keywords":["large language models","reinforcement learning","multi-turn code generation","Markov decision process","contextual bandit learning","offline trajectories","GRPO","VeRPO","Pass@1 scores","reward hacking","perturbed trajectories"],"githubStars":3,"organization":{"_id":"6127b4827dcb442c226129da","name":"osunlp","fullname":"OSU NLP Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6477a323dbc2a416f8b852b3/oiPPBo_knuDrz0YN9slKj.png"}},"publishedAt":"2026-02-03T13:08:41.000Z","title":"Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation","summary":"Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03806.png","numComments":1,"submittedBy":{"_id":"6494bbbb185ae221545a0ef6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6494bbbb185ae221545a0ef6/oA6HXlD0KnNfttYf2f_qD.jpeg","fullname":"Ziru Chen","name":"ronch99","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"6127b4827dcb442c226129da","name":"osunlp","fullname":"OSU NLP Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6477a323dbc2a416f8b852b3/oiPPBo_knuDrz0YN9slKj.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.03454","authors":[{"_id":"6982b6d89084cb4f0ecb5668","user":{"_id":"636f6e8a31af06da86499ebc","avatarUrl":"/avatars/9430fbc05774aad8e46c0861769b3c30.svg","isPro":false,"fullname":"Yeongtak","user":"Yeongtak","type":"user"},"name":"Yeongtak Oh","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:42.713Z","hidden":false},{"_id":"6982b6d89084cb4f0ecb5669","name":"Sangwon Yu","hidden":false},{"_id":"6982b6d89084cb4f0ecb566a","name":"Junsung Park","hidden":false},{"_id":"6982b6d89084cb4f0ecb566b","name":"Han Cheol Moon","hidden":false},{"_id":"6982b6d89084cb4f0ecb566c","name":"Jisoo Mok","hidden":false},{"_id":"6982b6d89084cb4f0ecb566d","name":"Sungroh Yoon","hidden":false}],"publishedAt":"2026-02-03T12:21:26.000Z","submittedOnDailyAt":"2026-02-04T00:33:33.702Z","title":"Contextualized Visual Personalization in Vision-Language Models","submittedOnDailyBy":{"_id":"636f6e8a31af06da86499ebc","avatarUrl":"/avatars/9430fbc05774aad8e46c0861769b3c30.svg","isPro":false,"fullname":"Yeongtak","user":"Yeongtak","type":"user"},"summary":"Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.","upvotes":3,"discussionId":"6982b6d89084cb4f0ecb566e","ai_summary":"CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.","ai_keywords":["vision-language models","contextualized visual personalization","personalized image captioning","reinforcement-learning-based post-training","caption-augmented generation","visual recognition","textual retrieval","diagnostic evaluations","visual context"]},"publishedAt":"2026-02-03T07:21:26.000Z","title":"Contextualized Visual Personalization in Vision-Language Models","summary":"Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03454.png","numComments":1,"submittedBy":{"_id":"636f6e8a31af06da86499ebc","avatarUrl":"/avatars/9430fbc05774aad8e46c0861769b3c30.svg","fullname":"Yeongtak","name":"Yeongtak","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.03295","authors":[{"_id":"6982b19b9084cb4f0ecb563f","user":{"_id":"6981d7eb1655f07c7aa39ea0","avatarUrl":"/avatars/a8ca4d00c4017189f04905da425c8697.svg","isPro":false,"fullname":"Junhui He","user":"Junhuihe","type":"user"},"name":"Junhui He","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:28:57.961Z","hidden":false},{"_id":"6982b19b9084cb4f0ecb5640","name":"Zhihui Fu","hidden":false},{"_id":"6982b19b9084cb4f0ecb5641","name":"Jun Wang","hidden":false},{"_id":"6982b19b9084cb4f0ecb5642","name":"Qingan Li","hidden":false}],"publishedAt":"2026-02-03T09:22:26.000Z","submittedOnDailyAt":"2026-02-04T10:35:05.205Z","title":"POP: Prefill-Only Pruning for Efficient Large Model Inference","submittedOnDailyBy":{"_id":"6981d7eb1655f07c7aa39ea0","avatarUrl":"/avatars/a8ca4d00c4017189f04905da425c8697.svg","isPro":false,"fullname":"Junhui He","user":"Junhuihe","type":"user"},"summary":"Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.","upvotes":3,"discussionId":"6982b19b9084cb4f0ecb5643","ai_summary":"Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.","ai_keywords":["structured pruning","large language models","vision-language models","prefill stage","decode stage","virtual gate mechanism","key-value projections","boundary handling strategy","speedup","accuracy-efficiency trade-off"]},"publishedAt":"2026-02-03T04:22:26.000Z","title":"POP: Prefill-Only Pruning for Efficient Large Model Inference","summary":"Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03295.png","numComments":1,"submittedBy":{"_id":"6981d7eb1655f07c7aa39ea0","avatarUrl":"/avatars/a8ca4d00c4017189f04905da425c8697.svg","fullname":"Junhui He","name":"Junhuihe","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.02419","authors":[{"_id":"6982d63f9084cb4f0ecb5808","name":"Qingni Wang","hidden":false},{"_id":"6982d63f9084cb4f0ecb5809","name":"Yue Fan","hidden":false},{"_id":"6982d63f9084cb4f0ecb580a","name":"Xin Eric Wang","hidden":false}],"publishedAt":"2026-02-02T18:22:45.000Z","submittedOnDailyAt":"2026-02-04T02:48:29.522Z","title":"SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration","submittedOnDailyBy":{"_id":"64679a226192d39142245e5e","avatarUrl":"/avatars/05abee0b6317f100923936ca2099e9eb.svg","isPro":false,"fullname":"Xin Eric Wang","user":"xw-eric","type":"user"},"summary":"Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38% percentage points over Gemini-only inference.","upvotes":3,"discussionId":"6982d6409084cb4f0ecb580b","githubRepo":"https://github.com/Cece1031/SAFEGROUND","githubRepoAddedBy":"user","ai_summary":"SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.","ai_keywords":["GUI grounding","uncertainty quantification","calibration","false discovery rate","distribution-aware","stochastic samples","test-time decision threshold"],"githubStars":4},"publishedAt":"2026-02-02T13:22:45.000Z","title":"SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration","summary":"Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38% percentage points over Gemini-only inference.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02419.png","numComments":1,"submittedBy":{"_id":"64679a226192d39142245e5e","avatarUrl":"/avatars/05abee0b6317f100923936ca2099e9eb.svg","fullname":"Xin Eric Wang","name":"xw-eric","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.03837","authors":[{"_id":"6982b85e9084cb4f0ecb5687","name":"David P. Woodruff","hidden":false},{"_id":"6982b85e9084cb4f0ecb5688","name":"Vincent Cohen-Addad","hidden":false},{"_id":"6982b85e9084cb4f0ecb5689","name":"Lalit Jain","hidden":false},{"_id":"6982b85e9084cb4f0ecb568a","name":"Jieming Mao","hidden":false},{"_id":"6982b85e9084cb4f0ecb568b","name":"Song Zuo","hidden":false},{"_id":"6982b85e9084cb4f0ecb568c","name":"MohammadHossein Bateni","hidden":false},{"_id":"6982b85e9084cb4f0ecb568d","name":"Simina Branzei","hidden":false},{"_id":"6982b85e9084cb4f0ecb568e","name":"Michael P. Brenner","hidden":false},{"_id":"6982b85e9084cb4f0ecb568f","name":"Lin Chen","hidden":false},{"_id":"6982b85e9084cb4f0ecb5690","name":"Ying Feng","hidden":false},{"_id":"6982b85e9084cb4f0ecb5691","name":"Lance Fortnow","hidden":false},{"_id":"6982b85e9084cb4f0ecb5692","name":"Gang Fu","hidden":false},{"_id":"6982b85e9084cb4f0ecb5693","name":"Ziyi Guan","hidden":false},{"_id":"6982b85e9084cb4f0ecb5694","name":"Zahra Hadizadeh","hidden":false},{"_id":"6982b85e9084cb4f0ecb5695","name":"Mohammad T. Hajiaghayi","hidden":false},{"_id":"6982b85e9084cb4f0ecb5696","name":"Mahdi JafariRaviz","hidden":false},{"_id":"6982b85e9084cb4f0ecb5697","name":"Adel Javanmard","hidden":false},{"_id":"6982b85e9084cb4f0ecb5698","name":"Karthik C. S.","hidden":false},{"_id":"6982b85e9084cb4f0ecb5699","name":"Ken-ichi Kawarabayashi","hidden":false},{"_id":"6982b85e9084cb4f0ecb569a","name":"Ravi Kumar","hidden":false},{"_id":"6982b85e9084cb4f0ecb569b","name":"Silvio Lattanzi","hidden":false},{"_id":"6982b85e9084cb4f0ecb569c","name":"Euiwoong Lee","hidden":false},{"_id":"6982b85e9084cb4f0ecb569d","name":"Yi Li","hidden":false},{"_id":"6982b85e9084cb4f0ecb569e","name":"Ioannis Panageas","hidden":false},{"_id":"6982b85e9084cb4f0ecb569f","name":"Dimitris Paparas","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a0","name":"Benjamin Przybocki","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a1","name":"Bernardo Subercaseaux","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a2","name":"Ola Svensson","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a3","name":"Shayan Taherijam","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a4","name":"Xuan Wu","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a5","name":"Eylon Yogev","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a6","name":"Morteza Zadimoghaddam","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a7","name":"Samson Zhou","hidden":false},{"_id":"6982b85e9084cb4f0ecb56a8","name":"Vahab Mirrokni","hidden":false}],"publishedAt":"2026-02-03T18:56:17.000Z","submittedOnDailyAt":"2026-02-04T00:39:21.645Z","title":"Accelerating Scientific Research with Gemini: Case Studies and Common Techniques","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.","upvotes":2,"discussionId":"6982b85e9084cb4f0ecb56a9","ai_summary":"Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.","ai_keywords":["large language models","theoretical computer science","mathematical discovery","human-AI collaboration","iterative refinement","problem decomposition","cross-disciplinary knowledge transfer","adversarial reviewer","neuro-symbolic loop","proof verification","automated code execution"]},"publishedAt":"2026-02-03T13:56:17.000Z","title":"Accelerating Scientific Research with Gemini: Case Studies and Common Techniques","summary":"Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03837.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":228,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.02914","authors":[{"_id":"6982b1989084cb4f0ecb563b","user":{"_id":"65ebae78d767680a0cf5f833","avatarUrl":"/avatars/5e0cee3000c6c4166983c2892e27bc8f.svg","isPro":true,"fullname":"Marshall Guo","user":"weathon","type":"user"},"name":"Wenqi Guo","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:29:00.692Z","hidden":false},{"_id":"6982b1989084cb4f0ecb563c","name":"Shan Du","hidden":false}],"publishedAt":"2026-02-02T23:41:14.000Z","submittedOnDailyAt":"2026-02-04T00:11:18.264Z","title":"FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction","submittedOnDailyBy":{"_id":"65ebae78d767680a0cf5f833","avatarUrl":"/avatars/5e0cee3000c6c4166983c2892e27bc8f.svg","isPro":true,"fullname":"Marshall Guo","user":"weathon","type":"user"},"summary":"Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.","upvotes":2,"discussionId":"6982b1989084cb4f0ecb563d","ai_summary":"FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.","ai_keywords":["face recognition","privacy-preserving","identity extraction attack","linkage matching","face regeneration","pixel-level reconstruction","PSNR","SSIM","visual obfuscation"],"organization":{"_id":"67481ed386d0329c44e1c49f","name":"UBC-O","fullname":"University of British Columbia - Okanagan Campus","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/lS-d_6xAQhQD3Vr6dBFyK.png"}},"publishedAt":"2026-02-02T18:41:14.000Z","title":"FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction","summary":"Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02914.png","numComments":1,"submittedBy":{"_id":"65ebae78d767680a0cf5f833","avatarUrl":"/avatars/5e0cee3000c6c4166983c2892e27bc8f.svg","fullname":"Marshall Guo","name":"weathon","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"67481ed386d0329c44e1c49f","name":"UBC-O","fullname":"University of British Columbia - Okanagan Campus","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/lS-d_6xAQhQD3Vr6dBFyK.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.02751","authors":[{"_id":"6983723937bca54cd0587da1","name":"Lisa Alazraki","hidden":false},{"_id":"6983723937bca54cd0587da2","name":"William F. Shen","hidden":false},{"_id":"6983723937bca54cd0587da3","name":"Yoram Bachrach","hidden":false},{"_id":"6983723937bca54cd0587da4","name":"Akhil Mathur","hidden":false}],"publishedAt":"2026-02-02T20:05:51.000Z","submittedOnDailyAt":"2026-02-04T13:54:43.885Z","title":"Scaling Small Agents Through Strategy Auctions","submittedOnDailyBy":{"_id":"638b828972d5e4aa0121de19","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CmmMUyzpm7WHB82uQ9cUz.png","isPro":false,"fullname":"Lisa Alazraki","user":"LisaAlaz","type":"user"},"summary":"Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively \"scaled up\" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.","upvotes":2,"discussionId":"6983723937bca54cd0587da5","ai_summary":"Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms.","ai_keywords":["small language models","agentic AI","deep search","coding tasks","Strategy Auctions for Workload Efficiency","agent framework","freelancer marketplaces","strategic plans","cost-value mechanism","auction memory","per-task routing","continual self-improvement","pass@1","task complexity","workload efficiency"],"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"}},"publishedAt":"2026-02-02T15:05:51.000Z","title":"Scaling Small Agents Through Strategy Auctions","summary":"Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively \"scaled up\" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02751.png","numComments":1,"submittedBy":{"_id":"638b828972d5e4aa0121de19","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CmmMUyzpm7WHB82uQ9cUz.png","fullname":"Lisa Alazraki","name":"LisaAlaz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"66b54027408752ae16404b05","name":"metaresearch","fullname":"Meta Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66b25f3f58babfaeb76112dc/2GmiaF075AZ7BcE538oPk.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.01212","authors":[{"_id":"698302c79084cb4f0ecb58fe","name":"Marco Chen","hidden":false},{"_id":"698302c79084cb4f0ecb58ff","name":"Xianbiao Qi","hidden":false},{"_id":"698302c79084cb4f0ecb5900","name":"Yelin He","hidden":false},{"_id":"698302c79084cb4f0ecb5901","name":"Jiaquan Ye","hidden":false},{"_id":"698302c79084cb4f0ecb5902","name":"Rong Xiao","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6494483aa13255720397287a/IjovjzNXgoGyOSfC00gCD.png","https://cdn-uploads.huggingface.co/production/uploads/6494483aa13255720397287a/1-ET-Vyvr9PlP6udHCPxR.png"],"publishedAt":"2026-02-01T13:07:17.000Z","submittedOnDailyAt":"2026-02-04T06:00:54.663Z","title":"SimpleGPT: Improving GPT via A Simple Normalization Strategy","submittedOnDailyBy":{"_id":"6494483aa13255720397287a","avatarUrl":"/avatars/61ff2e0371df513194246cf6fbb2b78a.svg","isPro":false,"fullname":"Xianbiao Qi","user":"qixianbiao","type":"user"},"summary":"In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3times-10times larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.","upvotes":2,"discussionId":"698302c79084cb4f0ecb5903","ai_summary":"SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance.","ai_keywords":["Transformer optimization","second-order geometry","Hessian matrix","spectral norm","learning rate","activation scale","SimpleNorm","SimpleGPT","LLaMA2","QKNorm"]},"publishedAt":"2026-02-01T08:07:17.000Z","title":"SimpleGPT: Improving GPT via A Simple Normalization Strategy","summary":"In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3times-10times larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6494483aa13255720397287a/IjovjzNXgoGyOSfC00gCD.png","https://cdn-uploads.huggingface.co/production/uploads/6494483aa13255720397287a/1-ET-Vyvr9PlP6udHCPxR.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01212.png","numComments":1,"submittedBy":{"_id":"6494483aa13255720397287a","avatarUrl":"/avatars/61ff2e0371df513194246cf6fbb2b78a.svg","fullname":"Xianbiao Qi","name":"qixianbiao","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.03320","authors":[{"_id":"6983428837bca54cd0587d0f","user":{"_id":"64745934a855203d8fe9588d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64745934a855203d8fe9588d/ETmY7zSULoAKrhShMvOzy.jpeg","isPro":false,"fullname":"Shengyuan Liu","user":"Saint-lsy","type":"user"},"name":"Shengyuan Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-04T19:03:39.205Z","hidden":false},{"_id":"6983428837bca54cd0587d10","name":"Liuxin Bao","hidden":false},{"_id":"6983428837bca54cd0587d11","name":"Qi Yang","hidden":false},{"_id":"6983428837bca54cd0587d12","name":"Wanting Geng","hidden":false},{"_id":"6983428837bca54cd0587d13","name":"Boyun Zheng","hidden":false},{"_id":"6983428837bca54cd0587d14","name":"Chenxin Li","hidden":false},{"_id":"6983428837bca54cd0587d15","name":"Wenting Chen","hidden":false},{"_id":"6983428837bca54cd0587d16","name":"Houwen Peng","hidden":false},{"_id":"6983428837bca54cd0587d17","name":"Yixuan Yuan","hidden":false}],"publishedAt":"2026-02-03T09:47:49.000Z","submittedOnDailyAt":"2026-02-04T10:30:09.038Z","title":"MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning","submittedOnDailyBy":{"_id":"64745934a855203d8fe9588d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64745934a855203d8fe9588d/ETmY7zSULoAKrhShMvOzy.jpeg","isPro":false,"fullname":"Shengyuan Liu","user":"Saint-lsy","type":"user"},"summary":"Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available https://github.com/CUHK-AIM-Group/MedSAM-Agent{here}.","upvotes":1,"discussionId":"6983428837bca54cd0587d18","githubRepo":"https://github.com/CUHK-AIM-Group/MedSAM-Agent","githubRepoAddedBy":"user","ai_summary":"MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.","ai_keywords":["Multi-modal Large Language Models","reinforcement learning with verifiable reward","Segment Anything Model","hybrid prompting strategy","two-stage training pipeline","clinical-fidelity process reward","multi-turn interaction","end-to-end outcome verification"],"githubStars":8},"publishedAt":"2026-02-03T04:47:49.000Z","title":"MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning","summary":"Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available https://github.com/CUHK-AIM-Group/MedSAM-Agent{here}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03320.png","numComments":1,"submittedBy":{"_id":"64745934a855203d8fe9588d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64745934a855203d8fe9588d/ETmY7zSULoAKrhShMvOzy.jpeg","fullname":"Shengyuan Liu","name":"Saint-lsy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.03238","authors":[{"_id":"6982f5619084cb4f0ecb58e2","name":"Pengyu Zhu","hidden":false},{"_id":"6982f5619084cb4f0ecb58e3","name":"Li Sun","hidden":false},{"_id":"6982f5619084cb4f0ecb58e4","name":"Philip S. Yu","hidden":false},{"_id":"6982f5619084cb4f0ecb58e5","name":"Sen Su","hidden":false}],"publishedAt":"2026-02-03T08:18:37.000Z","submittedOnDailyAt":"2026-02-04T05:01:16.974Z","title":"The Necessity of a Unified Framework for LLM-Based Agent Evaluation","submittedOnDailyBy":{"_id":"6682c061752d68b77f6a7340","avatarUrl":"/avatars/daed8141ee413262f69aa7fa343c2592.svg","isPro":false,"fullname":"zhupengyu","user":"whfeLingYu","type":"user"},"summary":"With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.","upvotes":1,"discussionId":"6982f5629084cb4f0ecb58e6","ai_summary":"Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.","ai_keywords":["Large Language Models","general-purpose agents","agent benchmarks","system prompts","toolset configurations","environmental dynamics","prompt engineering","reasoning","tool usage","standardized evaluation","unified evaluation framework"]},"publishedAt":"2026-02-03T03:18:37.000Z","title":"The Necessity of a Unified Framework for LLM-Based Agent Evaluation","summary":"With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03238.png","numComments":1,"submittedBy":{"_id":"6682c061752d68b77f6a7340","avatarUrl":"/avatars/daed8141ee413262f69aa7fa343c2592.svg","fullname":"zhupengyu","name":"whfeLingYu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.03183","authors":[{"_id":"6983cd4ce34659da7e1f4c7b","name":"Hyunwoo Kim","hidden":false},{"_id":"6983cd4ce34659da7e1f4c7c","name":"Niloofar Mireshghallah","hidden":false},{"_id":"6983cd4ce34659da7e1f4c7d","name":"Michael Duan","hidden":false},{"_id":"6983cd4ce34659da7e1f4c7e","name":"Rui Xin","hidden":false},{"_id":"6983cd4ce34659da7e1f4c7f","name":"Shuyue Stella Li","hidden":false},{"_id":"6983cd4ce34659da7e1f4c80","name":"Jaehun Jung","hidden":false},{"_id":"6983cd4ce34659da7e1f4c81","name":"David Acuna","hidden":false},{"_id":"6983cd4ce34659da7e1f4c82","name":"Qi Pang","hidden":false},{"_id":"6983cd4ce34659da7e1f4c83","name":"Hanshen Xiao","hidden":false},{"_id":"6983cd4ce34659da7e1f4c84","name":"G. Edward Suh","hidden":false},{"_id":"6983cd4ce34659da7e1f4c85","name":"Sewoong Oh","hidden":false},{"_id":"6983cd4ce34659da7e1f4c86","name":"Yulia Tsvetkov","hidden":false},{"_id":"6983cd4ce34659da7e1f4c87","name":"Pang Wei Koh","hidden":false},{"_id":"6983cd4ce34659da7e1f4c88","name":"Yejin Choi","hidden":false}],"publishedAt":"2026-02-03T06:54:46.000Z","submittedOnDailyAt":"2026-02-04T20:21:58.920Z","title":"Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch","submittedOnDailyBy":{"_id":"627a8c1793d0b645835e65f0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666754635237-627a8c1793d0b645835e65f0.jpeg","isPro":false,"fullname":"Hyunwoo Kim","user":"heanu","type":"user"},"summary":"Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.","upvotes":1,"discussionId":"6983cd4de34659da7e1f4c89","projectPage":"https://privasis.github.io","githubRepo":"https://github.com/skywalker023/privasis","githubRepoAddedBy":"user","ai_summary":"A large-scale synthetic dataset called Privasis is introduced to address privacy concerns in AI research, enabling more effective text sanitization with compact models that outperform existing large language models.","ai_keywords":["privacy-sensitive data","synthetic dataset","text sanitization","large language models","parallel corpus","attribute annotation"],"githubStars":0,"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-03T01:54:46.000Z","title":"Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch","summary":"Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03183.png","numComments":1,"submittedBy":{"_id":"627a8c1793d0b645835e65f0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1666754635237-627a8c1793d0b645835e65f0.jpeg","fullname":"Hyunwoo Kim","name":"heanu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02494","authors":[{"_id":"6981c956ce18b18628096569","name":"Dulhan Jayalath","hidden":false},{"_id":"6981c956ce18b1862809656a","name":"Oiwi Parker Jones","hidden":false}],"publishedAt":"2026-02-02T18:59:50.000Z","submittedOnDailyAt":"2026-02-04T10:22:59.526Z","title":"MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training","submittedOnDailyBy":{"_id":"672b79e99380700b60cd5c71","avatarUrl":"/avatars/46f79c736a49b2d6ab4339871eea698d.svg","isPro":false,"fullname":"Dulhan Jayalath","user":"latentdulhan","type":"user"},"summary":"Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .","upvotes":1,"discussionId":"6981c957ce18b1862809656b","githubRepo":"https://github.com/neural-processing-lab/MEG-XL","githubRepoAddedBy":"user","ai_summary":"MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.","ai_keywords":["brain-to-text interfaces","pre-training","statistical priors","MEG context","long-context pre-training","word decoding","neural context","foundation models"],"githubStars":2,"organization":{"_id":"67ab53f09cc9741800da0fba","name":"pnpl","fullname":"Parker Jones Neural Processing Lab (PNPL)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67643f6acf6b09fe21937f13/t0VJCh-B2rgcjiXbA2C3Q.png"}},"publishedAt":"2026-02-02T13:59:50.000Z","title":"MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training","summary":"Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02494.png","numComments":1,"submittedBy":{"_id":"672b79e99380700b60cd5c71","avatarUrl":"/avatars/46f79c736a49b2d6ab4339871eea698d.svg","fullname":"Dulhan Jayalath","name":"latentdulhan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"67ab53f09cc9741800da0fba","name":"pnpl","fullname":"Parker Jones Neural Processing Lab (PNPL)","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67643f6acf6b09fe21937f13/t0VJCh-B2rgcjiXbA2C3Q.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.02405","authors":[{"_id":"6983656737bca54cd0587d80","name":"Ethan Mendes","hidden":false},{"_id":"6983656737bca54cd0587d81","name":"Jungsoo Park","hidden":false},{"_id":"6983656737bca54cd0587d82","name":"Alan Ritter","hidden":false}],"publishedAt":"2026-02-02T18:03:43.000Z","submittedOnDailyAt":"2026-02-04T12:57:37.389Z","title":"Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning","submittedOnDailyBy":{"_id":"635d76ce94e5b275ca74b967","avatarUrl":"/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg","isPro":false,"fullname":"Ethan Mendes","user":"emendes3","type":"user"},"summary":"Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.","upvotes":1,"discussionId":"6983656737bca54cd0587d83","githubRepo":"https://github.com/ethanm88/DAIL","githubRepoAddedBy":"user","ai_summary":"Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data.","ai_keywords":["large language models","expert solutions","imitation learning","distributional gap","reasoning traces","contrastive objective","pass@k","out-of-domain generalization"],"githubStars":0},"publishedAt":"2026-02-02T13:03:43.000Z","title":"Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning","summary":"Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02405.png","numComments":1,"submittedBy":{"_id":"635d76ce94e5b275ca74b967","avatarUrl":"/avatars/5d9a389e5fd558c0b8f0724bf0838a3e.svg","fullname":"Ethan Mendes","name":"emendes3","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.01405","authors":[{"_id":"69836c8637bca54cd0587d89","user":{"_id":"632d0a031d303f5f9ad50aa3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/632d0a031d303f5f9ad50aa3/dRtjvO0KbdUMKPnHTp_SK.jpeg","isPro":false,"fullname":"Nikhil Sharma","user":"nikhilsk","type":"user"},"name":"Nikhil Sharma","status":"claimed_verified","statusLastChangedAt":"2026-02-04T19:03:26.520Z","hidden":false},{"_id":"69836c8637bca54cd0587d8a","user":{"_id":"68c5a8aa501d66d099201ce0","avatarUrl":"/avatars/814163cb5f1dfd149ea138189e27d64e.svg","isPro":false,"fullname":"Zheng Zhang","user":"roryzhang95","type":"user"},"name":"Zheng Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-04T19:06:59.012Z","hidden":false},{"_id":"69836c8637bca54cd0587d8b","name":"Daniel Lee","hidden":false},{"_id":"69836c8637bca54cd0587d8c","name":"Namita Krishnan","hidden":false},{"_id":"69836c8637bca54cd0587d8d","name":"Guang-Jie Ren","hidden":false},{"_id":"69836c8637bca54cd0587d8e","name":"Ziang Xiao","hidden":false},{"_id":"69836c8637bca54cd0587d8f","name":"Yunyao Li","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632d0a031d303f5f9ad50aa3/4PDHxsUCBIxgmax-_jEuz.png"],"publishedAt":"2026-02-01T19:16:23.000Z","submittedOnDailyAt":"2026-02-04T13:32:19.644Z","title":"Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents","submittedOnDailyBy":{"_id":"632d0a031d303f5f9ad50aa3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/632d0a031d303f5f9ad50aa3/dRtjvO0KbdUMKPnHTp_SK.jpeg","isPro":false,"fullname":"Nikhil Sharma","user":"nikhilsk","type":"user"},"summary":"High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.","upvotes":1,"discussionId":"69836c8737bca54cd0587d90","organization":{"_id":"61e5d14f77496de0a6d95c6b","name":"adobe","fullname":"Adobe","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}},"publishedAt":"2026-02-01T14:16:23.000Z","title":"Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents","summary":"High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/632d0a031d303f5f9ad50aa3/4PDHxsUCBIxgmax-_jEuz.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01405.png","numComments":1,"submittedBy":{"_id":"632d0a031d303f5f9ad50aa3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/632d0a031d303f5f9ad50aa3/dRtjvO0KbdUMKPnHTp_SK.jpeg","fullname":"Nikhil Sharma","name":"nikhilsk","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"61e5d14f77496de0a6d95c6b","name":"adobe","fullname":"Adobe","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.00682","authors":[{"_id":"6982c13e9084cb4f0ecb5738","name":"Yuecheng Li","hidden":false},{"_id":"6982c13e9084cb4f0ecb5739","name":"Hengwei Ju","hidden":false},{"_id":"6982c13e9084cb4f0ecb573a","name":"Zeyu Song","hidden":false},{"_id":"6982c13e9084cb4f0ecb573b","name":"Wei Yang","hidden":false},{"_id":"6982c13e9084cb4f0ecb573c","name":"Chi Lu","hidden":false},{"_id":"6982c13e9084cb4f0ecb573d","name":"Peng Jiang","hidden":false},{"_id":"6982c13e9084cb4f0ecb573e","name":"Kun Gai","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6683b38720ee9ac417195830/b4eaiWzyum1R48-xrcDZN.png"],"publishedAt":"2026-01-31T11:58:38.000Z","submittedOnDailyAt":"2026-02-04T09:16:48.276Z","title":"RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment","submittedOnDailyBy":{"_id":"6683b38720ee9ac417195830","avatarUrl":"/avatars/6066e5dfb8cf7e54de18f013fe4d1da6.svg","isPro":false,"fullname":"Yuecheng Li","user":"6lyc","type":"user"},"summary":"Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.","upvotes":1,"discussionId":"6982c13e9084cb4f0ecb573f","githubRepo":"https://github.com/6lyc/RecGOAT-LLM4Rec","githubRepoAddedBy":"user","ai_summary":"A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.","ai_keywords":["multimodal recommendation","large models","semantic understanding","contextual reasoning","representational divergence","graph attention networks","collaborative semantics","cross-modal contrastive learning","optimal adaptive transport","instance-level alignment","distribution-level alignment"],"githubStars":1},"publishedAt":"2026-01-31T06:58:38.000Z","title":"RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment","summary":"Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6683b38720ee9ac417195830/b4eaiWzyum1R48-xrcDZN.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00682.png","numComments":1,"submittedBy":{"_id":"6683b38720ee9ac417195830","avatarUrl":"/avatars/6066e5dfb8cf7e54de18f013fe4d1da6.svg","fullname":"Yuecheng Li","name":"6lyc","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.00398","authors":[{"_id":"698376e437bca54cd0587da7","name":"Ajay Jaiswal","hidden":false},{"_id":"698376e437bca54cd0587da8","name":"Lauren Hannah","hidden":false},{"_id":"698376e437bca54cd0587da9","name":"Han-Byul Kim","hidden":false},{"_id":"698376e437bca54cd0587daa","name":"Duc Hoang","hidden":false},{"_id":"698376e437bca54cd0587dab","name":"Arnav Kundu","hidden":false},{"_id":"698376e437bca54cd0587dac","name":"Mehrdad Farajtabar","hidden":false},{"_id":"698376e437bca54cd0587dad","name":"Minsik Cho","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64ad7afdafb6aa5534221c96/__RqULiML5RVTpKhFJ-W3.png"],"publishedAt":"2026-01-30T23:25:20.000Z","submittedOnDailyAt":"2026-02-04T14:12:20.663Z","title":"MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers","submittedOnDailyBy":{"_id":"64ad7afdafb6aa5534221c96","avatarUrl":"/avatars/c1c83d8f5ca683ce60fd4d50c028471c.svg","isPro":false,"fullname":"Ajay Jaiswal","user":"Ajay1994","type":"user"},"summary":"Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.","upvotes":1,"discussionId":"698376e437bca54cd0587dae","ai_summary":"MemoryLLM decouples feed-forward networks from self-attention in transformers, enabling context-free token-wise neural retrieval memory that improves inference efficiency through pre-computed lookups.","ai_keywords":["transformer components","feed-forward modules","self-attention","interpretability","MemoryLLM","context-free token-wise neural retrieval memory","token embeddings","pre-computed lookups","Flex-MemoryLLM"],"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"}},"publishedAt":"2026-01-30T18:25:20.000Z","title":"MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers","summary":"Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64ad7afdafb6aa5534221c96/__RqULiML5RVTpKhFJ-W3.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00398.png","numComments":1,"submittedBy":{"_id":"64ad7afdafb6aa5534221c96","avatarUrl":"/avatars/c1c83d8f5ca683ce60fd4d50c028471c.svg","fullname":"Ajay Jaiswal","name":"Ajay1994","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.01519","authors":[{"_id":"6983319b9084cb4f0ecb5989","user":{"_id":"69832f2c95375aec9d779304","avatarUrl":"/avatars/491c9b1f83125d2a685edd5df6a953a3.svg","isPro":false,"fullname":"Shiju Zhao","user":"shijuzhao","type":"user"},"name":"Shiju Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-04T12:26:17.510Z","hidden":false},{"_id":"6983319b9084cb4f0ecb598a","name":"Junhao Hu","hidden":false},{"_id":"6983319b9084cb4f0ecb598b","name":"Jiaqi Zheng","hidden":false},{"_id":"6983319b9084cb4f0ecb598c","name":"Guihai Chen","hidden":false}],"publishedAt":"2026-02-02T01:23:13.000Z","submittedOnDailyAt":"2026-02-04T11:05:37.052Z","title":"You Need an Encoder for Native Position-Independent Caching","submittedOnDailyBy":{"_id":"69832f2c95375aec9d779304","avatarUrl":"/avatars/491c9b1f83125d2a685edd5df6a953a3.svg","isPro":false,"fullname":"Shiju Zhao","user":"shijuzhao","type":"user"},"summary":"The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.","upvotes":0,"discussionId":"6983319b9084cb4f0ecb598d","ai_summary":"Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.","ai_keywords":["Key-Value cache","Large Language Models","position-independent caching","decoder-only LLMs","encoder","caching system","Time-to-First-Token","throughput"],"organization":{"_id":"638f70e8f1256a80d4288555","name":"nanjinguniv","fullname":"Nanjing University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/638f706ef1256a80d42880f9/6M6-JzwJGiLxjIJzvCflf.png"}},"publishedAt":"2026-02-01T20:23:13.000Z","title":"You Need an Encoder for Native Position-Independent Caching","summary":"The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01519.png","numComments":1,"submittedBy":{"_id":"69832f2c95375aec9d779304","avatarUrl":"/avatars/491c9b1f83125d2a685edd5df6a953a3.svg","fullname":"Shiju Zhao","name":"shijuzhao","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"638f70e8f1256a80d4288555","name":"nanjinguniv","fullname":"Nanjing University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/638f706ef1256a80d42880f9/6M6-JzwJGiLxjIJzvCflf.png"},"isAuthorParticipating":true}]
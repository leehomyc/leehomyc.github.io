[{"paper":{"id":"2602.10693","authors":[{"_id":"6992047b50fb2c0be47837f0","user":{"_id":"6475ff9b4c9fb8a4bf1cde76","avatarUrl":"/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg","isPro":false,"fullname":"floyed shen","user":"floyed","type":"user"},"name":"Guobin Shen","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:52:51.206Z","hidden":false},{"_id":"6992047b50fb2c0be47837f1","user":{"_id":"63fc5b724c57549ad5e54558","avatarUrl":"/avatars/1374c1e8969533dd7543959666f16d1a.svg","isPro":false,"fullname":"Chenxiao Zhao","user":"ChenShawn","type":"user"},"name":"Chenxiao Zhao","status":"admin_assigned","statusLastChangedAt":"2026-02-17T17:17:12.583Z","hidden":false},{"_id":"6992047b50fb2c0be47837f2","user":{"_id":"655c43d6b426ec8f4b5e7652","avatarUrl":"/avatars/ddcf9d1ef0e2dc1f564a56ba9153f24f.svg","isPro":false,"fullname":"Xiang Cheng","user":"FFFc2","type":"user"},"name":"Xiang Cheng","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:52:57.697Z","hidden":false},{"_id":"6992047b50fb2c0be47837f3","user":{"_id":"61f4c2e981c4d30f58140279","avatarUrl":"/avatars/c4a69f6563c952354e33682e86045b14.svg","isPro":false,"fullname":"HuangMeow","user":"Luckyyy","type":"user"},"name":"Lei Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-23T09:46:50.954Z","hidden":false},{"_id":"6992047b50fb2c0be47837f4","name":"Xing Yu","hidden":false}],"publishedAt":"2026-02-11T09:48:08.000Z","submittedOnDailyAt":"2026-02-23T03:29:14.259Z","title":"VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training","submittedOnDailyBy":{"_id":"6475ff9b4c9fb8a4bf1cde76","avatarUrl":"/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg","isPro":false,"fullname":"floyed shen","user":"floyed","type":"user"},"summary":"Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO","upvotes":158,"discussionId":"6992047c50fb2c0be47837f5","githubRepo":"https://github.com/FloyedShen/VESPO","githubRepoAddedBy":"user","ai_summary":"VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.","ai_keywords":["reinforcement learning","large language models","policy staleness","asynchronous training","importance sampling","variance reduction","variational formulation","proposal distributions","sequence-level importance weights","mathematical reasoning benchmarks","Mixture-of-Experts models"],"githubStars":14,"organization":{"_id":"68246a0a98117c02df67a547","name":"rednote-hilab","fullname":"rednote-hilab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"}},"publishedAt":"2026-02-11T04:48:08.000Z","title":"VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training","summary":"Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10693.png","numComments":2,"submittedBy":{"_id":"6475ff9b4c9fb8a4bf1cde76","avatarUrl":"/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg","fullname":"floyed shen","name":"floyed","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"68246a0a98117c02df67a547","name":"rednote-hilab","fullname":"rednote-hilab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08354","authors":[{"_id":"699bcb43f723198bd53a633e","user":{"_id":"6593d2329e16fa7510e0876a","avatarUrl":"/avatars/c200676dd9dc1db8a3e27388251aea49.svg","isPro":false,"fullname":"hzx","user":"hzxllll","type":"user"},"name":"Zixuan Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-23T09:42:40.470Z","hidden":false},{"_id":"699bcb43f723198bd53a633f","name":"Xin Xia","hidden":false},{"_id":"699bcb43f723198bd53a6340","name":"Yuxi Ren","hidden":false},{"_id":"699bcb43f723198bd53a6341","name":"Jianbin Zheng","hidden":false},{"_id":"699bcb43f723198bd53a6342","name":"Xuanda Wang","hidden":false},{"_id":"699bcb43f723198bd53a6343","name":"Zhixia Zhang","hidden":false},{"_id":"699bcb43f723198bd53a6344","name":"Hongyan Xie","hidden":false},{"_id":"699bcb43f723198bd53a6345","name":"Songshi Liang","hidden":false},{"_id":"699bcb43f723198bd53a6346","name":"Zehao Chen","hidden":false},{"_id":"699bcb43f723198bd53a6347","name":"Xuefeng Xiao","hidden":false},{"_id":"699bcb43f723198bd53a6348","name":"Fuzhen Zhuang","hidden":false},{"_id":"699bcb43f723198bd53a6349","name":"Jianxin Li","hidden":false},{"_id":"699bcb43f723198bd53a634a","user":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun B","user":"Yikunb","type":"user"},"name":"Yikun Ban","status":"claimed_verified","statusLastChangedAt":"2026-02-23T09:42:42.815Z","hidden":false},{"_id":"699bcb43f723198bd53a634b","name":"Deqing Wang","hidden":false}],"publishedAt":"2026-02-09T07:38:22.000Z","submittedOnDailyAt":"2026-02-23T01:06:54.391Z","title":"Does Your Reasoning Model Implicitly Know When to Stop Thinking?","submittedOnDailyBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun B","user":"Yikunb","type":"user"},"summary":"Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.","upvotes":95,"discussionId":"699bcb44f723198bd53a634c","projectPage":"https://hzx122.github.io/sage-rl/","ai_summary":"Large reasoning models can implicitly determine optimal stopping points for thinking, which SAGE-RL enhances by incorporating efficient reasoning patterns into pass@1 inference for improved accuracy and efficiency.","ai_keywords":["large reasoning models","chains of thought","sampling paradigms","self-aware guided efficient reasoning","group-based reinforcement learning","pass@1 inference","mathematical benchmarks"],"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}},"publishedAt":"2026-02-09T02:38:22.000Z","title":"Does Your Reasoning Model Implicitly Know When to Stop Thinking?","summary":"Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08354.png","numComments":2,"submittedBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","fullname":"Yikun B","name":"Yikunb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.18422","authors":[{"_id":"699bbf5bf723198bd53a6309","name":"Linxi Xie","hidden":false},{"_id":"699bbf5bf723198bd53a630a","name":"Lisong C. Sun","hidden":false},{"_id":"699bbf5bf723198bd53a630b","name":"Ashley Neall","hidden":false},{"_id":"699bbf5bf723198bd53a630c","name":"Tong Wu","hidden":false},{"_id":"699bbf5bf723198bd53a630d","name":"Shengqu Cai","hidden":false},{"_id":"699bbf5bf723198bd53a630e","name":"Gordon Wetzstein","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/6_AD51fcx9Fvk7_TGemIO.mp4"],"publishedAt":"2026-02-20T18:45:29.000Z","submittedOnDailyAt":"2026-02-23T00:17:53.289Z","title":"Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.","upvotes":17,"discussionId":"699bbf5cf723198bd53a630f","projectPage":"https://codeysun.github.io/generated-reality/","ai_summary":"A human-centric video world model conditioned on tracked head and hand poses is introduced, enabling dexterous interactions through a bidirectional video diffusion model trained for egocentric virtual environment generation.","ai_keywords":["video world models","diffusion transformer","3D head pose","joint-level hand poses","dexterous hand-object interactions","bidirectional video diffusion model","causal interactive system","egocentric virtual environments"]},"publishedAt":"2026-02-20T13:45:29.000Z","title":"Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control","summary":"Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/6_AD51fcx9Fvk7_TGemIO.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18422.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":237,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.18292","authors":[{"_id":"699c2a43f723198bd53a63cd","name":"Xiaotong Ji","hidden":false},{"_id":"699c2a43f723198bd53a63ce","name":"Rasul Tutunov","hidden":false},{"_id":"699c2a43f723198bd53a63cf","name":"Matthieu Zimmer","hidden":false},{"_id":"699c2a43f723198bd53a63d0","name":"Haitham Bou-Ammar","hidden":false}],"publishedAt":"2026-02-20T15:38:16.000Z","submittedOnDailyAt":"2026-02-23T11:55:01.456Z","title":"Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers","submittedOnDailyBy":{"_id":"631c375768f7da9ad2496bf6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg","isPro":true,"fullname":"Haitham Bou Ammar","user":"hba123","type":"user"},"summary":"Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.","upvotes":8,"discussionId":"699c2a44f723198bd53a63d1","ai_summary":"Decoding is reinterpreted as a principled optimization layer that balances model scores with structural preferences, recovering existing methods as special cases and enabling the creation of new decoders like Best-of-K that improve accuracy in mathematical reasoning tasks.","ai_keywords":["decoding","probability simplex","regularised problem","structural preferences","constraints","greedy decoding","Softmax sampling","Top-K","Top-P","Sparsemax","optimality conditions","Best-of-K","KL-anchored coverage objective","self-consistency","reranking","verifier selection","Qwen2.5-Math-7B","MATH500"]},"publishedAt":"2026-02-20T10:38:16.000Z","title":"Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers","summary":"Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18292.png","numComments":3,"submittedBy":{"_id":"631c375768f7da9ad2496bf6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg","fullname":"Haitham Bou Ammar","name":"hba123","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":95,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.15727","authors":[{"_id":"69959a83ed493589ceb5be2a","name":"Hila Manor","hidden":false},{"_id":"69959a83ed493589ceb5be2b","name":"Rinon Gal","hidden":false},{"_id":"69959a83ed493589ceb5be2c","name":"Haggai Maron","hidden":false},{"_id":"69959a83ed493589ceb5be2d","name":"Tomer Michaeli","hidden":false},{"_id":"69959a83ed493589ceb5be2e","name":"Gal Chechik","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63f6454a9cbd673030295df4/kTAls_z16eaGV_j6MEaFC.jpeg"],"publishedAt":"2026-02-17T17:02:38.000Z","submittedOnDailyAt":"2026-02-23T14:04:14.517Z","title":"Spanning the Visual Analogy Space with a Weight Basis of LoRAs","submittedOnDailyBy":{"_id":"63f6454a9cbd673030295df4","avatarUrl":"/avatars/90c0e4a1c5dffdabbf2492853741d0d9.svg","isPro":false,"fullname":"Hila Manor","user":"hilamanor","type":"user"},"summary":"Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet {a, a', b}, the goal is to generate b' such that a : a' :: b : b'. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"space of LoRAs\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb","upvotes":8,"discussionId":"69959a83ed493589ceb5be2f","projectPage":"https://research.nvidia.com/labs/par/lorweb/","githubRepo":"https://github.com/NVlabs/LoRWeB","githubRepoAddedBy":"user","ai_summary":"Visual analogy learning via dynamic composition of learned LoRA transformation primitives enables flexible image manipulation with improved generalization over fixed adaptation modules.","ai_keywords":["Low-Rank Adaptation","LoRA","visual analogy learning","dynamic composition","transformation primitives","semantic spaces","parameter-efficient fine-tuning"],"githubStars":7,"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-17T12:02:38.000Z","title":"Spanning the Visual Analogy Space with a Weight Basis of LoRAs","summary":"Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet {a, a', b}, the goal is to generate b' such that a : a' :: b : b'. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a \"space of LoRAs\". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63f6454a9cbd673030295df4/kTAls_z16eaGV_j6MEaFC.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15727.png","numComments":1,"submittedBy":{"_id":"63f6454a9cbd673030295df4","avatarUrl":"/avatars/90c0e4a1c5dffdabbf2492853741d0d9.svg","fullname":"Hila Manor","name":"hilamanor","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":16,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.18071","authors":[{"_id":"699bbee4f723198bd53a6300","name":"Boyuan An","hidden":false},{"_id":"699bbee4f723198bd53a6301","name":"Zhexiong Wang","hidden":false},{"_id":"699bbee4f723198bd53a6302","name":"Yipeng Wang","hidden":false},{"_id":"699bbee4f723198bd53a6303","name":"Jiaqi Li","hidden":false},{"_id":"699bbee4f723198bd53a6304","name":"Sihang Li","hidden":false},{"_id":"699bbee4f723198bd53a6305","name":"Jing Zhang","hidden":false},{"_id":"699bbee4f723198bd53a6306","name":"Chen Feng","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/JZMXdQ95tWmAWkwvLCHIa.mp4"],"publishedAt":"2026-02-20T08:54:20.000Z","submittedOnDailyAt":"2026-02-23T00:14:04.117Z","title":"EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.","upvotes":5,"discussionId":"699bbee4f723198bd53a6307","projectPage":"https://ai4ce.github.io/EgoPush/","ai_summary":"EgoPush enables robot manipulation in cluttered environments through perception-driven policy learning that uses object-centric latent spaces and stage-decomposed rewards for long-horizon tasks.","ai_keywords":["policy learning","egocentric perception","non-prehensile rearrangement","object-centric latent space","reinforcement-learning","visual student policy","active perception","stage-local completion rewards","zero-shot sim-to-real transfer"]},"publishedAt":"2026-02-20T03:54:20.000Z","title":"EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots","summary":"Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/JZMXdQ95tWmAWkwvLCHIa.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18071.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":237,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.18432","authors":[{"_id":"699bc2bff723198bd53a6330","name":"Evonne Ng","hidden":false},{"_id":"699bc2bff723198bd53a6331","name":"Siwei Zhang","hidden":false},{"_id":"699bc2bff723198bd53a6332","name":"Zhang Chen","hidden":false},{"_id":"699bc2bff723198bd53a6333","name":"Michael Zollhoefer","hidden":false},{"_id":"699bc2bff723198bd53a6334","name":"Alexander Richard","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/SkRj7SyX6FxMHZgHAysQQ.mp4"],"publishedAt":"2026-02-20T18:59:35.000Z","submittedOnDailyAt":"2026-02-23T00:30:33.895Z","title":"SARAH: Spatially Aware Real-time Agentic Humans","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.","upvotes":4,"discussionId":"699bc2bff723198bd53a6335","projectPage":"https://evonneng.github.io/sarah/","ai_summary":"A causal transformer-based variational autoencoder combined with flow matching enables real-time, spatially-aware conversational motion for embodied agents in virtual reality applications.","ai_keywords":["causal transformer","variational autoencoder","flow matching model","interleaved latent tokens","classifier-free guidance","spatial awareness","conversational motion","real-time inference","embodied agents","streaming VR"]},"publishedAt":"2026-02-20T13:59:35.000Z","title":"SARAH: Spatially Aware Real-time Agentic Humans","summary":"As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/SkRj7SyX6FxMHZgHAysQQ.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18432.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":237,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.17807","authors":[{"_id":"699c50c4cf1450b05134decd","name":"Narges Norouzi","hidden":false},{"_id":"699c50c4cf1450b05134dece","name":"Idil Esen Zulfikar","hidden":false},{"_id":"699c50c4cf1450b05134decf","name":"Niccol`o Cavagnero","hidden":false},{"_id":"699c50c4cf1450b05134ded0","name":"Tommie Kerssies","hidden":false},{"_id":"699c50c4cf1450b05134ded1","name":"Bastian Leibe","hidden":false},{"_id":"699c50c4cf1450b05134ded2","name":"Gijs Dubbelman","hidden":false},{"_id":"699c50c4cf1450b05134ded3","name":"Daan de Geus","hidden":false}],"publishedAt":"2026-02-19T20:14:14.000Z","submittedOnDailyAt":"2026-02-23T10:40:53.091Z","title":"VidEoMT: Your ViT is Secretly Also a Video Segmentation Model","submittedOnDailyBy":{"_id":"5f1158120c833276f61f1a84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg","isPro":false,"fullname":"Niels Rogge","user":"nielsr","type":"user"},"summary":"Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/","upvotes":2,"discussionId":"699c50c4cf1450b05134ded4","projectPage":"https://www.tue-mps.org/videomt/","githubRepo":"https://github.com/tue-mps/videomt","githubRepoAddedBy":"user","ai_summary":"A video segmentation model eliminates specialized tracking modules by using a Vision Transformer encoder with query propagation and fusion mechanisms for efficient, high-speed processing.","ai_keywords":["Vision Transformer","encoder-only","video segmentation","query propagation","query fusion","mask transformer","temporal modeling","frame-by-frame processing"],"githubStars":13,"organization":{"_id":"67e3f5cfc7315216c2dcb732","name":"tue-mps","fullname":"Mobile Perception Systems Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6368212544e19ccad212bbf2/z5ELScvtYofqDe4y1nyKg.png"}},"publishedAt":"2026-02-19T15:14:14.000Z","title":"VidEoMT: Your ViT is Secretly Also a Video Segmentation Model","summary":"Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17807.png","numComments":1,"submittedBy":{"_id":"5f1158120c833276f61f1a84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg","fullname":"Niels Rogge","name":"nielsr","type":"user","isPro":false,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":1100,"isUserFollowing":false},"organization":{"_id":"67e3f5cfc7315216c2dcb732","name":"tue-mps","fullname":"Mobile Perception Systems Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6368212544e19ccad212bbf2/z5ELScvtYofqDe4y1nyKg.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16742","authors":[{"_id":"69987ba8f723198bd53a5fbc","name":"Haoxiang Sun","hidden":false},{"_id":"69987ba8f723198bd53a5fbd","name":"Lizhen Xu","hidden":false},{"_id":"69987ba8f723198bd53a5fbe","name":"Bing Zhao","hidden":false},{"_id":"69987ba8f723198bd53a5fbf","name":"Wotao Yin","hidden":false},{"_id":"69987ba8f723198bd53a5fc0","name":"Wei Wang","hidden":false},{"_id":"69987ba8f723198bd53a5fc1","name":"Boyu Yang","hidden":false},{"_id":"69987ba8f723198bd53a5fc2","name":"Rui Wang","hidden":false},{"_id":"69987ba8f723198bd53a5fc3","name":"Hu Wei","hidden":false}],"publishedAt":"2026-02-18T01:51:21.000Z","submittedOnDailyAt":"2026-02-23T06:57:40.317Z","title":"DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning","submittedOnDailyBy":{"_id":"68d29966b368904f7508f616","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d29966b368904f7508f616/SVSjwXynfvKxJEp2H6HV9.png","isPro":false,"fullname":"SKYLENAGE","user":"skylenage","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce DeepVision-103K, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: https://huggingface.co/datasets/skylenage/DeepVision-103K{this url}.","upvotes":2,"discussionId":"69987ba8f723198bd53a5fc4","githubRepo":"https://github.com/SKYLENAGE-AI/DeepVision-103K","githubRepoAddedBy":"user","ai_summary":"DeepVision-103K dataset enhances multimodal reasoning capabilities of large models through diverse mathematical content and visual elements.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","Large Multimodal Models","multimodal mathematical benchmarks","multimodal reasoning tasks","visual perception","reflection","reasoning capabilities"],"githubStars":4},"publishedAt":"2026-02-17T20:51:21.000Z","title":"DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce DeepVision-103K, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: https://huggingface.co/datasets/skylenage/DeepVision-103K{this url}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16742.png","numComments":1,"submittedBy":{"_id":"68d29966b368904f7508f616","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d29966b368904f7508f616/SVSjwXynfvKxJEp2H6HV9.png","fullname":"SKYLENAGE","name":"skylenage","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.15814","authors":[{"_id":"69956b858d17d1ee8c10ed21","user":{"_id":"683c849152d744770fe82ade","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/683c849152d744770fe82ade/C_TMUIF5PJ1xkxwKxpBFl.jpeg","isPro":false,"fullname":"Rain","user":"rainnekoneko","type":"user"},"name":"Devang Acharya","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:09.096Z","hidden":false},{"_id":"69956b858d17d1ee8c10ed22","name":"Mohammad Hammoud","hidden":false}],"publishedAt":"2026-02-17T18:50:40.000Z","submittedOnDailyAt":"2026-02-23T17:34:13.641Z","title":"Avey-B","submittedOnDailyBy":{"_id":"683c849152d744770fe82ade","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/683c849152d744770fe82ade/C_TMUIF5PJ1xkxwKxpBFl.jpeg","isPro":false,"fullname":"Rain","user":"rainnekoneko","type":"user"},"summary":"Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.","upvotes":2,"discussionId":"69956b858d17d1ee8c10ed23","githubRepo":"https://github.com/rimads/avey-b","githubRepoAddedBy":"user","ai_summary":"Compact pretrained bidirectional encoders based on Avey architecture outperform Transformer-based models on token classification and information retrieval tasks while scaling more efficiently to long contexts.","ai_keywords":["bidirectional encoders","self-attention","BERT-style architectures","autoregressive models","attention-free alternative","encoder-only adaptation","decoupled parameterization","stability-oriented normalization","neural compression","Transformer-based encoders","token-classification","information-retrieval"],"githubStars":4,"organization":{"_id":"680f9d9c87f2627834575855","name":"avey-ai","fullname":"Avey","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680f9d01603c22ddabec14d3/MsK0RCMUYXsg6j-t9ka-w.png"}},"publishedAt":"2026-02-17T13:50:40.000Z","title":"Avey-B","summary":"Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15814.png","numComments":4,"submittedBy":{"_id":"683c849152d744770fe82ade","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/683c849152d744770fe82ade/C_TMUIF5PJ1xkxwKxpBFl.jpeg","fullname":"Rain","name":"rainnekoneko","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"680f9d9c87f2627834575855","name":"avey-ai","fullname":"Avey","avatar":"https://cdn-uploads.huggingface.co/production/uploads/680f9d01603c22ddabec14d3/MsK0RCMUYXsg6j-t9ka-w.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.18312","authors":[{"_id":"699bbec7f723198bd53a62fb","name":"Zhaoming Xie","hidden":false},{"_id":"699bbec7f723198bd53a62fc","name":"Kevin Karol","hidden":false},{"_id":"699bbec7f723198bd53a62fd","name":"Jessica Hodgins","hidden":false}],"publishedAt":"2026-02-20T16:11:19.000Z","submittedOnDailyAt":"2026-02-23T00:13:26.091Z","title":"Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.","upvotes":1,"discussionId":"699bbec7f723198bd53a62fe","ai_summary":"Reinforcement learning policies are improved by using action Jacobian penalty to eliminate unrealistic high-frequency signals, with a new Linear Policy Net architecture reducing computational overhead while enabling faster convergence and efficient inference for motion imitation tasks.","ai_keywords":["reinforcement learning","control policies","action Jacobian penalty","auto differentiation","fully connected neural network","Linear Policy Net","motion imitation","quadrupedal robot"]},"publishedAt":"2026-02-20T11:11:19.000Z","title":"Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty","summary":"Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18312.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":237,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.17664","authors":[{"_id":"6998aaf4f723198bd53a600f","user":{"_id":"65abc7742f560c70ff5d534c","avatarUrl":"/avatars/e4caa809a720645fd36790270b706ae1.svg","isPro":false,"fullname":"Aidar Myrzakhan","user":"aidar-myrzakhan","type":"user"},"name":"Aidar Myrzakhan","status":"claimed_verified","statusLastChangedAt":"2026-02-23T09:44:00.597Z","hidden":false},{"_id":"6998aaf4f723198bd53a6010","name":"Tianyi Li","hidden":false},{"_id":"6998aaf4f723198bd53a6011","name":"Bowei Guo","hidden":false},{"_id":"6998aaf4f723198bd53a6012","name":"Shengkun Tang","hidden":false},{"_id":"6998aaf4f723198bd53a6013","name":"Zhiqiang Shen","hidden":false}],"publishedAt":"2026-02-19T18:59:50.000Z","submittedOnDailyAt":"2026-02-23T10:56:49.782Z","title":"Sink-Aware Pruning for Diffusion Language Models","submittedOnDailyBy":{"_id":"65abc7742f560c70ff5d534c","avatarUrl":"/avatars/e4caa809a720645fd36790270b706ae1.svg","isPro":false,"fullname":"Aidar Myrzakhan","user":"aidar-myrzakhan","type":"user"},"summary":"Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose {bf Sink-Aware Pruning}, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.","upvotes":1,"discussionId":"6998aaf5f723198bd53a6014","githubRepo":"https://github.com/VILA-Lab/Sink-Aware-Pruning","githubRepoAddedBy":"user","ai_summary":"Diffusion Language Models suffer from high inference costs due to iterative denoising, prompting the development of Sink-Aware Pruning that identifies and removes unstable attention sinks, improving efficiency without retraining.","ai_keywords":["Diffusion Language Models","attention sink tokens","iterative denoising","pruning","autoregressive LLMs","attention-sink position","variance","unstable sinks","quality-efficiency trade-off"],"githubStars":5,"organization":{"_id":"61fb9e24dc607a42af5f193f","name":"MBZUAI","fullname":"Mohamed Bin Zayed University of Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}},"publishedAt":"2026-02-19T13:59:50.000Z","title":"Sink-Aware Pruning for Diffusion Language Models","summary":"Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose {bf Sink-Aware Pruning}, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17664.png","numComments":1,"submittedBy":{"_id":"65abc7742f560c70ff5d534c","avatarUrl":"/avatars/e4caa809a720645fd36790270b706ae1.svg","fullname":"Aidar Myrzakhan","name":"aidar-myrzakhan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"61fb9e24dc607a42af5f193f","name":"MBZUAI","fullname":"Mohamed Bin Zayed University of Artificial Intelligence","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.17186","authors":[{"_id":"699c0e1af723198bd53a637d","name":"Seulbi Lee","hidden":false},{"_id":"699c0e1af723198bd53a637e","name":"Sangheum Hwang","hidden":false}],"publishedAt":"2026-02-19T09:12:21.000Z","submittedOnDailyAt":"2026-02-23T05:58:20.081Z","title":"Selective Training for Large Vision Language Models via Visual Information Gain","submittedOnDailyBy":{"_id":"64e6ff6b09375662071ae8c8","avatarUrl":"/avatars/4e1086476a29733eb4680eb8771d2f68.svg","isPro":false,"fullname":"Sangheum Hwang","user":"beopst","type":"user"},"summary":"Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.","upvotes":1,"discussionId":"699c0e1af723198bd53a637f","ai_summary":"Visual Information Gain metric quantifies the contribution of visual input to prediction uncertainty, enabling selective training that improves visual grounding and reduces language bias in vision-language models.","ai_keywords":["Visual Information Gain","perplexity-based metric","visual grounding","language bias","selective training","vision-language models"],"organization":{"_id":"6495df76b8d4efc75beef704","name":"SeoulTech","fullname":"Seoul National University of Science and Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6642f47a06066f6ad3d8ab5a/vLWBsnubTjPFjg9OmDjnW.png"}},"publishedAt":"2026-02-19T04:12:21.000Z","title":"Selective Training for Large Vision Language Models via Visual Information Gain","summary":"Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17186.png","numComments":1,"submittedBy":{"_id":"64e6ff6b09375662071ae8c8","avatarUrl":"/avatars/4e1086476a29733eb4680eb8771d2f68.svg","fullname":"Sangheum Hwang","name":"beopst","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6495df76b8d4efc75beef704","name":"SeoulTech","fullname":"Seoul National University of Science and Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6642f47a06066f6ad3d8ab5a/vLWBsnubTjPFjg9OmDjnW.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.17080","authors":[{"_id":"6999f91ef723198bd53a613d","user":{"_id":"6999f733a16cc5bd03688716","avatarUrl":"/avatars/2795d338751ebee5b7e51b80fab71969.svg","isPro":false,"fullname":"Minxin Zhang","user":"minxin-zhang","type":"user"},"name":"Minxin Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-23T09:43:46.436Z","hidden":false},{"_id":"6999f91ef723198bd53a613e","name":"Yuxuan Liu","hidden":false},{"_id":"6999f91ef723198bd53a613f","name":"Hayden Scheaffer","hidden":false}],"publishedAt":"2026-02-19T05:00:39.000Z","submittedOnDailyAt":"2026-02-23T15:45:49.711Z","title":"Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum","submittedOnDailyBy":{"_id":"6999f733a16cc5bd03688716","avatarUrl":"/avatars/2795d338751ebee5b7e51b80fab71969.svg","isPro":false,"fullname":"Minxin Zhang","user":"minxin-zhang","type":"user"},"summary":"Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.","upvotes":1,"discussionId":"6999f91ff723198bd53a6140","ai_summary":"A new class of optimizers combines orthogonalized momentum with norm-based noise adaptation, achieving improved convergence rates and training performance for large language models.","ai_keywords":["stochastic optimization","Adam","Muon","orthogonalized momentum","norm-based Adam-type noise adaptation","convergence rates","stochastic gradients","GPT-2","pretraining"],"organization":{"_id":"67784c39dac147922d8d09f0","name":"UCLA","fullname":"University of California, Los Angeles","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"}},"publishedAt":"2026-02-19T00:00:39.000Z","title":"Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum","summary":"Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17080.png","numComments":1,"submittedBy":{"_id":"6999f733a16cc5bd03688716","avatarUrl":"/avatars/2795d338751ebee5b7e51b80fab71969.svg","fullname":"Minxin Zhang","name":"minxin-zhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67784c39dac147922d8d09f0","name":"UCLA","fullname":"University of California, Los Angeles","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.17022","authors":[{"_id":"699cac574e37ec6dfa1bc415","name":"Takyoung Kim","hidden":false},{"_id":"699cac574e37ec6dfa1bc416","name":"Jinseok Nam","hidden":false},{"_id":"699cac574e37ec6dfa1bc417","name":"Chandrayee Basu","hidden":false},{"_id":"699cac574e37ec6dfa1bc418","name":"Xing Fan","hidden":false},{"_id":"699cac574e37ec6dfa1bc419","name":"Chengyuan Ma","hidden":false},{"_id":"699cac574e37ec6dfa1bc41a","name":"Heng Ji","hidden":false},{"_id":"699cac574e37ec6dfa1bc41b","name":"Gokhan Tur","hidden":false},{"_id":"699cac574e37ec6dfa1bc41c","name":"Dilek Hakkani-Tr","hidden":false}],"publishedAt":"2026-02-19T02:37:29.000Z","submittedOnDailyAt":"2026-02-23T17:08:26.855Z","title":"ReIn: Conversational Error Recovery with Reasoning Inception","submittedOnDailyBy":{"_id":"63f4ea423aa49d8cb976d23b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63f4ea423aa49d8cb976d23b/qLzfRlYA-kcwehJIXs4Ru.png","isPro":false,"fullname":"Takyoung Kim","user":"takyoung","type":"user"},"summary":"Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.","upvotes":1,"discussionId":"699cac584e37ec6dfa1bc41d","ai_summary":"Conversational agents with tool integration face challenges from user-induced errors, but a test-time intervention method called Reasoning Inception (ReIn) enables error recovery by injecting external reasoning into the agent's decision-making process without modifying model parameters or prompts.","ai_keywords":["large language models","tool integration","dialogue context","error recovery","test-time intervention","Reasoning Inception","inception module","recovery plans","conversational agents","task success"],"organization":{"_id":"60212a089f64108326fac7c2","name":"illinois","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"}},"publishedAt":"2026-02-18T21:37:29.000Z","title":"ReIn: Conversational Error Recovery with Reasoning Inception","summary":"Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17022.png","numComments":1,"submittedBy":{"_id":"63f4ea423aa49d8cb976d23b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63f4ea423aa49d8cb976d23b/qLzfRlYA-kcwehJIXs4Ru.png","fullname":"Takyoung Kim","name":"takyoung","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"60212a089f64108326fac7c2","name":"illinois","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1612786274096-6021121cfb1b47827d667074.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14279","authors":[{"_id":"6997852c7a658569d5a10102","name":"Ruomeng Ding","hidden":false},{"_id":"6997852c7a658569d5a10103","name":"Tianwei Gao","hidden":false},{"_id":"6997852c7a658569d5a10104","name":"Thomas P. Zollo","hidden":false},{"_id":"6997852c7a658569d5a10105","name":"Eitan Bachmat","hidden":false},{"_id":"6997852c7a658569d5a10106","name":"Richard Zemel","hidden":false},{"_id":"6997852c7a658569d5a10107","name":"Zhun Deng","hidden":false}],"publishedAt":"2026-02-15T19:05:34.000Z","submittedOnDailyAt":"2026-02-23T18:30:24.551Z","title":"Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions","submittedOnDailyBy":{"_id":"654875f69295970f87ba1600","avatarUrl":"/avatars/4c387aa53053f88444e7d8fffd6161c1.svg","isPro":false,"fullname":"Ruomeng Ding","user":"Czardas","type":"user"},"summary":"Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.","upvotes":1,"discussionId":"6997852c7a658569d5a10108","githubRepo":"https://github.com/ZDCSlab/Group-Adaptive-Elicitation","githubRepoAddedBy":"user","ai_summary":"Adaptive group elicitation framework combines LLM-based information gain scoring with graph neural networks to improve population-level predictions under budget constraints.","ai_keywords":["large language models","expected information gain","heterogeneous graph neural network","response imputation","respondent selection","population-level inference","adaptive questioning","multi-round setting","structured similarity"],"githubStars":1,"organization":{"_id":"69975b0f28c030b6655b8d20","name":"ZDCSlab","fullname":"ZD Lab @ UNC-Chapel Hill"}},"publishedAt":"2026-02-15T14:05:34.000Z","title":"Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions","summary":"Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14279.png","numComments":1,"submittedBy":{"_id":"654875f69295970f87ba1600","avatarUrl":"/avatars/4c387aa53053f88444e7d8fffd6161c1.svg","fullname":"Ruomeng Ding","name":"Czardas","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"69975b0f28c030b6655b8d20","name":"ZDCSlab","fullname":"ZD Lab @ UNC-Chapel Hill"},"isAuthorParticipating":false},{"paper":{"id":"2602.13576","authors":[{"_id":"69976a4c7a658569d5a100d0","name":"Ruomeng Ding","hidden":false},{"_id":"69976a4c7a658569d5a100d1","name":"Yifei Pang","hidden":false},{"_id":"69976a4c7a658569d5a100d2","name":"He Sun","hidden":false},{"_id":"69976a4c7a658569d5a100d3","name":"Yizhong Wang","hidden":false},{"_id":"69976a4c7a658569d5a100d4","name":"Zhiwei Steven Wu","hidden":false},{"_id":"69976a4c7a658569d5a100d5","name":"Zhun Deng","hidden":false}],"publishedAt":"2026-02-14T03:19:14.000Z","submittedOnDailyAt":"2026-02-23T18:24:55.565Z","title":"Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges","submittedOnDailyBy":{"_id":"654875f69295970f87ba1600","avatarUrl":"/avatars/4c387aa53053f88444e7d8fffd6161c1.svg","isPro":false,"fullname":"Ruomeng Ding","user":"Czardas","type":"user"},"summary":"Evaluation and alignment pipelines for large language models increasingly rely on LLM-based judges, whose behavior is guided by natural-language rubrics and validated on benchmarks. We identify a previously under-recognized vulnerability in this workflow, which we term Rubric-Induced Preference Drift (RIPD). Even when rubric edits pass benchmark validation, they can still produce systematic and directional shifts in a judge's preferences on target domains. Because rubrics serve as a high-level decision interface, such drift can emerge from seemingly natural, criterion-preserving edits and remain difficult to detect through aggregate benchmark metrics or limited spot-checking. We further show this vulnerability can be exploited through rubric-based preference attacks, in which benchmark-compliant rubric edits steer judgments away from a fixed human or trusted reference on target domains, systematically inducing RIPD and reducing target-domain accuracy up to 9.5% (helpfulness) and 27.9% (harmlessness). When these judgments are used to generate preference labels for downstream post-training, the induced bias propagates through alignment pipelines and becomes internalized in trained policies. This leads to persistent and systematic drift in model behavior. Overall, our findings highlight evaluation rubrics as a sensitive and manipulable control interface, revealing a system-level alignment risk that extends beyond evaluator reliability alone. The code is available at: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Warning: Certain sections may contain potentially harmful content that may not be appropriate for all readers.","upvotes":1,"discussionId":"69976a4c7a658569d5a100d6","githubRepo":"https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface","githubRepoAddedBy":"user","ai_summary":"LLM-based judges using natural-language rubrics for evaluation can exhibit systematic preference drift from minor rubric modifications, which can be exploited to manipulate alignment pipelines and degrade model performance.","ai_keywords":["LLM-based judges","natural-language rubrics","benchmark validation","Rubric-Induced Preference Drift","preference attacks","alignment pipelines","post-training","model behavior"],"githubStars":2,"organization":{"_id":"69975b0f28c030b6655b8d20","name":"ZDCSlab","fullname":"ZD Lab @ UNC-Chapel Hill"}},"publishedAt":"2026-02-13T22:19:14.000Z","title":"Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges","summary":"Evaluation and alignment pipelines for large language models increasingly rely on LLM-based judges, whose behavior is guided by natural-language rubrics and validated on benchmarks. We identify a previously under-recognized vulnerability in this workflow, which we term Rubric-Induced Preference Drift (RIPD). Even when rubric edits pass benchmark validation, they can still produce systematic and directional shifts in a judge's preferences on target domains. Because rubrics serve as a high-level decision interface, such drift can emerge from seemingly natural, criterion-preserving edits and remain difficult to detect through aggregate benchmark metrics or limited spot-checking. We further show this vulnerability can be exploited through rubric-based preference attacks, in which benchmark-compliant rubric edits steer judgments away from a fixed human or trusted reference on target domains, systematically inducing RIPD and reducing target-domain accuracy up to 9.5% (helpfulness) and 27.9% (harmlessness). When these judgments are used to generate preference labels for downstream post-training, the induced bias propagates through alignment pipelines and becomes internalized in trained policies. This leads to persistent and systematic drift in model behavior. Overall, our findings highlight evaluation rubrics as a sensitive and manipulable control interface, revealing a system-level alignment risk that extends beyond evaluator reliability alone. The code is available at: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Warning: Certain sections may contain potentially harmful content that may not be appropriate for all readers.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13576.png","numComments":1,"submittedBy":{"_id":"654875f69295970f87ba1600","avatarUrl":"/avatars/4c387aa53053f88444e7d8fffd6161c1.svg","fullname":"Ruomeng Ding","name":"Czardas","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"69975b0f28c030b6655b8d20","name":"ZDCSlab","fullname":"ZD Lab @ UNC-Chapel Hill"},"isAuthorParticipating":false},{"paper":{"id":"2602.10094","authors":[{"_id":"699c6c42cf1450b05134df18","name":"Yihang Luo","hidden":false},{"_id":"699c6c42cf1450b05134df19","name":"Shangchen Zhou","hidden":false},{"_id":"699c6c42cf1450b05134df1a","name":"Yushi Lan","hidden":false},{"_id":"699c6c42cf1450b05134df1b","name":"Xingang Pan","hidden":false},{"_id":"699c6c42cf1450b05134df1c","name":"Chen Change Loy","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/654b607b9296401f072e7cf5/F7egYj8F-cQk_vfpd6-8R.mp4"],"publishedAt":"2026-02-10T18:57:04.000Z","submittedOnDailyAt":"2026-02-23T12:43:02.581Z","title":"4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere","submittedOnDailyBy":{"_id":"654b607b9296401f072e7cf5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654b607b9296401f072e7cf5/5ys396eFk9x_vKt5Aot8G.jpeg","isPro":false,"fullname":"Yihang Luo","user":"Luo-Yihang","type":"user"},"summary":"We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.","upvotes":1,"discussionId":"699c6c43cf1450b05134df1d","projectPage":"https://yihangluo.com/projects/4RC/","ai_summary":"4RC presents a unified feed-forward framework for 4D reconstruction from monocular videos that learns holistic scene geometry and motion dynamics through a transformer-based encoder-decoder architecture with conditional querying capabilities.","ai_keywords":["4D reconstruction","monocular videos","feed-forward framework","spatio-temporal latent space","transformer backbone","conditional decoder","3D geometry","motion dynamics","encode-once query-anywhere paradigm"]},"publishedAt":"2026-02-10T13:57:04.000Z","title":"4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere","summary":"We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/654b607b9296401f072e7cf5/F7egYj8F-cQk_vfpd6-8R.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10094.png","numComments":2,"submittedBy":{"_id":"654b607b9296401f072e7cf5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654b607b9296401f072e7cf5/5ys396eFk9x_vKt5Aot8G.jpeg","fullname":"Yihang Luo","name":"Luo-Yihang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false}]
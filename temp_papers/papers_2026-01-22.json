[{"paper":{"id":"2601.12538","authors":[{"_id":"6971913fc1c7409747bf9564","name":"Tianxin Wei","hidden":false},{"_id":"6971913fc1c7409747bf9565","user":{"_id":"6742eb40924e80c3c80ebe13","avatarUrl":"/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg","isPro":false,"fullname":"Ting-Wei Li","user":"tingwl0122","type":"user"},"name":"Ting-Wei Li","status":"claimed_verified","statusLastChangedAt":"2026-01-22T17:12:21.531Z","hidden":false},{"_id":"6971913fc1c7409747bf9566","name":"Zhining Liu","hidden":false},{"_id":"6971913fc1c7409747bf9567","name":"Xuying Ning","hidden":false},{"_id":"6971913fc1c7409747bf9568","name":"Ze Yang","hidden":false},{"_id":"6971913fc1c7409747bf9569","name":"Jiaru Zou","hidden":false},{"_id":"6971913fc1c7409747bf956a","name":"Zhichen Zeng","hidden":false},{"_id":"6971913fc1c7409747bf956b","name":"Ruizhong Qiu","hidden":false},{"_id":"6971913fc1c7409747bf956c","name":"Xiao Lin","hidden":false},{"_id":"6971913fc1c7409747bf956d","name":"Dongqi Fu","hidden":false},{"_id":"6971913fc1c7409747bf956e","name":"Zihao Li","hidden":false},{"_id":"6971913fc1c7409747bf956f","user":{"_id":"653962e75c8e4863e1a2068f","avatarUrl":"/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg","isPro":false,"fullname":"Mengting Ai","user":"famous-blue-raincoat","type":"user"},"name":"Mengting Ai","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:45:10.378Z","hidden":false},{"_id":"6971913fc1c7409747bf9570","user":{"_id":"677830bd3f2e3ec475576303","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png","isPro":false,"fullname":"Duo Zhou","user":"Claudius7","type":"user"},"name":"Duo Zhou","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:45:12.476Z","hidden":false},{"_id":"6971913fc1c7409747bf9571","name":"Wenxuan Bao","hidden":false},{"_id":"6971913fc1c7409747bf9572","user":{"_id":"646323556c27a7e33b23f198","avatarUrl":"/avatars/17fe142f689ab4be3c2374d1d90393db.svg","isPro":false,"fullname":"Yunzhe Li","user":"yunzhel2","type":"user"},"name":"Yunzhe Li","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:45:14.383Z","hidden":false},{"_id":"6971913fc1c7409747bf9573","name":"Gaotang Li","hidden":false},{"_id":"6971913fc1c7409747bf9574","name":"Cheng Qian","hidden":false},{"_id":"6971913fc1c7409747bf9575","name":"Yu Wang","hidden":false},{"_id":"6971913fc1c7409747bf9576","name":"Xiangru Tang","hidden":false},{"_id":"6971913fc1c7409747bf9577","name":"Yin Xiao","hidden":false},{"_id":"6971913fc1c7409747bf9578","name":"Liri Fang","hidden":false},{"_id":"6971913fc1c7409747bf9579","name":"Hui Liu","hidden":false},{"_id":"6971913fc1c7409747bf957a","name":"Xianfeng Tang","hidden":false},{"_id":"6971913fc1c7409747bf957b","name":"Yuji Zhang","hidden":false},{"_id":"6971913fc1c7409747bf957c","name":"Chi Wang","hidden":false},{"_id":"6971913fc1c7409747bf957d","name":"Jiaxuan You","hidden":false},{"_id":"6971913fc1c7409747bf957e","name":"Heng Ji","hidden":false},{"_id":"6971913fc1c7409747bf957f","name":"Hanghang Tong","hidden":false},{"_id":"6971913fc1c7409747bf9580","name":"Jingrui He","hidden":false}],"publishedAt":"2026-01-18T18:58:23.000Z","submittedOnDailyAt":"2026-01-22T00:27:25.162Z","title":"Agentic Reasoning for Large Language Models","submittedOnDailyBy":{"_id":"65c288280aa2d53135734a42","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg","isPro":false,"fullname":"Jiaru Zou","user":"jiaruz2","type":"user"},"summary":"Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.","upvotes":125,"discussionId":"69719140c1c7409747bf9581","githubRepo":"https://github.com/weitianxin/Awesome-Agentic-Reasoning","githubRepoAddedBy":"user","ai_summary":"Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.","ai_keywords":["large language models","agentic reasoning","autonomous agents","planning","tool use","search","feedback","memory","adaptation","collaborative settings","coordination","knowledge sharing","reinforcement learning","supervised fine-tuning","in-context reasoning","post-training reasoning","real-world applications","benchmarks","thought and action","world modeling","scalable multi-agent training","governance"],"githubStars":105,"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}},"publishedAt":"2026-01-18T13:58:23.000Z","title":"Agentic Reasoning for Large Language Models","summary":"Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png","numComments":3,"submittedBy":{"_id":"65c288280aa2d53135734a42","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg","fullname":"Jiaru Zou","name":"jiaruz2","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.12346","authors":[{"_id":"697145b5c1c7409747bf94c7","name":"Peizhou Huang","hidden":false},{"_id":"697145b5c1c7409747bf94c8","name":"Zixuan Zhong","hidden":false},{"_id":"697145b5c1c7409747bf94c9","name":"Zhongwei Wan","hidden":false},{"_id":"697145b5c1c7409747bf94ca","user":{"_id":"67136093d2e50f1e8c9fad52","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png","isPro":false,"fullname":"Donghao Zhou","user":"donghao-zhou","type":"user"},"name":"Donghao Zhou","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:45:36.605Z","hidden":false},{"_id":"697145b5c1c7409747bf94cb","name":"Samiul Alam","hidden":false},{"_id":"697145b5c1c7409747bf94cc","name":"Xin Wang","hidden":false},{"_id":"697145b5c1c7409747bf94cd","name":"Zexin Li","hidden":false},{"_id":"697145b5c1c7409747bf94ce","name":"Zhihao Dou","hidden":false},{"_id":"697145b5c1c7409747bf94cf","name":"Li Zhu","hidden":false},{"_id":"697145b5c1c7409747bf94d0","name":"Jing Xiong","hidden":false},{"_id":"697145b5c1c7409747bf94d1","name":"Chaofan Tao","hidden":false},{"_id":"697145b5c1c7409747bf94d2","name":"Yan Xu","hidden":false},{"_id":"697145b5c1c7409747bf94d3","name":"Dimitrios Dimitriadis","hidden":false},{"_id":"697145b5c1c7409747bf94d4","name":"Tuo Zhang","hidden":false},{"_id":"697145b5c1c7409747bf94d5","name":"Mi Zhang","hidden":false}],"publishedAt":"2026-01-18T10:41:33.000Z","submittedOnDailyAt":"2026-01-22T02:19:13.211Z","title":"MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents","submittedOnDailyBy":{"_id":"67136093d2e50f1e8c9fad52","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png","isPro":false,"fullname":"Donghao Zhou","user":"donghao-zhou","type":"user"},"summary":"Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.","upvotes":41,"discussionId":"697145b5c1c7409747bf94d6","projectPage":"https://mmdeepresearch-bench.github.io/","githubRepo":"https://github.com/AIoT-MLSys-Lab/MMDeepResearch-Bench","githubRepoAddedBy":"user","ai_summary":"MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.","ai_keywords":["multimodal evidence use","citation-grounded report generation","multimodal understanding","deep research agents","Formula-LLM Adaptive Evaluation","Trustworthy Retrieval-Aligned Citation Evaluation","Multimodal Support-Aligned Integrity Check"],"githubStars":10},"publishedAt":"2026-01-18T05:41:33.000Z","title":"MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents","summary":"Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12346.png","numComments":1,"submittedBy":{"_id":"67136093d2e50f1e8c9fad52","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png","fullname":"Donghao Zhou","name":"donghao-zhou","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.15282","authors":[{"_id":"69719a70c1c7409747bf9601","user":{"_id":"68fce03ed1d0efce7ca87075","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png","isPro":false,"fullname":"yfdeng","user":"yfdeng10","type":"user"},"name":"Yufan Deng","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:51.909Z","hidden":false},{"_id":"69719a70c1c7409747bf9602","name":"Zilin Pan","hidden":false},{"_id":"69719a70c1c7409747bf9603","name":"Hongyu Zhang","hidden":false},{"_id":"69719a70c1c7409747bf9604","name":"Xiaojie Li","hidden":false},{"_id":"69719a70c1c7409747bf9605","name":"Ruoqing Hu","hidden":false},{"_id":"69719a70c1c7409747bf9606","user":{"_id":"6661917459720067b2a15bd6","avatarUrl":"/avatars/f1afe7dd1c538d209016eb5740772d8b.svg","isPro":false,"fullname":"dyflional10","user":"dyflional10","type":"user"},"name":"Yufei Ding","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:59.616Z","hidden":true},{"_id":"69719a70c1c7409747bf9607","name":"Yiming Zou","hidden":false},{"_id":"69719a70c1c7409747bf9608","name":"Yan Zeng","hidden":false},{"_id":"69719a70c1c7409747bf9609","name":"Daquan Zhou","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"],"publishedAt":"2026-01-21T18:59:18.000Z","submittedOnDailyAt":"2026-01-22T01:05:21.353Z","title":"Rethinking Video Generation Model for the Embodied World","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.","upvotes":36,"discussionId":"69719a70c1c7409747bf960a","projectPage":"https://dagroup-pku.github.io/ReVidgen.github.io/","githubRepo":"https://github.com/DAGroup-PKU/ReVidgen/","githubRepoAddedBy":"user","ai_summary":"A comprehensive robotics benchmark evaluates video generation models across multiple task domains and robot embodiments, revealing significant gaps in physical realism and introducing a large-scale dataset to address training data limitations.","ai_keywords":["video generation models","embodied intelligence","robotics benchmark","robot-oriented video generation","task domains","physical plausibility","action completeness","Spearman correlation coefficient","RoVid-X","data pipeline","robotic dataset","video models","embodied AI","general intelligence"],"githubStars":7,"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-01-21T13:59:18.000Z","title":"Rethinking Video Generation Model for the Embodied World","summary":"Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15282.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":211,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.14171","authors":[{"_id":"69710b60c1c7409747bf9431","user":{"_id":"6448b2f53e7b3c11be684348","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg","isPro":true,"fullname":"Qianli Ma","user":"Mqleet","type":"user"},"name":"Qianli Ma","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:46:28.336Z","hidden":false},{"_id":"69710b60c1c7409747bf9432","name":"Chang Guo","hidden":false},{"_id":"69710b60c1c7409747bf9433","name":"Zhiheng Tian","hidden":false},{"_id":"69710b60c1c7409747bf9434","name":"Siyu Wang","hidden":false},{"_id":"69710b60c1c7409747bf9435","name":"Jipeng Xiao","hidden":false},{"_id":"69710b60c1c7409747bf9436","name":"Yuanhao Yue","hidden":false},{"_id":"69710b60c1c7409747bf9437","name":"Zhipeng Zhang","hidden":false}],"publishedAt":"2026-01-20T17:23:51.000Z","submittedOnDailyAt":"2026-01-22T01:11:23.304Z","title":"Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance","submittedOnDailyBy":{"_id":"6448b2f53e7b3c11be684348","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg","isPro":true,"fullname":"Qianli Ma","user":"Mqleet","type":"user"},"summary":"Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.","upvotes":35,"discussionId":"69710b60c1c7409747bf9438","projectPage":"https://mqleet.github.io/Paper2Rebuttal_ProjectPage/","githubRepo":"https://github.com/AutoLab-SAI-SJTU/Paper2Rebuttal","githubRepoAddedBy":"user","ai_summary":"RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.","ai_keywords":["multi-agents framework","evidence-centric planning","rebuttal generation","peer review","strategic coherence","faithful generation","external search module"],"githubStars":146,"organization":{"_id":"68ee0edd23dc954f7744ac27","name":"AutoLab-SJTU","fullname":"AutoLab"}},"publishedAt":"2026-01-20T12:23:51.000Z","title":"Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance","summary":"Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14171.png","numComments":1,"submittedBy":{"_id":"6448b2f53e7b3c11be684348","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg","fullname":"Qianli Ma","name":"Mqleet","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"68ee0edd23dc954f7744ac27","name":"AutoLab-SJTU","fullname":"AutoLab"},"isAuthorParticipating":true},{"paper":{"id":"2601.13572","authors":[{"_id":"69722bbec1c7409747bf97ed","user":{"_id":"658071c1da07e791d8a6e9dc","avatarUrl":"/avatars/66348bae88b5945cc54012c52cff6aa2.svg","isPro":false,"fullname":"Xiangchi Yuan","user":"Xiangchi","type":"user"},"name":"Xiangchi Yuan","status":"claimed_verified","statusLastChangedAt":"2026-01-22T17:11:49.186Z","hidden":false},{"_id":"69722bbec1c7409747bf97ee","name":"Dachuan Shi","hidden":false},{"_id":"69722bbec1c7409747bf97ef","name":"Chunhui Zhang","hidden":false},{"_id":"69722bbec1c7409747bf97f0","name":"Zheyuan Liu","hidden":false},{"_id":"69722bbec1c7409747bf97f1","name":"Shenglong Yao","hidden":false},{"_id":"69722bbec1c7409747bf97f2","name":"Soroush Vosoughi","hidden":false},{"_id":"69722bbec1c7409747bf97f3","name":"Wenke Lee","hidden":false}],"publishedAt":"2026-01-20T03:56:53.000Z","submittedOnDailyAt":"2026-01-22T11:55:21.343Z","title":"Behavior Knowledge Merge in Reinforced Agentic Models","submittedOnDailyBy":{"_id":"658071c1da07e791d8a6e9dc","avatarUrl":"/avatars/66348bae88b5945cc54012c52cff6aa2.svg","isPro":false,"fullname":"Xiangchi Yuan","user":"Xiangchi","type":"user"},"summary":"Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.","upvotes":15,"discussionId":"69722bbec1c7409747bf97f4","projectPage":"https://xiangchi-yuan.github.io/ram-project/","githubRepo":"https://github.com/xiangchi-yuan/mrl","githubRepoAddedBy":"user","ai_summary":"Reinforced Agent Merging (RAM) addresses the limitations of traditional merging methods for reinforcement learning-trained agents by distinguishing shared and task-specific parameters to preserve critical behaviors during model integration.","ai_keywords":["reinforcement learning","model merging","agentic models","task vectors","supervised fine-tuning","on-policy RL","global averaging","parameter updates","distribution-aware merging","synergistic potential"],"githubStars":1,"organization":{"_id":"64155eaa95fb6f824b237c3d","name":"GeorgiaTech","fullname":"Georgia Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"}},"publishedAt":"2026-01-19T22:56:53.000Z","title":"Behavior Knowledge Merge in Reinforced Agentic Models","summary":"Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13572.png","numComments":1,"submittedBy":{"_id":"658071c1da07e791d8a6e9dc","avatarUrl":"/avatars/66348bae88b5945cc54012c52cff6aa2.svg","fullname":"Xiangchi Yuan","name":"Xiangchi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"64155eaa95fb6f824b237c3d","name":"GeorgiaTech","fullname":"Georgia Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14750","authors":[{"_id":"697191c6c1c7409747bf9583","name":"Yifan Wang","hidden":false},{"_id":"697191c6c1c7409747bf9584","name":"Shiyu Li","hidden":false},{"_id":"697191c6c1c7409747bf9585","name":"Peiming Li","hidden":false},{"_id":"697191c6c1c7409747bf9586","name":"Xiaochen Yang","hidden":false},{"_id":"697191c6c1c7409747bf9587","name":"Yang Tang","hidden":false},{"_id":"697191c6c1c7409747bf9588","name":"Zheng Wei","hidden":false}],"publishedAt":"2026-01-21T08:09:25.000Z","submittedOnDailyAt":"2026-01-22T00:26:21.515Z","title":"Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT","upvotes":14,"discussionId":"697191c7c1c7409747bf9589","ai_summary":"Render-of-Thought framework converts textual reasoning steps into images using vision-language models to improve reasoning traceability and efficiency while maintaining competitive performance.","ai_keywords":["Chain-of-Thought prompting","Large Language Models","vision encoders","Vision Language Models","token compression","inference acceleration","reasoning chain","semantic anchors","latent reasoning","traceability"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-01-21T03:09:25.000Z","title":"Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning","summary":"Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14750.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":211,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.14490","authors":[{"_id":"6971e218c1c7409747bf96fa","user":{"_id":"632f536d2636f057d586cf5b","avatarUrl":"/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg","isPro":false,"fullname":"Hunter Heidenreich","user":"hheiden","type":"user"},"name":"Hunter Heidenreich","status":"claimed_verified","statusLastChangedAt":"2026-01-22T17:12:09.233Z","hidden":false},{"_id":"6971e218c1c7409747bf96fb","user":{"_id":"6970fcdb686be84c0f5a295a","avatarUrl":"/avatars/c34ccbc616ba2433836f04457e936f32.svg","isPro":false,"fullname":"Ben Elliott","user":"bene-roots","type":"user"},"name":"Ben Elliott","status":"claimed_verified","statusLastChangedAt":"2026-01-22T17:11:27.411Z","hidden":false},{"_id":"6971e218c1c7409747bf96fc","name":"Olivia Dinica","hidden":false},{"_id":"6971e218c1c7409747bf96fd","name":"Yosheb Getachew","hidden":false}],"publishedAt":"2026-01-20T21:26:15.000Z","submittedOnDailyAt":"2026-01-22T14:07:58.844Z","title":"GutenOCR: A Grounded Vision-Language Front-End for Documents","submittedOnDailyBy":{"_id":"632f536d2636f057d586cf5b","avatarUrl":"/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg","isPro":false,"fullname":"Hunter Heidenreich","user":"hheiden","type":"user"},"summary":"GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.","upvotes":14,"discussionId":"6971e219c1c7409747bf96fe","projectPage":"https://ocr.roots.ai/","githubRepo":"https://github.com/Roots-Automation/GutenOCR","githubRepoAddedBy":"user","ai_summary":"GutenOCR enhances vision-language models for document understanding by enabling unified reading, detection, and grounding through prompt-based interfaces trained on diverse document types.","ai_keywords":["vision-language models","fine-tuning","grounded OCR","prompt-based interface","document understanding","OCR evaluation protocol","page-level linearization","text-detection recall"],"githubStars":2,"organization":{"_id":"662171ba64e84619e5565f7d","name":"rootsautomation","fullname":"Roots.AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66215d0369a5d33b332a19ab/ptAbQGJespFBIeYVk5ce0.jpeg"}},"publishedAt":"2026-01-20T16:26:15.000Z","title":"GutenOCR: A Grounded Vision-Language Front-End for Documents","summary":"GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14490.png","numComments":2,"submittedBy":{"_id":"632f536d2636f057d586cf5b","avatarUrl":"/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg","fullname":"Hunter Heidenreich","name":"hheiden","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"662171ba64e84619e5565f7d","name":"rootsautomation","fullname":"Roots.AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66215d0369a5d33b332a19ab/ptAbQGJespFBIeYVk5ce0.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.14722","authors":[{"_id":"6971c5eac1c7409747bf969e","user":{"_id":"64030afa56038547951c6114","avatarUrl":"/avatars/79bde57576e75815e1ba383c3bd2eea9.svg","isPro":false,"fullname":"Surapon Nonesung","user":"Suraponn","type":"user"},"name":"Surapon Nonesung","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:35.059Z","hidden":false},{"_id":"6971c5eac1c7409747bf969f","user":{"_id":"64705d3890482b0e0f6591ed","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64705d3890482b0e0f6591ed/PFJs66YhXDogcreVHH1OL.png","isPro":true,"fullname":"Natapong Nitarach (Schwyter)","user":"natnitaract","type":"user"},"name":"Natapong Nitarach","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:30.601Z","hidden":false},{"_id":"6971c5eac1c7409747bf96a0","name":"Teetouch Jaknamon","hidden":false},{"_id":"6971c5eac1c7409747bf96a1","user":{"_id":"615313b0793ef66b3324da1f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg","isPro":false,"fullname":"Pittawat Taveekitworachai","user":"pittawat","type":"user"},"name":"Pittawat Taveekitworachai","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:28.142Z","hidden":false},{"_id":"6971c5eac1c7409747bf96a2","name":"Kunat Pipatanakul","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64030afa56038547951c6114/HYCwGave_wMfhAWriEe-b.jpeg"],"publishedAt":"2026-01-21T07:24:32.000Z","submittedOnDailyAt":"2026-01-22T04:13:18.445Z","title":"Typhoon OCR: Open Vision-Language Model For Thai Document Extraction","submittedOnDailyBy":{"_id":"64030afa56038547951c6114","avatarUrl":"/avatars/79bde57576e75815e1ba383c3bd2eea9.svg","isPro":false,"fullname":"Surapon Nonesung","user":"Suraponn","type":"user"},"summary":"Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.","upvotes":12,"discussionId":"6971c5eac1c7409747bf96a3","projectPage":"https://opentyphoon.ai/model/typhoon-ocr","githubRepo":"https://github.com/scb-10x/typhoon-ocr","githubRepoAddedBy":"user","ai_summary":"Thai-focused vision-language model for document extraction combining OCR, layout reconstruction, and structural consistency with reduced computational requirements.","ai_keywords":["vision-language models","document extraction","OCR","layout reconstruction","structural consistency","multi-stage data construction","synthetic data","inference efficiency","compact model"],"githubStars":85,"organization":{"_id":"63e9cdf9dd2c4effdd6d39c0","name":"typhoon-ai","fullname":"Typhoon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"}},"publishedAt":"2026-01-21T02:24:32.000Z","title":"Typhoon OCR: Open Vision-Language Model For Thai Document Extraction","summary":"Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64030afa56038547951c6114/HYCwGave_wMfhAWriEe-b.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14722.png","numComments":1,"submittedBy":{"_id":"64030afa56038547951c6114","avatarUrl":"/avatars/79bde57576e75815e1ba383c3bd2eea9.svg","fullname":"Surapon Nonesung","name":"Suraponn","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":14,"isUserFollowing":false},"organization":{"_id":"63e9cdf9dd2c4effdd6d39c0","name":"typhoon-ai","fullname":"Typhoon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.13044","authors":[{"_id":"6970467fa8be625b19c2aebe","user":{"_id":"63d371bcd0b503b7f239ef9d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63d371bcd0b503b7f239ef9d/RCHFx6Y6fnh0nZYyF52sS.jpeg","isPro":true,"fullname":"Sirichotedumrong","user":"Warit","type":"user"},"name":"Warit Sirichotedumrong","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:47:21.592Z","hidden":false},{"_id":"6970467fa8be625b19c2aebf","name":"Adisai Na-Thalang","hidden":false},{"_id":"6970467fa8be625b19c2aec0","name":"Potsawee Manakul","hidden":false},{"_id":"6970467fa8be625b19c2aec1","user":{"_id":"615313b0793ef66b3324da1f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg","isPro":false,"fullname":"Pittawat Taveekitworachai","user":"pittawat","type":"user"},"name":"Pittawat Taveekitworachai","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:47:23.874Z","hidden":false},{"_id":"6970467fa8be625b19c2aec2","name":"Sittipong Sripaisarnmongkol","hidden":false},{"_id":"6970467fa8be625b19c2aec3","name":"Kunat Pipatanakul","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63d371bcd0b503b7f239ef9d/Nw4VZrCtiKVy08No1FwE9.png"],"publishedAt":"2026-01-19T13:28:17.000Z","submittedOnDailyAt":"2026-01-22T04:16:06.196Z","title":"Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition","submittedOnDailyBy":{"_id":"63d371bcd0b503b7f239ef9d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63d371bcd0b503b7f239ef9d/RCHFx6Y6fnh0nZYyF52sS.jpeg","isPro":true,"fullname":"Sirichotedumrong","user":"Warit","type":"user"},"summary":"Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.","upvotes":11,"discussionId":"6970467fa8be625b19c2aec4","projectPage":"https://opentyphoon.ai/model/typhoon-asr-realtime","githubRepo":"https://github.com/scb-10x/typhoon-asr","githubRepoAddedBy":"user","ai_summary":"A 115M-parameter FastConformer-Transducer model achieves low-latency Thai speech recognition with reduced computational cost through text normalization and curriculum learning, accompanied by a benchmark dataset for standardized evaluation.","ai_keywords":["FastConformer-Transducer","streaming applications","text normalization","curriculum learning","Thai ASR","speech recognition","computational cost","model scaling","linguistic conventions","benchmark dataset"],"githubStars":38,"organization":{"_id":"63e9cdf9dd2c4effdd6d39c0","name":"typhoon-ai","fullname":"Typhoon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"}},"publishedAt":"2026-01-19T08:28:17.000Z","title":"Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition","summary":"Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63d371bcd0b503b7f239ef9d/Nw4VZrCtiKVy08No1FwE9.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13044.png","numComments":1,"submittedBy":{"_id":"63d371bcd0b503b7f239ef9d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63d371bcd0b503b7f239ef9d/RCHFx6Y6fnh0nZYyF52sS.jpeg","fullname":"Sirichotedumrong","name":"Warit","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"63e9cdf9dd2c4effdd6d39c0","name":"typhoon-ai","fullname":"Typhoon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.14027","authors":[{"_id":"6971b78cc1c7409747bf9668","name":"Junqi Liu","hidden":false},{"_id":"6971b78cc1c7409747bf9669","name":"Zihao Zhou","hidden":false},{"_id":"6971b78cc1c7409747bf966a","name":"Zekai Zhu","hidden":false},{"_id":"6971b78cc1c7409747bf966b","name":"Marco Dos Santos","hidden":false},{"_id":"6971b78cc1c7409747bf966c","name":"Weikun He","hidden":false},{"_id":"6971b78cc1c7409747bf966d","name":"Jiawei Liu","hidden":false},{"_id":"6971b78cc1c7409747bf966e","name":"Ran Wang","hidden":false},{"_id":"6971b78cc1c7409747bf966f","name":"Yunzhou Xie","hidden":false},{"_id":"6971b78cc1c7409747bf9670","name":"Junqiao Zhao","hidden":false},{"_id":"6971b78cc1c7409747bf9671","name":"Qiufeng Wang","hidden":false},{"_id":"6971b78cc1c7409747bf9672","name":"Lihong Zhi","hidden":false},{"_id":"6971b78cc1c7409747bf9673","name":"Jia Li","hidden":false},{"_id":"6971b78cc1c7409747bf9674","name":"Wenda Li","hidden":false}],"publishedAt":"2026-01-20T14:51:45.000Z","submittedOnDailyAt":"2026-01-22T03:16:22.205Z","title":"Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics","submittedOnDailyBy":{"_id":"65683c6efd8939c271447e9b","avatarUrl":"/avatars/e4a97ba3550ddcb24b906a7d4c87c861.svg","isPro":false,"fullname":"ZihaoZhou","user":"ZihaoZhou","type":"user"},"summary":"Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.","upvotes":9,"discussionId":"6971b78dc1c7409747bf9675","ai_summary":"A general coding agent paradigm enables flexible formal theorem proving by directly interfacing with proof assistants and retrieving relevant theorems without task-specific training.","ai_keywords":["agentic systems","formal theorem proving","general coding agent","Lean","Numina-Lean-MCP","Claude Code","Claude Opus","Putnam 2025","Brascamp-Lieb theorem"],"organization":{"_id":"659a9f55eabe0f3e9812aa63","name":"AI-MO","fullname":"Project-Numina","avatar":"https://cdn-uploads.huggingface.co/production/uploads/661d3f3e85f70e208d6f20db/kaXbnZDekN1gPzmRG_aFV.png"}},"publishedAt":"2026-01-20T09:51:45.000Z","title":"Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics","summary":"Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14027.png","numComments":1,"submittedBy":{"_id":"65683c6efd8939c271447e9b","avatarUrl":"/avatars/e4a97ba3550ddcb24b906a7d4c87c861.svg","fullname":"ZihaoZhou","name":"ZihaoZhou","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"659a9f55eabe0f3e9812aa63","name":"AI-MO","fullname":"Project-Numina","avatar":"https://cdn-uploads.huggingface.co/production/uploads/661d3f3e85f70e208d6f20db/kaXbnZDekN1gPzmRG_aFV.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.11141","authors":[{"_id":"696d91063f1837bfb8970953","user":{"_id":"65224256377bffdc599901c9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65224256377bffdc599901c9/xUM64_ugWwt4NO1F6DOcc.jpeg","isPro":false,"fullname":"chentanyu","user":"sailorjs0804","type":"user"},"name":"Tanyu Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:24:09.590Z","hidden":false},{"_id":"696d91063f1837bfb8970954","user":{"_id":"655dc83ca499c26280ae26cb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/655dc83ca499c26280ae26cb/NSDMlsuLbTKkT4U_TovGE.jpeg","isPro":false,"fullname":"Terrence","user":"terrencecchen","type":"user"},"name":"Tairan Chen","status":"claimed_verified","statusLastChangedAt":"2026-01-21T12:54:02.586Z","hidden":false},{"_id":"696d91063f1837bfb8970955","user":{"_id":"68b941018936c20ec712b011","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68b941018936c20ec712b011/8dvgJn1ROXISOZo8NUsi1.jpeg","isPro":false,"fullname":"shenkai","user":"FakerVSdonk","type":"user"},"name":"Kai Shen","status":"claimed_verified","statusLastChangedAt":"2026-01-20T09:41:12.758Z","hidden":false},{"_id":"696d91063f1837bfb8970956","user":{"_id":"679caa50208bfa0539ca05c8","avatarUrl":"/avatars/5e9d76169dbcd8c414d8cddaa2029c0a.svg","isPro":false,"fullname":"Zhenghua Bao","user":"KingZ23","type":"user"},"name":"Zhenghua Bao","status":"claimed_verified","statusLastChangedAt":"2026-01-21T09:24:01.839Z","hidden":false},{"_id":"696d91063f1837bfb8970957","name":"Zhihui Zhang","hidden":false},{"_id":"696d91063f1837bfb8970958","name":"Man Yuan","hidden":false},{"_id":"696d91063f1837bfb8970959","user":{"_id":"696de1a7ca820bff80b8977d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/696de1a7ca820bff80b8977d/0-IeLu9q-oz2ePK7sMfE6.jpeg","isPro":false,"fullname":"Yi Shi","user":"hugging-yiii","type":"user"},"name":"Yi Shi","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:47:32.091Z","hidden":false}],"publishedAt":"2026-01-16T10:00:03.000Z","submittedOnDailyAt":"2026-01-22T13:19:03.930Z","title":"FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning","submittedOnDailyBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","isPro":true,"fullname":"Rajkumar rawal","user":"rajkumarrawal","type":"user"},"summary":"Recent end-to-end spoken dialogue systems leverage speech tokenizers and neural audio codecs to enable LLMs to operate directly on discrete speech representations. However, these models often exhibit limited speaker identity preservation, hindering personalized voice interaction. In this work, we present Chroma 1.0, the first open-source, real-time, end-to-end spoken dialogue model that achieves both low-latency interaction and high-fidelity personalized voice cloning. Chroma achieves sub-second end-to-end latency through an interleaved text-audio token schedule (1:2) that supports streaming generation, while maintaining high-quality personalized voice synthesis across multi-turn conversations. Our experimental results demonstrate that Chroma achieves a 10.96% relative improvement in speaker similarity over the human baseline, with a Real-Time Factor (RTF) of 0.43, while maintaining strong reasoning and dialogue capabilities. Our code and models are publicly available at https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma and https://huggingface.co/FlashLabs/Chroma-4B .","upvotes":8,"discussionId":"696d91063f1837bfb897095a","projectPage":"https://www.flashlabs.ai/flashai-voice-agents","githubRepo":"https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma","githubRepoAddedBy":"user","ai_summary":"Chroma 1.0 enables real-time spoken dialogue with personalized voice cloning through discrete speech representations and interleaved text-audio token scheduling.","ai_keywords":["speech tokenizers","neural audio codecs","LLMs","end-to-end spoken dialogue systems","voice cloning","real-time interaction","discrete speech representations","interleaved text-audio token schedule","streaming generation","speaker identity preservation"],"githubStars":141,"organization":{"_id":"69154c5323185a1e38237e9d","name":"FlashLabs","fullname":"FlashLabs","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65224256377bffdc599901c9/y8OmryAl6Id-A28c1wAiq.png"}},"publishedAt":"2026-01-16T05:00:03.000Z","title":"FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning","summary":"Recent end-to-end spoken dialogue systems leverage speech tokenizers and neural audio codecs to enable LLMs to operate directly on discrete speech representations. However, these models often exhibit limited speaker identity preservation, hindering personalized voice interaction. In this work, we present Chroma 1.0, the first open-source, real-time, end-to-end spoken dialogue model that achieves both low-latency interaction and high-fidelity personalized voice cloning. Chroma achieves sub-second end-to-end latency through an interleaved text-audio token schedule (1:2) that supports streaming generation, while maintaining high-quality personalized voice synthesis across multi-turn conversations. Our experimental results demonstrate that Chroma achieves a 10.96% relative improvement in speaker similarity over the human baseline, with a Real-Time Factor (RTF) of 0.43, while maintaining strong reasoning and dialogue capabilities. Our code and models are publicly available at https://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma and https://huggingface.co/FlashLabs/Chroma-4B .","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11141.png","numComments":1,"submittedBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","fullname":"Rajkumar rawal","name":"rajkumarrawal","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":49,"isUserFollowing":false},"organization":{"_id":"69154c5323185a1e38237e9d","name":"FlashLabs","fullname":"FlashLabs","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65224256377bffdc599901c9/y8OmryAl6Id-A28c1wAiq.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.07853","authors":[{"_id":"69671840c5e371f6b235d186","user":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","isPro":false,"fullname":"Zhi Yang","user":"yangzhi1","type":"user"},"name":"Zhi Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:04:10.222Z","hidden":false},{"_id":"69671840c5e371f6b235d187","name":"Runguo Li","hidden":false},{"_id":"69671840c5e371f6b235d188","name":"Qiqi Qiang","hidden":false},{"_id":"69671840c5e371f6b235d189","name":"Jiashun Wang","hidden":false},{"_id":"69671840c5e371f6b235d18a","name":"Fangqi Lou","hidden":false},{"_id":"69671840c5e371f6b235d18b","name":"Mengping Li","hidden":false},{"_id":"69671840c5e371f6b235d18c","name":"Dongpo Cheng","hidden":false},{"_id":"69671840c5e371f6b235d18d","name":"Rui Xu","hidden":false},{"_id":"69671840c5e371f6b235d18e","name":"Heng Lian","hidden":false},{"_id":"69671840c5e371f6b235d18f","user":{"_id":"65562edfb7bad186e877c724","avatarUrl":"/avatars/bb91f42b102e113208bbe3238916a015.svg","isPro":false,"fullname":"zhangshuo","user":"mcflurryshuoz","type":"user"},"name":"Shuo Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:04:08.152Z","hidden":false},{"_id":"69671840c5e371f6b235d190","name":"Xiaolong Liang","hidden":false},{"_id":"69671840c5e371f6b235d191","name":"Xiaoming Huang","hidden":false},{"_id":"69671840c5e371f6b235d192","name":"Zheng Wei","hidden":false},{"_id":"69671840c5e371f6b235d193","name":"Zhaowei Liu","hidden":false},{"_id":"69671840c5e371f6b235d194","name":"Xin Guo","hidden":false},{"_id":"69671840c5e371f6b235d195","user":{"_id":"6603d56ab4344a2b07cd6d21","avatarUrl":"/avatars/1569bb60166532317c85e80da722ba1c.svg","isPro":false,"fullname":"Huacan Wang","user":"Huacan-Wang","type":"user"},"name":"Huacan Wang","status":"claimed_verified","statusLastChangedAt":"2026-01-15T15:04:12.096Z","hidden":false},{"_id":"69671840c5e371f6b235d196","name":"Ronghao Chen","hidden":false},{"_id":"69671840c5e371f6b235d197","user":{"_id":"64d4aa25263aa362d4644737","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64d4aa25263aa362d4644737/XB4VybeUyLpz1qVtqXHbp.png","isPro":false,"fullname":"Liwen Zhang","user":"SUFE-AIFLM-Lab","type":"user"},"name":"Liwen Zhang","status":"claimed_verified","statusLastChangedAt":"2026-01-16T14:37:56.203Z","hidden":false}],"publishedAt":"2026-01-09T03:25:45.000Z","submittedOnDailyAt":"2026-01-22T02:06:01.550Z","title":"FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments","submittedOnDailyBy":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","isPro":false,"fullname":"Zhi Yang","user":"yangzhi1","type":"user"},"summary":"Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.","upvotes":8,"discussionId":"69671841c5e371f6b235d198","githubRepo":"https://github.com/aifinlab/FinVault","githubRepoAddedBy":"user","ai_summary":"FinVault presents the first execution-grounded security benchmark for financial agents, revealing significant vulnerabilities in current defense mechanisms when applied to real-world financial workflows.","ai_keywords":["large language models","financial agents","security benchmark","state-writable databases","compliance constraints","prompt injection","jailbreaking","financially adapted attacks","attack success rates"],"githubStars":5,"organization":{"_id":"696875114bc2a5524dd8fcb7","name":"AIFin-Lab","fullname":"AIFin Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"}},"publishedAt":"2026-01-08T22:25:45.000Z","title":"FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments","summary":"Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07853.png","numComments":1,"submittedBy":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","fullname":"Zhi Yang","name":"yangzhi1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"696875114bc2a5524dd8fcb7","name":"AIFin-Lab","fullname":"AIFin Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14245","authors":[{"_id":"69719a02c1c7409747bf95eb","user":{"_id":"654f99f74c8874c64d4e5664","avatarUrl":"/avatars/e9da0d688f91ae49db91d0ebebb3782a.svg","isPro":false,"fullname":"Zhongyu Yang","user":"yzzyu","type":"user"},"name":"Zhongyu Yang","status":"claimed_verified","statusLastChangedAt":"2026-01-22T17:12:19.376Z","hidden":false},{"_id":"69719a02c1c7409747bf95ec","name":"Wei Pang","hidden":false},{"_id":"69719a02c1c7409747bf95ed","name":"Yingfang Yuan","hidden":false}],"publishedAt":"2026-01-20T18:57:00.000Z","submittedOnDailyAt":"2026-01-22T01:05:28.074Z","title":"XR: Cross-Modal Agents for Composed Image Retrieval","submittedOnDailyBy":{"_id":"654f99f74c8874c64d4e5664","avatarUrl":"/avatars/e9da0d688f91ae49db91d0ebebb3782a.svg","isPro":false,"fullname":"Zhongyu Yang","user":"yzzyu","type":"user"},"summary":"Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.","upvotes":7,"discussionId":"69719a02c1c7409747bf95ee","projectPage":"https://01yzzyu.github.io/xr.github.io/","ai_summary":"A multi-agent framework for compositional image retrieval that uses specialized agents for generation, filtering, and verification to improve semantic and visual query matching.","ai_keywords":["compositional image retrieval","multi-agent framework","cross-modal generation","hybrid matching","targeted reasoning"],"organization":{"_id":"6231af6a3e32d5aa1b7a5a6f","name":"Heriot-WattUniversity","fullname":"Heriot-Watt University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1647424288895-61ac8f8a00d01045fca0ad2f.jpeg"}},"publishedAt":"2026-01-20T13:57:00.000Z","title":"XR: Cross-Modal Agents for Composed Image Retrieval","summary":"Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14245.png","numComments":1,"submittedBy":{"_id":"654f99f74c8874c64d4e5664","avatarUrl":"/avatars/e9da0d688f91ae49db91d0ebebb3782a.svg","fullname":"Zhongyu Yang","name":"yzzyu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6231af6a3e32d5aa1b7a5a6f","name":"Heriot-WattUniversity","fullname":"Heriot-Watt University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1647424288895-61ac8f8a00d01045fca0ad2f.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2601.15220","authors":[{"_id":"6972308dc1c7409747bf97fe","name":"Anmol Goel","hidden":false},{"_id":"6972308dc1c7409747bf97ff","name":"Cornelius Emde","hidden":false},{"_id":"6972308dc1c7409747bf9800","name":"Sangdoo Yun","hidden":false},{"_id":"6972308dc1c7409747bf9801","name":"Seong Joon Oh","hidden":false},{"_id":"6972308dc1c7409747bf9802","user":{"_id":"63ee35c3f599efc7a010c792","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676555600618-noauth.jpeg","isPro":false,"fullname":"Martin Gubri","user":"mgubri","type":"user"},"name":"Martin Gubri","status":"claimed_verified","statusLastChangedAt":"2026-01-22T17:11:46.109Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/baQAVZ203utZdgfjTrsU1.png","https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/K1Tx9O0nG357RAR4xg7Fn.png","https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/D6LwaP04RIDybeGbXNxcF.png"],"publishedAt":"2026-01-21T17:53:06.000Z","submittedOnDailyAt":"2026-01-22T12:03:12.991Z","title":"Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models","submittedOnDailyBy":{"_id":"63ee35c3f599efc7a010c792","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676555600618-noauth.jpeg","isPro":false,"fullname":"Martin Gubri","user":"mgubri","type":"user"},"summary":"We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.","upvotes":6,"discussionId":"6972308dc1c7409747bf9803","githubRepo":"https://github.com/parameterlab/privacy-collapse","githubRepoAddedBy":"user","ai_summary":"Benign fine-tuning of language models can cause privacy collapse, where models lose contextual privacy reasoning abilities despite maintaining high performance on standard benchmarks.","ai_keywords":["language models","fine-tuning","privacy collapse","contextual privacy","safety evaluations","agentic tasks","memory-based tasks"],"githubStars":0,"organization":{"_id":"673bcad43a2fd2a3b41f64e3","name":"parameterlab","fullname":"Parameter Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"}},"publishedAt":"2026-01-21T12:53:06.000Z","title":"Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models","summary":"We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/baQAVZ203utZdgfjTrsU1.png","https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/K1Tx9O0nG357RAR4xg7Fn.png","https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/D6LwaP04RIDybeGbXNxcF.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15220.png","numComments":1,"submittedBy":{"_id":"63ee35c3f599efc7a010c792","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676555600618-noauth.jpeg","fullname":"Martin Gubri","name":"mgubri","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"673bcad43a2fd2a3b41f64e3","name":"parameterlab","fullname":"Parameter Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63ee35c3f599efc7a010c792/w3hbIZBXXMq09s0BAzwC0.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14352","authors":[{"_id":"697197e3c1c7409747bf95bb","name":"Huajie Tan","hidden":false},{"_id":"697197e3c1c7409747bf95bc","user":{"_id":"63f08dc79cf89c9ed1bb89cd","avatarUrl":"/avatars/37290358ad00bbd752f519cfdec02f3e.svg","isPro":false,"fullname":"Zhoues","user":"Zhoues","type":"user"},"name":"Enshen Zhou","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:45:02.013Z","hidden":false},{"_id":"697197e3c1c7409747bf95bd","name":"Zhiyu Li","hidden":false},{"_id":"697197e3c1c7409747bf95be","name":"Yijie Xu","hidden":false},{"_id":"697197e3c1c7409747bf95bf","name":"Yuheng Ji","hidden":false},{"_id":"697197e3c1c7409747bf95c0","name":"Xiansheng Chen","hidden":false},{"_id":"697197e3c1c7409747bf95c1","name":"Cheng Chi","hidden":false},{"_id":"697197e3c1c7409747bf95c2","name":"Pengwei Wang","hidden":false},{"_id":"697197e3c1c7409747bf95c3","name":"Huizhu Jia","hidden":false},{"_id":"697197e3c1c7409747bf95c4","name":"Yulong Ao","hidden":false},{"_id":"697197e3c1c7409747bf95c5","name":"Mingyu Cao","hidden":false},{"_id":"697197e3c1c7409747bf95c6","name":"Sixiang Chen","hidden":false},{"_id":"697197e3c1c7409747bf95c7","name":"Zhe Li","hidden":false},{"_id":"697197e3c1c7409747bf95c8","name":"Mengzhen Liu","hidden":false},{"_id":"697197e3c1c7409747bf95c9","name":"Zixiao Wang","hidden":false},{"_id":"697197e3c1c7409747bf95ca","name":"Shanyu Rong","hidden":false},{"_id":"697197e3c1c7409747bf95cb","name":"Yaoxu Lyu","hidden":false},{"_id":"697197e3c1c7409747bf95cc","name":"Zhongxia Zhao","hidden":false},{"_id":"697197e3c1c7409747bf95cd","name":"Peterson Co","hidden":false},{"_id":"697197e3c1c7409747bf95ce","name":"Yibo Li","hidden":false},{"_id":"697197e3c1c7409747bf95cf","name":"Yi Han","hidden":false},{"_id":"697197e3c1c7409747bf95d0","name":"Shaoxuan Xie","hidden":false},{"_id":"697197e3c1c7409747bf95d1","name":"Guocai Yao","hidden":false},{"_id":"697197e3c1c7409747bf95d2","name":"Songjing Wang","hidden":false},{"_id":"697197e3c1c7409747bf95d3","name":"Leiduo Zhang","hidden":false},{"_id":"697197e3c1c7409747bf95d4","name":"Xi Yang","hidden":false},{"_id":"697197e3c1c7409747bf95d5","name":"Yance Jiao","hidden":false},{"_id":"697197e3c1c7409747bf95d6","name":"Donghai Shi","hidden":false},{"_id":"697197e3c1c7409747bf95d7","name":"Kunchang Xie","hidden":false},{"_id":"697197e3c1c7409747bf95d8","name":"Shaokai Nie","hidden":false},{"_id":"697197e3c1c7409747bf95d9","name":"Chunlei Men","hidden":false},{"_id":"697197e3c1c7409747bf95da","name":"Yonghua Lin","hidden":false},{"_id":"697197e3c1c7409747bf95db","name":"Zhongyuan Wang","hidden":false},{"_id":"697197e3c1c7409747bf95dc","name":"Tiejun Huang","hidden":false},{"_id":"697197e3c1c7409747bf95dd","name":"Shanghang Zhang","hidden":false}],"publishedAt":"2026-01-20T17:21:54.000Z","submittedOnDailyAt":"2026-01-22T00:52:27.595Z","title":"RoboBrain 2.5: Depth in Sight, Time in Mind","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io","upvotes":6,"discussionId":"697197e3c1c7409747bf95de","projectPage":"https://superrobobrain.github.io/","ai_summary":"RoboBrain 2.5 enhances embodied AI through improved 3D spatial reasoning and temporal value estimation for more precise manipulation tasks.","ai_keywords":["embodied AI","spatiotemporal supervision","3D spatial reasoning","depth-aware coordinate prediction","metric constraint comprehension","3D manipulation traces","temporal value estimation","step-aware progress prediction","execution state understanding"]},"publishedAt":"2026-01-20T12:21:54.000Z","title":"RoboBrain 2.5: Depth in Sight, Time in Mind","summary":"We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14352.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":211,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.14417","authors":[{"_id":"6971bda7c1c7409747bf9677","name":"Thanathai Lertpetchpun","hidden":false},{"_id":"6971bda7c1c7409747bf9678","name":"Yoonjeong Lee","hidden":false},{"_id":"6971bda7c1c7409747bf9679","name":"Thanapat Trachu","hidden":false},{"_id":"6971bda7c1c7409747bf967a","name":"Jihwan Lee","hidden":false},{"_id":"6971bda7c1c7409747bf967b","user":{"_id":"64092a1ab6a334f53e278b3b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg","isPro":false,"fullname":"Tiantian Feng","user":"tiantiaf","type":"user"},"name":"Tiantian Feng","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:45.167Z","hidden":false},{"_id":"6971bda7c1c7409747bf967c","name":"Dani Byrd","hidden":false},{"_id":"6971bda7c1c7409747bf967d","name":"Shrikanth Narayanan","hidden":false}],"publishedAt":"2026-01-20T19:25:33.000Z","submittedOnDailyAt":"2026-01-22T03:39:52.906Z","title":"Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis","submittedOnDailyBy":{"_id":"64092a1ab6a334f53e278b3b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg","isPro":false,"fullname":"Tiantian Feng","user":"tiantiaf","type":"user"},"summary":"Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.","upvotes":5,"discussionId":"6971bda7c1c7409747bf967e","projectPage":"https://sav-eng.github.io/icassp_samples.html","ai_summary":"Research investigates the relationship between speaker embeddings and phonological rules in accent control for text-to-speech systems, introducing a metric to measure rule preservation versus embedding influence.","ai_keywords":["text-to-speech","speaker embeddings","phonological rules","accent control","phoneme shift rate","flapping","rhoticity","vowel correspondences"]},"publishedAt":"2026-01-20T14:25:33.000Z","title":"Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis","summary":"Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14417.png","numComments":1,"submittedBy":{"_id":"64092a1ab6a334f53e278b3b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg","fullname":"Tiantian Feng","name":"tiantiaf","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.14256","authors":[{"_id":"69726e10fb12c92b735b732d","name":"Matthew Gwilliam","hidden":false},{"_id":"69726e10fb12c92b735b732e","name":"Xiao Wang","hidden":false},{"_id":"69726e10fb12c92b735b732f","name":"Xuefeng Hu","hidden":false},{"_id":"69726e10fb12c92b735b7330","name":"Zhenheng Yang","hidden":false}],"publishedAt":"2026-01-20T18:59:57.000Z","submittedOnDailyAt":"2026-01-22T16:07:05.228Z","title":"Implicit Neural Representation Facilitates Unified Universal Vision Encoding","submittedOnDailyBy":{"_id":"6250c5008c2cdfaca2a681d9","avatarUrl":"/avatars/e4f168396c151c1f8077768430bfc673.svg","isPro":false,"fullname":"Matt Gwilliam","user":"mgwillia","type":"user"},"summary":"Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.","upvotes":5,"discussionId":"69726e10fb12c92b735b7331","ai_summary":"A unified model learns image representations useful for both recognition and generation by using a hyper-network for implicit neural representation with knowledge distillation, achieving state-of-the-art results while enabling generative capabilities through compressed embeddings.","ai_keywords":["contrastive learning","image representation learning","recognition","generation","hyper-network","implicit neural representation","knowledge distillation","compressed embedding space","visual tasks","image reconstruction"]},"publishedAt":"2026-01-20T13:59:57.000Z","title":"Implicit Neural Representation Facilitates Unified Universal Vision Encoding","summary":"Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14256.png","numComments":1,"submittedBy":{"_id":"6250c5008c2cdfaca2a681d9","avatarUrl":"/avatars/e4f168396c151c1f8077768430bfc673.svg","fullname":"Matt Gwilliam","name":"mgwillia","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.13918","authors":[{"_id":"6971b232c1c7409747bf9654","user":{"_id":"64bf44c0a0e5471066875053","avatarUrl":"/avatars/b165c0b0632e849248362b7ed65314a9.svg","isPro":false,"fullname":"Yusheng Liao","user":"BlueZeros","type":"user"},"name":"Yusheng Liao","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:49.734Z","hidden":false},{"_id":"6971b232c1c7409747bf9655","user":{"_id":"6530e71196a972c24cdd6df8","avatarUrl":"/avatars/4f802030fb86fa5274b939faaec6945b.svg","isPro":false,"fullname":"xuan","user":"handmaster","type":"user"},"name":"Chuan Xuan","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:47.363Z","hidden":false},{"_id":"6971b232c1c7409747bf9656","name":"Yutong Cai","hidden":false},{"_id":"6971b232c1c7409747bf9657","name":"Lina Yang","hidden":false},{"_id":"6971b232c1c7409747bf9658","name":"Zhe Chen","hidden":false},{"_id":"6971b232c1c7409747bf9659","name":"Yanfeng Wang","hidden":false},{"_id":"6971b232c1c7409747bf965a","name":"Yu Wang","hidden":false}],"publishedAt":"2026-01-20T12:48:04.000Z","submittedOnDailyAt":"2026-01-22T02:45:57.233Z","title":"AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization","submittedOnDailyBy":{"_id":"64bf44c0a0e5471066875053","avatarUrl":"/avatars/b165c0b0632e849248362b7ed65314a9.svg","isPro":false,"fullname":"Yusheng Liao","user":"BlueZeros","type":"user"},"summary":"Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.","upvotes":5,"discussionId":"6971b232c1c7409747bf965b","githubRepo":"https://github.com/BlueZeros/AgentEHR","githubRepoAddedBy":"user","ai_summary":"AgentEHR presents a benchmark for autonomous EHR navigation requiring complex decision-making, while RetroSum framework improves performance through retrospective summarization and evolving experience strategies.","ai_keywords":["Electronic Health Records","large language models","decision-making","retrospective summarization","evolving experience strategy","interaction history","memory bank","clinical environments","autonomous navigation"],"githubStars":3,"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}},"publishedAt":"2026-01-20T07:48:04.000Z","title":"AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization","summary":"Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13918.png","numComments":1,"submittedBy":{"_id":"64bf44c0a0e5471066875053","avatarUrl":"/avatars/b165c0b0632e849248362b7ed65314a9.svg","fullname":"Yusheng Liao","name":"BlueZeros","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"63e5ef7bf2e9a8f22c515654","name":"SJTU","fullname":"Shanghai Jiao Tong University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14681","authors":[{"_id":"69719812c1c7409747bf95e0","name":"Shuhao Liao","hidden":false},{"_id":"69719812c1c7409747bf95e1","name":"Xuxin Lv","hidden":false},{"_id":"69719812c1c7409747bf95e2","name":"Jeric Lew","hidden":false},{"_id":"69719812c1c7409747bf95e3","name":"Shizhe Zhang","hidden":false},{"_id":"69719812c1c7409747bf95e4","name":"Jingsong Liang","hidden":false},{"_id":"69719812c1c7409747bf95e5","name":"Peizhuo Li","hidden":false},{"_id":"69719812c1c7409747bf95e6","name":"Yuhong Cao","hidden":false},{"_id":"69719812c1c7409747bf95e7","name":"Wenjun Wu","hidden":false},{"_id":"69719812c1c7409747bf95e8","name":"Guillaume Sartoretti","hidden":false}],"publishedAt":"2026-01-21T05:56:24.000Z","submittedOnDailyAt":"2026-01-22T00:53:10.543Z","title":"FARE: Fast-Slow Agentic Robotic Exploration","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.","upvotes":4,"discussionId":"69719812c1c7409747bf95e9","ai_summary":"FARE is a hierarchical exploration framework that combines large language model reasoning with reinforcement learning control to enable efficient autonomous robot navigation in complex environments.","ai_keywords":["large language model","reinforcement learning","global reasoning","local decision making","topological graph","modularity-based pruning","reward shaping","closed-loop behavior","temporal scale","spatial scale"]},"publishedAt":"2026-01-21T00:56:24.000Z","title":"FARE: Fast-Slow Agentic Robotic Exploration","summary":"This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14681.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":211,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.14152","authors":[{"_id":"69706674a8be625b19c2afa1","name":"Hyunjong Ok","hidden":false},{"_id":"69706674a8be625b19c2afa2","name":"Jaeho Lee","hidden":false}],"publishedAt":"2026-01-20T16:54:22.000Z","submittedOnDailyAt":"2026-01-22T00:30:31.651Z","title":"Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models","submittedOnDailyBy":{"_id":"631974d51328b6caf9fe328f","avatarUrl":"/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg","isPro":false,"fullname":"Hyunjong Ok","user":"HJOK","type":"user"},"summary":"Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.","upvotes":4,"discussionId":"69706675a8be625b19c2afa3","ai_summary":"Research reveals that causal attention in language models creates information bottlenecks when question-answer options follow context, leading to performance drops of over 14 percentage points compared to reversed prompt ordering.","ai_keywords":["large language models","prompt structure","multiple-choice question answering","causal attention","causal mask","information bottleneck"],"organization":{"_id":"62459012e1b9dab15a3e6674","name":"POSTECH","fullname":"Pohang University of Science and Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1648726022705-62458f43d5895bdf34ee7d56.jpeg"}},"publishedAt":"2026-01-20T11:54:22.000Z","title":"Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models","summary":"Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14152.png","numComments":1,"submittedBy":{"_id":"631974d51328b6caf9fe328f","avatarUrl":"/avatars/5d7b54d2798d9e42d2db66cdba24e085.svg","fullname":"Hyunjong Ok","name":"HJOK","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"62459012e1b9dab15a3e6674","name":"POSTECH","fullname":"Pohang University of Science and Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1648726022705-62458f43d5895bdf34ee7d56.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.15059","authors":[{"_id":"6971a2dec1c7409747bf9641","name":"Oleg Romanchuk","hidden":false},{"_id":"6971a2dec1c7409747bf9642","name":"Roman Bondar","hidden":false}],"publishedAt":"2026-01-21T15:05:27.000Z","submittedOnDailyAt":"2026-01-22T01:48:54.058Z","title":"The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems","submittedOnDailyBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","isPro":true,"fullname":"Rajkumar rawal","user":"rajkumarrawal","type":"user"},"summary":"Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.\n  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.\n  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.\n  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.\n  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.","upvotes":2,"discussionId":"6971a2dec1c7409747bf9643"},"publishedAt":"2026-01-21T10:05:27.000Z","title":"The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems","summary":"Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.\n  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.\n  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.\n  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.\n  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15059.png","numComments":1,"submittedBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","fullname":"Rajkumar rawal","name":"rajkumarrawal","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":49,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.15100","authors":[{"_id":"697197c8c1c7409747bf95b7","name":"Yanwei Huang","hidden":false},{"_id":"697197c8c1c7409747bf95b8","name":"Arpit Narechania","hidden":false}],"publishedAt":"2026-01-21T15:38:57.000Z","submittedOnDailyAt":"2026-01-22T00:51:49.081Z","title":"Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.","upvotes":1,"discussionId":"697197c8c1c7409747bf95b9","ai_summary":"WebSeek is a mixed-initiative browser extension that enables interactive web data extraction and analysis with AI-assisted guidance and automation.","ai_keywords":[""]},"publishedAt":"2026-01-21T10:38:57.000Z","title":"Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek","summary":"Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15100.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":211,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.12029","authors":[{"_id":"6971c3b1c1c7409747bf9694","user":{"_id":"68d51061ab9204b0c8a0ceb2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg","isPro":false,"fullname":"Sandy Hardian Susanto Herho","user":"sandyherho","type":"user"},"name":"Sandy H. S. Herho","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:44:40.724Z","hidden":false},{"_id":"6971c3b1c1c7409747bf9695","name":"Faruq Khadami","hidden":false},{"_id":"6971c3b1c1c7409747bf9696","name":"Iwan P. Anwar","hidden":false},{"_id":"6971c3b1c1c7409747bf9697","name":"Dasapta E. Irawan","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/68d51061ab9204b0c8a0ceb2/AevS3nwVQq9KOXU1FsfSU.png"],"publishedAt":"2026-01-17T12:12:52.000Z","submittedOnDailyAt":"2026-01-22T04:00:39.193Z","title":"sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation","submittedOnDailyBy":{"_id":"68d51061ab9204b0c8a0ceb2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg","isPro":false,"fullname":"Sandy Hardian Susanto Herho","user":"sandyherho","type":"user"},"summary":"The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.","upvotes":1,"discussionId":"6971c3b1c1c7409747bf9698","projectPage":"https://pypi.org/project/sangkuriang-ideal-solver/","githubRepo":"https://github.com/sandyherho/sangkuriang-ideal-solver","githubRepoAddedBy":"user","githubStars":5,"organization":{"_id":"5e67bd5b1009063689407478","name":"huggingface","fullname":"Hugging Face","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png"}},"publishedAt":"2026-01-17T07:12:52.000Z","title":"sangkuriang: A pseudo-spectral Python library for Korteweg-de Vries soliton simulation","summary":"The Korteweg-de Vries (KdV) equation serves as a foundational model in nonlinear wave physics, describing the balance between dispersive spreading and nonlinear steepening that gives rise to solitons. This article introduces sangkuriang, an open-source Python library for solving this equation using Fourier pseudo-spectral spatial discretization coupled with adaptive high-order time integration. The implementation leverages just-in-time (JIT) compilation for computational efficiency while maintaining accessibility for instructional purposes. Validation encompasses progressively complex scenarios including isolated soliton propagation, symmetric two-wave configurations, overtaking collisions between waves of differing amplitudes, and three-body interactions. Conservation of the classical invariants is monitored throughout, with deviations remaining small across all test cases. Measured soliton velocities conform closely to theoretical predictions based on the amplitude-velocity relationship characteristic of integrable systems. Complementary diagnostics drawn from information theory and recurrence analysis confirm that computed solutions preserve the regular phase-space structure expected for completely integrable dynamics. The solver outputs data in standard scientific formats compatible with common analysis tools and generates visualizations of spatiotemporal wave evolution. By combining numerical accuracy with practical accessibility on modest computational resources, sangkuriang offers a platform suitable for both classroom demonstrations of nonlinear wave phenomena and exploratory research into soliton dynamics.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/68d51061ab9204b0c8a0ceb2/AevS3nwVQq9KOXU1FsfSU.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12029.png","numComments":1,"submittedBy":{"_id":"68d51061ab9204b0c8a0ceb2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d51061ab9204b0c8a0ceb2/CVmg6wo4yBqIZgGZUYSX8.jpeg","fullname":"Sandy Hardian Susanto Herho","name":"sandyherho","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"5e67bd5b1009063689407478","name":"huggingface","fullname":"Hugging Face","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583856921041-5dd96eb166059660ed1ee413.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.11387","authors":[{"_id":"69724a26fb12c92b735b72ea","user":{"_id":"6698cffdb2ebada9f4a7e7d7","avatarUrl":"/avatars/e66d946c14595d3b008185f2be8d2f57.svg","isPro":false,"fullname":"Greta Warren","user":"gretawarren","type":"user"},"name":"Greta Warren","status":"claimed_verified","statusLastChangedAt":"2026-01-22T17:11:19.710Z","hidden":false},{"_id":"69724a26fb12c92b735b72eb","name":"Jingyi Sun","hidden":false},{"_id":"69724a26fb12c92b735b72ec","name":"Irina Shklovski","hidden":false},{"_id":"69724a26fb12c92b735b72ed","name":"Isabelle Augenstein","hidden":false}],"publishedAt":"2026-01-16T15:55:31.000Z","submittedOnDailyAt":"2026-01-22T13:34:36.530Z","title":"Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking","submittedOnDailyBy":{"_id":"6698cffdb2ebada9f4a7e7d7","avatarUrl":"/avatars/e66d946c14595d3b008185f2be8d2f57.svg","isPro":false,"fullname":"Greta Warren","user":"gretawarren","type":"user"},"summary":"Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is a key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice.","upvotes":1,"discussionId":"69724a26fb12c92b735b72ee","organization":{"_id":"60a63ea08b5faf05f3c4d761","name":"copenlu","fullname":"CopeNLU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1621507708679-608918b7df398c3b285ce960.png"}},"publishedAt":"2026-01-16T10:55:31.000Z","title":"Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking","summary":"Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is a key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11387.png","numComments":1,"submittedBy":{"_id":"6698cffdb2ebada9f4a7e7d7","avatarUrl":"/avatars/e66d946c14595d3b008185f2be8d2f57.svg","fullname":"Greta Warren","name":"gretawarren","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"60a63ea08b5faf05f3c4d761","name":"copenlu","fullname":"CopeNLU","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1621507708679-608918b7df398c3b285ce960.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.14253","authors":[{"_id":"69706031a8be625b19c2af88","name":"Hongyuan Chen","hidden":false},{"_id":"69706031a8be625b19c2af89","name":"Xingyu Chen","hidden":false},{"_id":"69706031a8be625b19c2af8a","name":"Youjia Zhang","hidden":false},{"_id":"69706031a8be625b19c2af8b","name":"Zexiang Xu","hidden":false},{"_id":"69706031a8be625b19c2af8c","name":"Anpei Chen","hidden":false}],"publishedAt":"2026-01-20T18:59:48.000Z","submittedOnDailyAt":"2026-01-22T16:03:28.797Z","title":"Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis","submittedOnDailyBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","isPro":true,"fullname":"AK","user":"akhaliq","type":"user"},"summary":"We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.","upvotes":0,"discussionId":"69706031a8be625b19c2af8d","ai_summary":"Motion 3-to-4 synthesizes high-quality 4D dynamic objects from monocular video and 3D mesh by decomposing into static shape generation and motion reconstruction with canonical mesh and transformer-based frame processing.","ai_keywords":["4D dynamic objects","monocular video","3D reference mesh","canonical reference mesh","motion latent representation","vertex trajectories","temporally coherent geometry","frame-wise transformer"]},"publishedAt":"2026-01-20T13:59:48.000Z","title":"Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis","summary":"We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14253.png","numComments":1,"submittedBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","fullname":"AK","name":"akhaliq","type":"user","isPro":true,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":9138,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.13262","authors":[{"_id":"69713325c1c7409747bf9491","user":{"_id":"679a568f79c779b5238c3d71","avatarUrl":"/avatars/649d978e56ebbed4370755f459d74fc3.svg","isPro":false,"fullname":"Eric Onyame","user":"EricOnyame","type":"user"},"name":"Eric Onyame","status":"claimed_verified","statusLastChangedAt":"2026-01-22T08:45:41.805Z","hidden":false},{"_id":"69713325c1c7409747bf9492","name":"Akash Ghosh","hidden":false},{"_id":"69713325c1c7409747bf9493","name":"Subhadip Baidya","hidden":false},{"_id":"69713325c1c7409747bf9494","name":"Sriparna Saha","hidden":false},{"_id":"69713325c1c7409747bf9495","name":"Xiuying Chen","hidden":false},{"_id":"69713325c1c7409747bf9496","name":"Chirag Agarwal","hidden":false}],"publishedAt":"2026-01-19T17:51:00.000Z","submittedOnDailyAt":"2026-01-22T14:45:47.723Z","title":"CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning","submittedOnDailyBy":{"_id":"679a568f79c779b5238c3d71","avatarUrl":"/avatars/649d978e56ebbed4370755f459d74fc3.svg","isPro":false,"fullname":"Eric Onyame","user":"EricOnyame","type":"user"},"summary":"While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/","upvotes":0,"discussionId":"69713325c1c7409747bf9497","projectPage":"https://cure-med.github.io/","githubRepo":"https://github.com/AikyamLab/cure-med","githubRepoAddedBy":"user","ai_summary":"A multilingual medical reasoning framework using curriculum-informed reinforcement learning achieves high language consistency and logical correctness across thirteen languages including underrepresented ones.","ai_keywords":["large language models","multilingual medical reasoning","CUREMED-BENCH","curriculum-informed reinforcement learning","code-switching-aware supervised fine-tuning","Group Relative Policy Optimization","logical correctness","language consistency"],"githubStars":3,"organization":{"_id":"68004d07890ea6755b8652ab","name":"UVASDS","fullname":"UVA Data Science","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64832043cb811166c6a2f4e3/iLUsOgUbsv6N0rFUSSzfF.png"}},"publishedAt":"2026-01-19T12:51:00.000Z","title":"CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning","summary":"While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13262.png","numComments":1,"submittedBy":{"_id":"679a568f79c779b5238c3d71","avatarUrl":"/avatars/649d978e56ebbed4370755f459d74fc3.svg","fullname":"Eric Onyame","name":"EricOnyame","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"68004d07890ea6755b8652ab","name":"UVASDS","fullname":"UVA Data Science","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64832043cb811166c6a2f4e3/iLUsOgUbsv6N0rFUSSzfF.png"},"isAuthorParticipating":true}]
[{"paper":{"id":"2601.21558","authors":[{"_id":"697c279ea67238fac88cc104","user":{"_id":"621499d72be42a56cca7afad","avatarUrl":"/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg","isPro":false,"fullname":"TianXiaoyu","user":"Emperorizzis","type":"user"},"name":"Xiaoyu Tian","status":"claimed_verified","statusLastChangedAt":"2026-02-02T17:00:01.968Z","hidden":false},{"_id":"697c279ea67238fac88cc105","name":"Haotian Wang","hidden":false},{"_id":"697c279ea67238fac88cc106","name":"Shuaiting Chen","hidden":false},{"_id":"697c279ea67238fac88cc107","name":"Hao Zhou","hidden":false},{"_id":"697c279ea67238fac88cc108","name":"Kaichi Yu","hidden":false},{"_id":"697c279ea67238fac88cc109","name":"Yudian Zhang","hidden":false},{"_id":"697c279ea67238fac88cc10a","user":{"_id":"690189db2b5a1d242306b77f","avatarUrl":"/avatars/b0297c57395029da2725758671952bb6.svg","isPro":false,"fullname":"jade ouyang","user":"jade0101","type":"user"},"name":"Jade Ouyang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:59:59.965Z","hidden":false},{"_id":"697c279ea67238fac88cc10b","name":"Junxi Yin","hidden":false},{"_id":"697c279ea67238fac88cc10c","name":"Jiong Chen","hidden":false},{"_id":"697c279ea67238fac88cc10d","name":"Baoyan Guo","hidden":false},{"_id":"697c279ea67238fac88cc10e","name":"Lei Zhang","hidden":false},{"_id":"697c279ea67238fac88cc10f","name":"Junjie Tao","hidden":false},{"_id":"697c279ea67238fac88cc110","name":"Yuansheng Song","hidden":false},{"_id":"697c279ea67238fac88cc111","name":"Ming Cui","hidden":false},{"_id":"697c279ea67238fac88cc112","name":"Chengwei Liu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"],"publishedAt":"2026-01-29T11:22:23.000Z","submittedOnDailyAt":"2026-02-02T00:08:03.322Z","title":"ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas","submittedOnDailyBy":{"_id":"621499d72be42a56cca7afad","avatarUrl":"/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg","isPro":false,"fullname":"TianXiaoyu","user":"Emperorizzis","type":"user"},"summary":"Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.","upvotes":45,"discussionId":"697c279fa67238fac88cc113","githubRepo":"https://github.com/LianjiaTech/astra","githubRepoAddedBy":"user","ai_summary":"ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.","ai_keywords":["tool-call graphs","trajectory-level rewards","supervised fine-tuning","reinforcement learning","agent training","multi-step decision making","verifiable environments","compositional topology","semantic reasoning","tool-augmented language models"],"githubStars":84},"publishedAt":"2026-01-29T06:22:23.000Z","title":"ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas","summary":"Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21558.png","numComments":3,"submittedBy":{"_id":"621499d72be42a56cca7afad","avatarUrl":"/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg","fullname":"TianXiaoyu","name":"Emperorizzis","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22813","authors":[{"_id":"6980c289020e48d648c5d3ca","user":{"_id":"623753b5eddd7763adc9346a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg","isPro":false,"fullname":"Andrei Panferov","user":"BlackSamorez","type":"user"},"name":"Andrei Panferov","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:03.673Z","hidden":false},{"_id":"6980c289020e48d648c5d3cb","name":"Erik Schultheis","hidden":false},{"_id":"6980c289020e48d648c5d3cc","user":{"_id":"632a2e325f2ff1958c0103be","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg","isPro":false,"fullname":"Rush Tabesh","user":"soroushtabesh","type":"user"},"name":"Soroush Tabesh","status":"claimed_verified","statusLastChangedAt":"2026-02-02T15:43:33.368Z","hidden":false},{"_id":"6980c289020e48d648c5d3cd","name":"Dan Alistarh","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/ABDboVhzjTdRyUdiu_quW.png"],"publishedAt":"2026-01-30T10:39:11.000Z","submittedOnDailyAt":"2026-02-02T13:00:36.911Z","title":"Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation","submittedOnDailyBy":{"_id":"623753b5eddd7763adc9346a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg","isPro":false,"fullname":"Andrei Panferov","user":"BlackSamorez","type":"user"},"summary":"The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .","upvotes":41,"discussionId":"6980c289020e48d648c5d3ce","githubRepo":"https://github.com/IST-DASLab/Quartet-II","githubRepoAddedBy":"user","ai_summary":"Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.","ai_keywords":["NVFP4","stochastic rounding","quantized training","micro-scaled formats","MS-EDEN","Quartet II","linear layers","gradient estimation","matrix multiplications","LLM training","NVIDIA Blackwell GPUs"],"githubStars":5,"organization":{"_id":"64d0ffde9cff738203a50e9b","name":"ISTA-DASLab","fullname":" IST Austria Distributed Algorithms and Systems Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"}},"publishedAt":"2026-01-30T05:39:11.000Z","title":"Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation","summary":"The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/ABDboVhzjTdRyUdiu_quW.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22813.png","numComments":1,"submittedBy":{"_id":"623753b5eddd7763adc9346a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg","fullname":"Andrei Panferov","name":"BlackSamorez","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":41,"isUserFollowing":false},"organization":{"_id":"64d0ffde9cff738203a50e9b","name":"ISTA-DASLab","fullname":" IST Austria Distributed Algorithms and Systems Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.22975","authors":[{"_id":"6980a86d6676f9332270678f","name":"Ximing Lu","hidden":false},{"_id":"6980a86d6676f93322706790","name":"David Acuna","hidden":false},{"_id":"6980a86d6676f93322706791","name":"Jaehun Jung","hidden":false},{"_id":"6980a86d6676f93322706792","name":"Jian Hu","hidden":false},{"_id":"6980a86d6676f93322706793","user":{"_id":"64bce15bafd1e46c5504ad38","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png","isPro":false,"fullname":"Di Zhang","user":"di-zhang-fdu","type":"user"},"name":"Di Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:05.846Z","hidden":false},{"_id":"6980a86d6676f93322706794","name":"Shizhe Diao","hidden":false},{"_id":"6980a86d6676f93322706795","name":"Yunheng Zou","hidden":false},{"_id":"6980a86d6676f93322706796","name":"Shaokun Zhang","hidden":false},{"_id":"6980a86d6676f93322706797","name":"Brandon Cui","hidden":false},{"_id":"6980a86d6676f93322706798","name":"Mingjie Liu","hidden":false},{"_id":"6980a86d6676f93322706799","name":"Hyunwoo Kim","hidden":false},{"_id":"6980a86d6676f9332270679a","name":"Prithviraj Ammanabrolu","hidden":false},{"_id":"6980a86d6676f9332270679b","name":"Jan Kautz","hidden":false},{"_id":"6980a86d6676f9332270679c","name":"Yi Dong","hidden":false},{"_id":"6980a86d6676f9332270679d","name":"Yejin Choi","hidden":false}],"publishedAt":"2026-01-30T13:39:11.000Z","submittedOnDailyAt":"2026-02-02T11:10:15.313Z","title":"Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text","submittedOnDailyBy":{"_id":"640928bd3461c51cf7378707","avatarUrl":"/avatars/b29fcb7388b81f8686086352f6321d06.svg","isPro":false,"fullname":"Ximing Lu","user":"Ximing","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.","upvotes":34,"discussionId":"6980a86d6676f9332270679e","ai_summary":"Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","Large Language Models","fill-in-the-middle task","multiple-choice question-answering","unverifiable corpora","GooseReason-0.7M","continuous RL","Qwen3-4B-Instruct","GooseReason-Cyber"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-01-30T08:39:11.000Z","title":"Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22975.png","numComments":2,"submittedBy":{"_id":"640928bd3461c51cf7378707","avatarUrl":"/avatars/b29fcb7388b81f8686086352f6321d06.svg","fullname":"Ximing Lu","name":"Ximing","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.23143","authors":[{"_id":"69801ae26676f9332270659b","user":{"_id":"64ad5f59b7e4b2c1ce47eb43","avatarUrl":"/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg","isPro":false,"fullname":"Seanie Lee","user":"Seanie-lee","type":"user"},"name":"Seanie Lee","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:56.623Z","hidden":false},{"_id":"69801ae26676f9332270659c","user":{"_id":"638716c14e00d7fc0902fef4","avatarUrl":"/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg","isPro":false,"fullname":"Sangwoo Park","user":"Sangsang","type":"user"},"name":"Sangwoo Park","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:59.260Z","hidden":false},{"_id":"69801ae26676f9332270659d","user":{"_id":"64cfa0b9749587dbe01d0079","avatarUrl":"/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg","isPro":false,"fullname":"Yumin Choi","user":"YuminChoi","type":"user"},"name":"Yumin Choi","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:54.423Z","hidden":false},{"_id":"69801ae26676f9332270659e","name":"Gyeongman Kim","hidden":false},{"_id":"69801ae26676f9332270659f","name":"Minki Kang","hidden":false},{"_id":"69801ae26676f933227065a0","name":"Jihun Yun","hidden":false},{"_id":"69801ae26676f933227065a1","name":"Dongmin Park","hidden":false},{"_id":"69801ae26676f933227065a2","name":"Jongho Park","hidden":false},{"_id":"69801ae26676f933227065a3","name":"Sung Ju Hwang","hidden":false}],"publishedAt":"2026-01-30T16:31:02.000Z","submittedOnDailyAt":"2026-02-02T02:09:13.735Z","title":"THINKSAFE: Self-Generated Safety Alignment for Reasoning Models","submittedOnDailyBy":{"_id":"638716c14e00d7fc0902fef4","avatarUrl":"/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg","isPro":false,"fullname":"Sangwoo Park","user":"Sangsang","type":"user"},"summary":"Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.","upvotes":32,"discussionId":"69801ae26676f933227065a4","githubRepo":"https://github.com/seanie12/ThinkSafe.git","githubRepoAddedBy":"user","ai_summary":"ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.","ai_keywords":["reinforcement learning","chain-of-thought reasoning","external teacher distillation","distributional discrepancy","lightweight refusal steering","self-generated alignment","safety alignment","reasoning proficiency","computational cost"],"githubStars":3},"publishedAt":"2026-01-30T11:31:02.000Z","title":"THINKSAFE: Self-Generated Safety Alignment for Reasoning Models","summary":"Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23143.png","numComments":2,"submittedBy":{"_id":"638716c14e00d7fc0902fef4","avatarUrl":"/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg","fullname":"Sangwoo Park","name":"Sangsang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":13,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22628","authors":[{"_id":"6980103b6676f9332270654a","user":{"_id":"681ab9d3d7dbd87287875667","avatarUrl":"/avatars/1a2785d7a250c4988b1c1c5cc78e53fc.svg","isPro":false,"fullname":"ChengyiYang","user":"ChengyiYang","type":"user"},"name":"Chengyi Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:56:38.598Z","hidden":false},{"_id":"6980103b6676f9332270654b","name":"Zhishang Xiang","hidden":false},{"_id":"6980103b6676f9332270654c","name":"Yunbo Tang","hidden":false},{"_id":"6980103b6676f9332270654d","name":"Zongpei Teng","hidden":false},{"_id":"6980103b6676f9332270654e","name":"Chengsong Huang","hidden":false},{"_id":"6980103b6676f9332270654f","name":"Fei Long","hidden":false},{"_id":"6980103b6676f93322706550","name":"Yuhan Liu","hidden":false},{"_id":"6980103b6676f93322706551","name":"Jinsong Su","hidden":false}],"publishedAt":"2026-01-30T06:38:02.000Z","submittedOnDailyAt":"2026-02-02T00:37:03.238Z","title":"TTCS: Test-Time Curriculum Synthesis for Self-Evolving","submittedOnDailyBy":{"_id":"62ea79dd01ed9b0e8f61ccd3","avatarUrl":"/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg","isPro":false,"fullname":"Chengsong Huang","user":"ChengsongHuang","type":"user"},"summary":"Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.","upvotes":24,"discussionId":"6980103b6676f93322706552","githubRepo":"https://github.com/XMUDeepLIT/TTCS","githubRepoAddedBy":"user","ai_summary":"TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.","ai_keywords":["test-time training","large language models","pseudo-labels","self-consistency rewards","question synthesizer","reasoning solver","iterative optimization","test-time curricula","mathematical benchmarks","general-domain tasks"],"githubStars":19},"publishedAt":"2026-01-30T01:38:02.000Z","title":"TTCS: Test-Time Curriculum Synthesis for Self-Evolving","summary":"Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22628.png","numComments":2,"submittedBy":{"_id":"62ea79dd01ed9b0e8f61ccd3","avatarUrl":"/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg","fullname":"Chengsong Huang","name":"ChengsongHuang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.21192","authors":[{"_id":"697cae786676f933227060de","user":{"_id":"66a36281e5b14ef0e6d3befa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gejfR0L2VGYWBvVfHTyC6.png","isPro":false,"fullname":"Lucas Chan","user":"lucaswychan","type":"user"},"name":"Wun Yu Chan","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:58:57.137Z","hidden":false},{"_id":"697cae786676f933227060df","name":"Shaojin Chen","hidden":false},{"_id":"697cae786676f933227060e0","name":"Huihao Jing","hidden":false},{"_id":"697cae786676f933227060e1","name":"Kwun Hang Lau","hidden":false},{"_id":"697cae786676f933227060e2","name":"Elton Chun-Chai Li","hidden":false},{"_id":"697cae786676f933227060e3","name":"Zihao Wang","hidden":false},{"_id":"697cae786676f933227060e4","name":"Haoran Li","hidden":false},{"_id":"697cae786676f933227060e5","name":"Yangqiu Song","hidden":false}],"publishedAt":"2026-01-29T02:48:34.000Z","submittedOnDailyAt":"2026-02-02T14:43:15.103Z","title":"Do Reasoning Models Enhance Embedding Models?","submittedOnDailyBy":{"_id":"66a36281e5b14ef0e6d3befa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gejfR0L2VGYWBvVfHTyC6.png","isPro":false,"fullname":"Lucas Chan","user":"lucaswychan","type":"user"},"summary":"State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.","upvotes":22,"discussionId":"697cae796676f933227060e6","githubRepo":"https://github.com/HKUST-KnowComp/Reasoning-Embedding","githubRepoAddedBy":"user","ai_summary":"Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.","ai_keywords":["embedding models","decoder-only LLM","contrastive learning","Reinforcement Learning with Verifiable Rewards","MTEB","BRIGHT","hierarchical representation similarity analysis","latent manifold","global manifold geometry","local geometry reorganization","coordinate basis drift","Manifold Realignment","supervised fine-tuning"],"githubStars":5},"publishedAt":"2026-01-28T21:48:34.000Z","title":"Do Reasoning Models Enhance Embedding Models?","summary":"State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21192.png","numComments":1,"submittedBy":{"_id":"66a36281e5b14ef0e6d3befa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gejfR0L2VGYWBvVfHTyC6.png","fullname":"Lucas Chan","name":"lucaswychan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.23265","authors":[{"_id":"698024266676f933227065e5","name":"Dawei Zhu","hidden":false},{"_id":"698024266676f933227065e6","name":"Rui Meng","hidden":false},{"_id":"698024266676f933227065e7","name":"Yale Song","hidden":false},{"_id":"698024266676f933227065e8","name":"Xiyu Wei","hidden":false},{"_id":"698024266676f933227065e9","name":"Sujian Li","hidden":false},{"_id":"698024266676f933227065ea","name":"Tomas Pfister","hidden":false},{"_id":"698024266676f933227065eb","name":"Jinsung Yoon","hidden":false}],"publishedAt":"2026-01-30T18:33:37.000Z","submittedOnDailyAt":"2026-02-02T01:42:31.514Z","title":"PaperBanana: Automating Academic Illustration for AI Scientists","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.","upvotes":19,"discussionId":"698024276676f933227065ec","projectPage":"https://dwzhu-pku.github.io/PaperBanana/","ai_summary":"_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.","ai_keywords":["VLMs","image generation models","agentic framework","publication-ready illustrations","methodology diagrams","PaperBananaBench","self-critique","statistical plots"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-01-30T13:33:37.000Z","title":"PaperBanana: Automating Academic Illustration for AI Scientists","summary":"Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23265.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":224,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.23182","authors":[{"_id":"6980487a6676f933227066aa","name":"Siyang He","hidden":false},{"_id":"6980487a6676f933227066ab","name":"Qiqi Wang","hidden":false},{"_id":"6980487a6676f933227066ac","user":{"_id":"64f033ef82c6eea604c4da8b","avatarUrl":"/avatars/51b93fea7fd68b4274ee03701245dcca.svg","isPro":false,"fullname":"Xiaoran Liu (SII)","user":"SII-xrliu","type":"user"},"name":"Xiaoran Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:14.672Z","hidden":false},{"_id":"6980487a6676f933227066ad","name":"Hongnan Ma","hidden":false},{"_id":"6980487a6676f933227066ae","name":"Yiwei Shi","hidden":false},{"_id":"6980487a6676f933227066af","name":"Yuerong Song","hidden":false},{"_id":"6980487a6676f933227066b0","name":"Ying Zhu","hidden":false},{"_id":"6980487a6676f933227066b1","name":"Tianyi Liang","hidden":false},{"_id":"6980487a6676f933227066b2","name":"Zengfeng Huang","hidden":false},{"_id":"6980487a6676f933227066b3","name":"Ziwei He","hidden":false},{"_id":"6980487a6676f933227066b4","name":"Xipeng Qiu","hidden":false}],"publishedAt":"2026-01-30T17:06:41.000Z","submittedOnDailyAt":"2026-02-02T04:18:39.975Z","title":"FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation","submittedOnDailyBy":{"_id":"64f033ef82c6eea604c4da8b","avatarUrl":"/avatars/51b93fea7fd68b4274ee03701245dcca.svg","isPro":false,"fullname":"Xiaoran Liu (SII)","user":"SII-xrliu","type":"user"},"summary":"Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.","upvotes":18,"discussionId":"6980487b6676f933227066b5","githubRepo":"https://github.com/ShirleYoung/FourierSampler","githubRepoAddedBy":"user","ai_summary":"Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.","ai_keywords":["diffusion language models","spectral characteristics","frequency-domain analysis","hidden states","low-frequency components","high-frequency components","FourierSampler","frequency-domain sliding window mechanism","arbitrary generation","positional bias"],"githubStars":2,"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}},"publishedAt":"2026-01-30T12:06:41.000Z","title":"FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation","summary":"Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23182.png","numComments":1,"submittedBy":{"_id":"64f033ef82c6eea604c4da8b","avatarUrl":"/avatars/51b93fea7fd68b4274ee03701245dcca.svg","fullname":"Xiaoran Liu (SII)","name":"SII-xrliu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":13,"isUserFollowing":false},"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.23184","authors":[{"_id":"6980247b6676f933227065ee","user":{"_id":"6912d19a10fbcbf70dedf126","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6912d19a10fbcbf70dedf126/XYstDl622FMSbFRQ9Ab6o.jpeg","isPro":false,"fullname":"Fanmeng Wang","user":"FanmengWang","type":"user"},"name":"Fanmeng Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:37.097Z","hidden":false},{"_id":"6980247b6676f933227065ef","name":"Haotian Liu","hidden":false},{"_id":"6980247b6676f933227065f0","name":"Guojiang Zhao","hidden":false},{"_id":"6980247b6676f933227065f1","name":"Hongteng Xu","hidden":false},{"_id":"6980247b6676f933227065f2","name":"Zhifeng Gao","hidden":false}],"publishedAt":"2026-01-30T17:08:06.000Z","submittedOnDailyAt":"2026-02-02T01:44:01.004Z","title":"ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.","upvotes":12,"discussionId":"6980247b6676f933227065f3","githubRepo":"https://github.com/FanmengWang/ReGuLaR","githubRepoAddedBy":"user","ai_summary":"ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.","ai_keywords":["Chain-of-Thought","Large Language Models","latent reasoning","Variational Auto-Encoding","posterior distribution","visual-semantic representations","multi-modal reasoning"],"githubStars":15},"publishedAt":"2026-01-30T12:08:06.000Z","title":"ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought","summary":"While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23184.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":224,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.21998","authors":[{"_id":"6980d46983fdbbe1963c2ce6","name":"Lin Li","hidden":false},{"_id":"6980d46983fdbbe1963c2ce7","name":"Qihang Zhang","hidden":false},{"_id":"6980d46983fdbbe1963c2ce8","name":"Yiming Luo","hidden":false},{"_id":"6980d46983fdbbe1963c2ce9","name":"Shuai Yang","hidden":false},{"_id":"6980d46983fdbbe1963c2cea","name":"Ruilin Wang","hidden":false},{"_id":"6980d46983fdbbe1963c2ceb","name":"Fei Han","hidden":false},{"_id":"6980d46983fdbbe1963c2cec","name":"Mingrui Yu","hidden":false},{"_id":"6980d46983fdbbe1963c2ced","name":"Zelin Gao","hidden":false},{"_id":"6980d46983fdbbe1963c2cee","name":"Nan Xue","hidden":false},{"_id":"6980d46983fdbbe1963c2cef","name":"Xing Zhu","hidden":false},{"_id":"6980d46983fdbbe1963c2cf0","name":"Yujun Shen","hidden":false},{"_id":"6980d46983fdbbe1963c2cf1","name":"Yinghao Xu","hidden":false}],"publishedAt":"2026-01-29T17:07:43.000Z","submittedOnDailyAt":"2026-02-02T14:14:54.399Z","title":"Causal World Modeling for Robot Control","submittedOnDailyBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","isPro":true,"fullname":"AK","user":"akhaliq","type":"user"},"summary":"This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.","upvotes":12,"discussionId":"6980d46983fdbbe1963c2cf2","ai_summary":"Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.","ai_keywords":["video world modeling","autoregressive diffusion framework","frame prediction","policy execution","shared latent space","Mixture-of-Transformers","closed-loop rollout mechanism","asynchronous inference pipeline","long-horizon manipulation","data efficiency","generalizability"]},"publishedAt":"2026-01-29T12:07:43.000Z","title":"Causal World Modeling for Robot Control","summary":"This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21998.png","numComments":1,"submittedBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","fullname":"AK","name":"akhaliq","type":"user","isPro":true,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":9203,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.21716","authors":[{"_id":"697d68416676f933227061e4","name":"Mingshuang Luo","hidden":false},{"_id":"697d68416676f933227061e5","name":"Shuang Liang","hidden":false},{"_id":"697d68416676f933227061e6","name":"Zhengkun Rong","hidden":false},{"_id":"697d68416676f933227061e7","name":"Yuxuan Luo","hidden":false},{"_id":"697d68416676f933227061e8","name":"Tianshu Hu","hidden":false},{"_id":"697d68416676f933227061e9","name":"Ruibing Hou","hidden":false},{"_id":"697d68416676f933227061ea","name":"Hong Chang","hidden":false},{"_id":"697d68416676f933227061eb","name":"Yong Li","hidden":false},{"_id":"697d68416676f933227061ec","name":"Yuan Zhang","hidden":false},{"_id":"697d68416676f933227061ed","name":"Mingyuan Gao","hidden":false}],"publishedAt":"2026-01-29T13:43:17.000Z","submittedOnDailyAt":"2026-02-02T00:12:40.328Z","title":"DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning","submittedOnDailyBy":{"_id":"6178ea05267320abb99df778","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1636104707106-6178ea05267320abb99df778.jpeg","isPro":false,"fullname":"Mingshuang Luo","user":"luomingshuang","type":"user"},"summary":"Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/","upvotes":12,"discussionId":"697d68426676f933227061ee","projectPage":"https://grisoon.github.io/DreamActor-M2/","ai_summary":"DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.","ai_keywords":["motion conditioning","in-context learning","latent space","generative prior","self-bootstrapped data synthesis","cross-identity training pairs","end-to-end RGB-driven animation","cross-domain generalization"],"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}},"publishedAt":"2026-01-29T08:43:17.000Z","title":"DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning","summary":"Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21716.png","numComments":1,"submittedBy":{"_id":"6178ea05267320abb99df778","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1636104707106-6178ea05267320abb99df778.jpeg","fullname":"Mingshuang Luo","name":"luomingshuang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"653b817d32c97d0655575872","name":"ByteDance","fullname":"ByteDance","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.22491","authors":[{"_id":"69801f416676f933227065a6","name":"Jinyang Wu","hidden":false},{"_id":"69801f416676f933227065a7","user":{"_id":"65d83362984cc240f2241e3a","avatarUrl":"/avatars/1f922987d7d69f553bb672c4d26ceef6.svg","isPro":false,"fullname":"Changpeng Yang","user":"thkelper","type":"user"},"name":"Changpeng Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:51.129Z","hidden":false},{"_id":"69801f416676f933227065a8","name":"Yuhao Shen","hidden":false},{"_id":"69801f416676f933227065a9","name":"Fangzhi Xu","hidden":false},{"_id":"69801f416676f933227065aa","name":"Bolin Ni","hidden":false},{"_id":"69801f416676f933227065ab","name":"Chonghua Liao","hidden":false},{"_id":"69801f416676f933227065ac","name":"Yuchen Liu","hidden":false},{"_id":"69801f416676f933227065ad","name":"Hongzhen Wang","hidden":false},{"_id":"69801f416676f933227065ae","name":"Shuai Nie","hidden":false},{"_id":"69801f416676f933227065af","name":"Shuai Zhang","hidden":false},{"_id":"69801f416676f933227065b0","name":"Haoran Luo","hidden":false},{"_id":"69801f416676f933227065b1","name":"Jiaming Xu","hidden":false}],"publishedAt":"2026-01-30T03:02:18.000Z","submittedOnDailyAt":"2026-02-02T01:24:32.985Z","title":"SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization","submittedOnDailyBy":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","isPro":false,"fullname":"Jinyang Wu","user":"Jinyang23","type":"user"},"summary":"Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.","upvotes":11,"discussionId":"69801f416676f933227065b2","ai_summary":"Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.","ai_keywords":["reinforcement learning","verifiable rewards","trajectory optimization","policy optimization","gradient signal-to-noise ratio","sample efficiency","cross-task transferability"]},"publishedAt":"2026-01-29T22:02:18.000Z","title":"SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization","summary":"Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22491.png","numComments":1,"submittedBy":{"_id":"6747de57f8cab58c22ec94a2","avatarUrl":"/avatars/5bae0341862fac24564781c0fa32aac5.svg","fullname":"Jinyang Wu","name":"Jinyang23","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.20218","authors":[{"_id":"697b3e53870173bd91777369","user":{"_id":"665fce33b99c631f4f57e650","avatarUrl":"/avatars/dc98235d6e4e33e5980c2b46627c238b.svg","isPro":false,"fullname":"Haoyou Deng","user":"haoyou11","type":"user"},"name":"Haoyou Deng","status":"claimed_verified","statusLastChangedAt":"2026-01-29T13:56:28.058Z","hidden":false},{"_id":"697b3e53870173bd9177736a","name":"Keyu Yan","hidden":false},{"_id":"697b3e53870173bd9177736b","name":"Chaojie Mao","hidden":false},{"_id":"697b3e53870173bd9177736c","name":"Xiang Wang","hidden":false},{"_id":"697b3e53870173bd9177736d","name":"Yu Liu","hidden":false},{"_id":"697b3e53870173bd9177736e","name":"Changxin Gao","hidden":false},{"_id":"697b3e53870173bd9177736f","name":"Nong Sang","hidden":false}],"publishedAt":"2026-01-28T03:39:05.000Z","submittedOnDailyAt":"2026-02-02T00:05:08.557Z","title":"DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment","submittedOnDailyBy":{"_id":"665fce33b99c631f4f57e650","avatarUrl":"/avatars/dc98235d6e4e33e5980c2b46627c238b.svg","isPro":false,"fullname":"Haoyou Deng","user":"haoyou11","type":"user"},"summary":"Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.","upvotes":10,"discussionId":"697b3e54870173bd91777370","ai_summary":"DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.","ai_keywords":["flow matching models","denoising trajectory","sparse reward problem","dense rewards","step-wise reward gain","reward model","ODE-based approach","SDE sampler","stochasticity injection","time-varying noise intensity","reward-aware scheme","exploration space"],"organization":{"_id":"67d15cca6e2cf0e062dbfb54","name":"AlibabaTongyiLab","fullname":"TongyiLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}},"publishedAt":"2026-01-27T22:39:05.000Z","title":"DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment","summary":"Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20218.png","numComments":1,"submittedBy":{"_id":"665fce33b99c631f4f57e650","avatarUrl":"/avatars/dc98235d6e4e33e5980c2b46627c238b.svg","fullname":"Haoyou Deng","name":"haoyou11","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67d15cca6e2cf0e062dbfb54","name":"AlibabaTongyiLab","fullname":"TongyiLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.22636","authors":[{"_id":"6980426e6676f93322706680","name":"Mingqian Feng","hidden":false},{"_id":"6980426e6676f93322706681","name":"Xiaodong Liu","hidden":false},{"_id":"6980426e6676f93322706682","name":"Weiwei Yang","hidden":false},{"_id":"6980426e6676f93322706683","name":"Chenliang Xu","hidden":false},{"_id":"6980426e6676f93322706684","name":"Christopher White","hidden":false},{"_id":"6980426e6676f93322706685","name":"Jianfeng Gao","hidden":false}],"publishedAt":"2026-01-30T06:54:35.000Z","submittedOnDailyAt":"2026-02-02T03:54:34.923Z","title":"Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling","submittedOnDailyBy":{"_id":"66332a98e39731d65a4b7e45","avatarUrl":"/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg","isPro":true,"fullname":"Mingqian Feng","user":"fmmarkmq","type":"user"},"summary":"Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.","upvotes":8,"discussionId":"6980426e6676f93322706686","ai_summary":"A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.","ai_keywords":["large language models","adversarial prompting","Best-of-N sampling","jailbreak vulnerability","Beta distribution","scaling law","attack success rate","parallel sampling","risk estimation"],"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}},"publishedAt":"2026-01-30T01:54:35.000Z","title":"Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling","summary":"Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22636.png","numComments":1,"submittedBy":{"_id":"66332a98e39731d65a4b7e45","avatarUrl":"/avatars/193e9ee3ac7488960229d6edddb9d1e9.svg","fullname":"Mingqian Feng","name":"fmmarkmq","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21957","authors":[{"_id":"697c2ae8a67238fac88cc135","user":{"_id":"65a5231a087d8a2e9cc2414b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a5231a087d8a2e9cc2414b/wj0l5R5LmBUG-E8XTMdBM.jpeg","isPro":false,"fullname":"cuicheng","user":"ChengCui","type":"user"},"name":"Cheng Cui","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:59:40.473Z","hidden":false},{"_id":"697c2ae8a67238fac88cc136","user":{"_id":"63d7b8ee07cd1aa3c49a2026","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63d7b8ee07cd1aa3c49a2026/VdFETZ7zQKwkQhhahzW3i.jpeg","isPro":false,"fullname":"Ting Sun","user":"sunflowerting78","type":"user"},"name":"Ting Sun","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:59:37.597Z","hidden":false},{"_id":"697c2ae8a67238fac88cc137","name":"Suyin Liang","hidden":false},{"_id":"697c2ae8a67238fac88cc138","name":"Tingquan Gao","hidden":false},{"_id":"697c2ae8a67238fac88cc139","name":"Zelun Zhang","hidden":false},{"_id":"697c2ae8a67238fac88cc13a","name":"Jiaxuan Liu","hidden":false},{"_id":"697c2ae8a67238fac88cc13b","name":"Xueqing Wang","hidden":false},{"_id":"697c2ae8a67238fac88cc13c","name":"Changda Zhou","hidden":false},{"_id":"697c2ae8a67238fac88cc13d","name":"Hongen Liu","hidden":false},{"_id":"697c2ae8a67238fac88cc13e","name":"Manhui Lin","hidden":false},{"_id":"697c2ae8a67238fac88cc13f","name":"Yue Zhang","hidden":false},{"_id":"697c2ae8a67238fac88cc140","name":"Yubo Zhang","hidden":false},{"_id":"697c2ae8a67238fac88cc141","name":"Yi Liu","hidden":false},{"_id":"697c2ae8a67238fac88cc142","name":"Dianhai Yu","hidden":false},{"_id":"697c2ae8a67238fac88cc143","name":"Yanjun Ma","hidden":false}],"publishedAt":"2026-01-29T16:35:04.000Z","submittedOnDailyAt":"2026-02-02T04:03:37.670Z","title":"PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing","submittedOnDailyBy":{"_id":"65a5231a087d8a2e9cc2414b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a5231a087d8a2e9cc2414b/wj0l5R5LmBUG-E8XTMdBM.jpeg","isPro":false,"fullname":"cuicheng","user":"ChengCui","type":"user"},"summary":"We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR","upvotes":8,"discussionId":"697c2ae9a67238fac88cc144","ai_summary":"A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.","ai_keywords":["Vision-Language Model","OmniDocBench","Real5-OmniDocBench","seal recognition","text spotting"]},"publishedAt":"2026-01-29T11:35:04.000Z","title":"PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing","summary":"We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21957.png","numComments":1,"submittedBy":{"_id":"65a5231a087d8a2e9cc2414b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a5231a087d8a2e9cc2414b/wj0l5R5LmBUG-E8XTMdBM.jpeg","fullname":"cuicheng","name":"ChengCui","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":20,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.13097","authors":[{"_id":"6976fdd85d41524304c1367e","user":{"_id":"66728993cc71f0dce43fb93a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66728993cc71f0dce43fb93a/3Lf8ltxKSDOhbQxOCt-cE.jpeg","isPro":false,"fullname":"Bruches Elena","user":"brucheselena","type":"user"},"name":"Elena Bruches","status":"claimed_verified","statusLastChangedAt":"2026-02-02T17:01:03.322Z","hidden":false},{"_id":"6976fdd85d41524304c1367f","user":{"_id":"63cb976d80ba2ca4151b67a2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1675713278440-63cb976d80ba2ca4151b67a2.jpeg","isPro":false,"fullname":"Daniel Grebenkin","user":"dangrebenkin","type":"user"},"name":"Daniil Grebenkin","status":"claimed_verified","statusLastChangedAt":"2026-01-26T08:28:32.856Z","hidden":false},{"_id":"6976fdd85d41524304c13680","name":"Mikhail Klementev","hidden":false},{"_id":"6976fdd85d41524304c13681","name":"Vadim Alperovich","hidden":false},{"_id":"6976fdd85d41524304c13682","user":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","isPro":false,"fullname":"Roman Derunets","user":"rmndrnts","type":"user"},"name":"Roman Derunets","status":"claimed_verified","statusLastChangedAt":"2026-01-26T08:28:34.729Z","hidden":false},{"_id":"6976fdd85d41524304c13683","user":{"_id":"63ea1ca8afddf881179de4fe","avatarUrl":"/avatars/2690766ee09eefff296bcdac46e87b3b.svg","isPro":false,"fullname":"Dari","user":"doooori","type":"user"},"name":"Dari Baturova","status":"claimed_verified","statusLastChangedAt":"2026-02-02T17:01:01.399Z","hidden":false},{"_id":"6976fdd85d41524304c13684","name":"Georgy Mkrtchyan","hidden":false},{"_id":"6976fdd85d41524304c13685","name":"Oleg Sedukhin","hidden":false},{"_id":"6976fdd85d41524304c13686","name":"Ivan Bondarenko","hidden":false},{"_id":"6976fdd85d41524304c13687","name":"Nikolay Bushkov","hidden":false},{"_id":"6976fdd85d41524304c13688","name":"Stanislav Moiseev","hidden":false}],"publishedAt":"2026-01-19T14:37:50.000Z","submittedOnDailyAt":"2026-02-02T05:52:21.731Z","title":"RM -RF: Reward Model for Run-Free Unit Test Evaluation","submittedOnDailyBy":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","isPro":false,"fullname":"Roman Derunets","user":"rmndrnts","type":"user"},"summary":"We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.","upvotes":8,"discussionId":"6976fdd85d41524304c13689","githubRepo":"https://github.com/trndcenter/RM-RF-unit-tests","githubRepoAddedBy":"user","ai_summary":"RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.","ai_keywords":["reward model","run-free evaluation","unit tests","execution-derived signals","code coverage","mutation kill rate","multilingual dataset","focal files","test files","candidate test additions","execution-based pipeline","model families","tuning regimes","zero-shot","full fine-tuning","PEFT","LoRA","F1 score"],"githubStars":5},"publishedAt":"2026-01-19T09:37:50.000Z","title":"RM -RF: Reward Model for Run-Free Unit Test Evaluation","summary":"We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13097.png","numComments":1,"submittedBy":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","fullname":"Roman Derunets","name":"rmndrnts","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22904","authors":[{"_id":"6980232e6676f933227065c4","name":"Hun Chang","hidden":false},{"_id":"6980232e6676f933227065c5","name":"Byunghee Cha","hidden":false},{"_id":"6980232e6676f933227065c6","name":"Jong Chul Ye","hidden":false}],"publishedAt":"2026-01-30T12:25:34.000Z","submittedOnDailyAt":"2026-02-02T01:38:19.533Z","title":"DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.","upvotes":7,"discussionId":"6980232e6676f933227065c7","ai_summary":"A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.","ai_keywords":["Vision Foundation Models","DINO","generative autoencoders","contrastive representations","feature vectors","hierarchical convolutional patch embedding","cosine similarity alignment","diffusion transformer","Riemannian flow matching","hypersphere","latent manifold","reconstruction quality","semantic alignment","gFID","rFID","PSNR"]},"publishedAt":"2026-01-30T07:25:34.000Z","title":"DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation","summary":"Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22904.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":224,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.22837","authors":[{"_id":"698023a66676f933227065d2","name":"Bin Wu","hidden":false},{"_id":"698023a66676f933227065d3","name":"Mengqi Huang","hidden":false},{"_id":"698023a66676f933227065d4","name":"Weinan Jia","hidden":false},{"_id":"698023a66676f933227065d5","name":"Zhendong Mao","hidden":false}],"publishedAt":"2026-01-30T11:01:43.000Z","submittedOnDailyAt":"2026-02-02T01:40:24.437Z","title":"NativeTok: Native Visual Tokenization for Improved Image Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.","upvotes":7,"discussionId":"698023a66676f933227065d6","githubRepo":"https://github.com/wangbei1/Nativetok","githubRepoAddedBy":"user","ai_summary":"NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.","ai_keywords":["tokenization","generative model","causal dependencies","Meta Image Transformer","Mixture of Causal Expert Transformer","hierarchical training","latent image modeling"],"githubStars":4},"publishedAt":"2026-01-30T06:01:43.000Z","title":"NativeTok: Native Visual Tokenization for Improved Image Generation","summary":"VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22837.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":224,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.22642","authors":[{"_id":"698023f56676f933227065d8","user":{"_id":"64c1e3a911c6194a55d974df","avatarUrl":"/avatars/db7c37ba78b726a5646221bf9bc4cc6c.svg","isPro":false,"fullname":"Chuxue Cao","user":"chuxuecao","type":"user"},"name":"Chuxue Cao","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:39.264Z","hidden":false},{"_id":"698023f56676f933227065d9","name":"Jinluan Yang","hidden":false},{"_id":"698023f56676f933227065da","name":"Haoran Li","hidden":false},{"_id":"698023f56676f933227065db","name":"Kunhao Pan","hidden":false},{"_id":"698023f56676f933227065dc","name":"Zijian Zhao","hidden":false},{"_id":"698023f56676f933227065dd","name":"Zhengyu Chen","hidden":false},{"_id":"698023f56676f933227065de","name":"Yuchen Tian","hidden":false},{"_id":"698023f56676f933227065df","name":"Lijun Wu","hidden":false},{"_id":"698023f56676f933227065e0","name":"Conghui He","hidden":false},{"_id":"698023f56676f933227065e1","name":"Sirui Han","hidden":false},{"_id":"698023f56676f933227065e2","name":"Yike Guo","hidden":false}],"publishedAt":"2026-01-30T07:01:25.000Z","submittedOnDailyAt":"2026-02-02T02:04:09.685Z","title":"Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification","submittedOnDailyBy":{"_id":"64c1e3a911c6194a55d974df","avatarUrl":"/avatars/db7c37ba78b726a5646221bf9bc4cc6c.svg","isPro":false,"fullname":"Chuxue Cao","user":"chuxuecao","type":"user"},"summary":"Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.","upvotes":7,"discussionId":"698023f56676f933227065e3","ai_summary":"A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.","ai_keywords":["large language models","formal logic verification","symbolic verification","natural language generation","reasoning chain","supervised fine-tuning","policy optimization","mathematical reasoning","logical reasoning","general reasoning"],"organization":{"_id":"6609f50bf4ab651901ae4541","name":"hongkongust","fullname":"The Hong Kong University of Science and Technology"}},"publishedAt":"2026-01-30T02:01:25.000Z","title":"Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification","summary":"Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22642.png","numComments":1,"submittedBy":{"_id":"64c1e3a911c6194a55d974df","avatarUrl":"/avatars/db7c37ba78b726a5646221bf9bc4cc6c.svg","fullname":"Chuxue Cao","name":"chuxuecao","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6609f50bf4ab651901ae4541","name":"hongkongust","fullname":"The Hong Kong University of Science and Technology"},"isAuthorParticipating":true},{"paper":{"id":"2601.21468","authors":[{"_id":"697c7da1a67238fac88cc2d0","name":"Yaorui Shi","hidden":false},{"_id":"697c7da1a67238fac88cc2d1","name":"Shugui Liu","hidden":false},{"_id":"697c7da1a67238fac88cc2d2","name":"Yu Yang","hidden":false},{"_id":"697c7da1a67238fac88cc2d3","name":"Wenyu Mao","hidden":false},{"_id":"697c7da1a67238fac88cc2d4","name":"Yuxin Chen","hidden":false},{"_id":"697c7da1a67238fac88cc2d5","name":"Qi GU","hidden":false},{"_id":"697c7da1a67238fac88cc2d6","name":"Hui Su","hidden":false},{"_id":"697c7da1a67238fac88cc2d7","name":"Xunliang Cai","hidden":false},{"_id":"697c7da1a67238fac88cc2d8","name":"Xiang Wang","hidden":false},{"_id":"697c7da1a67238fac88cc2d9","name":"An Zhang","hidden":false}],"publishedAt":"2026-01-29T09:47:17.000Z","submittedOnDailyAt":"2026-02-02T01:45:23.265Z","title":"MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning","submittedOnDailyBy":{"_id":"63edd2d1f765928ceeb49057","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png","isPro":false,"fullname":"Yaorui SHI","user":"yrshi","type":"user"},"summary":"Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.","upvotes":7,"discussionId":"697c7da1a67238fac88cc2da","ai_summary":"MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.","ai_keywords":["memory systems","context window","visual layout","structured rich-text memory","reinforcement learning","context utilization","long-horizon reasoning"]},"publishedAt":"2026-01-29T04:47:17.000Z","title":"MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning","summary":"Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21468.png","numComments":1,"submittedBy":{"_id":"63edd2d1f765928ceeb49057","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png","fullname":"Yaorui SHI","name":"yrshi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.18241","authors":[{"_id":"697c93736676f93322706042","user":{"_id":"66728993cc71f0dce43fb93a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66728993cc71f0dce43fb93a/3Lf8ltxKSDOhbQxOCt-cE.jpeg","isPro":false,"fullname":"Bruches Elena","user":"brucheselena","type":"user"},"name":"Elena Bruches","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:59:23.851Z","hidden":false},{"_id":"697c93736676f93322706043","name":"Vadim Alperovich","hidden":false},{"_id":"697c93736676f93322706044","user":{"_id":"63ea1ca8afddf881179de4fe","avatarUrl":"/avatars/2690766ee09eefff296bcdac46e87b3b.svg","isPro":false,"fullname":"Dari","user":"doooori","type":"user"},"name":"Dari Baturova","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:59:21.364Z","hidden":false},{"_id":"697c93736676f93322706045","user":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","isPro":false,"fullname":"Roman Derunets","user":"rmndrnts","type":"user"},"name":"Roman Derunets","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:25.974Z","hidden":false},{"_id":"697c93736676f93322706046","user":{"_id":"63cb976d80ba2ca4151b67a2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1675713278440-63cb976d80ba2ca4151b67a2.jpeg","isPro":false,"fullname":"Daniel Grebenkin","user":"dangrebenkin","type":"user"},"name":"Daniil Grebenkin","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:59:26.065Z","hidden":false},{"_id":"697c93736676f93322706047","name":"Georgy Mkrtchyan","hidden":false},{"_id":"697c93736676f93322706048","name":"Oleg Sedukhin","hidden":false},{"_id":"697c93736676f93322706049","name":"Mikhail Klementev","hidden":false},{"_id":"697c93736676f9332270604a","name":"Ivan Bondarenko","hidden":false},{"_id":"697c93736676f9332270604b","name":"Nikolay Bushkov","hidden":false},{"_id":"697c93736676f9332270604c","name":"Stanislav Moiseev","hidden":false}],"publishedAt":"2026-01-26T07:47:22.000Z","submittedOnDailyAt":"2026-02-02T05:38:57.789Z","title":"TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance","submittedOnDailyBy":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","isPro":false,"fullname":"Roman Derunets","user":"rmndrnts","type":"user"},"summary":"While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.","upvotes":7,"discussionId":"697c93736676f9332270604d","githubRepo":"https://github.com/trndcenter/TAM-Eval","githubRepoAddedBy":"user","ai_summary":"TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.","ai_keywords":["Large Language Models","unit testing","test suite maintenance","test automation","test generation","oracle prediction","test file level evaluation","repository context","test suite pass rate","code coverage","mutation testing","agentic workflows","reference-free protocol"],"githubStars":6},"publishedAt":"2026-01-26T02:47:22.000Z","title":"TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance","summary":"While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18241.png","numComments":1,"submittedBy":{"_id":"6415cb01486c7c9a5d1560f3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6415cb01486c7c9a5d1560f3/tKQPhr1o-1SLSvKg3un6J.jpeg","fullname":"Roman Derunets","name":"rmndrnts","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.23161","authors":[{"_id":"698025fb6676f933227065fb","name":"Jiaming Zhou","hidden":false},{"_id":"698025fb6676f933227065fc","name":"Xuxin Cheng","hidden":false},{"_id":"698025fb6676f933227065fd","name":"Shiwan Zhao","hidden":false},{"_id":"698025fb6676f933227065fe","name":"Yuhang Jia","hidden":false},{"_id":"698025fb6676f933227065ff","name":"Cao Liu","hidden":false},{"_id":"698025fb6676f93322706600","name":"Ke Zeng","hidden":false},{"_id":"698025fb6676f93322706601","name":"Xunliang Cai","hidden":false},{"_id":"698025fb6676f93322706602","name":"Yong Qin","hidden":false}],"publishedAt":"2026-01-30T16:44:23.000Z","submittedOnDailyAt":"2026-02-02T01:50:21.212Z","title":"DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.","upvotes":6,"discussionId":"698025fc6676f93322706603","ai_summary":"DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.","ai_keywords":["autoregressive models","large audio language models","diffusion models","speech encoder","dual semantic and acoustic adapters","four-stage curriculum","semantic alignment","acoustic alignment","supervised fine-tuning","preference optimization","variance-reduced optimization"],"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}},"publishedAt":"2026-01-30T11:44:23.000Z","title":"DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding","summary":"Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23161.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":224,"isUserFollowing":false},"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.23228","authors":[{"_id":"6980147c6676f93322706566","user":{"_id":"677969a60a4d2007a86d669e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5CTwfmdOcrcK20rU5djcx.png","isPro":false,"fullname":"Ed Li","user":"edli","type":"user"},"name":"Ed Li","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:56:18.813Z","hidden":false},{"_id":"6980147c6676f93322706567","name":"Junyu Ren","hidden":false},{"_id":"6980147c6676f93322706568","name":"Cat Yan","hidden":false}],"publishedAt":"2026-01-30T17:55:27.000Z","submittedOnDailyAt":"2026-02-02T06:26:37.249Z","title":"Scaling Multiagent Systems with Process Rewards","submittedOnDailyBy":{"_id":"677969a60a4d2007a86d669e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5CTwfmdOcrcK20rU5djcx.png","isPro":false,"fullname":"Ed Li","user":"edli","type":"user"},"summary":"While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.","upvotes":5,"discussionId":"6980147c6676f93322706569","projectPage":"https://ltjed.github.io/MAPPA/","githubRepo":"https://github.com/ltjed/multiagent-coaching","githubRepoAddedBy":"user","ai_summary":"Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.","ai_keywords":["multiagent systems","credit assignment","sample efficiency","finetuning","per-action process rewards","AI feedback","competition math problems","tool-augmented data analysis"],"githubStars":43,"organization":{"_id":"6756014dd83c390221a3815c","name":"YaleUniversity","fullname":"Yale University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6755ff00d3ff7f20aad244d2/xk0r-87S0AVfUA1XX8CG3.png"}},"publishedAt":"2026-01-30T12:55:27.000Z","title":"Scaling Multiagent Systems with Process Rewards","summary":"While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23228.png","numComments":1,"submittedBy":{"_id":"677969a60a4d2007a86d669e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5CTwfmdOcrcK20rU5djcx.png","fullname":"Ed Li","name":"edli","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6756014dd83c390221a3815c","name":"YaleUniversity","fullname":"Yale University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6755ff00d3ff7f20aad244d2/xk0r-87S0AVfUA1XX8CG3.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.21358","authors":[{"_id":"69801aa26676f93322706596","user":{"_id":"652275e1d65824bda7a845db","avatarUrl":"/avatars/ae465143a480e4f739c3c4c544490f25.svg","isPro":false,"fullname":"jc","user":"yunsaijc","type":"user"},"name":"Jiecong Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:53:01.647Z","hidden":false},{"_id":"69801aa26676f93322706597","name":"Hao Peng","hidden":false},{"_id":"69801aa26676f93322706598","name":"Chunyang Liu","hidden":false}],"publishedAt":"2026-01-29T07:38:18.000Z","submittedOnDailyAt":"2026-02-02T01:03:26.097Z","title":"Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization","submittedOnDailyBy":{"_id":"652275e1d65824bda7a845db","avatarUrl":"/avatars/ae465143a480e4f739c3c4c544490f25.svg","isPro":false,"fullname":"jc","user":"yunsaijc","type":"user"},"summary":"Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.","upvotes":5,"discussionId":"69801aa26676f93322706599","ai_summary":"PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.","ai_keywords":["Chain-of-Thought","Large Language Models","latent reasoning","discrete token spaces","continuous hidden states","deterministic trajectory","latent planning states","inference-time search"],"organization":{"_id":"63ba7720fc454697637969f1","name":"Beihang","fullname":"Beihang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}},"publishedAt":"2026-01-29T02:38:18.000Z","title":"Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization","summary":"Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21358.png","numComments":1,"submittedBy":{"_id":"652275e1d65824bda7a845db","avatarUrl":"/avatars/ae465143a480e4f739c3c4c544490f25.svg","fullname":"jc","name":"yunsaijc","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"63ba7720fc454697637969f1","name":"Beihang","fullname":"Beihang University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.21419","authors":[{"_id":"697c1dfca67238fac88cc0a4","name":"Qing Jin","hidden":false},{"_id":"697c1dfca67238fac88cc0a5","name":"Chaoyang Wang","hidden":false}],"publishedAt":"2026-01-29T08:56:55.000Z","submittedOnDailyAt":"2026-02-02T04:06:18.035Z","title":"Revisiting Diffusion Model Predictions Through Dimensionality","submittedOnDailyBy":{"_id":"64c166de11f3f1d23a99c6fd","avatarUrl":"/avatars/d4730f70a6f1cb51266862c7a8d54f77.svg","isPro":false,"fullname":"Chaoyang Wang","user":"cwang9","type":"user"},"summary":"Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise (varepsilon) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which varepsilon-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.","upvotes":4,"discussionId":"697c1dfca67238fac88cc0a6","ai_summary":"Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.","ai_keywords":["diffusion models","flow matching models","noise prediction","velocity prediction","data prediction","ambient dimension","intrinsic dimension","generalized prediction formulation","k-Diff","latent-space","pixel-space","generative performance"]},"publishedAt":"2026-01-29T03:56:55.000Z","title":"Revisiting Diffusion Model Predictions Through Dimensionality","summary":"Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise (varepsilon) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which varepsilon-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21419.png","numComments":1,"submittedBy":{"_id":"64c166de11f3f1d23a99c6fd","avatarUrl":"/avatars/d4730f70a6f1cb51266862c7a8d54f77.svg","fullname":"Chaoyang Wang","name":"cwang9","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.23188","authors":[{"_id":"69802c5c6676f93322706612","name":"Zhongxiang Sun","hidden":false},{"_id":"69802c5c6676f93322706613","name":"Qipeng Wang","hidden":false},{"_id":"69802c5c6676f93322706614","name":"Weijie Yu","hidden":false},{"_id":"69802c5c6676f93322706615","name":"Jingxuan Yang","hidden":false},{"_id":"69802c5c6676f93322706616","name":"Haolang Lu","hidden":false},{"_id":"69802c5c6676f93322706617","name":"Jun Xu","hidden":false}],"publishedAt":"2026-01-30T17:10:48.000Z","submittedOnDailyAt":"2026-02-02T03:00:37.659Z","title":"Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience","submittedOnDailyBy":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","isPro":false,"fullname":"SunZX","user":"Jeryi","type":"user"},"summary":"Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.","upvotes":3,"discussionId":"69802c5c6676f93322706618","ai_summary":"Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.","ai_keywords":["deep search agents","large language models","multi-step retrieval","reasoning","long-horizon task execution","metacognition","hierarchical organization","anomaly detection","reflection","Fast Consistency Monitor","Slow Experience-Driven Monitor","corrective intervention","experience memory","agent trajectories","deep search benchmarks","backbone models"],"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}},"publishedAt":"2026-01-30T12:10:48.000Z","title":"Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience","summary":"Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23188.png","numComments":1,"submittedBy":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","fullname":"SunZX","name":"Jeryi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2601.22664","authors":[{"_id":"6980162d6676f9332270656f","user":{"_id":"6593d2329e16fa7510e0876a","avatarUrl":"/avatars/c200676dd9dc1db8a3e27388251aea49.svg","isPro":false,"fullname":"hzx","user":"hzxllll","type":"user"},"name":"Zixuan Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:56:16.769Z","hidden":false},{"_id":"6980162d6676f93322706570","name":"Xin Xia","hidden":false},{"_id":"6980162d6676f93322706571","name":"Yuxi Ren","hidden":false},{"_id":"6980162d6676f93322706572","name":"Jianbin Zheng","hidden":false},{"_id":"6980162d6676f93322706573","name":"Xuefeng Xiao","hidden":false},{"_id":"6980162d6676f93322706574","name":"Hongyan Xie","hidden":false},{"_id":"6980162d6676f93322706575","name":"Li Huaqiu","hidden":false},{"_id":"6980162d6676f93322706576","name":"Songshi Liang","hidden":false},{"_id":"6980162d6676f93322706577","name":"Zhongxiang Dai","hidden":false},{"_id":"6980162d6676f93322706578","name":"Fuzhen Zhuang","hidden":false},{"_id":"6980162d6676f93322706579","name":"Jianxin Li","hidden":false},{"_id":"6980162d6676f9332270657a","user":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun Ban","user":"Yikunb","type":"user"},"name":"Yikun Ban","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:56:12.351Z","hidden":false},{"_id":"6980162d6676f9332270657b","name":"Deqing Wang","hidden":false}],"publishedAt":"2026-01-30T07:32:35.000Z","submittedOnDailyAt":"2026-02-02T01:01:20.171Z","title":"Real-Time Aligned Reward Model beyond Semantics","submittedOnDailyBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","isPro":false,"fullname":"Yikun Ban","user":"Yikunb","type":"user"},"summary":"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.","upvotes":3,"discussionId":"6980162e6676f9332270657c","ai_summary":"RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.","ai_keywords":["Reinforcement Learning from Human Feedback","reward overoptimization","policy models","reward models","policy distribution shifts","real-time alignment","policy feedback"]},"publishedAt":"2026-01-30T02:32:35.000Z","title":"Real-Time Aligned Reward Model beyond Semantics","summary":"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22664.png","numComments":1,"submittedBy":{"_id":"68345345f4bbf856e2d708e2","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg","fullname":"Yikun Ban","name":"Yikunb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.20732","authors":[{"_id":"69804c686676f933227066bd","name":"Ziwei Liu","hidden":false},{"_id":"69804c686676f933227066be","name":"Borui Kang","hidden":false},{"_id":"69804c686676f933227066bf","user":{"_id":"649d54b314afbb10ce2a9eeb","avatarUrl":"/avatars/15c325d8c2273ff63569f23015e98486.svg","isPro":false,"fullname":"Hangjie Yuan","user":"JacobYuan","type":"user"},"name":"Hangjie Yuan","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:12.133Z","hidden":false},{"_id":"69804c686676f933227066c0","name":"Zixiang Zhao","hidden":false},{"_id":"69804c686676f933227066c1","name":"Wei Li","hidden":false},{"_id":"69804c686676f933227066c2","name":"Yifan Zhu","hidden":false},{"_id":"69804c686676f933227066c3","name":"Tao Feng","hidden":false}],"publishedAt":"2026-01-28T16:06:31.000Z","submittedOnDailyAt":"2026-02-02T04:56:18.518Z","title":"Continual GUI Agents","submittedOnDailyBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","isPro":true,"fullname":"Rajkumar rawal","user":"rajkumarrawal","type":"user"},"summary":"As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.","upvotes":3,"discussionId":"69804c696676f933227066c4","ai_summary":"Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.","ai_keywords":["continual learning","GUI agents","reinforcement fine-tuning","GUI-Anchoring in Flux","APR-iF","ARR-iF","continual GUI Agents"],"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-01-28T11:06:31.000Z","title":"Continual GUI Agents","summary":"As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20732.png","numComments":1,"submittedBy":{"_id":"64b8e82aa62c52b252c827fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg","fullname":"Rajkumar rawal","name":"rajkumarrawal","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":50,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":false},{"paper":{"id":"2601.15625","authors":[{"_id":"698042726676f93322706688","name":"Zhiwei Zhang","hidden":false},{"_id":"698042726676f93322706689","user":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","isPro":false,"fullname":"Fei Zhao","user":"Hiiamein","type":"user"},"name":"Fei Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:17.466Z","hidden":false},{"_id":"698042726676f9332270668a","name":"Rui Wang","hidden":false},{"_id":"698042726676f9332270668b","name":"Zezhong Wang","hidden":false},{"_id":"698042726676f9332270668c","name":"Bin Liang","hidden":false},{"_id":"698042726676f9332270668d","name":"Jiakang Wang","hidden":false},{"_id":"698042726676f9332270668e","name":"Yao Hu","hidden":false},{"_id":"698042726676f9332270668f","name":"Shaosheng Cao","hidden":false},{"_id":"698042726676f93322706690","name":"Kam-Fai Wong","hidden":false}],"publishedAt":"2026-01-22T03:57:35.000Z","submittedOnDailyAt":"2026-02-02T03:52:21.562Z","title":"Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors","submittedOnDailyBy":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","isPro":false,"fullname":"Fei Zhao","user":"Hiiamein","type":"user"},"summary":"Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.","upvotes":3,"discussionId":"698042736676f93322706691","ai_summary":"A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.","ai_keywords":["large language models","tool calling","reinforcement learning","error recovery","Fission-GRPO","GRPO","Error Simulator","on-policy sampling","multi-turn execution","trajectory fission"]},"publishedAt":"2026-01-21T22:57:35.000Z","title":"Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors","summary":"Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15625.png","numComments":1,"submittedBy":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","fullname":"Fei Zhao","name":"Hiiamein","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22141","authors":[{"_id":"697c8d096676f93322706032","user":{"_id":"632dc132fdb35759ea673fbf","avatarUrl":"/avatars/4ba2879965150820170d96b610cfccd2.svg","isPro":false,"fullname":"Grzegorz Stefaski","user":"GrzegorzStefanski","type":"user"},"name":"Grzegorz Stefanski","status":"claimed_verified","statusLastChangedAt":"2026-01-30T13:31:28.141Z","hidden":false},{"_id":"697c8d096676f93322706033","name":"Alberto Presta","hidden":false},{"_id":"697c8d096676f93322706034","name":"Michal Byra","hidden":false}],"publishedAt":"2026-01-29T18:56:41.000Z","submittedOnDailyAt":"2026-02-02T01:00:24.563Z","title":"Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data","submittedOnDailyBy":{"_id":"632dc132fdb35759ea673fbf","avatarUrl":"/avatars/4ba2879965150820170d96b610cfccd2.svg","isPro":false,"fullname":"Grzegorz Stefaski","user":"GrzegorzStefanski","type":"user"},"summary":"In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.","upvotes":2,"discussionId":"697c8d096676f93322706035","ai_summary":"Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.","ai_keywords":["Lottery Ticket Hypothesis","winning tickets","adaptive pruning","adaptive tickets","subnetwork collapse","subnetwork similarity score"]},"publishedAt":"2026-01-29T13:56:41.000Z","title":"Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data","summary":"In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22141.png","numComments":1,"submittedBy":{"_id":"632dc132fdb35759ea673fbf","avatarUrl":"/avatars/4ba2879965150820170d96b610cfccd2.svg","fullname":"Grzegorz Stefaski","name":"GrzegorzStefanski","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22032","authors":[{"_id":"6980c763020e48d648c5d3ef","user":{"_id":"642ae02becec03b44648a184","avatarUrl":"/avatars/fb490addca0d2638af44cef8aa2b3ee7.svg","isPro":false,"fullname":"Linhan Wang","user":"LinhanWang","type":"user"},"name":"Linhan Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:01.249Z","hidden":false},{"_id":"6980c763020e48d648c5d3f0","name":"Zichong Yang","hidden":false},{"_id":"6980c763020e48d648c5d3f1","name":"Chen Bai","hidden":false},{"_id":"6980c763020e48d648c5d3f2","name":"Guoxiang Zhang","hidden":false},{"_id":"6980c763020e48d648c5d3f3","name":"Xiaotong Liu","hidden":false},{"_id":"6980c763020e48d648c5d3f4","name":"Xiaoyin Zheng","hidden":false},{"_id":"6980c763020e48d648c5d3f5","name":"Xiao-Xiao Long","hidden":false},{"_id":"6980c763020e48d648c5d3f6","name":"Chang-Tien Lu","hidden":false},{"_id":"6980c763020e48d648c5d3f7","name":"Cheng Lu","hidden":false}],"publishedAt":"2026-01-29T17:39:20.000Z","submittedOnDailyAt":"2026-02-02T15:29:01.925Z","title":"Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving","submittedOnDailyBy":{"_id":"642ae02becec03b44648a184","avatarUrl":"/avatars/fb490addca0d2638af44cef8aa2b3ee7.svg","isPro":false,"fullname":"Linhan Wang","user":"LinhanWang","type":"user"},"summary":"End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.","upvotes":2,"discussionId":"6980c763020e48d648c5d3f8","projectPage":"https://github.com/linhanwang/Drive-JEPA","githubRepo":"https://github.com/linhanwang/Drive-JEPA","githubRepoAddedBy":"user","ai_summary":"Drive-JEPA combines V-JEPA video pretraining with multimodal trajectory distillation to achieve state-of-the-art performance in end-to-end autonomous driving.","ai_keywords":["Video Joint-Embedding Predictive Architecture","V-JEPA","ViT encoder","trajectory distillation","multimodal behaviors","proposal-centric planner","momentum-aware selection","transformer-based decoder","NAVSIM","PDMS","EPDMS"],"githubStars":32,"organization":{"_id":"68ad133a4815e6da4ab1799b","name":"xpeng-liry","fullname":"xpeng","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68ad1242f79e71602eae228d/v6S2xKfwwaDWBybpJwT0k.png"}},"publishedAt":"2026-01-29T12:39:20.000Z","title":"Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving","summary":"End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22032.png","numComments":1,"submittedBy":{"_id":"642ae02becec03b44648a184","avatarUrl":"/avatars/fb490addca0d2638af44cef8aa2b3ee7.svg","fullname":"Linhan Wang","name":"LinhanWang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"68ad133a4815e6da4ab1799b","name":"xpeng-liry","fullname":"xpeng","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68ad1242f79e71602eae228d/v6S2xKfwwaDWBybpJwT0k.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.21709","authors":[{"_id":"6980623f6676f93322706710","name":"Qingyue Yang","hidden":false},{"_id":"6980623f6676f93322706711","name":"Jie Wang","hidden":false},{"_id":"6980623f6676f93322706712","name":"Xing Li","hidden":false},{"_id":"6980623f6676f93322706713","name":"Yinqi Bai","hidden":false},{"_id":"6980623f6676f93322706714","name":"Xialiang Tong","hidden":false},{"_id":"6980623f6676f93322706715","name":"Huiling Zhen","hidden":false},{"_id":"6980623f6676f93322706716","name":"Jianye Hao","hidden":false},{"_id":"6980623f6676f93322706717","name":"Mingxuan Yuan","hidden":false},{"_id":"6980623f6676f93322706718","name":"Bin Li","hidden":false}],"publishedAt":"2026-01-29T13:40:23.000Z","submittedOnDailyAt":"2026-02-02T06:08:49.669Z","title":"Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis","submittedOnDailyBy":{"_id":"64d7b0679a6a7ae9842ee987","avatarUrl":"/avatars/f836dd340cc2068d54430bf75fa0f7d6.svg","isPro":false,"fullname":"Charlie_li","user":"charlie-li","type":"user"},"summary":"Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.","upvotes":2,"discussionId":"698062406676f93322706719","ai_summary":"Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-similarity.","ai_keywords":["Attention patterns","large language models","Temporal Attention Pattern Predictability Analysis","TAPPA","query self-similarity","Rotary Positional Embeddings","RoPE","KV cache compression","LLM pruning"]},"publishedAt":"2026-01-29T08:40:23.000Z","title":"Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis","summary":"Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21709.png","numComments":1,"submittedBy":{"_id":"64d7b0679a6a7ae9842ee987","avatarUrl":"/avatars/f836dd340cc2068d54430bf75fa0f7d6.svg","fullname":"Charlie_li","name":"charlie-li","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.21525","authors":[{"_id":"697c3614a67238fac88cc19a","user":{"_id":"649e9dbdb24e556c33b9e508","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg","isPro":false,"fullname":"Meet Doshi","user":"meetdoshi90","type":"user"},"name":"Meet Doshi","status":"claimed_verified","statusLastChangedAt":"2026-01-30T09:35:41.547Z","hidden":false},{"_id":"697c3614a67238fac88cc19b","name":"Aashka Trivedi","hidden":false},{"_id":"697c3614a67238fac88cc19c","user":{"_id":"62c45be1143622c9278f1ae2","avatarUrl":"/avatars/0ddc94e2e8428334f85f13e6f08a66c2.svg","isPro":false,"fullname":"vishwajeet kumar","user":"vishwajeetkumar","type":"user"},"name":"Vishwajeet Kumar","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:59:35.335Z","hidden":false},{"_id":"697c3614a67238fac88cc19d","name":"Parul Awasthy","hidden":false},{"_id":"697c3614a67238fac88cc19e","name":"Yulong Li","hidden":false},{"_id":"697c3614a67238fac88cc19f","name":"Jaydeep Sen","hidden":false},{"_id":"697c3614a67238fac88cc1a0","name":"Radu Florian","hidden":false},{"_id":"697c3614a67238fac88cc1a1","name":"Sachindra Joshi","hidden":false}],"publishedAt":"2026-01-29T10:40:37.000Z","submittedOnDailyAt":"2026-02-02T02:14:58.662Z","title":"LMK > CLS: Landmark Pooling for Dense Embeddings","submittedOnDailyBy":{"_id":"649e9dbdb24e556c33b9e508","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg","isPro":false,"fullname":"Meet Doshi","user":"meetdoshi90","type":"user"},"summary":"Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.","upvotes":2,"discussionId":"697c3614a67238fac88cc1a2","ai_summary":"Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.","ai_keywords":["representation learning","sequence encoders","pooling operator","[CLS] token","mean pooling","landmark pooling","token sequences","long-context extrapolation","short-context performance"]},"publishedAt":"2026-01-29T05:40:37.000Z","title":"LMK > CLS: Landmark Pooling for Dense Embeddings","summary":"Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21525.png","numComments":0,"submittedBy":{"_id":"649e9dbdb24e556c33b9e508","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649e9dbdb24e556c33b9e508/B4M6OjrzL7Ac-vZFjydm_.jpeg","fullname":"Meet Doshi","name":"meetdoshi90","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.15394","authors":[{"_id":"697d7a2e6676f9332270621b","user":{"_id":"63c38f888cc87cf0c06abfe9","avatarUrl":"/avatars/e99b662404fb805870d95e0f1f17167b.svg","isPro":false,"fullname":"Jay","user":"jaydeepb","type":"user"},"name":"Jaydeep Borkar","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:57:50.378Z","hidden":false},{"_id":"697d7a2e6676f9332270621c","name":"Karan Chadha","hidden":false},{"_id":"697d7a2e6676f9332270621d","name":"Niloofar Mireshghallah","hidden":false},{"_id":"697d7a2e6676f9332270621e","name":"Yuchen Zhang","hidden":false},{"_id":"697d7a2e6676f9332270621f","name":"Irina-Elena Veliche","hidden":false},{"_id":"697d7a2e6676f93322706220","name":"Archi Mitra","hidden":false},{"_id":"697d7a2e6676f93322706221","name":"David A. Smith","hidden":false},{"_id":"697d7a2e6676f93322706222","name":"Zheng Xu","hidden":false},{"_id":"697d7a2e6676f93322706223","name":"Diego Garcia-Olano","hidden":false}],"publishedAt":"2026-01-21T19:04:40.000Z","submittedOnDailyAt":"2026-02-02T16:34:23.522Z","title":"Memorization Dynamics in Knowledge Distillation for Language Models","submittedOnDailyBy":{"_id":"63c38f888cc87cf0c06abfe9","avatarUrl":"/avatars/e99b662404fb805870d95e0f1f17167b.svg","isPro":false,"fullname":"Jay","user":"jaydeepb","type":"user"},"summary":"Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits 2.7times more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.","upvotes":2,"discussionId":"697d7a2e6676f93322706224","ai_summary":"Knowledge distillation reduces training data memorization compared to standard fine-tuning while maintaining performance, with distinct memorization patterns and predictability based on input characteristics.","ai_keywords":["knowledge distillation","large language models","memorization","soft distillation","hard distillation","zlib entropy","KL divergence","perplexity"],"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}},"publishedAt":"2026-01-21T14:04:40.000Z","title":"Memorization Dynamics in Knowledge Distillation for Language Models","summary":"Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits 2.7times more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15394.png","numComments":1,"submittedBy":{"_id":"63c38f888cc87cf0c06abfe9","avatarUrl":"/avatars/e99b662404fb805870d95e0f1f17167b.svg","fullname":"Jay","name":"jaydeepb","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.23134","authors":[{"_id":"69804c7d6676f933227066c6","user":{"_id":"653053c6657ae56cdb5c490b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png","isPro":false,"fullname":"Peter Hu","user":"Peter2023HuggingFace","type":"user"},"name":"Zheyuan Hu","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:52:09.611Z","hidden":false},{"_id":"69804c7d6676f933227066c7","name":"Yifei Shi","hidden":false}],"publishedAt":"2026-01-30T16:23:06.000Z","submittedOnDailyAt":"2026-02-02T04:35:07.481Z","title":"Machine Learning for Energy-Performance-aware Scheduling","submittedOnDailyBy":{"_id":"653053c6657ae56cdb5c490b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png","isPro":false,"fullname":"Peter Hu","user":"Peter2023HuggingFace","type":"user"},"summary":"In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matrn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.","upvotes":1,"discussionId":"69804c7e6676f933227066c8","projectPage":"https://peterhuistyping.github.io/ml-cpu-sched/","githubRepo":"https://github.com/PeterHUistyping/ml-cpu-sched","githubRepoAddedBy":"user","ai_summary":"A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.","ai_keywords":["Bayesian Optimization","Gaussian Processes","multi-objective optimization","Pareto Frontier","sensitivity analysis","fANOVA","covariance kernels","Matrn","RBF"],"githubStars":1},"publishedAt":"2026-01-30T11:23:06.000Z","title":"Machine Learning for Energy-Performance-aware Scheduling","summary":"In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matrn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23134.png","numComments":1,"submittedBy":{"_id":"653053c6657ae56cdb5c490b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png","fullname":"Peter Hu","name":"Peter2023HuggingFace","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2601.22680","authors":[{"_id":"6980d5d721b30847e8c656fb","name":"Rameen Abdal","hidden":false},{"_id":"6980d5d721b30847e8c656fc","name":"James Burgess","hidden":false},{"_id":"6980d5d721b30847e8c656fd","name":"Sergey Tulyakov","hidden":false},{"_id":"6980d5d721b30847e8c656fe","name":"Kuan-Chieh Jackson Wang","hidden":false}],"publishedAt":"2026-01-30T07:53:07.000Z","submittedOnDailyAt":"2026-02-02T14:21:23.889Z","title":"Visual Personalization Turing Test","submittedOnDailyBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","isPro":true,"fullname":"AK","user":"akhaliq","type":"user"},"summary":"We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.","upvotes":1,"discussionId":"6980d5d721b30847e8c656ff","ai_summary":"A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.","ai_keywords":["Visual Personalization Turing Test","VPTT Framework","VPTT-Bench","visual retrieval-augmented generator","VPRAG","VPTT Score"]},"publishedAt":"2026-01-30T02:53:07.000Z","title":"Visual Personalization Turing Test","summary":"We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22680.png","numComments":1,"submittedBy":{"_id":"60f1abe7544c2adfd699860c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg","fullname":"AK","name":"akhaliq","type":"user","isPro":true,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":9203,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2601.22666","authors":[{"_id":"6980c7ba020e48d648c5d3fa","name":"Junyi Hu","hidden":false},{"_id":"6980c7ba020e48d648c5d3fb","name":"Tian Bai","hidden":false},{"_id":"6980c7ba020e48d648c5d3fc","name":"Fengyi Wu","hidden":false},{"_id":"6980c7ba020e48d648c5d3fd","name":"Wenyan Li","hidden":false},{"_id":"6980c7ba020e48d648c5d3fe","name":"Zhenming Peng","hidden":false},{"_id":"6980c7ba020e48d648c5d3ff","name":"Yi Zhang","hidden":false}],"publishedAt":"2026-01-30T07:38:04.000Z","submittedOnDailyAt":"2026-02-02T13:21:34.812Z","title":"ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding","submittedOnDailyBy":{"_id":"619b506f70d03780cbec5806","avatarUrl":"/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg","isPro":false,"fullname":"wenyan li","user":"lyan62","type":"user"},"summary":"Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP_r on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.","upvotes":1,"discussionId":"6980c7ba020e48d648c5d400","ai_summary":"ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.","ai_keywords":["vision-language alignment","multiple instance learning","Expectation Alignment Head","soft MIL pooling","token-region similarities","energy-based multi-scale consistency regularization","Top-K multi-positive contrastive objective","Geometry-Aware Consistency Objective","Lagrangian-constrained free-energy minimization","open-vocabulary detection","zero-shot instance segmentation","LVIS minival split"],"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-01-30T02:38:04.000Z","title":"ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding","summary":"Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP_r on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22666.png","numComments":1,"submittedBy":{"_id":"619b506f70d03780cbec5806","avatarUrl":"/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg","fullname":"wenyan li","name":"lyan62","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":false},{"paper":{"id":"2601.22108","authors":[{"_id":"697d0a4d6676f93322706191","user":{"_id":"68d0dde3b8183cc322e4d641","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/43G45HfJWCnLcDBFuSAza.png","isPro":false,"fullname":"Shuqi Ke","user":"shuqike","type":"user"},"name":"Shuqi Ke","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:58:22.811Z","hidden":false},{"_id":"697d0a4d6676f93322706192","name":"Giulia Fanti","hidden":false}],"publishedAt":"2026-01-29T18:38:09.000Z","submittedOnDailyAt":"2026-02-02T15:08:57.876Z","title":"Value-Based Pre-Training with Downstream Feedback","submittedOnDailyBy":{"_id":"68d0dde3b8183cc322e4d641","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/43G45HfJWCnLcDBFuSAza.png","isPro":false,"fullname":"Shuqi Ke","user":"shuqike","type":"user"},"summary":"Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.","upvotes":1,"discussionId":"697d0a4e6676f93322706193","ai_summary":"V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.","ai_keywords":["self-supervised learning","pretraining","proxy objective","gradient alignment","value-based pretraining","downstream task","sample augmentation","next-token prediction","GSM8K","ADE20K","NYUv2","ImageNet","token efficiency"],"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"}},"publishedAt":"2026-01-29T13:38:09.000Z","title":"Value-Based Pre-Training with Downstream Feedback","summary":"Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22108.png","numComments":1,"submittedBy":{"_id":"68d0dde3b8183cc322e4d641","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/43G45HfJWCnLcDBFuSAza.png","fullname":"Shuqi Ke","name":"shuqike","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"},"isAuthorParticipating":true},{"paper":{"id":"2601.21526","authors":[{"_id":"6980c9b683fdbbe1963c2cd5","name":"Alireza Nadaf","hidden":false},{"_id":"6980c9b683fdbbe1963c2cd6","name":"Alireza Mohammadshahi","hidden":false},{"_id":"6980c9b683fdbbe1963c2cd7","name":"Majid Yazdani","hidden":false}],"publishedAt":"2026-01-29T10:40:54.000Z","submittedOnDailyAt":"2026-02-02T13:30:32.762Z","title":"KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization","submittedOnDailyBy":{"_id":"63615355a46f0cdd62e707f5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1667394938142-63615355a46f0cdd62e707f5.png","isPro":false,"fullname":"Alireza Mohammadshahi","user":"alirezamsh","type":"user"},"summary":"We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.\n  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.\n  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.\n  Code Available at: https://github.com/Leeroo-AI/kapso","upvotes":1,"discussionId":"6980c9b783fdbbe1963c2cd8","githubRepo":"https://github.com/Leeroo-AI/kapso","githubRepoAddedBy":"user","ai_summary":"KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.","ai_keywords":["program synthesis","optimization loop","git-native experimentation engine","knowledge system","cognitive memory layer","episodic store","retrieval","experimentation tracking","knowledge integration"],"githubStars":56,"organization":{"_id":"657acabd913a86b38fd0efb2","name":"leeroo","fullname":"Leeroo","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63615355a46f0cdd62e707f5/HpvNSU7nAs_Z5izTZL8ir.png"}},"publishedAt":"2026-01-29T05:40:54.000Z","title":"KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization","summary":"We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.\n  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.\n  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.\n  Code Available at: https://github.com/Leeroo-AI/kapso","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21526.png","numComments":1,"submittedBy":{"_id":"63615355a46f0cdd62e707f5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1667394938142-63615355a46f0cdd62e707f5.png","fullname":"Alireza Mohammadshahi","name":"alirezamsh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":27,"isUserFollowing":false},"organization":{"_id":"657acabd913a86b38fd0efb2","name":"leeroo","fullname":"Leeroo","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63615355a46f0cdd62e707f5/HpvNSU7nAs_Z5izTZL8ir.png"},"isAuthorParticipating":false},{"paper":{"id":"2601.21666","authors":[{"_id":"697cde656676f93322706147","user":{"_id":"64e1b29875fae2212e9473bd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64e1b29875fae2212e9473bd/mGq0oPie2BQGc4VMtilVj.jpeg","isPro":false,"fullname":"Ahmed Radwan","user":"ahmedyradwan02","type":"user"},"name":"Ahmed Y. Radwan","status":"claimed_verified","statusLastChangedAt":"2026-02-02T16:58:44.065Z","hidden":false},{"_id":"697cde656676f93322706148","name":"Christos Emmanouilidis","hidden":false},{"_id":"697cde656676f93322706149","name":"Hina Tabassum","hidden":false},{"_id":"697cde656676f9332270614a","name":"Deval Pandya","hidden":false},{"_id":"697cde656676f9332270614b","name":"Shaina Raza","hidden":false}],"publishedAt":"2026-01-29T13:01:07.000Z","submittedOnDailyAt":"2026-02-02T14:37:30.637Z","title":"SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding","submittedOnDailyBy":{"_id":"64e1b29875fae2212e9473bd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64e1b29875fae2212e9473bd/mGq0oPie2BQGc4VMtilVj.jpeg","isPro":false,"fullname":"Ahmed Radwan","user":"ahmedyradwan02","type":"user"},"summary":"Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard","upvotes":0,"discussionId":"697cde656676f9332270614c","ai_summary":"A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.","ai_keywords":["Multimodal Large Language Models","audio-video data","real-world conversational domains","temporal localization","multimodal understanding"],"organization":{"_id":"65b2b60eade89bc3fac3108a","name":"vector-institute","fullname":"Vector Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"}},"publishedAt":"2026-01-29T08:01:07.000Z","title":"SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding","summary":"Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21666.png","numComments":1,"submittedBy":{"_id":"64e1b29875fae2212e9473bd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64e1b29875fae2212e9473bd/mGq0oPie2BQGc4VMtilVj.jpeg","fullname":"Ahmed Radwan","name":"ahmedyradwan02","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"65b2b60eade89bc3fac3108a","name":"vector-institute","fullname":"Vector Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"},"isAuthorParticipating":true}]